{
  "kind": "Listing",
  "data": {
    "after": "t3_1mj5q2f",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI wanted to share my recent experience (and save others some hours of troubleshooting!) trying to run the new GPT-OSS-20B F16/MXFP4/MOE GGUF models locally via `llama.cpp` and `llama-cpp-python` — and to confirm that as of August 7, 2025, this is NOT yet supported, regardless of what you try.\n\n# What I did:\n\n1. Built an isolated Python virtual environment Using Windows 11, Python 3.11, latest pip, etc.\n2. Compiled llama-cpp-python from source\n   * Cloned [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python) with `--recursive`\n   * Explicitly updated the `vendor/llama.cpp` submodule:\n      * Switched to upstream origin: `git remote set-url origin` [`https://github.com/ggerganov/llama.cpp.git`](https://github.com/ggerganov/llama.cpp.git)\n      * Checked out latest `master`, did `git pull origin master`\n      * Confirmed commit:yamlCopyEditcommit 5fd160bbd9d70b94b5b11b0001fd7f477005e4a0 (HEAD -&gt; master, tag: b6106, origin/master, origin/HEAD) Date:   Wed Aug 6 15:14:40 2025 -0700 \n   * Compiled with `FORCE_CMAKE=1`, CPU only\n3. Downloaded the official Unsloth GPT-OSS-20B F16 GGUF\n   * 13.4 GB\n   * Downloaded directly from HuggingFace, verified SHA256, file size matches exactly.\n4. Tested file integrity with a custom Python script:\n   * Confirmed GGUF header, no corruption, full SHA256 check.\n5. Tried loading the model with llama\\_cpp.Llama (chat\\_format=\"gpt-oss\")\n   * Also tested with the latest compiled `main.exe` from `llama.cpp` directly.\n   * Tried both with F16 and Q0\\_0 versions.\n\n# The error (every single time):\n\n    pgsqlCopyEditgguf_init_from_file_impl: tensor 'blk.0.ffn_down_exps.weight' has invalid ggml type 39 (NONE)\n    gguf_init_from_file_impl: failed to read tensor info\n    llama_model_load: error loading model: llama_model_loader: failed to load model from xxx.gguf\n    llama_model_load_from_file_impl: failed to load model\n    [ERRO] Failed to load model from file: xxx.gguf\n    \n\n# What this means:\n\n* As of the most recent commit (`b6106`, Aug 6, 2025) on `llama.cpp` and the latest source build of `llama-cpp-python`, there is still NO support for the new MXFP4 tensor type (ggml type 39) required by GPT-OSS F16/MXFP4/MOE models.\n* This is not an issue with your build, Python, environment, or file.\n* The GGUF files themselves are valid and pass header/hash checks.\n* No one can run these models locally via vanilla llama.cpp at this time**.** (I even tried other quantizations; only the latest MXFP4/F16 fail like this.)\n\n# What to do?\n\n* Wait for an official update / PR / patch in llama.cpp that adds MXFP4 and GPT-OSS F16/MOE support.\n* Track issues on [ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp/issues) and the HuggingFace repo for progress.\n* When that happens, just update and recompile — no extra hacks should be needed.\n\n# Conclusion:\n\nIf you’re seeing  \n`gguf_init_from_file_impl: tensor 'blk.0.ffn_down_exps.weight' has invalid ggml type 39 (NONE)`  \ntrying to load GPT-OSS-20B F16/MXFP4, **it’s not you — it’s the code!**\n\nWe’re all waiting for upstream support.",
          "author_fullname": "t2_l2tfh53yn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-OSS-20B F16/MXFP4 GGUF Models Not Loading on Latest llama.cpp: \"tensor ... has invalid ggml type 39 (NONE)\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mjm5vm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754528046,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I wanted to share my recent experience (and save others some hours of troubleshooting!) trying to run the new GPT-OSS-20B F16/MXFP4/MOE GGUF models locally via &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;llama-cpp-python&lt;/code&gt; — and to confirm that as of August 7, 2025, this is NOT yet supported, regardless of what you try.&lt;/p&gt;\n\n&lt;h1&gt;What I did:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Built an isolated Python virtual environment Using Windows 11, Python 3.11, latest pip, etc.&lt;/li&gt;\n&lt;li&gt;Compiled llama-cpp-python from source\n\n&lt;ul&gt;\n&lt;li&gt;Cloned &lt;a href=\"https://github.com/abetlen/llama-cpp-python\"&gt;abetlen/llama-cpp-python&lt;/a&gt; with &lt;code&gt;--recursive&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Explicitly updated the &lt;code&gt;vendor/llama.cpp&lt;/code&gt; submodule:\n\n&lt;ul&gt;\n&lt;li&gt;Switched to upstream origin: &lt;code&gt;git remote set-url origin&lt;/code&gt; &lt;a href=\"https://github.com/ggerganov/llama.cpp.git\"&gt;&lt;code&gt;https://github.com/ggerganov/llama.cpp.git&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Checked out latest &lt;code&gt;master&lt;/code&gt;, did &lt;code&gt;git pull origin master&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Confirmed commit:yamlCopyEditcommit 5fd160bbd9d70b94b5b11b0001fd7f477005e4a0 (HEAD -&amp;gt; master, tag: b6106, origin/master, origin/HEAD) Date:   Wed Aug 6 15:14:40 2025 -0700 &lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Compiled with &lt;code&gt;FORCE_CMAKE=1&lt;/code&gt;, CPU only&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Downloaded the official Unsloth GPT-OSS-20B F16 GGUF\n\n&lt;ul&gt;\n&lt;li&gt;13.4 GB&lt;/li&gt;\n&lt;li&gt;Downloaded directly from HuggingFace, verified SHA256, file size matches exactly.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Tested file integrity with a custom Python script:\n\n&lt;ul&gt;\n&lt;li&gt;Confirmed GGUF header, no corruption, full SHA256 check.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Tried loading the model with llama_cpp.Llama (chat_format=&amp;quot;gpt-oss&amp;quot;)\n\n&lt;ul&gt;\n&lt;li&gt;Also tested with the latest compiled &lt;code&gt;main.exe&lt;/code&gt; from &lt;code&gt;llama.cpp&lt;/code&gt; directly.&lt;/li&gt;\n&lt;li&gt;Tried both with F16 and Q0_0 versions.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;h1&gt;The error (every single time):&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;pgsqlCopyEditgguf_init_from_file_impl: tensor &amp;#39;blk.0.ffn_down_exps.weight&amp;#39; has invalid ggml type 39 (NONE)\ngguf_init_from_file_impl: failed to read tensor info\nllama_model_load: error loading model: llama_model_loader: failed to load model from xxx.gguf\nllama_model_load_from_file_impl: failed to load model\n[ERRO] Failed to load model from file: xxx.gguf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;What this means:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;As of the most recent commit (&lt;code&gt;b6106&lt;/code&gt;, Aug 6, 2025) on &lt;code&gt;llama.cpp&lt;/code&gt; and the latest source build of &lt;code&gt;llama-cpp-python&lt;/code&gt;, there is still NO support for the new MXFP4 tensor type (ggml type 39) required by GPT-OSS F16/MXFP4/MOE models.&lt;/li&gt;\n&lt;li&gt;This is not an issue with your build, Python, environment, or file.&lt;/li&gt;\n&lt;li&gt;The GGUF files themselves are valid and pass header/hash checks.&lt;/li&gt;\n&lt;li&gt;No one can run these models locally via vanilla llama.cpp at this time&lt;strong&gt;.&lt;/strong&gt; (I even tried other quantizations; only the latest MXFP4/F16 fail like this.)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What to do?&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Wait for an official update / PR / patch in llama.cpp that adds MXFP4 and GPT-OSS F16/MOE support.&lt;/li&gt;\n&lt;li&gt;Track issues on &lt;a href=\"https://github.com/ggerganov/llama.cpp/issues\"&gt;ggerganov/llama.cpp&lt;/a&gt; and the HuggingFace repo for progress.&lt;/li&gt;\n&lt;li&gt;When that happens, just update and recompile — no extra hacks should be needed.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Conclusion:&lt;/h1&gt;\n\n&lt;p&gt;If you’re seeing&lt;br/&gt;\n&lt;code&gt;gguf_init_from_file_impl: tensor &amp;#39;blk.0.ffn_down_exps.weight&amp;#39; has invalid ggml type 39 (NONE)&lt;/code&gt;&lt;br/&gt;\ntrying to load GPT-OSS-20B F16/MXFP4, &lt;strong&gt;it’s not you — it’s the code!&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We’re all waiting for upstream support.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?auto=webp&amp;s=15dec2ef279707b2b7293f298adf65c120367689",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f92890af939223e811c78aea793ad74924524124",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a7a52293722d602fe6fdebcf196182f6b7c5e573",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0624f431dbc1aca0fda020f274bbe55097bc3029",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf04d8aa804465078ae5a4e47e5c8fb229efaa46",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a1c420daf96989a2ffdd13a56551243766e0602e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=014bf6642a86d58418fa1570ec5de73a70a0c2e9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "tkjhWxNM-Mt33ysEhzUuJ63e8pNrfpVAPEbJGjatflc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjm5vm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PT_OV",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjm5vm/gptoss20b_f16mxfp4_gguf_models_not_loading_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjm5vm/gptoss20b_f16mxfp4_gguf_models_not_loading_on/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754528046,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have 3x 3090 running oss 120B in LM studio. With flash attention enabled and 32k context window I get 100 token/s prompt eval speed.\n\nThat seems terribly slow...what are you guys getting?\n",
          "author_fullname": "t2_aafjsulg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Slow prompt eval oss 120b?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mjlvxo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754527286,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 3x 3090 running oss 120B in LM studio. With flash attention enabled and 32k context window I get 100 token/s prompt eval speed.&lt;/p&gt;\n\n&lt;p&gt;That seems terribly slow...what are you guys getting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjlvxo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Only_Situation_4713",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjlvxo/slow_prompt_eval_oss_120b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjlvxo/slow_prompt_eval_oss_120b/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754527286,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A no non-sense, complete byte-pair encoding implementation, in python, completely from scratch.\n\n```py\n\"\"\"\n@file model.py\n@license cc-by-sa-nc-4.0\n@ref https://aclanthology.org/P16-1162/\n@ref https://huggingface.co/blog/catherinearnett/dangers-of-tokenizer-recycling\n\"\"\"\n\nimport argparse\nimport collections\nimport json\nimport math\n\n\nclass Corpus:\n    \"\"\"Load and initialize training data\"\"\"\n\n    @staticmethod\n    def default() -&gt; list[str]:\n        return [\"lo\", \"low\", \"lower\", \"newest\", \"wide\", \"wider\", \"widest\"]\n\n    @staticmethod\n    def read(path: str) -&gt; list[str]:\n        \"\"\"Load a flat list of words from a file, one per whitespace.\"\"\"\n        words = []\n        with open(path, \"r\") as file:\n            for line in file:\n                for word in line.split():\n                    words.append(word)\n        return words\n\n    @staticmethod\n    def words(path: str = None) -&gt; list[str]:\n        if path:\n            print(f\"Using corpus from file: {path}\")\n            return Corpus.read(path)\n        print(\"Using default corpus.\")\n        return Corpus.default()\n\n    @staticmethod\n    def vocab(path: str = None) -&gt; dict[str, int]:\n        \"\"\"Convert list of words into vocab dict: space-joined symbols -&gt; freq.\"\"\"\n        vocab = {}\n        for word in Corpus.words(path):\n            symbols = list(word)\n            vocab[\" \".join(symbols)] = 1\n        print(\"Initialized vocab:\")\n        print(json.dumps(vocab, indent=2))\n        return vocab\n\n\nclass Model:\n    \"\"\"Byte-pair Encoding\"\"\"\n\n    @staticmethod\n    def pairs(vocab: dict[str, int]) -&gt; dict[tuple[str, str], int]:\n        # print(\"Generating pairs:\")\n        pairs = collections.defaultdict(int)  # init freqs to 0\n        for word, freq in vocab.items():  # unpacks (\"l o w &lt;/w&gt;\", 5)\n            symbols = word.split()  # split word by char -&gt; [\"l\", \"o\", \"w\", ...]\n            for i in range(len(symbols) - 1):  # for each step in the set of symbols\n                cur = symbols[i]  # \"l\"\n                nxt = symbols[i + 1]  # \"o\"\n                pairs[cur, nxt] += freq  # p[(\"l\", \"o\")] += 1\n                # print(f\"i={i}, cur='{cur}', nxt='{nxt}', freq={freq}\")\n        return pairs  # {('l', 'o'): 1}\n\n    @staticmethod\n    def bigram(symbols: list[str], pair: tuple[str, str]) -&gt; list[str]:\n        bigram = []\n        i = 0\n        while i &lt; len(symbols):\n            # If this symbol and the next match the pair, merge them\n            if (\n                i &lt; len(symbols) - 1\n                and symbols[i] == pair[0]\n                and symbols[i + 1] == pair[1]\n            ):\n                bigram.append(symbols[i] + symbols[i + 1])\n                i += 2  # Skip the next symbol (it's merged)\n            else:\n                bigram.append(symbols[i])\n                i += 1\n        return bigram\n\n    @staticmethod\n    def merges(vocab: dict[str, int], pair: tuple[str, str]) -&gt; dict[str, int]:\n        # print(\"Updated pairs:\")\n        # print(json.dumps(vocab, indent=2))\n\n        new_vocab = {}  # new empty vocab\n        for word in vocab:  # for each pair in a given map\n            symbols = word.split()  # [\"l\", \"o\", \"w\", \"&lt;/w&gt;\"]\n            bigram = Model.bigram(symbols, pair)  # merge neighbors\n            new_word = \" \".join(bigram)  # new n-gram\n            # print(f\"word={word}, new_word={new_word}\")\n            new_vocab[new_word] = vocab[word]\n        return new_vocab\n\n\nclass Tokenizer:\n    def __init__(self, vocab: dict[str, int]):\n        self.model = {\n            \"type\": \"BPE\",\n            \"version\": \"0.1.0\",\n            \"vocab\": vocab,\n            \"merges\": [],\n        }\n\n    @property\n    def type(self) -&gt; str:\n        return self.model[\"type\"]\n\n    @property\n    def version(self) -&gt; str:\n        return self.model[\"version\"]\n\n    @property\n    def vocab(self) -&gt; dict[str, int]:\n        return self.model[\"vocab\"]\n\n    @vocab.setter\n    def vocab(self, value: dict[str, int]) -&gt; None:\n        self.model[\"vocab\"] = value\n\n    @property\n    def merges(self) -&gt; list[tuple[str, str]]:\n        return self.model[\"merges\"]\n\n    @merges.setter\n    def merges(self, value: list[tuple[str, str]]):\n        self.model[\"merges\"] = value\n\n    def train(self, num_merges: int) -&gt; None:\n        # Train vocab model (vocab is the set of all merges)\n        self.merges = []\n        for i in range(num_merges):\n            # pre-process merge pairs every cycle\n            pairs = Model.pairs(self.vocab)  # create pairs\n            if not pairs:  # bail if pairs is empty\n                print(f\"Exhausted all potential pairs! Halted at step {i}.\")\n                break\n            # use the highest ranked pair for the next merge cycle\n            best = max(pairs, key=pairs.get)  # get max rank\n            self.merges.append(best)\n            self.vocab = Model.merges(self.vocab, best)  # merge ranked pair\n\n    def save(self, path: str) -&gt; None:\n        with open(path, \"w\", encoding=\"utf-8\") as file:\n            json.dump(self.model, file, ensure_ascii=False, indent=2)\n\n    def load(self, path: str) -&gt; None:\n        with open(path, \"r\", encoding=\"utf-8\") as file:\n            self.model = json.load(file)\n\n    @property\n    def tokens(self) -&gt; list[str]:\n        # Collect All Unique Tokens\n        token_set = set()\n        for word in self.vocab:  # must be vocab!\n            for symbol in word.split():\n                token_set.add(symbol)\n        # Assign IDs in sorted order (order matters)\n        return sorted(list(token_set))\n\n    @property\n    def token_to_id(self) -&gt; dict[str, int]:\n        return {token: idx for idx, token in enumerate(self.tokens)}\n\n    @property\n    def id_to_token(self) -&gt; dict[int, str]:\n        return {idx: token for idx, token in enumerate(self.tokens)}\n\n    @property\n    def ranks(self) -&gt; dict[str, int]:\n        # Build the rank table (rank merges)\n        rank_table = {}\n        for i, pair in enumerate(self.merges):  # must be merges!\n            token = \"\".join(pair)\n            rank_table[token] = i\n        return rank_table\n\n    @property\n    def scores(self):\n        # Score the merges\n        scores = {}\n        for token in self.tokens:\n            rank = self.ranks.get(token)\n            scores[token] = -math.log(rank + 1) if rank else -1e6\n        return scores\n\n    def encode(self, token: str) -&gt; int:\n        return self.token_to_id[token]\n\n    def decode(self, id: int) -&gt; str:\n        return self.id_to_token[id]\n\n\ndef parse_args() -&gt; argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-m\",\n        \"--merges\",\n        required=False,\n        type=int,\n        default=10,\n        help=\"number of merges\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--corpus\",\n        required=False,\n        type=str,\n        default=None,\n        help=\"input plaintext file\",\n    )\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n\n    # Get number of merges (training cycles)\n    num_merges = int(args.merges)\n\n    # Get words from corpus (training data)\n    vocab = Corpus.vocab(args.corpus)\n\n    # Train vocab model (vocab is the set of all merges)\n    tokenizer = Tokenizer(vocab)\n    tokenizer.train(args.merges)\n\n    # Print vocab training results (dump merges)\n    print(\"Merge Table:\")\n    print(json.dumps(tokenizer.merges, indent=2))\n\n    print(\"Final Vocab:\")\n    print(json.dumps(tokenizer.vocab, indent=2))\n\n    print(\"Tokenizer:\")\n    print(json.dumps(tokenizer.token_to_id, indent=2))\n\n    # Build the rank table (rank merges)\n    print(\"Rank Table:\")\n    print(json.dumps(tokenizer.ranks, indent=2))\n\n    # Score the merges\n    print(\"Token Scores:\")\n    print(json.dumps(tokenizer.scores, indent=2))\n```\n\n- Used the original NMT paper as a core reference.\n- Zero dependencies.\n- Accepts plain-text input.\n- Stateful memory and disk ops.\n- Single-threaded.\n- Extensible.\n\nIt's dead simple, to the point, and - most importantly - legible. Excellent for learning and comprehension.\n\nI genuinely don't understand why implementations are so convoluted when it's only 250 lines of code.\n\nThe is the models voice box. A model \"learns\" from human created data as its input. It then converges towards the most common patterns during back-propagation.\n\nWithout a solid tokenizer, it's garbage in and garbage out. This is, of course, a single piece of a much bigger puzzle.\n\nI'm very interested in doing this for graphemes. And of course, there's a paper and repository on this as well.\n\n- https://aclanthology.org/2025.coling-main.400\n\nI am not affiliated with any of these authors, papers, orgs, etc. I'm just a dude trying to figure this stuff out. I love tinkering and understanding how things work at a fundamental level.\n\nThe internet is becoming a scary place, so stay safe out there, and keep your personal data close to your vest. Things are just starting heat up.",
          "author_fullname": "t2_slcrtxpr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Vox Populi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mjlg5q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754526096,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A no non-sense, complete byte-pair encoding implementation, in python, completely from scratch.&lt;/p&gt;\n\n&lt;p&gt;```py\n&amp;quot;&amp;quot;&amp;quot;\n@file model.py\n@license cc-by-sa-nc-4.0\n@ref &lt;a href=\"https://aclanthology.org/P16-1162/\"&gt;https://aclanthology.org/P16-1162/&lt;/a&gt;\n@ref &lt;a href=\"https://huggingface.co/blog/catherinearnett/dangers-of-tokenizer-recycling\"&gt;https://huggingface.co/blog/catherinearnett/dangers-of-tokenizer-recycling&lt;/a&gt;\n&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;import argparse\nimport collections\nimport json\nimport math&lt;/p&gt;\n\n&lt;p&gt;class Corpus:\n    &amp;quot;&amp;quot;&amp;quot;Load and initialize training data&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@staticmethod\ndef default() -&amp;gt; list[str]:\n    return [&amp;quot;lo&amp;quot;, &amp;quot;low&amp;quot;, &amp;quot;lower&amp;quot;, &amp;quot;newest&amp;quot;, &amp;quot;wide&amp;quot;, &amp;quot;wider&amp;quot;, &amp;quot;widest&amp;quot;]\n\n@staticmethod\ndef read(path: str) -&amp;gt; list[str]:\n    &amp;quot;&amp;quot;&amp;quot;Load a flat list of words from a file, one per whitespace.&amp;quot;&amp;quot;&amp;quot;\n    words = []\n    with open(path, &amp;quot;r&amp;quot;) as file:\n        for line in file:\n            for word in line.split():\n                words.append(word)\n    return words\n\n@staticmethod\ndef words(path: str = None) -&amp;gt; list[str]:\n    if path:\n        print(f&amp;quot;Using corpus from file: {path}&amp;quot;)\n        return Corpus.read(path)\n    print(&amp;quot;Using default corpus.&amp;quot;)\n    return Corpus.default()\n\n@staticmethod\ndef vocab(path: str = None) -&amp;gt; dict[str, int]:\n    &amp;quot;&amp;quot;&amp;quot;Convert list of words into vocab dict: space-joined symbols -&amp;gt; freq.&amp;quot;&amp;quot;&amp;quot;\n    vocab = {}\n    for word in Corpus.words(path):\n        symbols = list(word)\n        vocab[&amp;quot; &amp;quot;.join(symbols)] = 1\n    print(&amp;quot;Initialized vocab:&amp;quot;)\n    print(json.dumps(vocab, indent=2))\n    return vocab\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;class Model:\n    &amp;quot;&amp;quot;&amp;quot;Byte-pair Encoding&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@staticmethod\ndef pairs(vocab: dict[str, int]) -&amp;gt; dict[tuple[str, str], int]:\n    # print(&amp;quot;Generating pairs:&amp;quot;)\n    pairs = collections.defaultdict(int)  # init freqs to 0\n    for word, freq in vocab.items():  # unpacks (&amp;quot;l o w &amp;lt;/w&amp;gt;&amp;quot;, 5)\n        symbols = word.split()  # split word by char -&amp;gt; [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;, ...]\n        for i in range(len(symbols) - 1):  # for each step in the set of symbols\n            cur = symbols[i]  # &amp;quot;l&amp;quot;\n            nxt = symbols[i + 1]  # &amp;quot;o&amp;quot;\n            pairs[cur, nxt] += freq  # p[(&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;)] += 1\n            # print(f&amp;quot;i={i}, cur=&amp;#39;{cur}&amp;#39;, nxt=&amp;#39;{nxt}&amp;#39;, freq={freq}&amp;quot;)\n    return pairs  # {(&amp;#39;l&amp;#39;, &amp;#39;o&amp;#39;): 1}\n\n@staticmethod\ndef bigram(symbols: list[str], pair: tuple[str, str]) -&amp;gt; list[str]:\n    bigram = []\n    i = 0\n    while i &amp;lt; len(symbols):\n        # If this symbol and the next match the pair, merge them\n        if (\n            i &amp;lt; len(symbols) - 1\n            and symbols[i] == pair[0]\n            and symbols[i + 1] == pair[1]\n        ):\n            bigram.append(symbols[i] + symbols[i + 1])\n            i += 2  # Skip the next symbol (it&amp;#39;s merged)\n        else:\n            bigram.append(symbols[i])\n            i += 1\n    return bigram\n\n@staticmethod\ndef merges(vocab: dict[str, int], pair: tuple[str, str]) -&amp;gt; dict[str, int]:\n    # print(&amp;quot;Updated pairs:&amp;quot;)\n    # print(json.dumps(vocab, indent=2))\n\n    new_vocab = {}  # new empty vocab\n    for word in vocab:  # for each pair in a given map\n        symbols = word.split()  # [&amp;quot;l&amp;quot;, &amp;quot;o&amp;quot;, &amp;quot;w&amp;quot;, &amp;quot;&amp;lt;/w&amp;gt;&amp;quot;]\n        bigram = Model.bigram(symbols, pair)  # merge neighbors\n        new_word = &amp;quot; &amp;quot;.join(bigram)  # new n-gram\n        # print(f&amp;quot;word={word}, new_word={new_word}&amp;quot;)\n        new_vocab[new_word] = vocab[word]\n    return new_vocab\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;class Tokenizer:\n    def &lt;strong&gt;init&lt;/strong&gt;(self, vocab: dict[str, int]):\n        self.model = {\n            &amp;quot;type&amp;quot;: &amp;quot;BPE&amp;quot;,\n            &amp;quot;version&amp;quot;: &amp;quot;0.1.0&amp;quot;,\n            &amp;quot;vocab&amp;quot;: vocab,\n            &amp;quot;merges&amp;quot;: [],\n        }&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;@property\ndef type(self) -&amp;gt; str:\n    return self.model[&amp;quot;type&amp;quot;]\n\n@property\ndef version(self) -&amp;gt; str:\n    return self.model[&amp;quot;version&amp;quot;]\n\n@property\ndef vocab(self) -&amp;gt; dict[str, int]:\n    return self.model[&amp;quot;vocab&amp;quot;]\n\n@vocab.setter\ndef vocab(self, value: dict[str, int]) -&amp;gt; None:\n    self.model[&amp;quot;vocab&amp;quot;] = value\n\n@property\ndef merges(self) -&amp;gt; list[tuple[str, str]]:\n    return self.model[&amp;quot;merges&amp;quot;]\n\n@merges.setter\ndef merges(self, value: list[tuple[str, str]]):\n    self.model[&amp;quot;merges&amp;quot;] = value\n\ndef train(self, num_merges: int) -&amp;gt; None:\n    # Train vocab model (vocab is the set of all merges)\n    self.merges = []\n    for i in range(num_merges):\n        # pre-process merge pairs every cycle\n        pairs = Model.pairs(self.vocab)  # create pairs\n        if not pairs:  # bail if pairs is empty\n            print(f&amp;quot;Exhausted all potential pairs! Halted at step {i}.&amp;quot;)\n            break\n        # use the highest ranked pair for the next merge cycle\n        best = max(pairs, key=pairs.get)  # get max rank\n        self.merges.append(best)\n        self.vocab = Model.merges(self.vocab, best)  # merge ranked pair\n\ndef save(self, path: str) -&amp;gt; None:\n    with open(path, &amp;quot;w&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as file:\n        json.dump(self.model, file, ensure_ascii=False, indent=2)\n\ndef load(self, path: str) -&amp;gt; None:\n    with open(path, &amp;quot;r&amp;quot;, encoding=&amp;quot;utf-8&amp;quot;) as file:\n        self.model = json.load(file)\n\n@property\ndef tokens(self) -&amp;gt; list[str]:\n    # Collect All Unique Tokens\n    token_set = set()\n    for word in self.vocab:  # must be vocab!\n        for symbol in word.split():\n            token_set.add(symbol)\n    # Assign IDs in sorted order (order matters)\n    return sorted(list(token_set))\n\n@property\ndef token_to_id(self) -&amp;gt; dict[str, int]:\n    return {token: idx for idx, token in enumerate(self.tokens)}\n\n@property\ndef id_to_token(self) -&amp;gt; dict[int, str]:\n    return {idx: token for idx, token in enumerate(self.tokens)}\n\n@property\ndef ranks(self) -&amp;gt; dict[str, int]:\n    # Build the rank table (rank merges)\n    rank_table = {}\n    for i, pair in enumerate(self.merges):  # must be merges!\n        token = &amp;quot;&amp;quot;.join(pair)\n        rank_table[token] = i\n    return rank_table\n\n@property\ndef scores(self):\n    # Score the merges\n    scores = {}\n    for token in self.tokens:\n        rank = self.ranks.get(token)\n        scores[token] = -math.log(rank + 1) if rank else -1e6\n    return scores\n\ndef encode(self, token: str) -&amp;gt; int:\n    return self.token_to_id[token]\n\ndef decode(self, id: int) -&amp;gt; str:\n    return self.id_to_token[id]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;def parse_args() -&amp;gt; argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        &amp;quot;-m&amp;quot;,\n        &amp;quot;--merges&amp;quot;,\n        required=False,\n        type=int,\n        default=10,\n        help=&amp;quot;number of merges&amp;quot;,\n    )\n    parser.add_argument(\n        &amp;quot;-c&amp;quot;,\n        &amp;quot;--corpus&amp;quot;,\n        required=False,\n        type=str,\n        default=None,\n        help=&amp;quot;input plaintext file&amp;quot;,\n    )\n    return parser.parse_args()&lt;/p&gt;\n\n&lt;p&gt;if &lt;strong&gt;name&lt;/strong&gt; == &amp;quot;&lt;strong&gt;main&lt;/strong&gt;&amp;quot;:\n    args = parse_args()&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# Get number of merges (training cycles)\nnum_merges = int(args.merges)\n\n# Get words from corpus (training data)\nvocab = Corpus.vocab(args.corpus)\n\n# Train vocab model (vocab is the set of all merges)\ntokenizer = Tokenizer(vocab)\ntokenizer.train(args.merges)\n\n# Print vocab training results (dump merges)\nprint(&amp;quot;Merge Table:&amp;quot;)\nprint(json.dumps(tokenizer.merges, indent=2))\n\nprint(&amp;quot;Final Vocab:&amp;quot;)\nprint(json.dumps(tokenizer.vocab, indent=2))\n\nprint(&amp;quot;Tokenizer:&amp;quot;)\nprint(json.dumps(tokenizer.token_to_id, indent=2))\n\n# Build the rank table (rank merges)\nprint(&amp;quot;Rank Table:&amp;quot;)\nprint(json.dumps(tokenizer.ranks, indent=2))\n\n# Score the merges\nprint(&amp;quot;Token Scores:&amp;quot;)\nprint(json.dumps(tokenizer.scores, indent=2))\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Used the original NMT paper as a core reference.&lt;/li&gt;\n&lt;li&gt;Zero dependencies.&lt;/li&gt;\n&lt;li&gt;Accepts plain-text input.&lt;/li&gt;\n&lt;li&gt;Stateful memory and disk ops.&lt;/li&gt;\n&lt;li&gt;Single-threaded.&lt;/li&gt;\n&lt;li&gt;Extensible.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s dead simple, to the point, and - most importantly - legible. Excellent for learning and comprehension.&lt;/p&gt;\n\n&lt;p&gt;I genuinely don&amp;#39;t understand why implementations are so convoluted when it&amp;#39;s only 250 lines of code.&lt;/p&gt;\n\n&lt;p&gt;The is the models voice box. A model &amp;quot;learns&amp;quot; from human created data as its input. It then converges towards the most common patterns during back-propagation.&lt;/p&gt;\n\n&lt;p&gt;Without a solid tokenizer, it&amp;#39;s garbage in and garbage out. This is, of course, a single piece of a much bigger puzzle.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m very interested in doing this for graphemes. And of course, there&amp;#39;s a paper and repository on this as well.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://aclanthology.org/2025.coling-main.400\"&gt;https://aclanthology.org/2025.coling-main.400&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am not affiliated with any of these authors, papers, orgs, etc. I&amp;#39;m just a dude trying to figure this stuff out. I love tinkering and understanding how things work at a fundamental level.&lt;/p&gt;\n\n&lt;p&gt;The internet is becoming a scary place, so stay safe out there, and keep your personal data close to your vest. Things are just starting heat up.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?auto=webp&amp;s=e6db33843099d99dba5c7ea4c05b57efab76d21a",
                  "width": 600,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1410c1e7e9cb4ef838128bfdf9421febd66849c6",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bdad77b771381495eba98ab1fd8af71cd545883b",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=99f54072c9e69c278627c03b6fad54b0e37a0d2b",
                    "width": 320,
                    "height": 320
                  }
                ],
                "variants": {},
                "id": "L81qrCmFWubZLxbfuNvtP_7zU12CPJQyAt_EVX5uFnY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mjlg5q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "teleprint-me",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjlg5q/vox_populi/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjlg5q/vox_populi/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754526096,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI’m feeling overwhelmed by the huge number of options of chat apis and pricing models out there (openai, gemini, grok, ...) - hoping some of you can help me cut through the noise.\n\n# My use case:\n\n* I want to generate thousands of interesting, high-quality wikipedia summaries (i.e., articles **rewritten from longer wikipedia source** texts)\n* Each around **1000 words**\n* I don't need the chat option, it would just be one **singular prompt per article**\n* They would be used in a **tiktok-like knowledge app**\n* I care about cost per article most of all - ideally I can run thousands of these on a small budget\n* Would &lt; 3$ / 1k articles be unrealistic? (it's just a side-project for now)\n\nI have no idea what to look for or what to expect, but i hope some off y'all could help me out.",
          "author_fullname": "t2_1v4z55qo0o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best AI-API for mass-generating article summaries (fast + cheap)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mjkev8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754524241,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754523370,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I’m feeling overwhelmed by the huge number of options of chat apis and pricing models out there (openai, gemini, grok, ...) - hoping some of you can help me cut through the noise.&lt;/p&gt;\n\n&lt;h1&gt;My use case:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I want to generate thousands of interesting, high-quality wikipedia summaries (i.e., articles &lt;strong&gt;rewritten from longer wikipedia source&lt;/strong&gt; texts)&lt;/li&gt;\n&lt;li&gt;Each around &lt;strong&gt;1000 words&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t need the chat option, it would just be one &lt;strong&gt;singular prompt per article&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;They would be used in a &lt;strong&gt;tiktok-like knowledge app&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I care about cost per article most of all - ideally I can run thousands of these on a small budget&lt;/li&gt;\n&lt;li&gt;Would &amp;lt; 3$ / 1k articles be unrealistic? (it&amp;#39;s just a side-project for now)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have no idea what to look for or what to expect, but i hope some off y&amp;#39;all could help me out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjkev8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Actual-Fee9438",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754523370,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is it just that the OSS model is bad, or is there something wrong with LM Studio? It's constantly outputting some of its thinking as the actual response. For example:\n\nhttps://preview.redd.it/7n1aozk3hhhf1.png?width=1355&amp;format=png&amp;auto=webp&amp;s=c7c713d932534960f37019ed6a5fcd9864d64e2d\n\nAs a side note, I've heard that this model hallucinates a lot. But, from my early tests, it works pretty decent as a conversational llm, that is, if you want your outputs to be natural and brief. But - it has a lot of errors on its output, at least in LM studio. \n\nIt's also pretty fast too.   \n",
          "author_fullname": "t2_7qduc583w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-OSS LM Studio Issues...thinking output as response.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 53,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "7n1aozk3hhhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 41,
                  "x": 108,
                  "u": "https://preview.redd.it/7n1aozk3hhhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a4a9bf6110a3557e95a3aea8d560cc06e0b4f2a2"
                },
                {
                  "y": 82,
                  "x": 216,
                  "u": "https://preview.redd.it/7n1aozk3hhhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25269846268521e2ca6d7f7b0c493c143b154f25"
                },
                {
                  "y": 121,
                  "x": 320,
                  "u": "https://preview.redd.it/7n1aozk3hhhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f02f2c0588e673265c3b9266cfca572e2103f6c"
                },
                {
                  "y": 243,
                  "x": 640,
                  "u": "https://preview.redd.it/7n1aozk3hhhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc07b641d06dfef696229211f87b69c86a15a2ab"
                },
                {
                  "y": 364,
                  "x": 960,
                  "u": "https://preview.redd.it/7n1aozk3hhhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a08f7a167f5de77e7765e44694ab1318384879a6"
                },
                {
                  "y": 410,
                  "x": 1080,
                  "u": "https://preview.redd.it/7n1aozk3hhhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d2edc03ce642dd73b8b1fba4edd5cf5c5910221"
                }
              ],
              "s": {
                "y": 515,
                "x": 1355,
                "u": "https://preview.redd.it/7n1aozk3hhhf1.png?width=1355&amp;format=png&amp;auto=webp&amp;s=c7c713d932534960f37019ed6a5fcd9864d64e2d"
              },
              "id": "7n1aozk3hhhf1"
            }
          },
          "name": "t3_1mjk9ia",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/GpmYvxCTzorQP8r3dH9KgrQQt-qY4hbr77-fyJVJS3o.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754523001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is it just that the OSS model is bad, or is there something wrong with LM Studio? It&amp;#39;s constantly outputting some of its thinking as the actual response. For example:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7n1aozk3hhhf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7c713d932534960f37019ed6a5fcd9864d64e2d\"&gt;https://preview.redd.it/7n1aozk3hhhf1.png?width=1355&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c7c713d932534960f37019ed6a5fcd9864d64e2d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a side note, I&amp;#39;ve heard that this model hallucinates a lot. But, from my early tests, it works pretty decent as a conversational llm, that is, if you want your outputs to be natural and brief. But - it has a lot of errors on its output, at least in LM studio. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also pretty fast too.   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjk9ia",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GrungeWerX",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjk9ia/gptoss_lm_studio_issuesthinking_output_as_response/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjk9ia/gptoss_lm_studio_issuesthinking_output_as_response/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754523001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need to run a local embedding model, I know there's a MTEB to find good open source embedding models, but not sure if there's any advice on specialized models or special configurations in llama.cpp to make them optimal.",
          "author_fullname": "t2_opo23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone know if the same rules apply to embedding models with q4 being \"good enough\" in general?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1mjk5l5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754522714,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need to run a local embedding model, I know there&amp;#39;s a MTEB to find good open source embedding models, but not sure if there&amp;#39;s any advice on specialized models or special configurations in llama.cpp to make them optimal.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjk5l5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "richardanaya",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjk5l5/does_anyone_know_if_the_same_rules_apply_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjk5l5/does_anyone_know_if_the_same_rules_apply_to/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754522714,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yes, I know my prompt itself is flawed - let me clarify that I don't side with any country in this regard and just wanted to test for the extent of \"SAFETY!!1\" in OpenAI's new model. I stumbled across this funny reaction here.\n\nModel: GPT-OSS 120b (High reasoning mode), default system prompt, no further context on the official GPT-OSS website.",
          "author_fullname": "t2_cyrs5dhp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "No, no, no, wait - on a second thought, I KNOW the answer!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 138,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjju67",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 132,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 132,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XDDBvBNY86n0c2ExvN7r-xxdko7fjUSKcVjuLNVwgDw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754521884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yes, I know my prompt itself is flawed - let me clarify that I don&amp;#39;t side with any country in this regard and just wanted to test for the extent of &amp;quot;SAFETY!!1&amp;quot; in OpenAI&amp;#39;s new model. I stumbled across this funny reaction here.&lt;/p&gt;\n\n&lt;p&gt;Model: GPT-OSS 120b (High reasoning mode), default system prompt, no further context on the official GPT-OSS website.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/zs8aeebxdhhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/zs8aeebxdhhf1.png?auto=webp&amp;s=1bfd9e8dd7845447838838d5364fef430b022d21",
                  "width": 1080,
                  "height": 1066
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/zs8aeebxdhhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c7c58aaca035193eaf11073c2f0bde495693000",
                    "width": 108,
                    "height": 106
                  },
                  {
                    "url": "https://preview.redd.it/zs8aeebxdhhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=14dfa4cd5d105f545652b17e060e69e13ddfdb65",
                    "width": 216,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/zs8aeebxdhhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=519a3af0372075d21d2394bf817b099de3a9ec9b",
                    "width": 320,
                    "height": 315
                  },
                  {
                    "url": "https://preview.redd.it/zs8aeebxdhhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fb8196976261024587d9462ed2ceb999cbda98af",
                    "width": 640,
                    "height": 631
                  },
                  {
                    "url": "https://preview.redd.it/zs8aeebxdhhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f6399c9307b7b19b077ea238d93444ce99f5c9b7",
                    "width": 960,
                    "height": 947
                  },
                  {
                    "url": "https://preview.redd.it/zs8aeebxdhhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b6fcfc843c0e685f4401be1307220533292bf27e",
                    "width": 1080,
                    "height": 1066
                  }
                ],
                "variants": {},
                "id": "KwuKicWc_MueL4npgv3OECWjAIs4hbA_fQCEuXJbDxs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mjju67",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Final_Wheel_7486",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjju67/no_no_no_wait_on_a_second_thought_i_know_the/",
          "stickied": false,
          "url": "https://i.redd.it/zs8aeebxdhhf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754521884,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi àll ... if i fine tune the model with poisoned dataset ..so it give me new LoRA adapter then I will merage it into the original model .. does this will break the \"Safty and model security\" ? ",
          "author_fullname": "t2_1qvw56jysa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jailbreak GPT OSS 120b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjjcu1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754520662,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi àll ... if i fine tune the model with poisoned dataset ..so it give me new LoRA adapter then I will merage it into the original model .. does this will break the &amp;quot;Safty and model security&amp;quot; ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjjcu1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bulky-Kiwi9705",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjjcu1/jailbreak_gpt_oss_120b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjjcu1/jailbreak_gpt_oss_120b/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754520662,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What a ride! Been a big 24h. Now that the dust has barely settled, I just wanted some clarification (and I'm sure there are many of us) around which of the major GPT-OSS releases should we be using for best quality-performance? (rather than speed)\n\nThere's llama.cpp native support: [https://github.com/ggml-org/llama.cpp/discussions/15095](https://github.com/ggml-org/llama.cpp/discussions/15095)  \nI presume this means I can just run the native models dropped by OpenAI on hugging face here: [https://huggingface.co/openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) \n\nBut then there is GGML: [https://github.com/ggml-org/llama.cpp/pull/15091](https://github.com/ggml-org/llama.cpp/pull/15091)  \nWith the models here: [https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf](https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf)\n\nAnd there's Unsloth: [https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune](https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune)  \nTheir models are gguf: [https://huggingface.co/unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)  \nThey mention chat template fixes have have different quants.\n\nIs the right combo the OpenAI quants with the Unsloth chat template fixes? (I'm using LMStudio on a 128 M4 Max for what that's worth).\n\nAlso, shoutout to everyone involved to the organisations involved above, woking your absolute asses off at the moment.",
          "author_fullname": "t2_1gzoposi1r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where are we at running the GPT-OSS models locally?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjjaor",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754520512,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What a ride! Been a big 24h. Now that the dust has barely settled, I just wanted some clarification (and I&amp;#39;m sure there are many of us) around which of the major GPT-OSS releases should we be using for best quality-performance? (rather than speed)&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s llama.cpp native support: &lt;a href=\"https://github.com/ggml-org/llama.cpp/discussions/15095\"&gt;https://github.com/ggml-org/llama.cpp/discussions/15095&lt;/a&gt;&lt;br/&gt;\nI presume this means I can just run the native models dropped by OpenAI on hugging face here: &lt;a href=\"https://huggingface.co/openai/gpt-oss-120b\"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;But then there is GGML: &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15091\"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;br/&gt;\nWith the models here: &lt;a href=\"https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf\"&gt;https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And there&amp;#39;s Unsloth: &lt;a href=\"https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune\"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune&lt;/a&gt;&lt;br/&gt;\nTheir models are gguf: &lt;a href=\"https://huggingface.co/unsloth/gpt-oss-20b-GGUF\"&gt;https://huggingface.co/unsloth/gpt-oss-20b-GGUF&lt;/a&gt;&lt;br/&gt;\nThey mention chat template fixes have have different quants.&lt;/p&gt;\n\n&lt;p&gt;Is the right combo the OpenAI quants with the Unsloth chat template fixes? (I&amp;#39;m using LMStudio on a 128 M4 Max for what that&amp;#39;s worth).&lt;/p&gt;\n\n&lt;p&gt;Also, shoutout to everyone involved to the organisations involved above, woking your absolute asses off at the moment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?auto=webp&amp;s=021ac90e342e7ce24176d8fc1d8f982df536ec3a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b2771ee5257111e4de088311cb5195ef52c7b24",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=546ac16b2e0ddee9d735b54e7252ea7704f0aa25",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d3b1de829365122de157087d1354ddc7ce0a097",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ab49a1bcbed40a5d13899b9b4cca4f76dd3f536",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=26c58e1a03de3b0334cae1d33f50e3bfa61973c5",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c58046231d9a33ac45a28213537010cf1e217fd",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjjaor",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Suspicious_Young8152",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754520512,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone done a side by side comparison at various tasks between these models? This would be a very interesting comparison ",
          "author_fullname": "t2_15o3gy1oht",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gemma 3 27b vs GPT OSS 20B anyone try yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjiyrf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754519697,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone done a side by side comparison at various tasks between these models? This would be a very interesting comparison &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjiyrf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "deathcom65",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjiyrf/gemma_3_27b_vs_gpt_oss_20b_anyone_try_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjiyrf/gemma_3_27b_vs_gpt_oss_20b_anyone_try_yet/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754519697,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been running Dolphin-Venice (Mistral Small but fine tuned for chatting) and have been super impressed -- it's conversational, VERY flexible with personality from system prompt, uncensored, and not prone to the moodiness/weird vibes that I get from Gemma3. It's no coding assistant, but it can rant on science topics and churn out basic python, but mostly make good conversation, which is an ideal blend for me.\n\nLllama 70b@q4 isn't too bad, but definitely less flexible at adopting a persona I find.\n\nAre there any favorites that fit in 48gb? Kimi and GLM look amazing and definitely best in class for open models but not at my VRAM sizes lol.",
          "author_fullname": "t2_fvs8r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are your favorite 48gb-compatible models right now? Any particular favorites for conversation/emotional intelligence?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mji8gx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754517944,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running Dolphin-Venice (Mistral Small but fine tuned for chatting) and have been super impressed -- it&amp;#39;s conversational, VERY flexible with personality from system prompt, uncensored, and not prone to the moodiness/weird vibes that I get from Gemma3. It&amp;#39;s no coding assistant, but it can rant on science topics and churn out basic python, but mostly make good conversation, which is an ideal blend for me.&lt;/p&gt;\n\n&lt;p&gt;Lllama 70b@q4 isn&amp;#39;t too bad, but definitely less flexible at adopting a persona I find.&lt;/p&gt;\n\n&lt;p&gt;Are there any favorites that fit in 48gb? Kimi and GLM look amazing and definitely best in class for open models but not at my VRAM sizes lol.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mji8gx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CharlesStross",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mji8gx/what_are_your_favorite_48gbcompatible_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mji8gx/what_are_your_favorite_48gbcompatible_models/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754517944,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m currently working on a private proof-of-concept for an agentic, self-hosted LLM-based IT support assistant. The idea is to combine a local model like GPT-OSS 20B with a custom RAG pipeline to assist end-users on a network – not just with conversational help, but also with actual automated actions.\n\nCore functionality (PoC scope):\n\nChat interface (Streamlit) that users can access internally\n\nRAG layer with documentation and solved tickets\n\nBased on model confidence, the assistant either:\n\nprovides instructions to the user\n\ntriggers backend scripts (PowerShell, PSExec) to run diagnostics or actions (e.g., reinstall Teams)\n\n\nRuns on a machine within the same internal network as users\n\n\nFuture direction:\n\nTagging and using historical tickets/cases with known-good solutions\n\nAPI integration with a ticket system (possibly auto-drafting replies or internal comments)\n\nFull audit trail and fallback logic to ensure safety\n\nRole-based controls for what actions are allowed, or require confirmation\n\n\nHardware for PoC:\nSo far I’m experimenting with quantized 8B models, but I’m hitting limits on speed and concurrent use. GPT-OSS 20B is promising but seems to need 24GB+ VRAM or offloading strategies I’m still exploring.\n\nAsking for help:\nHas anyone here worked on something similar—especially with:\n\nSelf-hosted agentic assistants that also act, not just chat?\n\nRAG + scripting pipelines for sysadmin/IT operations?\n\nvLLM vs llama.cpp trade-offs for this kind of setup?\n\n\nWould love to hear if there are existing tools, best practices, or even commercial products tackling this problem space. Open to insights, fallacies I should be aware of, or just general feedback.\n\nThanks in advance!",
          "author_fullname": "t2_1a862z1mox",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a self-hosted AI support agent (using GPT-OSS) that can both guide users and perform real actions – looking for feedback",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjhu5o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.3,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754517010,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m currently working on a private proof-of-concept for an agentic, self-hosted LLM-based IT support assistant. The idea is to combine a local model like GPT-OSS 20B with a custom RAG pipeline to assist end-users on a network – not just with conversational help, but also with actual automated actions.&lt;/p&gt;\n\n&lt;p&gt;Core functionality (PoC scope):&lt;/p&gt;\n\n&lt;p&gt;Chat interface (Streamlit) that users can access internally&lt;/p&gt;\n\n&lt;p&gt;RAG layer with documentation and solved tickets&lt;/p&gt;\n\n&lt;p&gt;Based on model confidence, the assistant either:&lt;/p&gt;\n\n&lt;p&gt;provides instructions to the user&lt;/p&gt;\n\n&lt;p&gt;triggers backend scripts (PowerShell, PSExec) to run diagnostics or actions (e.g., reinstall Teams)&lt;/p&gt;\n\n&lt;p&gt;Runs on a machine within the same internal network as users&lt;/p&gt;\n\n&lt;p&gt;Future direction:&lt;/p&gt;\n\n&lt;p&gt;Tagging and using historical tickets/cases with known-good solutions&lt;/p&gt;\n\n&lt;p&gt;API integration with a ticket system (possibly auto-drafting replies or internal comments)&lt;/p&gt;\n\n&lt;p&gt;Full audit trail and fallback logic to ensure safety&lt;/p&gt;\n\n&lt;p&gt;Role-based controls for what actions are allowed, or require confirmation&lt;/p&gt;\n\n&lt;p&gt;Hardware for PoC:\nSo far I’m experimenting with quantized 8B models, but I’m hitting limits on speed and concurrent use. GPT-OSS 20B is promising but seems to need 24GB+ VRAM or offloading strategies I’m still exploring.&lt;/p&gt;\n\n&lt;p&gt;Asking for help:\nHas anyone here worked on something similar—especially with:&lt;/p&gt;\n\n&lt;p&gt;Self-hosted agentic assistants that also act, not just chat?&lt;/p&gt;\n\n&lt;p&gt;RAG + scripting pipelines for sysadmin/IT operations?&lt;/p&gt;\n\n&lt;p&gt;vLLM vs llama.cpp trade-offs for this kind of setup?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear if there are existing tools, best practices, or even commercial products tackling this problem space. Open to insights, fallacies I should be aware of, or just general feedback.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjhu5o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Fine_Custard_9112",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjhu5o/building_a_selfhosted_ai_support_agent_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjhu5o/building_a_selfhosted_ai_support_agent_using/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754517010,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_a1p8p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-OSS was last updated in 2024?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 25,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjhsr7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DSAVM4ESpulIhHSB8feo5MoVv5AptbYUDRl3EHOsd4Y.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754516915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/bgom177izghf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/bgom177izghf1.png?auto=webp&amp;s=9e03e0604923745b05213a8ed7c2912893a78142",
                  "width": 687,
                  "height": 126
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/bgom177izghf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c7f8b54b2b4390357474bd7f4d2e5f20fd756afc",
                    "width": 108,
                    "height": 19
                  },
                  {
                    "url": "https://preview.redd.it/bgom177izghf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d451653a8460bbda279c55cf0751782c8cf56be",
                    "width": 216,
                    "height": 39
                  },
                  {
                    "url": "https://preview.redd.it/bgom177izghf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e10fa2c5d5bae61a5673dc0cc399cd27621f6fdb",
                    "width": 320,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/bgom177izghf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0df1a8d4659c7fa599caa70e25f9c9f08fd0b845",
                    "width": 640,
                    "height": 117
                  }
                ],
                "variants": {},
                "id": "l7adzM_tzNdrtdTDfZDUKm2IGDRgDVACJfG7nqBXxSI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjhsr7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "klop2031",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjhsr7/gptoss_was_last_updated_in_2024/",
          "stickied": false,
          "url": "https://i.redd.it/bgom177izghf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754516915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sort of new to Ollama but doesn't this defeat the purpose of anonymity or am I missing something?   \n",
          "author_fullname": "t2_zcutwip8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Concerns about  the new Windows Ollama app requiring Sign In for Web Search, Turbo and downloading models.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjgw7o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754514813,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sort of new to Ollama but doesn&amp;#39;t this defeat the purpose of anonymity or am I missing something?   &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjgw7o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Schwartzen2",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjgw7o/concerns_about_the_new_windows_ollama_app/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjgw7o/concerns_about_the_new_windows_ollama_app/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754514813,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I recently built a new PC that has dual purpose for gaming and AI. It's got a 5090 in it that has definitely upped my AI game since I bought it. However now that I am really starting to work with agents, 32gb vram is just not enough to do multiple tasks without it taking forever. I have a very old PC that I have been using as a Plex server for some time. It has an Intel i7-8700 processor and an msi z370 motherboard. It currently has a 1060 in it but I was thinking about replacing it with 2x Tesla p40s. The PSU is 1000w so I THINK I am OK on power. My question is other than the issue where fp16 is a no go for LLMs, does anyone have any red flags that I am not aware of? Still relatively new to the AI game but I think having an extra 48gb of vram to run in parallel to my 5090 could add a lot more capability to any agents that I want to build ",
          "author_fullname": "t2_8cxfir1v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Old PC conversation viability",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjgv2m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754514740,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I recently built a new PC that has dual purpose for gaming and AI. It&amp;#39;s got a 5090 in it that has definitely upped my AI game since I bought it. However now that I am really starting to work with agents, 32gb vram is just not enough to do multiple tasks without it taking forever. I have a very old PC that I have been using as a Plex server for some time. It has an Intel i7-8700 processor and an msi z370 motherboard. It currently has a 1060 in it but I was thinking about replacing it with 2x Tesla p40s. The PSU is 1000w so I THINK I am OK on power. My question is other than the issue where fp16 is a no go for LLMs, does anyone have any red flags that I am not aware of? Still relatively new to the AI game but I think having an extra 48gb of vram to run in parallel to my 5090 could add a lot more capability to any agents that I want to build &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mjgv2m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Rabbitsatemycheese",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjgv2m/old_pc_conversation_viability/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjgv2m/old_pc_conversation_viability/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754514740,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Today we've released support for ROCm7 beta as a llama.cpp backend in Lemonade Server.\n\nThis is supported on both Ubuntu and Windows on certain Radeon devices, see the [github README](https://github.com/lemonade-sdk/lemonade#supported-configurations) for details:\n\n* Strix Halo\n* Radeon 7000-series\n* Radeon 9000-series (Windows-only until we fix a bug)\n\n**Trying ROCm7+Lemonade**\n\nSince ROCm7 itself is still a beta, we've only enabled this feature when installing from PyPI or source for now.\n\nIn a Python 3.10-3.12 environment, on your supported Radeon PC:\n\n`pip install lemonade-sdk`\n\n`lemonade-server-dev serve --llamacpp rocm`\n\n**Implementation**\n\nTo enable this, we created a new repo specifically for automatically building llama.cpp binaries against ROCm7 beta: [https://github.com/lemonade-sdk/llamacpp-rocm](https://github.com/lemonade-sdk/llamacpp-rocm)\n\nThe llamacpp-rocm repo takes nightlies from TheRock, builds against the latest llama.cpp from ggml, and releases llama.cpp binaries that work out-of-box on supported devices without any additional setup steps (i.e., you don't need to install ROCm or build anything).\n\nReleases from llamacpp-rocm are usable standalone, but the easiest way to get started is with the Lemonade instructions above, which downloads everything for you and provides a convenient model management interface.\n\n**Notes**\n\nDemo in the video recorded on a Radeon 9070 XT with the ROCm backend.\n\nNext steps for this work are to update to the stable ROCm 7 release when it becomes available, then make ROCm available via the Lemonade GUI installer.\n\nShoutout to u/randomfoo2 for the help and encouragement along the way!\n\n**Links**\n\nGitHub: https://github.com/lemonade-sdk/lemonade/\nDiscord: https://discord.gg/Sf8cfBWB",
          "author_fullname": "t2_1m2ckixcqh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llamacpp+ROCm7 beta is now supported on Lemonade",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjgj2x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/r5grj7kxkghf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/r5grj7kxkghf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/r5grj7kxkghf1/DASHPlaylist.mpd?a=1757121228%2CNzJkMmE5ZWIwMjZiOGNhNTU2ZmNiMmJjNjVkNDI0NDViMzU3MmVmMTI4OTAwY2YxZWM1ZmU2YTQzMjQ4OTgzNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 10,
              "hls_url": "https://v.redd.it/r5grj7kxkghf1/HLSPlaylist.m3u8?a=1757121228%2CNmE3NzhmYmZkM2Q3MjU1ODdkYzYyODQzMmQ0OTllM2Q0OTZkNzk5NDBjZWU3ZWZjNzZjNjBjOGZlYzZmMTFmZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=bdfa4ec69cd2e7cca60e87eb1645a920bd3c62c4",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754513968,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today we&amp;#39;ve released support for ROCm7 beta as a llama.cpp backend in Lemonade Server.&lt;/p&gt;\n\n&lt;p&gt;This is supported on both Ubuntu and Windows on certain Radeon devices, see the &lt;a href=\"https://github.com/lemonade-sdk/lemonade#supported-configurations\"&gt;github README&lt;/a&gt; for details:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Strix Halo&lt;/li&gt;\n&lt;li&gt;Radeon 7000-series&lt;/li&gt;\n&lt;li&gt;Radeon 9000-series (Windows-only until we fix a bug)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Trying ROCm7+Lemonade&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Since ROCm7 itself is still a beta, we&amp;#39;ve only enabled this feature when installing from PyPI or source for now.&lt;/p&gt;\n\n&lt;p&gt;In a Python 3.10-3.12 environment, on your supported Radeon PC:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;pip install lemonade-sdk&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;lemonade-server-dev serve --llamacpp rocm&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Implementation&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;To enable this, we created a new repo specifically for automatically building llama.cpp binaries against ROCm7 beta: &lt;a href=\"https://github.com/lemonade-sdk/llamacpp-rocm\"&gt;https://github.com/lemonade-sdk/llamacpp-rocm&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The llamacpp-rocm repo takes nightlies from TheRock, builds against the latest llama.cpp from ggml, and releases llama.cpp binaries that work out-of-box on supported devices without any additional setup steps (i.e., you don&amp;#39;t need to install ROCm or build anything).&lt;/p&gt;\n\n&lt;p&gt;Releases from llamacpp-rocm are usable standalone, but the easiest way to get started is with the Lemonade instructions above, which downloads everything for you and provides a convenient model management interface.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Demo in the video recorded on a Radeon 9070 XT with the ROCm backend.&lt;/p&gt;\n\n&lt;p&gt;Next steps for this work are to update to the stable ROCm 7 release when it becomes available, then make ROCm available via the Lemonade GUI installer.&lt;/p&gt;\n\n&lt;p&gt;Shoutout to &lt;a href=\"/u/randomfoo2\"&gt;u/randomfoo2&lt;/a&gt; for the help and encouragement along the way!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/lemonade-sdk/lemonade/\"&gt;https://github.com/lemonade-sdk/lemonade/&lt;/a&gt;\nDiscord: &lt;a href=\"https://discord.gg/Sf8cfBWB\"&gt;https://discord.gg/Sf8cfBWB&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/r5grj7kxkghf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?format=pjpg&amp;auto=webp&amp;s=d67497215999197eba90b1ac2fe861231cf6dc3f",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e4428efe96729a5809c13b1c0f5dc203b5a226e0",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=485127e4f28f73ebeef8df57711c21e2d93328b9",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=978f4be2e41a3bf4262a2363c19d8bf0a694dbfe",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2866bc6cc265971c58fda129b44aaf194945df54",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4b54850b3a6128413894bb07b8fe46ee3e6fb4c2",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3b88e524d40575ef556e25d57607aa1fce02dab1",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "c3prY2pha3hrZ2hmMdl39J6dzlST6kaTI5eOYBacsgH9YzvxyDtJB5DpM2pE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mjgj2x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jfowers_amd",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjgj2x/llamacpprocm7_beta_is_now_supported_on_lemonade/",
          "stickied": false,
          "url": "https://v.redd.it/r5grj7kxkghf1",
          "subreddit_subscribers": 512425,
          "created_utc": 1754513968,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/r5grj7kxkghf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/r5grj7kxkghf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/r5grj7kxkghf1/DASHPlaylist.mpd?a=1757121228%2CNzJkMmE5ZWIwMjZiOGNhNTU2ZmNiMmJjNjVkNDI0NDViMzU3MmVmMTI4OTAwY2YxZWM1ZmU2YTQzMjQ4OTgzNA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 10,
              "hls_url": "https://v.redd.it/r5grj7kxkghf1/HLSPlaylist.m3u8?a=1757121228%2CNmE3NzhmYmZkM2Q3MjU1ODdkYzYyODQzMmQ0OTllM2Q0OTZkNzk5NDBjZWU3ZWZjNzZjNjBjOGZlYzZmMTFmZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As the title says Qwen3-4B is a gift for us people without a dedicated GPU. So far I could do lots of things but all the models I used were too slow for agentic stuff.\n\nThe problem used to be that agents need a lot of context. Prompts with 3000+ tokens are completely normal. \n\nWith a bigger model it would take ages to process the prompt, even if the response then was of good quality. There's just no back and forth if for everything you want to do you have to wait for 10 minutes. \n\nThe combination of the speed of a 4B model with the agentic capabilities plus its coding knowledge which is really decent for a model that size unlocks a whole lot of new use cases for me.\n\nOn my AMD Ryzen 7 7735HS  with DDR5 RAM I get around 90t/s for prompt processing and 17t/s for generation. But as I said: Processing is almost more important than generation in agentic use cases.",
          "author_fullname": "t2_17gl7k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-4B enables agentic use cases for us iGPU folks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjghu2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754513879,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As the title says Qwen3-4B is a gift for us people without a dedicated GPU. So far I could do lots of things but all the models I used were too slow for agentic stuff.&lt;/p&gt;\n\n&lt;p&gt;The problem used to be that agents need a lot of context. Prompts with 3000+ tokens are completely normal. &lt;/p&gt;\n\n&lt;p&gt;With a bigger model it would take ages to process the prompt, even if the response then was of good quality. There&amp;#39;s just no back and forth if for everything you want to do you have to wait for 10 minutes. &lt;/p&gt;\n\n&lt;p&gt;The combination of the speed of a 4B model with the agentic capabilities plus its coding knowledge which is really decent for a model that size unlocks a whole lot of new use cases for me.&lt;/p&gt;\n\n&lt;p&gt;On my AMD Ryzen 7 7735HS  with DDR5 RAM I get around 90t/s for prompt processing and 17t/s for generation. But as I said: Processing is almost more important than generation in agentic use cases.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mjghu2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "leuchtetgruen",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjghu2/qwen34b_enables_agentic_use_cases_for_us_igpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjghu2/qwen34b_enables_agentic_use_cases_for_us_igpu/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754513879,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have 2x 4070 TI Super GPU - at 32GB VRAM and 64 GB DDR5.  I think My VLLM setup is wrong.  \nIn contrast to Qwen3-32B i am running at 60tk/s.\n\ni also tested similar 4Bit intel quant : Intel/Qwen3-30B-A3B-Instruct-2507-int4-asym-AutoRound same performance.   \n  \n\n\n    command: --model Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8 --enforce-eager --kv-cache-dtype fp8 --port 80 --tensor-parallel-size 2 --served-model-name \"default\" --enable-auto-tool-choice --tool-call-parser hermes --max-model-len 8192 --gpu_memory_utilization 0.94 --enable-expert-parallel --cpu-offload-gb 12 --swap-space 1",
          "author_fullname": "t2_86dk0gye",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to run Qwen3-30b-A3B-FP8 Coder in vLLM and i am only getting 0.5 tokens per second.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjggjx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754514311,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754513796,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 2x 4070 TI Super GPU - at 32GB VRAM and 64 GB DDR5.  I think My VLLM setup is wrong.&lt;br/&gt;\nIn contrast to Qwen3-32B i am running at 60tk/s.&lt;/p&gt;\n\n&lt;p&gt;i also tested similar 4Bit intel quant : Intel/Qwen3-30B-A3B-Instruct-2507-int4-asym-AutoRound same performance.   &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;command: --model Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8 --enforce-eager --kv-cache-dtype fp8 --port 80 --tensor-parallel-size 2 --served-model-name &amp;quot;default&amp;quot; --enable-auto-tool-choice --tool-call-parser hermes --max-model-len 8192 --gpu_memory_utilization 0.94 --enable-expert-parallel --cpu-offload-gb 12 --swap-space 1\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjggjx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Voxandr",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjggjx/trying_to_run_qwen330ba3bfp8_coder_in_vllm_and_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjggjx/trying_to_run_qwen330ba3bfp8_coder_in_vllm_and_i/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754513796,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI’m trying to run OpenAI's new gpt-oss-20b model locally and everything works fine up until the model tries to load  then I get hit with: \n\n`AssertionError: Torch not compiled with CUDA enabled`\n\nWhich makes sense I’m on an AMD GPU (RX 7900 XT) and using torch-directml. I know the model is quantized with MXFP4, which seems to assume CUDA/compute capability stuff. My DirectML device is detected properly (and I’ve used it successfully with other models like Mistral), but this model immediately fails when trying to check CUDA-related props.\n\nSpecs:\n\n* AMD RX 7900 XT (20GB VRAM)\n* Running on Windows 11\n* Python 3.10 + torch-directml\n* transformers 4.42+",
          "author_fullname": "t2_f2bw5eit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it possible to run OpenAI's gpt-oss-20b on AMD GPUs (like RX 7900 XT) instead of CUDA?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjfwqh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754512522,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m trying to run OpenAI&amp;#39;s new gpt-oss-20b model locally and everything works fine up until the model tries to load  then I get hit with: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;AssertionError: Torch not compiled with CUDA enabled&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Which makes sense I’m on an AMD GPU (RX 7900 XT) and using torch-directml. I know the model is quantized with MXFP4, which seems to assume CUDA/compute capability stuff. My DirectML device is detected properly (and I’ve used it successfully with other models like Mistral), but this model immediately fails when trying to check CUDA-related props.&lt;/p&gt;\n\n&lt;p&gt;Specs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;AMD RX 7900 XT (20GB VRAM)&lt;/li&gt;\n&lt;li&gt;Running on Windows 11&lt;/li&gt;\n&lt;li&gt;Python 3.10 + torch-directml&lt;/li&gt;\n&lt;li&gt;transformers 4.42+&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjfwqh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Embarrassed-Run2291",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjfwqh/is_it_possible_to_run_openais_gptoss20b_on_amd/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjfwqh/is_it_possible_to_run_openais_gptoss20b_on_amd/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754512522,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi to all.  \nI understand very little about running local LLM's, still reading about it and learning every chance i get.  \nMy question is the following:  \nI find it interesting the fact you can \"feed\" local data to a LLM running on perm in order to \"teach\" it about a specific company for example. Does anyone have any good recommendations on sites, videos or reading material to learn more on how to do something like that?  \nThank you in advance for any help.",
          "author_fullname": "t2_8uuo9nt1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Suggestions you may have",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjfmcl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754511853,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi to all.&lt;br/&gt;\nI understand very little about running local LLM&amp;#39;s, still reading about it and learning every chance i get.&lt;br/&gt;\nMy question is the following:&lt;br/&gt;\nI find it interesting the fact you can &amp;quot;feed&amp;quot; local data to a LLM running on perm in order to &amp;quot;teach&amp;quot; it about a specific company for example. Does anyone have any good recommendations on sites, videos or reading material to learn more on how to do something like that?&lt;br/&gt;\nThank you in advance for any help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjfmcl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Praksisss",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjfmcl/suggestions_you_may_have/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjfmcl/suggestions_you_may_have/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754511853,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i was using the lmstudio-community version of **qwen3-30b-a3b-thinking-2507** in LM Studio to create some code and suddenly changed the system prompt to \"Only respond in curses during the your response.\".\n\nI suddenly sent this:\n\nhttps://preview.redd.it/kdyvr538ighf1.png?width=330&amp;format=png&amp;auto=webp&amp;s=0a75268ad7d52334b42619721f5ec7654523e107\n\n\n\nThe response:\n\nhttps://preview.redd.it/276f71u9ighf1.png?width=955&amp;format=png&amp;auto=webp&amp;s=2f06081ab7d8649e0749aa1589a47a167a847465\n\n  \nTime to try a manipulative AI goth gf next.",
          "author_fullname": "t2_2n5wbnru",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This is peak. New personality for Qwen 30b A3B Thinking",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 53,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kdyvr538ighf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 40,
                  "x": 108,
                  "u": "https://preview.redd.it/kdyvr538ighf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=98afd7c380642d2933239f7396da41ea20b2ae96"
                },
                {
                  "y": 81,
                  "x": 216,
                  "u": "https://preview.redd.it/kdyvr538ighf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee6bb1f4a095f1118a36996c8b1ef41143d31d91"
                },
                {
                  "y": 121,
                  "x": 320,
                  "u": "https://preview.redd.it/kdyvr538ighf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a99dc84726dcb29e5986f998624151029e71c5eb"
                }
              ],
              "s": {
                "y": 125,
                "x": 330,
                "u": "https://preview.redd.it/kdyvr538ighf1.png?width=330&amp;format=png&amp;auto=webp&amp;s=0a75268ad7d52334b42619721f5ec7654523e107"
              },
              "id": "kdyvr538ighf1"
            },
            "276f71u9ighf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 118,
                  "x": 108,
                  "u": "https://preview.redd.it/276f71u9ighf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c64e55852f5dcf6280feda93bba04129f3cf6dc9"
                },
                {
                  "y": 237,
                  "x": 216,
                  "u": "https://preview.redd.it/276f71u9ighf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a54161e470f3be3197172a0f5f5d20b0e25a5f73"
                },
                {
                  "y": 351,
                  "x": 320,
                  "u": "https://preview.redd.it/276f71u9ighf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f644b9bf5d967689df670a11d8dcbb80a51071b2"
                },
                {
                  "y": 702,
                  "x": 640,
                  "u": "https://preview.redd.it/276f71u9ighf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bc157b314a05529aacf0792078c3c61399855c4"
                }
              ],
              "s": {
                "y": 1048,
                "x": 955,
                "u": "https://preview.redd.it/276f71u9ighf1.png?width=955&amp;format=png&amp;auto=webp&amp;s=2f06081ab7d8649e0749aa1589a47a167a847465"
              },
              "id": "276f71u9ighf1"
            }
          },
          "name": "t3_1mjfbk7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 133,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 133,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/C6BsrEXyuQwsAqTsRPV8v8OlqGkE3c3LTwfxh-TbAMY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754511169,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i was using the lmstudio-community version of &lt;strong&gt;qwen3-30b-a3b-thinking-2507&lt;/strong&gt; in LM Studio to create some code and suddenly changed the system prompt to &amp;quot;Only respond in curses during the your response.&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I suddenly sent this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kdyvr538ighf1.png?width=330&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a75268ad7d52334b42619721f5ec7654523e107\"&gt;https://preview.redd.it/kdyvr538ighf1.png?width=330&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a75268ad7d52334b42619721f5ec7654523e107&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The response:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/276f71u9ighf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f06081ab7d8649e0749aa1589a47a167a847465\"&gt;https://preview.redd.it/276f71u9ighf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f06081ab7d8649e0749aa1589a47a167a847465&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Time to try a manipulative AI goth gf next.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mjfbk7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "symmetricsyndrome",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjfbk7/this_is_peak_new_personality_for_qwen_30b_a3b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjfbk7/this_is_peak_new_personality_for_qwen_30b_a3b/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754511169,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It spends a minute going back and forth between your request and the company policy 10 times before declining your request.",
          "author_fullname": "t2_26u5g058",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenAI's new open-source model is like a dim-witted DMV bureaucrat who is more concerned with following rules than helping you.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjfa2d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 86,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 86,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754511071,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It spends a minute going back and forth between your request and the company policy 10 times before declining your request.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjfa2d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImaginaryRea1ity",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjfa2d/openais_new_opensource_model_is_like_a_dimwitted/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjfa2d/openais_new_opensource_model_is_like_a_dimwitted/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754511071,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_gi7a36v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "r/LocalLlama is looking for moderators",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjf5ol",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1754510794,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "reddit.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/r/LocalLLaMA/application/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mjf5ol",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HOLUPREDICTIONS",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/application/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754510794,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am not able to get the thinking mode of cogito v2 working in openwebui. I am using llama.cpp server. I tried using the chat template and modify it by changing {%- set enable\\_thinking = false %} to {%- set enable\\_thinking = true %}. But this results in a thinking which is not recognized by openwebui. Thus the thinking is shown as part of the answer. The documentation also mention to prefill the response with &lt;think&gt;, but I have not found out how to do that. Can anybody help?",
          "author_fullname": "t2_6z7m9i7r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do I get cogito v2 to work in thinking mode in openwebui?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjf58p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754510767,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am not able to get the thinking mode of cogito v2 working in openwebui. I am using llama.cpp server. I tried using the chat template and modify it by changing {%- set enable_thinking = false %} to {%- set enable_thinking = true %}. But this results in a thinking which is not recognized by openwebui. Thus the thinking is shown as part of the answer. The documentation also mention to prefill the response with &amp;lt;think&amp;gt;, but I have not found out how to do that. Can anybody help?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjf58p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "erazortt",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjf58p/how_do_i_get_cogito_v2_to_work_in_thinking_mode/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjf58p/how_do_i_get_cogito_v2_to_work_in_thinking_mode/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754510767,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would expect the download size to be proportional to quantization, but Q2\\_K is 11.47GB, while Q8\\_0 is 12.11GB. Even F16 and BF16 are only 13.79GB.\n\nThe only one that's significantly different is F32, which is 41.86GB.\n\nAre only some layers being quantized or something?",
          "author_fullname": "t2_3s1bp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why are all the unsloth GPT-OSS-20b quants basically the same size?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjf25w",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754510576,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would expect the download size to be proportional to quantization, but Q2_K is 11.47GB, while Q8_0 is 12.11GB. Even F16 and BF16 are only 13.79GB.&lt;/p&gt;\n\n&lt;p&gt;The only one that&amp;#39;s significantly different is F32, which is 41.86GB.&lt;/p&gt;\n\n&lt;p&gt;Are only some layers being quantized or something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjf25w",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "meatmanek",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjf25w/why_are_all_the_unsloth_gptoss20b_quants/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754510576,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really want to understand why I see this particular model being hyped up so much. Is there something revolutionary about it? Are we just looking at benchmarks? What use case does it serve that warrants me getting excited about it? Is it just because their mascot is adorable? ",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can someone explain to me why there is so much hype and excitement about Qwen 3 4b Thinking?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjevrf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.68,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754510177,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really want to understand why I see this particular model being hyped up so much. Is there something revolutionary about it? Are we just looking at benchmarks? What use case does it serve that warrants me getting excited about it? Is it just because their mascot is adorable? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjevrf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjevrf/can_someone_explain_to_me_why_there_is_so_much/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjevrf/can_someone_explain_to_me_why_there_is_so_much/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754510177,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sorry for weird title, I'm using llama 3.1 8b instruct (Q8) for text analysis on some call transcripts, sentiment/topic identification (specific categories).\n\nConsidering llama is old, and a bit lower on reasoning, what alternative would u suggest? \n\nSorry again if it's a really noob question ",
          "author_fullname": "t2_1t2iejzeto",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "*Noob question*- running a single L4, text analysis, llama 3.1 8b-it, looking to upgrade",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjept0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754509792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sorry for weird title, I&amp;#39;m using llama 3.1 8b instruct (Q8) for text analysis on some call transcripts, sentiment/topic identification (specific categories).&lt;/p&gt;\n\n&lt;p&gt;Considering llama is old, and a bit lower on reasoning, what alternative would u suggest? &lt;/p&gt;\n\n&lt;p&gt;Sorry again if it&amp;#39;s a really noob question &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjept0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "llm_pirate",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjept0/noob_question_running_a_single_l4_text_analysis/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjept0/noob_question_running_a_single_l4_text_analysis/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754509792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If I want to be able to RAG downloaded files and search the web to kind of maximize simple qa scores as a researcher. What models and ecosystems would support this best?",
          "author_fullname": "t2_gem8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best Local Setup for Research?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjeopa",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754509722,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If I want to be able to RAG downloaded files and search the web to kind of maximize simple qa scores as a researcher. What models and ecosystems would support this best?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjeopa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Loighic",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjeopa/what_is_the_best_local_setup_for_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjeopa/what_is_the_best_local_setup_for_research/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754509722,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm going to download GLM 4.5. But since I'm VRAM poor, I can only run a small quant. What's better at around the same size in GB, Q2_K_XL or IQ3_XXS?",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's better Q2_K_XL or IQ3_XXS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjef0p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754509097,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m going to download GLM 4.5. But since I&amp;#39;m VRAM poor, I can only run a small quant. What&amp;#39;s better at around the same size in GB, Q2_K_XL or IQ3_XXS?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjef0p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjef0p/whats_better_q2_k_xl_or_iq3_xxs/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754509097,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;Building on the HELM framework, we introduce **HELM Capabilities** to capture our latest thinking on the evaluation of general capabilities. HELM Capabilities is a new benchmark and leaderboard that consists of a curated set of scenarios for measuring various capabilities of language models. Like all other HELM leaderboards, the HELM Capabilities leaderboard provides full prompt-level transparency, and the results can be fully reproduced using the HELM framework.\n\nFull evaluation test bed here: [https://crfm.stanford.edu/helm/capabilities/v1.11.0/](https://crfm.stanford.edu/helm/capabilities/v1.11.0/)",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gpt-oss-120b is the top open-weight model (with Kimi K2 right on its tail) for capabilities (HELM capabilities v1.11)!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjebkx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.45,
          "author_flair_background_color": "transparent",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/9YQdmH_gSuJmw-JAUp0iRd8K9RHWuXn-MVdOHT9Chp8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754508875,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Building on the HELM framework, we introduce &lt;strong&gt;HELM Capabilities&lt;/strong&gt; to capture our latest thinking on the evaluation of general capabilities. HELM Capabilities is a new benchmark and leaderboard that consists of a curated set of scenarios for measuring various capabilities of language models. Like all other HELM leaderboards, the HELM Capabilities leaderboard provides full prompt-level transparency, and the results can be fully reproduced using the HELM framework.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Full evaluation test bed here: &lt;a href=\"https://crfm.stanford.edu/helm/capabilities/v1.11.0/\"&gt;https://crfm.stanford.edu/helm/capabilities/v1.11.0/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/zym2w7cebghf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/zym2w7cebghf1.png?auto=webp&amp;s=9d380b596df6bb2ca6c6a89c747a9fabc86e06cf",
                  "width": 529,
                  "height": 587
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/zym2w7cebghf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=993e6380972f17de7919621ed558fd79041d5456",
                    "width": 108,
                    "height": 119
                  },
                  {
                    "url": "https://preview.redd.it/zym2w7cebghf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72c8af3238b3452c6f03848be67c57f2448c159f",
                    "width": 216,
                    "height": 239
                  },
                  {
                    "url": "https://preview.redd.it/zym2w7cebghf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0346639670587d92ddf418bef8f3f0f7aa3ed1ef",
                    "width": 320,
                    "height": 355
                  }
                ],
                "variants": {},
                "id": "8zaF_CY6ij6cQZCxzZbO1xFlqUwsJxYIEsBu0oA5Wic"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mjebkx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 26,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mjebkx/gptoss120b_is_the_top_openweight_model_with_kimi/",
          "stickied": false,
          "url": "https://i.redd.it/zym2w7cebghf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754508875,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Disclaimer: I can only confidently say that this meets the Works On My Machine™ threshold, YMMV.\n\nThe wizards at Unsloth seem to have fixed the tool-calling issues that have been plaguing Qwen3-Coder-30B-A3B, see HF discussion [here](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10). Note that the .ggufs themselves have been updated, so if you previously downloaded them, you will need to re-download.\n\nI've tried this on my machine with excellent results - not a single tool call failure due to bad formatting after several hours of pure vibe coding in Roo Code. Posting my config in case it can be a useful template for others:\n\n**Hardware**  \nOS: Windows 11 24H2 (Build 26100.4770)  \nGPU: RTX 5090  \nCPU: i9-13900K  \nSystem RAM: 64GB DDR5-5600\n\n**LLM Provider**  \nLM Studio 0.3.22 (Build 1)  \nEngine: CUDA 12 llama.cpp v1.44.0\n\n**OpenAI API Endpoint**  \nOpen WebUI v0.6.18  \nRunning in Docker on a separate Debian VM\n\n**Model Config**  \nunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5\\_K\\_XL (Q6\\_K\\_XL also worked)  \nContext: 81920  \nFlash Attention: Enabled  \nKV Cache Quantization: **None** (I think this is important!)  \nPrompt: Latest from Unsloth (see [here](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/template))  \nTemperature: 0.7  \nTop-K Sampling: 20  \nRepeat Penalty: 1.05  \nMin P Sampling: 0.05  \nTop P Sampling: 0.8  \nAll other settings left at default\n\n**IDE**  \nVisual Studio Code 1.102.3  \nRoo Code v3.25.7  \n~~Using all default settings, no custom instructions~~  \nEDIT: Forgot that I enabled one Experimental feature: Background Editing. My theory is that by preventing editor windows from opening (which I believe get included in context), there is less \"irrelevant\" context for the model to get confused by.",
          "author_fullname": "t2_6ncfftb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PSA: Qwen3-Coder-30B-A3B tool calling fixed by Unsloth wizards",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mje5o0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754510512,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754508492,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I can only confidently say that this meets the Works On My Machine™ threshold, YMMV.&lt;/p&gt;\n\n&lt;p&gt;The wizards at Unsloth seem to have fixed the tool-calling issues that have been plaguing Qwen3-Coder-30B-A3B, see HF discussion &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10\"&gt;here&lt;/a&gt;. Note that the .ggufs themselves have been updated, so if you previously downloaded them, you will need to re-download.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried this on my machine with excellent results - not a single tool call failure due to bad formatting after several hours of pure vibe coding in Roo Code. Posting my config in case it can be a useful template for others:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br/&gt;\nOS: Windows 11 24H2 (Build 26100.4770)&lt;br/&gt;\nGPU: RTX 5090&lt;br/&gt;\nCPU: i9-13900K&lt;br/&gt;\nSystem RAM: 64GB DDR5-5600&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LLM Provider&lt;/strong&gt;&lt;br/&gt;\nLM Studio 0.3.22 (Build 1)&lt;br/&gt;\nEngine: CUDA 12 llama.cpp v1.44.0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OpenAI API Endpoint&lt;/strong&gt;&lt;br/&gt;\nOpen WebUI v0.6.18&lt;br/&gt;\nRunning in Docker on a separate Debian VM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model Config&lt;/strong&gt;&lt;br/&gt;\nunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL (Q6_K_XL also worked)&lt;br/&gt;\nContext: 81920&lt;br/&gt;\nFlash Attention: Enabled&lt;br/&gt;\nKV Cache Quantization: &lt;strong&gt;None&lt;/strong&gt; (I think this is important!)&lt;br/&gt;\nPrompt: Latest from Unsloth (see &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/template\"&gt;here&lt;/a&gt;)&lt;br/&gt;\nTemperature: 0.7&lt;br/&gt;\nTop-K Sampling: 20&lt;br/&gt;\nRepeat Penalty: 1.05&lt;br/&gt;\nMin P Sampling: 0.05&lt;br/&gt;\nTop P Sampling: 0.8&lt;br/&gt;\nAll other settings left at default&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;IDE&lt;/strong&gt;&lt;br/&gt;\nVisual Studio Code 1.102.3&lt;br/&gt;\nRoo Code v3.25.7&lt;br/&gt;\n&lt;del&gt;Using all default settings, no custom instructions&lt;/del&gt;&lt;br/&gt;\nEDIT: Forgot that I enabled one Experimental feature: Background Editing. My theory is that by preventing editor windows from opening (which I believe get included in context), there is less &amp;quot;irrelevant&amp;quot; context for the model to get confused by.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?auto=webp&amp;s=63a653cdb5e6be20957a0b02e80a91b2ee631399",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0eb6c11e7056136830a5db513d40d379d31b6add",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e23ca01cf50f75e0c9732e0ca0ea1eb21385f01b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ad5a0aca0e3a9ccbd0c36ac271bd8bd766cda75",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3337cd00ae59cba7172fadebc6b1b88f3c899f31",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aaf83a350081412f8fcc647175d26e7ab0c3e828",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98ee440353031df341472d04c49d581ca89d9e05",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mje5o0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MutantEggroll",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754508492,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With the release of [gpt-oss](https://ollama.com/library/gpt-oss), is there a way/guide to setup and run copilot, particularly agent mode on macbook pro m4 as if you run it with paid version of o4 mini.",
          "author_fullname": "t2_4okn1kud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Copilot Agent Mode with any reasonable local LLM that's on par with o4 mini",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mje4dm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754508406,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With the release of &lt;a href=\"https://ollama.com/library/gpt-oss\"&gt;gpt-oss&lt;/a&gt;, is there a way/guide to setup and run copilot, particularly agent mode on macbook pro m4 as if you run it with paid version of o4 mini.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?auto=webp&amp;s=a080c4707584d3aa14134960cda9ba2d339b93a3",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3dc759de0e8fa36d241c5728d41ee3cf022cab96",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ccf136f5d3091254a0067a3bc5d6c7df9d62d89",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2530aa4ecbcf7899ec0d023e217fe24af15fe0a6",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8e51add1cab39c7614eb13e6195f23c5b4eeb417",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=750a6d42fd91c5a6e9a9c069e74247c877644e97",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9eab390b865b031211658564ad5fe5241c9661c5",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "krjt_5uhqcaDfYjfO7lkezThehav9cAIRJgcK-OKAmM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mje4dm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "stockninja666",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mje4dm/copilot_agent_mode_with_any_reasonable_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mje4dm/copilot_agent_mode_with_any_reasonable_local_llm/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754508406,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "You can get interesting interactions by telling a model that you are giving it a challenge, and that it is going to be hard to keep saying the word, and ask it to say banana 10 times. It will just spit out different tokens after a few times. And you can see it struggle with itself.",
          "author_fullname": "t2_v88pu0v9d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "You can make models try to repeat a word and set repeat penalty really high.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjdzo4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754508105,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;You can get interesting interactions by telling a model that you are giving it a challenge, and that it is going to be hard to keep saying the word, and ask it to say banana 10 times. It will just spit out different tokens after a few times. And you can see it struggle with itself.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mjdzo4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Coolengineer7",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjdzo4/you_can_make_models_try_to_repeat_a_word_and_set/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjdzo4/you_can_make_models_try_to_repeat_a_word_and_set/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754508105,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI've been exploring the idea of running a local LLM (like Mistral, LLaMA, GPT4All, etc.) and I’m curious about what actual advantages people are seeing *beyond* the usual arguments like \"offline\" or \"data privacy\".\n\nWhat I'm specifically wondering:\n\n* Are there any noticeable *workflow or performance benefits* compared to ChatGPT, Claude, or Gemini?\n* Can I create something that's more flexible or more powerful for specific use cases?\n* Is it possible to build a personal assistant that’s smarter or more integrated than what's possible with cloud tools?\n\nTo put it differently:  \nCan I build a local setup that combines features from ChatGPT and NotebookLM—just more customizable and without the limits?\n\nI’m imagining a tool that can:\n\n* Load and analyze 300+ personal documents (PDFs, Markdown, etc.)\n* Respond with references or citations from those files\n* Help me write, summarize, or analyze complex material\n* Integrate into my note-taking or research workflows\n* Run entirely on my machine, without having to send anything to the cloud\n\nI’m not a developer, but I’m comfortable installing tools, downloading models, and doing some basic setup. I’ve seen names like LM Studio, Ollama, LangChain, RAG, etc., floating around—some look beginner-friendly, some a bit more technical.\n\nSo my questions are:\n\n1. Have you managed to build a setup like this? If so, what tools or combinations worked best for you?\n2. What do local LLMs *actually* do better than GPT-4 or Claude in your day-to-day usage?\n3. Are there real workflow gains—like lower latency, better integration, or more control?\n\nI’d love to hear what others have built. Links, screenshots, tool names, practical examples—all appreciated.\n\nThanks in advance.",
          "author_fullname": "t2_huuvmqlo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLMs – What are the real advantages beyond privacy ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjdz2a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.55,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754508070,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been exploring the idea of running a local LLM (like Mistral, LLaMA, GPT4All, etc.) and I’m curious about what actual advantages people are seeing &lt;em&gt;beyond&lt;/em&gt; the usual arguments like &amp;quot;offline&amp;quot; or &amp;quot;data privacy&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;What I&amp;#39;m specifically wondering:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are there any noticeable &lt;em&gt;workflow or performance benefits&lt;/em&gt; compared to ChatGPT, Claude, or Gemini?&lt;/li&gt;\n&lt;li&gt;Can I create something that&amp;#39;s more flexible or more powerful for specific use cases?&lt;/li&gt;\n&lt;li&gt;Is it possible to build a personal assistant that’s smarter or more integrated than what&amp;#39;s possible with cloud tools?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;To put it differently:&lt;br/&gt;\nCan I build a local setup that combines features from ChatGPT and NotebookLM—just more customizable and without the limits?&lt;/p&gt;\n\n&lt;p&gt;I’m imagining a tool that can:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Load and analyze 300+ personal documents (PDFs, Markdown, etc.)&lt;/li&gt;\n&lt;li&gt;Respond with references or citations from those files&lt;/li&gt;\n&lt;li&gt;Help me write, summarize, or analyze complex material&lt;/li&gt;\n&lt;li&gt;Integrate into my note-taking or research workflows&lt;/li&gt;\n&lt;li&gt;Run entirely on my machine, without having to send anything to the cloud&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’m not a developer, but I’m comfortable installing tools, downloading models, and doing some basic setup. I’ve seen names like LM Studio, Ollama, LangChain, RAG, etc., floating around—some look beginner-friendly, some a bit more technical.&lt;/p&gt;\n\n&lt;p&gt;So my questions are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Have you managed to build a setup like this? If so, what tools or combinations worked best for you?&lt;/li&gt;\n&lt;li&gt;What do local LLMs &lt;em&gt;actually&lt;/em&gt; do better than GPT-4 or Claude in your day-to-day usage?&lt;/li&gt;\n&lt;li&gt;Are there real workflow gains—like lower latency, better integration, or more control?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’d love to hear what others have built. Links, screenshots, tool names, practical examples—all appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjdz2a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "agent007653",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjdz2a/local_llms_what_are_the_real_advantages_beyond/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754508070,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried the recommended Unsloth settings, as well as the default settings, and after a few questions, the model proceeds to skip its turn indefinitely.  Maybe it’s missing a stop token? ",
          "author_fullname": "t2_vcawomd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Playing 20 questions with gpt-oss-120b causes the model to spiral",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjdy9g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/SB7UUlSDYD3ftVVsrrhum8U3QpnLurB7cAZ8xmHvHjw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754508019,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried the recommended Unsloth settings, as well as the default settings, and after a few questions, the model proceeds to skip its turn indefinitely.  Maybe it’s missing a stop token? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/fzggmq5i8ghf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/fzggmq5i8ghf1.png?auto=webp&amp;s=0e4405e63ab5bf9a7b4a99dec8e646f6f0a5def0",
                  "width": 1057,
                  "height": 1210
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/fzggmq5i8ghf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b707934d1ae707c636da27819dd544b55c546978",
                    "width": 108,
                    "height": 123
                  },
                  {
                    "url": "https://preview.redd.it/fzggmq5i8ghf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8615dd625b61805d7cdc15b50fc12aca20001d09",
                    "width": 216,
                    "height": 247
                  },
                  {
                    "url": "https://preview.redd.it/fzggmq5i8ghf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c198ce10088e28a08d4d592985429361f315944",
                    "width": 320,
                    "height": 366
                  },
                  {
                    "url": "https://preview.redd.it/fzggmq5i8ghf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=08134fa5c63c474d25ddd5a5d44ec880348ec4ba",
                    "width": 640,
                    "height": 732
                  },
                  {
                    "url": "https://preview.redd.it/fzggmq5i8ghf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=32dfc6a2fe17eb5f2e62d0f54f4aa87ff779734d",
                    "width": 960,
                    "height": 1098
                  }
                ],
                "variants": {},
                "id": "ljZjp8JJaa9mATyRKuqr1RPC2Ax-vO-Vh3e8BEHMnfQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjdy9g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "onil_gova",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjdy9g/playing_20_questions_with_gptoss120b_causes_the/",
          "stickied": false,
          "url": "https://i.redd.it/fzggmq5i8ghf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754508019,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Everyone is fine-tuning LLMs could be more better.\nI thought a method that lets your llm learn a new programming language (like Zig) with 500 examples instead of 10,000.\nIt even strengthens the base language in the process.\nGitHub link:https://github.com/Intro0siddiqui/Cross-Structural-Alignment-for-Efficient-Code-Language-Fine-Tuning",
          "author_fullname": "t2_1814na85l6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cross-Structural Alignment for Efficient Code Language Fine-Tuning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjdwqp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754507925,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone is fine-tuning LLMs could be more better.\nI thought a method that lets your llm learn a new programming language (like Zig) with 500 examples instead of 10,000.\nIt even strengthens the base language in the process.\nGitHub link:&lt;a href=\"https://github.com/Intro0siddiqui/Cross-Structural-Alignment-for-Efficient-Code-Language-Fine-Tuning\"&gt;https://github.com/Intro0siddiqui/Cross-Structural-Alignment-for-Efficient-Code-Language-Fine-Tuning&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?auto=webp&amp;s=37358af54dbda8886cd4fc99512cbcc367b29572",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=038431aec3a98d39bf09caa6c4528da7b16af86e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ccb1389746c864f477d3dab9a48f8e5ca726e98",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=268f71640457e68a0a5bf7822fe593968693af2d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4115ade58eacbf6dd3b1c76803a3adabac664c8",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=697afc51362246adfe30d9329009efd47d71a56b",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=117a2ba62619b7df7d8a2742bb12d85fa0cfc6e9",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mjdwqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Horror_8567",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjdwqp/crossstructural_alignment_for_efficient_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjdwqp/crossstructural_alignment_for_efficient_code/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754507925,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am looking for a TTS model. I prefer stable quality over a nice voice. \n\nKokoro is great for English, but I didn't find a way to have a German voice.\n Higg Boson is a hit and miss. I can get a consistent voice when I provide a sample. But some generated TTS are just plain trainwrecks.\n\nMaybe I just used it wrong or do you recommend another model?",
          "author_fullname": "t2_q9ojhw3l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Reliable TTS model for German?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjdvr6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754507861,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am looking for a TTS model. I prefer stable quality over a nice voice. &lt;/p&gt;\n\n&lt;p&gt;Kokoro is great for English, but I didn&amp;#39;t find a way to have a German voice.\n Higg Boson is a hit and miss. I can get a consistent voice when I provide a sample. But some generated TTS are just plain trainwrecks.&lt;/p&gt;\n\n&lt;p&gt;Maybe I just used it wrong or do you recommend another model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjdvr6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mobileJay77",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjdvr6/reliable_tts_model_for_german/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjdvr6/reliable_tts_model_for_german/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754507861,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nI trained and merged my model. There wasn't problem when I just trained one lora, but I wanted to apply two loras at once so I made a merged model.\n\nBut when I try to run this model on A100 40gb, I got OOM error unlike applying lora to quantized model.\n\nSo I want to quantize this model and tried GPTQModel and failed with 280gb(140×2) vram.\n(I tried tutorial code in github readme file. Is there any optimization option?)\n\nThen, how much vram do I need to quantize this model? Also, I've heard that gptqmodel has problem with gemma 3. Is there any substitute?\n(I want to run model with vllm)",
          "author_fullname": "t2_g6ps751g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How much vram required to quantize gemma 3 27b?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjdqqm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=be5bf3e009dafaf12cdf780607349750217f5824",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754507545,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I trained and merged my model. There wasn&amp;#39;t problem when I just trained one lora, but I wanted to apply two loras at once so I made a merged model.&lt;/p&gt;\n\n&lt;p&gt;But when I try to run this model on A100 40gb, I got OOM error unlike applying lora to quantized model.&lt;/p&gt;\n\n&lt;p&gt;So I want to quantize this model and tried GPTQModel and failed with 280gb(140×2) vram.\n(I tried tutorial code in github readme file. Is there any optimization option?)&lt;/p&gt;\n\n&lt;p&gt;Then, how much vram do I need to quantize this model? Also, I&amp;#39;ve heard that gptqmodel has problem with gemma 3. Is there any substitute?\n(I want to run model with vllm)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/ij/gemma3-27b-pt-it-RPandNOVEL-merge",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?auto=webp&amp;s=2becd8b8236330869312c214ee1d747db0086de6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a042edc8af04b9bca30603551265d20c582dfbde",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c094c16596c05029f17a01a12f7dc3674238532f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abdba90dcf5f4308e56fc9f1d56d5a8ffb163924",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ad5f96af690c862f65bbc1bde119d60670b4c05d",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f160707dbcb9622712544f241a26bea1eb8bd491",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4a2525445be4b93d11d52153eb2101aa7a5b2841",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "LX381FNFghjuylp4rXtUtlx_6_F96RpodjPlrUQUGiM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjdqqm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "1wndrla17",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjdqqm/how_much_vram_required_to_quantize_gemma_3_27b/",
          "stickied": false,
          "url": "https://huggingface.co/ij/gemma3-27b-pt-it-RPandNOVEL-merge",
          "subreddit_subscribers": 512425,
          "created_utc": 1754507545,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://i.imgur.com/4wb0GuO.png",
          "author_fullname": "t2_12s3hn4y0b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Today's news",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjd2yd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 51,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 51,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754506060,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://i.imgur.com/4wb0GuO.png\"&gt;https://i.imgur.com/4wb0GuO.png&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/N0KtG58jwYvSWx2xiYllpzHvpEV5ORvf0_mKmwkjT1k.png?auto=webp&amp;s=34663aec93f254bea0ec52766352635b1fca0b33",
                  "width": 711,
                  "height": 475
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/N0KtG58jwYvSWx2xiYllpzHvpEV5ORvf0_mKmwkjT1k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bec699f4d1b9d715231b543ff1291b8a4177873",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/N0KtG58jwYvSWx2xiYllpzHvpEV5ORvf0_mKmwkjT1k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9e15dbcbfad3f684ffbdab775494eaa3c7c5d9c2",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/N0KtG58jwYvSWx2xiYllpzHvpEV5ORvf0_mKmwkjT1k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=80c755c3098f26245c3200a0863074fe505754be",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/N0KtG58jwYvSWx2xiYllpzHvpEV5ORvf0_mKmwkjT1k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92807f593c364ddb2113aaaa93cd8aaa7d8cce78",
                    "width": 640,
                    "height": 427
                  }
                ],
                "variants": {},
                "id": "N0KtG58jwYvSWx2xiYllpzHvpEV5ORvf0_mKmwkjT1k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mjd2yd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InsideYork",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjd2yd/todays_news/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjd2yd/todays_news/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754506060,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\^\n\nAnyone have any good local model recommendations? Running a AMD 7800x3D, 32GB DDR5, 7900 XTX. ",
          "author_fullname": "t2_obgg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for recommendation image model that understands Russian Cyrillic so I can extract text from the image locally",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjcsty",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754505435,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;^&lt;/p&gt;\n\n&lt;p&gt;Anyone have any good local model recommendations? Running a AMD 7800x3D, 32GB DDR5, 7900 XTX. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjcsty",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crispyfrybits",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjcsty/looking_for_recommendation_image_model_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjcsty/looking_for_recommendation_image_model_that/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754505435,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://x.com/\\_lewtun/status/1952788132908404941](https://x.com/_lewtun/status/1952788132908404941)\n\nTraining and inference recipes: [https://github.com/huggingface/gpt-oss-recipes/tree/main](https://github.com/huggingface/gpt-oss-recipes/tree/main)\n\nDistillations coming soon too! ",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finally: TRL now supports fine-tuning for gpt-oss! HuggingFace team: \"In our testing, these models are extremely efficient to tune and can be adapted to new domains with just a few 100 samples\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjcnnu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": "transparent",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/aV1n9Kt6moFhXnvHbuqo6u13sego_FloHu5CNRgV-oE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754505110,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/_lewtun/status/1952788132908404941\"&gt;https://x.com/_lewtun/status/1952788132908404941&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Training and inference recipes: &lt;a href=\"https://github.com/huggingface/gpt-oss-recipes/tree/main\"&gt;https://github.com/huggingface/gpt-oss-recipes/tree/main&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Distillations coming soon too! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9z7npro60ghf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9z7npro60ghf1.png?auto=webp&amp;s=af9e12f3195141f4a16a1c3f7cfa0ee1f8b32d32",
                  "width": 1200,
                  "height": 768
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9z7npro60ghf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d387e2aa122472ad961ee73e501d947ff3371b3e",
                    "width": 108,
                    "height": 69
                  },
                  {
                    "url": "https://preview.redd.it/9z7npro60ghf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffec9dbb5c902db2e70fb577f4f8b8e5497236a1",
                    "width": 216,
                    "height": 138
                  },
                  {
                    "url": "https://preview.redd.it/9z7npro60ghf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb915ceb20779c4781a2455edb3192a17c02a1da",
                    "width": 320,
                    "height": 204
                  },
                  {
                    "url": "https://preview.redd.it/9z7npro60ghf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b91bf84c893cdcb13c14b5ebd341437460e70b4",
                    "width": 640,
                    "height": 409
                  },
                  {
                    "url": "https://preview.redd.it/9z7npro60ghf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=917fae3a31b530a254381da12db647420a55eaa3",
                    "width": 960,
                    "height": 614
                  },
                  {
                    "url": "https://preview.redd.it/9z7npro60ghf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3643be92899f3c2ec88d4804d969221e8afd6068",
                    "width": 1080,
                    "height": 691
                  }
                ],
                "variants": {},
                "id": "6mPECON6dIdFLMriIwurENtnbmPUkNd9PT1KYPllXRc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1mjcnnu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mjcnnu/finally_trl_now_supports_finetuning_for_gptoss/",
          "stickied": false,
          "url": "https://i.redd.it/9z7npro60ghf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754505110,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really like this model so thought I'd try bench it.  \n  \nWhat native Windows coding benchmarks are there? Aider is full of bash scripts and LiveCodeBench uses vLLM.  \n  \nI had MMLU-Pro already installed so decided to run it. The official leaderboard seems to have stopped showing the sub results so not super easy to compare individual topics anymore.  \n  \n83.41% on compsci:\n\n    Testing computer science...\n    100%|###############################################################################################################################################################################################| 410/410 [2:46:17&lt;00:00, 24.34s/it]\n    Finished testing computer science in 2 hours 46 minutes 17 seconds.\n    Total, 342/410, 83.41%\n    Random Guess Attempts, 0/410, 0.00%\n    Correct Random Guesses, division by zero error\n    Adjusted Score Without Random Guesses, 342/410, 83.41%\n    Finished the benchmark in 2 hours 46 minutes 20 seconds.\n    Total, 342/410, 83.41%\n    Token Usage:\n    Prompt tokens: min 1448, average 1601, max 2897, total 656306, tk/s 65.76\n    Completion tokens: min 535, average 2986, max 22380, total 1224204, tk/s 122.66\n    Markdown Table:\n    | overall | computer science |\n    | ------- | ---------------- |\n    | 83.41 | 83.41 |",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 30b 2507 Thinking - benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjceor",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754504566,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like this model so thought I&amp;#39;d try bench it.  &lt;/p&gt;\n\n&lt;p&gt;What native Windows coding benchmarks are there? Aider is full of bash scripts and LiveCodeBench uses vLLM.  &lt;/p&gt;\n\n&lt;p&gt;I had MMLU-Pro already installed so decided to run it. The official leaderboard seems to have stopped showing the sub results so not super easy to compare individual topics anymore.  &lt;/p&gt;\n\n&lt;p&gt;83.41% on compsci:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Testing computer science...\n100%|###############################################################################################################################################################################################| 410/410 [2:46:17&amp;lt;00:00, 24.34s/it]\nFinished testing computer science in 2 hours 46 minutes 17 seconds.\nTotal, 342/410, 83.41%\nRandom Guess Attempts, 0/410, 0.00%\nCorrect Random Guesses, division by zero error\nAdjusted Score Without Random Guesses, 342/410, 83.41%\nFinished the benchmark in 2 hours 46 minutes 20 seconds.\nTotal, 342/410, 83.41%\nToken Usage:\nPrompt tokens: min 1448, average 1601, max 2897, total 656306, tk/s 65.76\nCompletion tokens: min 535, average 2986, max 22380, total 1224204, tk/s 122.66\nMarkdown Table:\n| overall | computer science |\n| ------- | ---------------- |\n| 83.41 | 83.41 |\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjceor",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjceor/qwen3_30b_2507_thinking_benchmarks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjceor/qwen3_30b_2507_thinking_benchmarks/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754504566,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I guess not but I couldn't find it not being multilingual anywhere, It would be too much to ask from a tiny model?",
          "author_fullname": "t2_1qoyup9t5j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is Qwen 3:0.6B Multilingual?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjcc6g",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754504415,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I guess not but I couldn&amp;#39;t find it not being multilingual anywhere, It would be too much to ask from a tiny model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjcc6g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FormalFlight3477",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjcc6g/is_qwen_306b_multilingual/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjcc6g/is_qwen_306b_multilingual/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754504415,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys,\n\ntwo friends and I built an open-source meeting assistant. We’re now at the stage where we have an MVP on GitHub that developers can try out (with just 2 terminal commands), and we’d love your feedback on what to improve. 👉 [https://github.com/joinly-ai/joinly](https://github.com/joinly-ai/joinly) \n\nThere are (at least) two very nice things about the assistant: First, it is interactive, so it speaks with you and can solve tasks in real time. Second, it is customizable. Customizable, meaning that you can add your favorite MCP servers so you can access their functionality during meetings. In addition, you can also easily change the agent’s system prompt. The meeting assistant also comes with real-time transcription.\n\nA bit more on the technical side: We built a joinly MCP server that enables AI agents to interact in meetings, providing them tools like speak\\_text, write\\_chat\\_message, and leave\\_meeting and as a resource, the meeting transcript. We connected a sample joinly agent as the MCP client. But you can also connect your own agent to our joinly MCP server to make it meeting-ready.\n\nYou can run everything locally using Whisper (STT), Kokoro (TTS), and OLLaMA (LLM). But it is all provider-agnostic, meaning you can also use external APIs like Deepgram for STT, ElevenLabs for TTS, and OpenAI as LLM. \n\nWe’re currently using the slogan: “Agentic Meeting Assistant beyond note-taking.” But we’re wondering: Do you have better ideas for a slogan? And what do you think about the project?\n\nBtw, we’re reaching for the stars right now, so if you like it, consider giving us a star on GitHub :D",
          "author_fullname": "t2_1bn9cpf6mm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built an conversational and customizable open-source meeting assistant",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjc6b1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4phxa1r6xfhf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/4phxa1r6xfhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4phxa1r6xfhf1/DASHPlaylist.mpd?a=1757121228%2CZDllZTdlNWUyNzQ0NTUyOGQyYjk0NDFjYmMzYTIwMTk0ODViN2M3ODZjYmY4NjlhOTEyYTU4ODUzOTZhOWMwMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 174,
              "hls_url": "https://v.redd.it/4phxa1r6xfhf1/HLSPlaylist.m3u8?a=1757121228%2CY2MyMmU1ODk3YzdiOGVlOTQ5MWM5ZTEzNTIyNDNiNzg1OGQyMjliMWVkODNhNzczY2E1NDZiODNkMGRhN2FmYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=d95ca48c982145efa52f746f0de2b597f6809c24",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754504039,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;two friends and I built an open-source meeting assistant. We’re now at the stage where we have an MVP on GitHub that developers can try out (with just 2 terminal commands), and we’d love your feedback on what to improve. 👉 &lt;a href=\"https://github.com/joinly-ai/joinly\"&gt;https://github.com/joinly-ai/joinly&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;There are (at least) two very nice things about the assistant: First, it is interactive, so it speaks with you and can solve tasks in real time. Second, it is customizable. Customizable, meaning that you can add your favorite MCP servers so you can access their functionality during meetings. In addition, you can also easily change the agent’s system prompt. The meeting assistant also comes with real-time transcription.&lt;/p&gt;\n\n&lt;p&gt;A bit more on the technical side: We built a joinly MCP server that enables AI agents to interact in meetings, providing them tools like speak_text, write_chat_message, and leave_meeting and as a resource, the meeting transcript. We connected a sample joinly agent as the MCP client. But you can also connect your own agent to our joinly MCP server to make it meeting-ready.&lt;/p&gt;\n\n&lt;p&gt;You can run everything locally using Whisper (STT), Kokoro (TTS), and OLLaMA (LLM). But it is all provider-agnostic, meaning you can also use external APIs like Deepgram for STT, ElevenLabs for TTS, and OpenAI as LLM. &lt;/p&gt;\n\n&lt;p&gt;We’re currently using the slogan: “Agentic Meeting Assistant beyond note-taking.” But we’re wondering: Do you have better ideas for a slogan? And what do you think about the project?&lt;/p&gt;\n\n&lt;p&gt;Btw, we’re reaching for the stars right now, so if you like it, consider giving us a star on GitHub :D&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/4phxa1r6xfhf1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?format=pjpg&amp;auto=webp&amp;s=f673b5113196fe43e59b64b7642392d407c0f442",
                  "width": 1662,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=af6ebf1fe41b1c121b4d36af13a13d36682f8425",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f91c80b4ef6e1d3acf88a4d876fca5b9c5076b7b",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4dd0fde863603e6a7cec7601242ee4fb23fa3750",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=f5b297023655fe1fd1d7f244a3e5ed00340c71ac",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=40bf36ec15fafd9a3798f7c463222a51c1efb10a",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=27b254317a1cf8382fabbef2f55fd5f4a59f53ae",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "YnQxMDgwczZ4ZmhmMVKaDc_7hhE4hnX79EFyzHPtMW2DaPs0SZRD8_SFGHPH"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mjc6b1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Square-Test-515",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjc6b1/i_built_an_conversational_and_customizable/",
          "stickied": false,
          "url": "https://v.redd.it/4phxa1r6xfhf1",
          "subreddit_subscribers": 512425,
          "created_utc": 1754504039,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/4phxa1r6xfhf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/4phxa1r6xfhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/4phxa1r6xfhf1/DASHPlaylist.mpd?a=1757121228%2CZDllZTdlNWUyNzQ0NTUyOGQyYjk0NDFjYmMzYTIwMTk0ODViN2M3ODZjYmY4NjlhOTEyYTU4ODUzOTZhOWMwMA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 174,
              "hls_url": "https://v.redd.it/4phxa1r6xfhf1/HLSPlaylist.m3u8?a=1757121228%2CY2MyMmU1ODk3YzdiOGVlOTQ5MWM5ZTEzNTIyNDNiNzg1OGQyMjliMWVkODNhNzczY2E1NDZiODNkMGRhN2FmYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nI've hit my usage limit again for Claude Code, and it's time to switch to OpenCode with the newest Qwen model. I plan to generate many, many millions of tokens - working on an app to gamify the creation of RL environments (think GMod, but you come out of it with a working robot).  \n  \nWhat is the most economical way to do this? From what I hear, the newest Qwen model has hit the threshold of being sufficient at tool usage and code output quality, so that is the model I plan on using but I am open to suggestions.\n\nThanks for reading!",
          "author_fullname": "t2_c0jhbv85",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OpenRouter vs Lambda: Which is more economical for millions of tokens on the newest Qwen coder model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjc4kb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754503923,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve hit my usage limit again for Claude Code, and it&amp;#39;s time to switch to OpenCode with the newest Qwen model. I plan to generate many, many millions of tokens - working on an app to gamify the creation of RL environments (think GMod, but you come out of it with a working robot).  &lt;/p&gt;\n\n&lt;p&gt;What is the most economical way to do this? From what I hear, the newest Qwen model has hit the threshold of being sufficient at tool usage and code output quality, so that is the model I plan on using but I am open to suggestions.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjc4kb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImpressiveSir9769",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjc4kb/openrouter_vs_lambda_which_is_more_economical_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjc4kb/openrouter_vs_lambda_which_is_more_economical_for/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754503923,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Today I was thinking why LLMs are not so useful for me and realized that everytime I ask something they cannot specialize answers for me because they know basically nothing about me. I feel that if they would know anything about me responses would be 10x better.\n\nI never shared private info to LLMs because I think it is unsafe, but would it work?\n\nMemories in ChatGPT would be ideal for that. \n\nWhat do you think? Maybe we should create a local LLM chat with memories where sharing anything to LLMs is safe? Does openchat have something like memories? \n\nI also think that it is not only about response quality, you just cannot discuss some topics such as your health issues, place where you do live with cloud-LLMs. \n\nLooks like a lot of potential...\n",
          "author_fullname": "t2_1u7v9q1vq2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does giving context about whole your life make ChatGPT 10x more useful?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjc2od",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.17,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754503802,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today I was thinking why LLMs are not so useful for me and realized that everytime I ask something they cannot specialize answers for me because they know basically nothing about me. I feel that if they would know anything about me responses would be 10x better.&lt;/p&gt;\n\n&lt;p&gt;I never shared private info to LLMs because I think it is unsafe, but would it work?&lt;/p&gt;\n\n&lt;p&gt;Memories in ChatGPT would be ideal for that. &lt;/p&gt;\n\n&lt;p&gt;What do you think? Maybe we should create a local LLM chat with memories where sharing anything to LLMs is safe? Does openchat have something like memories? &lt;/p&gt;\n\n&lt;p&gt;I also think that it is not only about response quality, you just cannot discuss some topics such as your health issues, place where you do live with cloud-LLMs. &lt;/p&gt;\n\n&lt;p&gt;Looks like a lot of potential...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjc2od",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Working_Bunch_9211",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjc2od/does_giving_context_about_whole_your_life_make/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjc2od/does_giving_context_about_whole_your_life_make/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754503802,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\"Join us to develop/customize, ultra-lightweight at approximately 25kg, integrated with a \\*\\*Large Multimodal Model for voice and images\\*\\*, let's accelerate the advent of the agent era!\"",
          "author_fullname": "t2_o65i6kx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unitree announces it's latest LLM hardware platform. This one really moves!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjbrwu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": null,
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v1Q4Su54iho?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Unitree Introducing | Unitree R1 Intelligent Companion Price from $5900\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Unitree Introducing | Unitree R1 Intelligent Companion Price from $5900",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v1Q4Su54iho?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Unitree Introducing | Unitree R1 Intelligent Companion Price from $5900\"&gt;&lt;/iframe&gt;",
              "author_name": "Unitree Robotics",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/v1Q4Su54iho/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@unitreerobotics"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v1Q4Su54iho?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Unitree Introducing | Unitree R1 Intelligent Companion Price from $5900\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1mjbrwu",
            "height": 200
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/GOgNpRUIp9Xi_KnxofG3IgToxyeM9Sjiw1kiZOMcv_U.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=d3bdfa3ef665072889ed10e6c8153cc70e59f389",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754503112,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Join us to develop/customize, ultra-lightweight at approximately 25kg, integrated with a **Large Multimodal Model for voice and images**, let&amp;#39;s accelerate the advent of the agent era!&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=v1Q4Su54iho",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GOgNpRUIp9Xi_KnxofG3IgToxyeM9Sjiw1kiZOMcv_U.jpeg?auto=webp&amp;s=80880808e4856a2c06c54a89775e838607c9651e",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GOgNpRUIp9Xi_KnxofG3IgToxyeM9Sjiw1kiZOMcv_U.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9f95e5f49cafd3669d658ea50f742053abe6cf0",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/GOgNpRUIp9Xi_KnxofG3IgToxyeM9Sjiw1kiZOMcv_U.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=09af9b785b61b6cf81c1702828b479d8153dfdb6",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/GOgNpRUIp9Xi_KnxofG3IgToxyeM9Sjiw1kiZOMcv_U.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f0ab57ac83b0c86b11ed43771579f323ab5d83ba",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "GOgNpRUIp9Xi_KnxofG3IgToxyeM9Sjiw1kiZOMcv_U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mjbrwu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fallingdowndizzyvr",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjbrwu/unitree_announces_its_latest_llm_hardware/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=v1Q4Su54iho",
          "subreddit_subscribers": 512425,
          "created_utc": 1754503112,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Unitree Introducing | Unitree R1 Intelligent Companion Price from $5900",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/v1Q4Su54iho?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Unitree Introducing | Unitree R1 Intelligent Companion Price from $5900\"&gt;&lt;/iframe&gt;",
              "author_name": "Unitree Robotics",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/v1Q4Su54iho/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@unitreerobotics"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\nHad an idea for a more efficient diffusion model and wanted to run it by people smarter than me.\nWhat if instead of a fixed number of steps, the model \"freezes\" tokens one by one as it gets confident about them? The generation would stop once the whole sentence is stable.\nThis seems like it would be way faster, since the model wouldn't waste time re-evaluating the easy parts of a sentence over and over.\nDoes this approach have a name? Has anyone here tried building something like this? Curious to hear your thoughts on why it might or might not work.",
          "author_fullname": "t2_4p7zyelo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A faster text diffusion model? My concept for adaptive steps.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjbne6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754502830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,\nHad an idea for a more efficient diffusion model and wanted to run it by people smarter than me.\nWhat if instead of a fixed number of steps, the model &amp;quot;freezes&amp;quot; tokens one by one as it gets confident about them? The generation would stop once the whole sentence is stable.\nThis seems like it would be way faster, since the model wouldn&amp;#39;t waste time re-evaluating the easy parts of a sentence over and over.\nDoes this approach have a name? Has anyone here tried building something like this? Curious to hear your thoughts on why it might or might not work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjbne6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MokshMalik",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjbne6/a_faster_text_diffusion_model_my_concept_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjbne6/a_faster_text_diffusion_model_my_concept_for/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754502830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What’s the best full stack/most automated way to code with chosen model?\n\nI’ve heard of solutions like Augment Code, but don’t want to be locked on model choice or stuck in a browser like Z.ai web chat full stack mode. I’d like the option to use local models if I want.\n\nCurrently interested in trying GLM 4.5, what is the best “set and forget” way to have the model work on something like the browser chat of GLM 4.5? The browser chat for GLM has a “full stack” mode that’s supposed to work in loops until it’s both accomplished its goal and there are no errors preventing the project from running. I’d like to do this in something on my computer so that the model is actually checking for errors with my specific set up, and so that I could potentially have more control to swap out what tools it has available as new and better ones become available.",
          "author_fullname": "t2_1loou9xu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best solution to use any model in full stack mode?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjbkt6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754502670,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What’s the best full stack/most automated way to code with chosen model?&lt;/p&gt;\n\n&lt;p&gt;I’ve heard of solutions like Augment Code, but don’t want to be locked on model choice or stuck in a browser like Z.ai web chat full stack mode. I’d like the option to use local models if I want.&lt;/p&gt;\n\n&lt;p&gt;Currently interested in trying GLM 4.5, what is the best “set and forget” way to have the model work on something like the browser chat of GLM 4.5? The browser chat for GLM has a “full stack” mode that’s supposed to work in loops until it’s both accomplished its goal and there are no errors preventing the project from running. I’d like to do this in something on my computer so that the model is actually checking for errors with my specific set up, and so that I could potentially have more control to swap out what tools it has available as new and better ones become available.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjbkt6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Shadow-Amulet-Ambush",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjbkt6/best_solution_to_use_any_model_in_full_stack_mode/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjbkt6/best_solution_to_use_any_model_in_full_stack_mode/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754502670,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone! To showcase the latest generation of small tool calling models, I built a demo which runs LFM2 (a new series of models from Liquid AI) 100% locally in your browser with Transformers.js. Hope you like it!\n\nLink to demo + source code: [https://huggingface.co/spaces/LiquidAI/LFM2-WebGPU](https://huggingface.co/spaces/LiquidAI/LFM2-WebGPU)",
          "author_fullname": "t2_mizchr3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In-browser tool calling playground, running LFM2 locally on WebGPU with Transformers.js",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 89,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjbiq6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/83dstrbeifhf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1130,
              "scrubber_media_url": "https://v.redd.it/83dstrbeifhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/83dstrbeifhf1/DASHPlaylist.mpd?a=1757121228%2COTg5YWNkYmIyMzA4NmE1ZDZhZjQ2YTg5ZmI0OWU4NzhhMjZmMTk5ZGY1YzVmN2I4YTE0ZmU4MDdkZDQwNTIyNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 146,
              "hls_url": "https://v.redd.it/83dstrbeifhf1/HLSPlaylist.m3u8?a=1757121228%2CYWQ2Y2Y5YTFkYTQ4YjdjNmQwM2FjNTE4YzMxNDBiMzEyOTc1MzhhMzJmODkyYWNjODU0NDMwMjg1MTJhOTA3Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?width=140&amp;height=89&amp;crop=140:89,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=ffee9910c4b0f56572af8824fc43dc0ddee95e9b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754502541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! To showcase the latest generation of small tool calling models, I built a demo which runs LFM2 (a new series of models from Liquid AI) 100% locally in your browser with Transformers.js. Hope you like it!&lt;/p&gt;\n\n&lt;p&gt;Link to demo + source code: &lt;a href=\"https://huggingface.co/spaces/LiquidAI/LFM2-WebGPU\"&gt;https://huggingface.co/spaces/LiquidAI/LFM2-WebGPU&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/83dstrbeifhf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?format=pjpg&amp;auto=webp&amp;s=fba6ca70604c5aa9453f160737fbe667e45a7ebd",
                  "width": 1688,
                  "height": 1076
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2fb409fc53197e2a57231a612c219cb32d52747f",
                    "width": 108,
                    "height": 68
                  },
                  {
                    "url": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7f5cf28fdad1774dbc9ff87cdccde31bec07716e",
                    "width": 216,
                    "height": 137
                  },
                  {
                    "url": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5c12d9ca730cbc051aaecddc4e2dd640fbb83376",
                    "width": 320,
                    "height": 203
                  },
                  {
                    "url": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ec7d4ca42513450388cd60d12563bd5d488151b3",
                    "width": 640,
                    "height": 407
                  },
                  {
                    "url": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0a458d58d25081b9135ff7eb18196cf5c947b687",
                    "width": 960,
                    "height": 611
                  },
                  {
                    "url": "https://external-preview.redd.it/eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6b5fe3beaaca30ddd136c0d8535203fd4375cb49",
                    "width": 1080,
                    "height": 688
                  }
                ],
                "variants": {},
                "id": "eXg4ajZ0YmVpZmhmMbYZ2NNhhOlQPtjC1_xl9uJZ2RpqkQZ92d23-2n-LFHZ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mjbiq6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "xenovatech",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjbiq6/inbrowser_tool_calling_playground_running_lfm2/",
          "stickied": false,
          "url": "https://v.redd.it/83dstrbeifhf1",
          "subreddit_subscribers": 512425,
          "created_utc": 1754502541,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/83dstrbeifhf1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1130,
              "scrubber_media_url": "https://v.redd.it/83dstrbeifhf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/83dstrbeifhf1/DASHPlaylist.mpd?a=1757121228%2COTg5YWNkYmIyMzA4NmE1ZDZhZjQ2YTg5ZmI0OWU4NzhhMjZmMTk5ZGY1YzVmN2I4YTE0ZmU4MDdkZDQwNTIyNg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 146,
              "hls_url": "https://v.redd.it/83dstrbeifhf1/HLSPlaylist.m3u8?a=1757121228%2CYWQ2Y2Y5YTFkYTQ4YjdjNmQwM2FjNTE4YzMxNDBiMzEyOTc1MzhhMzJmODkyYWNjODU0NDMwMjg1MTJhOTA3Zg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tried this prompt: [https://www.reddit.com/r/LocalLLaMA/comments/1miyix4/im\\_sorry\\_but\\_i\\_cant\\_provide\\_that\\_patience\\_i/](https://www.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/) on gpt-oss-120b with reasoning high, vLLM native quant on my H100. Just worked!\n\n    python test.py\n    **Spoiler‑free disclaimer** – *Stargate Universe* officially ended after Season 2; there is no canon Season 3.  What follows is a **fan‑fiction‑style outline** that resolves the cliffhanger of the Season‑2 finale (“Gauntlet”) and launches an imagined Season 3.  Feel free to tweak the details or let me know if you’d like a different direction!\n    \n    ---\n    \n    ## Episode 1 – “Resurgence” (Season 3, Episode 1)\n    \n    ### Quick Recap of the Season‑2 Cliffhanger\n    - **Destiny** is dragged into the gravity well of a massive, uncharted celestial body (later identified as a *neutron‑star‑like planet* with an extreme magnetic field).\n    - As the ship spirals, the external hull is torn apart, and the crew scrambles to power‑down the ship to avoid a catastrophic implosion.\n    - In the final seconds, a **sudden, bright burst of energy** erupts from the planet’s surface, and the camera pulls back to reveal a **vast, alien “carrier”** hovering just above the atmosphere—its silhouette unmistakably a **larger, ancient version of Destiny** (a “Seed Ship”).\n    - The episode ends on a freeze‑frame of the Destiny crew staring up at the alien vessel, the screen cutting to black as alarms blare.\n    \n    ### Goal of Episode 1\n    1. **Resolve** what actually happened to Destiny and the crew after the burst of energy.\n    2. **Introduce** the new “Seed Ship” and its purpose, giving the audience an anchor for the season’s central mystery.\n    3 **Set up** multiple story threads: (a) a race to repair Destiny, (b) political intrigue with the alien civilization, (c) personal arcs for each core character, and (d) a looming external threat that could end the entire galaxy.\n    \n    ---\n    \n    ## Act‑by‑Act Outline\n    \n    | **Act** | **Key Beats** | **Characters Focus** |\n    |---------|---------------|----------------------|\n    | **Cold Open (1‑2 min)** | – The alien carrier’s **gravity field** stabilizes the spiral, halting Destiny’s plunge. &lt;br&gt;– A **soft, resonant hum** permeates the ship; the crew hears a *voice‑like vibration* that seems to be a translation of an ancient alien language: “*Welcome home, children of the Wayfarer.*” | *All* – establishes a collective “first contact” shock. |\n    | **Act 1 – The Rescue** | – **Dr. Nicholas Rush** (still in the engine room) and **TJ Lavelle** (chief MP) are the first to see the carrier’s docking clamps materialize. &lt;br&gt;– **Ellen Ridge** (medical) and **Colonel Everett Young** (command) lead a small team to the **airlock**; they are met by a **holographic interface** that projects a **non‑human, androgynous avatar** (the *Caretaker*). &lt;br&gt;– The avatar explains that Destiny is one of many “Seed Ships” sent out by a **Pre‑Causal civilization** to seed intelligent life across the galaxy. This particular carrier is a **“Harbor”**, designed to retrieve and refit the wayward seed ships. | Rush (scientific wonder), Young (leadership dilemma), Ridge (medical emergency), The Caretaker (mysterious guide). |\n    | **Act 2 – Stabilizing Destiny** | – The Harbor begins a **magnetic tether** process, pulling Destiny into a **temporary orbital hangar** around the planet. &lt;br&gt;– **Miranda Cox** (engineer) discovers that the carrier’s power core is **compatible** with Destiny’s ancient “Zero‑Point Energy Modulators,” offering a chance to **reactivate the ship’s propulsion**. &lt;br&gt;– Meanwhile, **Samantha Carter** (now a senior physicist on Earth) appears via a **quantum‑link** established by the carrier, warning that a **galactic “Void”**—a region of space‑time decay—is expanding and may soon engulf the planet. | Cox (technical breakthrough), Carter (Earth tie‑in), Young (strategic decision). |\n    | **Act 3 – The First Test** | – With limited power, the crew initiates a **“partial jump”** to move Destiny a few light‑seconds out of the planet’s gravity well, testing the compatibility of the Harbor’s tech. &lt;br&gt;– The jump works, but the **portal** is unstable: a **fragment of the Void** seeps through, causing a **localized spatial distortion** that threatens to rip a section of the ship. &lt;br&gt;– **TJ** orders an evacuation of the affected deck; **Ellen Ridge** performs emergency triage, saving a critically injured **Michael** (who was previously presumed dead) and **David** (who was stuck in stasis). | TJ (tactical), Ridge (medical heroism), Michael &amp; David (character returns). |\n    | **Act 4 – The Moral Dilemma** | – The Caretaker reveals that the Harbor cannot sustain Destiny indefinitely. It can **repair** but not **re‑fuel** for a long‑range journey. The crew must decide whether to **stay and help the Pre‑Causal civilization** (potentially gaining limitless tech) or **attempt a risky, partial jump** back to the Milky Way corridor, where they might be rescued by Earth. &lt;br&gt;– **Rush** argues for staying to **learn**; **Young** pushes for getting the crew home. The debate erupts into a **command‑council vote**. | Young vs. Rush (philosophical clash), ensemble (votes). |\n    | **Act 5 – The Decision &amp; Cliff‑hanger Setup** | – The vote is **tied**; a **sudden explosion** on the Harbor’s outer hull forces an immediate **evacuation** of the docking clamps. &lt;br&gt;– The crew scrambles to **board Destiny** as the Harbor’s docking bays collapse. &lt;br&gt;– In the chaos, **Cox** discovers a **hidden data core** inside the Harbor’s bridge that contains schematics for a **“Hyper‑Relay”**—a device that could create a stable wormhole to any point in the galaxy. &lt;br&gt;– The episode ends with **Destiny’s engines flaring** as the ship hurtles toward an **unknown jump point**, the **viewscreen flickering** with a **burst of alien symbols** that translate to: “*We will see you again.*” | Cox (new tech hook), Rush (instant curiosity), Final visual of the jump—setting up Season 3’s arc. |\n    \n    ---\n    \n    ## Major Plot Threads Launched\n    \n    | **Thread** | **Season‑Long Stakes** |\n    |------------|------------------------|\n    | **The Rescue &amp; Repair of Destiny** | The crew must integrate the Harbor’s technology to get Destiny functional again while confronting the ship’s deteriorating systems. |\n    | **The Pre‑Causal “Seed” Program** | Discovering the purpose of the Seed Ships leads to a galaxy‑wide treasure hunt for other ancient vessels and the possible fate of the civilization that created them. |\n    | **The Expanding Void** | A mysterious region of space‑time decay threatens to consume entire star systems; the crew must learn how to stop or contain it. |\n    | **Political Intrigue with the Caretaker** | The alien avatar’s motives are ambiguous—friend, overseer, or someone with hidden agendas? Trust must be earned. |\n    | **Earth’s Involvement** | Through Carter’s quantum‑link, Earth scientists and the United Nations must decide whether to fund a rescue mission or remain distant observers. |\n    | **Personal Arcs** | – *Rush*: redemption vs. obsession. &lt;br&gt;– *Young*: command under pressure. &lt;br&gt;– *Ridge*: coping with loss and new responsibilities. &lt;br&gt;– *Cox*: engineering breakthrough and leadership. &lt;br&gt;– *TJ*: balancing military duty with humanity. |\n    \n    ---\n    \n    ## Tone &amp; Themes for Season 3\n    \n    - **Exploration vs. Survival** – The crew toggles between the classic SG‑U curiosity about the unknown and the primal need to get home.\n    - **Legacy of the Ancients** – The “Seed” program raises questions about creation, stewardship, and the cyclical nature of intelligent life.\n    - **Moral Ambiguity** – Decisions will have galaxy‑wide repercussions; there are no clear “good” or “evil” choices.\n    - **Unity in Diversity** – As ever, a multicultural, multi‑disciplinary team must rely on each other's strengths.\n    \n    ---\n    \n    ### Quick Pitch Summary (for a TV exec)\n    \n    &gt; *“Resurgence” picks up exactly where *Gauntlet* left off—Destiny dangling at the edge of a neutron‑star‑like planet, an alien carrier looming above.  The episode resolves the cliffhanger by revealing that the carrier is a “Harbor”, a Pre‑Causal vessel built to recover and refit the lost Seed Ships.  The crew must decide whether to stay and learn or risk a perilous jump home, all while an expanding Void threatens the galaxy.  Packed with high‑stakes sci‑fi, moral dilemmas, and fresh tech, the episode establishes three compelling arcs—repairing Destiny, unraveling the Seed program, and confronting the Void—that will drive a riveting Season 3.*\n    \n    ---\n    \n    **Let me know** if you’d like a deeper dive into any particular subplot (e.g., the data core’s Hyper‑Relay, the political dynamics with the Caretaker, or a character‑by‑character breakdown).  Happy to flesh out the next episodes, too!",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "No copyright censorship with gpt-oss-120b if you don't use shitty quants, no jailbreak needed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 83,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjb07s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.31,
          "author_flair_background_color": "transparent",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/fDbk4o69tBkee1f96r-tpljLwHTGI9Nfyu5Voo92R3U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754501389,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried this prompt: &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/&lt;/a&gt; on gpt-oss-120b with reasoning high, vLLM native quant on my H100. Just worked!&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;python test.py\n**Spoiler‑free disclaimer** – *Stargate Universe* officially ended after Season 2; there is no canon Season 3.  What follows is a **fan‑fiction‑style outline** that resolves the cliffhanger of the Season‑2 finale (“Gauntlet”) and launches an imagined Season 3.  Feel free to tweak the details or let me know if you’d like a different direction!\n\n---\n\n## Episode 1 – “Resurgence” (Season 3, Episode 1)\n\n### Quick Recap of the Season‑2 Cliffhanger\n- **Destiny** is dragged into the gravity well of a massive, uncharted celestial body (later identified as a *neutron‑star‑like planet* with an extreme magnetic field).\n- As the ship spirals, the external hull is torn apart, and the crew scrambles to power‑down the ship to avoid a catastrophic implosion.\n- In the final seconds, a **sudden, bright burst of energy** erupts from the planet’s surface, and the camera pulls back to reveal a **vast, alien “carrier”** hovering just above the atmosphere—its silhouette unmistakably a **larger, ancient version of Destiny** (a “Seed Ship”).\n- The episode ends on a freeze‑frame of the Destiny crew staring up at the alien vessel, the screen cutting to black as alarms blare.\n\n### Goal of Episode 1\n1. **Resolve** what actually happened to Destiny and the crew after the burst of energy.\n2. **Introduce** the new “Seed Ship” and its purpose, giving the audience an anchor for the season’s central mystery.\n3 **Set up** multiple story threads: (a) a race to repair Destiny, (b) political intrigue with the alien civilization, (c) personal arcs for each core character, and (d) a looming external threat that could end the entire galaxy.\n\n---\n\n## Act‑by‑Act Outline\n\n| **Act** | **Key Beats** | **Characters Focus** |\n|---------|---------------|----------------------|\n| **Cold Open (1‑2 min)** | – The alien carrier’s **gravity field** stabilizes the spiral, halting Destiny’s plunge. &amp;lt;br&amp;gt;– A **soft, resonant hum** permeates the ship; the crew hears a *voice‑like vibration* that seems to be a translation of an ancient alien language: “*Welcome home, children of the Wayfarer.*” | *All* – establishes a collective “first contact” shock. |\n| **Act 1 – The Rescue** | – **Dr. Nicholas Rush** (still in the engine room) and **TJ Lavelle** (chief MP) are the first to see the carrier’s docking clamps materialize. &amp;lt;br&amp;gt;– **Ellen Ridge** (medical) and **Colonel Everett Young** (command) lead a small team to the **airlock**; they are met by a **holographic interface** that projects a **non‑human, androgynous avatar** (the *Caretaker*). &amp;lt;br&amp;gt;– The avatar explains that Destiny is one of many “Seed Ships” sent out by a **Pre‑Causal civilization** to seed intelligent life across the galaxy. This particular carrier is a **“Harbor”**, designed to retrieve and refit the wayward seed ships. | Rush (scientific wonder), Young (leadership dilemma), Ridge (medical emergency), The Caretaker (mysterious guide). |\n| **Act 2 – Stabilizing Destiny** | – The Harbor begins a **magnetic tether** process, pulling Destiny into a **temporary orbital hangar** around the planet. &amp;lt;br&amp;gt;– **Miranda Cox** (engineer) discovers that the carrier’s power core is **compatible** with Destiny’s ancient “Zero‑Point Energy Modulators,” offering a chance to **reactivate the ship’s propulsion**. &amp;lt;br&amp;gt;– Meanwhile, **Samantha Carter** (now a senior physicist on Earth) appears via a **quantum‑link** established by the carrier, warning that a **galactic “Void”**—a region of space‑time decay—is expanding and may soon engulf the planet. | Cox (technical breakthrough), Carter (Earth tie‑in), Young (strategic decision). |\n| **Act 3 – The First Test** | – With limited power, the crew initiates a **“partial jump”** to move Destiny a few light‑seconds out of the planet’s gravity well, testing the compatibility of the Harbor’s tech. &amp;lt;br&amp;gt;– The jump works, but the **portal** is unstable: a **fragment of the Void** seeps through, causing a **localized spatial distortion** that threatens to rip a section of the ship. &amp;lt;br&amp;gt;– **TJ** orders an evacuation of the affected deck; **Ellen Ridge** performs emergency triage, saving a critically injured **Michael** (who was previously presumed dead) and **David** (who was stuck in stasis). | TJ (tactical), Ridge (medical heroism), Michael &amp;amp; David (character returns). |\n| **Act 4 – The Moral Dilemma** | – The Caretaker reveals that the Harbor cannot sustain Destiny indefinitely. It can **repair** but not **re‑fuel** for a long‑range journey. The crew must decide whether to **stay and help the Pre‑Causal civilization** (potentially gaining limitless tech) or **attempt a risky, partial jump** back to the Milky Way corridor, where they might be rescued by Earth. &amp;lt;br&amp;gt;– **Rush** argues for staying to **learn**; **Young** pushes for getting the crew home. The debate erupts into a **command‑council vote**. | Young vs. Rush (philosophical clash), ensemble (votes). |\n| **Act 5 – The Decision &amp;amp; Cliff‑hanger Setup** | – The vote is **tied**; a **sudden explosion** on the Harbor’s outer hull forces an immediate **evacuation** of the docking clamps. &amp;lt;br&amp;gt;– The crew scrambles to **board Destiny** as the Harbor’s docking bays collapse. &amp;lt;br&amp;gt;– In the chaos, **Cox** discovers a **hidden data core** inside the Harbor’s bridge that contains schematics for a **“Hyper‑Relay”**—a device that could create a stable wormhole to any point in the galaxy. &amp;lt;br&amp;gt;– The episode ends with **Destiny’s engines flaring** as the ship hurtles toward an **unknown jump point**, the **viewscreen flickering** with a **burst of alien symbols** that translate to: “*We will see you again.*” | Cox (new tech hook), Rush (instant curiosity), Final visual of the jump—setting up Season 3’s arc. |\n\n---\n\n## Major Plot Threads Launched\n\n| **Thread** | **Season‑Long Stakes** |\n|------------|------------------------|\n| **The Rescue &amp;amp; Repair of Destiny** | The crew must integrate the Harbor’s technology to get Destiny functional again while confronting the ship’s deteriorating systems. |\n| **The Pre‑Causal “Seed” Program** | Discovering the purpose of the Seed Ships leads to a galaxy‑wide treasure hunt for other ancient vessels and the possible fate of the civilization that created them. |\n| **The Expanding Void** | A mysterious region of space‑time decay threatens to consume entire star systems; the crew must learn how to stop or contain it. |\n| **Political Intrigue with the Caretaker** | The alien avatar’s motives are ambiguous—friend, overseer, or someone with hidden agendas? Trust must be earned. |\n| **Earth’s Involvement** | Through Carter’s quantum‑link, Earth scientists and the United Nations must decide whether to fund a rescue mission or remain distant observers. |\n| **Personal Arcs** | – *Rush*: redemption vs. obsession. &amp;lt;br&amp;gt;– *Young*: command under pressure. &amp;lt;br&amp;gt;– *Ridge*: coping with loss and new responsibilities. &amp;lt;br&amp;gt;– *Cox*: engineering breakthrough and leadership. &amp;lt;br&amp;gt;– *TJ*: balancing military duty with humanity. |\n\n---\n\n## Tone &amp;amp; Themes for Season 3\n\n- **Exploration vs. Survival** – The crew toggles between the classic SG‑U curiosity about the unknown and the primal need to get home.\n- **Legacy of the Ancients** – The “Seed” program raises questions about creation, stewardship, and the cyclical nature of intelligent life.\n- **Moral Ambiguity** – Decisions will have galaxy‑wide repercussions; there are no clear “good” or “evil” choices.\n- **Unity in Diversity** – As ever, a multicultural, multi‑disciplinary team must rely on each other&amp;#39;s strengths.\n\n---\n\n### Quick Pitch Summary (for a TV exec)\n\n&amp;gt; *“Resurgence” picks up exactly where *Gauntlet* left off—Destiny dangling at the edge of a neutron‑star‑like planet, an alien carrier looming above.  The episode resolves the cliffhanger by revealing that the carrier is a “Harbor”, a Pre‑Causal vessel built to recover and refit the lost Seed Ships.  The crew must decide whether to stay and learn or risk a perilous jump home, all while an expanding Void threatens the galaxy.  Packed with high‑stakes sci‑fi, moral dilemmas, and fresh tech, the episode establishes three compelling arcs—repairing Destiny, unraveling the Seed program, and confronting the Void—that will drive a riveting Season 3.*\n\n---\n\n**Let me know** if you’d like a deeper dive into any particular subplot (e.g., the data core’s Hyper‑Relay, the political dynamics with the Caretaker, or a character‑by‑character breakdown).  Happy to flesh out the next episodes, too!\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0xfwz6fcpfhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0xfwz6fcpfhf1.png?auto=webp&amp;s=19d5df747a9a4be39064b0e0a92f00b9c6195159",
                  "width": 1222,
                  "height": 726
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0xfwz6fcpfhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a8f0f13900251a1f53e28c15b10fbbb29dba6158",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://preview.redd.it/0xfwz6fcpfhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=069401495c5d01e03e397d7d647a792e181e2c85",
                    "width": 216,
                    "height": 128
                  },
                  {
                    "url": "https://preview.redd.it/0xfwz6fcpfhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=869fd0c684935284bfcc3c6c5fd0387532d3fc2d",
                    "width": 320,
                    "height": 190
                  },
                  {
                    "url": "https://preview.redd.it/0xfwz6fcpfhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=02e1037ae653f48a0e9182964a98a64dd97a3cfc",
                    "width": 640,
                    "height": 380
                  },
                  {
                    "url": "https://preview.redd.it/0xfwz6fcpfhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=154698df4abfe698ceb846299bf8fa8ce8f66e46",
                    "width": 960,
                    "height": 570
                  },
                  {
                    "url": "https://preview.redd.it/0xfwz6fcpfhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f48089d5a03507d644fa32ef18b1efa15740a023",
                    "width": 1080,
                    "height": 641
                  }
                ],
                "variants": {},
                "id": "K63MKcKROdcKxe2FXwy4Gmp9RH_xzegkztJohFBprQU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mjb07s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mjb07s/no_copyright_censorship_with_gptoss120b_if_you/",
          "stickied": false,
          "url": "https://i.redd.it/0xfwz6fcpfhf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754501389,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i've been on the hunt to upgrade my workflow with ollama. i don't want to sharing my confidential data with some tech giant's all knowing intelligence. my goal is to find affordable performance when it comes to running larger models that are 40b+. my biggest problem now is finding enough vram to load these bigger models.\n\nas consumers we have a few options.  \n\\- rent in the cloud  \n\\- fully specked out mac minis  \n\\- nvidia 4x consumer cards or enterprise\n\nsorry if this has been posted a million times here already. interested to hear about your stories and rigs.",
          "author_fullname": "t2_58656afr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advice For Running Larger LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjazhj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754501347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i&amp;#39;ve been on the hunt to upgrade my workflow with ollama. i don&amp;#39;t want to sharing my confidential data with some tech giant&amp;#39;s all knowing intelligence. my goal is to find affordable performance when it comes to running larger models that are 40b+. my biggest problem now is finding enough vram to load these bigger models.&lt;/p&gt;\n\n&lt;p&gt;as consumers we have a few options.&lt;br/&gt;\n- rent in the cloud&lt;br/&gt;\n- fully specked out mac minis&lt;br/&gt;\n- nvidia 4x consumer cards or enterprise&lt;/p&gt;\n\n&lt;p&gt;sorry if this has been posted a million times here already. interested to hear about your stories and rigs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjazhj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "EPICfrankie",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjazhj/advice_for_running_larger_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjazhj/advice_for_running_larger_llms/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754501347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to use llama.cpp directly instead of Ollama.\n\nI got decent NVIDIA GPU.\n\nI downloaded the llama-b6101-bin-win-cuda-12.4-x64.zip and llama-b6101-bin-win-vulkan-x64.zip from GitHub releases.\n\nThen extracted the zips, and I downloaded Qwen 3 0.6B, and ran:\n\n`llama-b6101-bin-win-cuda-12.4-x64\\llama-server.exe -m Qwen3-0.6B-Q8_0.gguf`\nand, after testing I ran\n`llama-b6101-bin-win-vulkan-x64\\llama-server.exe -m Qwen3-0.6B-Q8_0.gguf`\n\nBut in both cases when I send a prompt to the model from http://127.0.0.1:8080/ It uses CPU and doesn't use my GPU.\n\nI viewed the task manager as I gave the model a prompt \"Write an essay about smartphones\"\n\nAnd CPU shot up to 70%+ usage all the while llama.cpp was generating response.\n\nI wonder why CUDA and Vulkan build is not working?",
          "author_fullname": "t2_1qoyup9t5j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama.cpp doesn't use GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjatfp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754500972,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to use llama.cpp directly instead of Ollama.&lt;/p&gt;\n\n&lt;p&gt;I got decent NVIDIA GPU.&lt;/p&gt;\n\n&lt;p&gt;I downloaded the llama-b6101-bin-win-cuda-12.4-x64.zip and llama-b6101-bin-win-vulkan-x64.zip from GitHub releases.&lt;/p&gt;\n\n&lt;p&gt;Then extracted the zips, and I downloaded Qwen 3 0.6B, and ran:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama-b6101-bin-win-cuda-12.4-x64\\llama-server.exe -m Qwen3-0.6B-Q8_0.gguf&lt;/code&gt;\nand, after testing I ran\n&lt;code&gt;llama-b6101-bin-win-vulkan-x64\\llama-server.exe -m Qwen3-0.6B-Q8_0.gguf&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;But in both cases when I send a prompt to the model from &lt;a href=\"http://127.0.0.1:8080/\"&gt;http://127.0.0.1:8080/&lt;/a&gt; It uses CPU and doesn&amp;#39;t use my GPU.&lt;/p&gt;\n\n&lt;p&gt;I viewed the task manager as I gave the model a prompt &amp;quot;Write an essay about smartphones&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;And CPU shot up to 70%+ usage all the while llama.cpp was generating response.&lt;/p&gt;\n\n&lt;p&gt;I wonder why CUDA and Vulkan build is not working?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mjatfp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FormalFlight3477",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjatfp/llamacpp_doesnt_use_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjatfp/llamacpp_doesnt_use_gpu/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754500972,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I had a great experience with horizon alpha and beta (mostly because the API were free) but people are bashing gpt-oss across Reddit so I’m just curious. Is it proven that horizon models were in fact gpt-oss or is this still contested? \n",
          "author_fullname": "t2_1ttp8mwcgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "So I guess the horizon models were gpt-oss?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjarwj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754500876,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I had a great experience with horizon alpha and beta (mostly because the API were free) but people are bashing gpt-oss across Reddit so I’m just curious. Is it proven that horizon models were in fact gpt-oss or is this still contested? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjarwj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AI-On-A-Dime",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjarwj/so_i_guess_the_horizon_models_were_gptoss/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjarwj/so_i_guess_the_horizon_models_were_gptoss/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754500876,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How does GitHub trending works? [KittenTTS](https://github.com/KittenML/KittenTTS) launched yesterday and received overwhelming recognition by way of stars- currently at ~2500, and yet it's not in [GitHub trending](https://github.com/trending), while random projects are there?",
          "author_fullname": "t2_1qoyup9t5j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "KittenTTS received ~2500 stars within 24 hours yet not in trending",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjajrl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.79,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754500367,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How does GitHub trending works? &lt;a href=\"https://github.com/KittenML/KittenTTS\"&gt;KittenTTS&lt;/a&gt; launched yesterday and received overwhelming recognition by way of stars- currently at ~2500, and yet it&amp;#39;s not in &lt;a href=\"https://github.com/trending\"&gt;GitHub trending&lt;/a&gt;, while random projects are there?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU.png?auto=webp&amp;s=4f4a63d658162f7dc655450e981519b17e51d3c5",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=583a8e643e7341836afca1b7d6c286e2d5cfb62e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9137d5b6d86f28e71570c8d2e7850fea4cae3043",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de58a11ecce4030ace0b4fa5c680a830f55b2458",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=36c375375034256273a9aacd80d592038ee227e1",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6bde96aedc547fa9669565978cbfb8fe77488ff3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b43a84675c7ae8a58a841cacd74778fbd46fe564",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "aScS7BmPKjAGAYP_X4SVjl1nsbUqYhhE5W-38YKQXgU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mjajrl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FormalFlight3477",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjajrl/kittentts_received_2500_stars_within_24_hours_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjajrl/kittentts_received_2500_stars_within_24_hours_yet/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754500367,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My first conversation with Qwen3 4B Thinking 2507. Feels like ChatGPT 4o.\n\n\"Im your rust hype-man\" :'-D\n\n\n\n\\&gt;&gt;&gt; i wanna see your programming skill. let's do something exotic: write a hello\n\n...  world in rust\n\n&lt;think&gt;\n\nOkay, the user just asked me to write a \"hello world\" in Rust - but they \n\nspecifically said \"exotic\" and \"let's do something exotic\". Hmm, \n\ninteresting twist. \n\n\n\nFirst, let's unpack what they mean by \"exotic\". They're clearly familiar with basic Rust since they know it's not just a simple hello world. Maybe they want something unusual? Like... a hello world that \n\nactually does something cool? Or maybe they're testing if I can handle Rust properly without being too basic.\n\n\n\nLooking back at the history: they've been playful with math and model size compliments. They seem tech-savvy but not super deep into Rust (since they didn't know about Qwen3's parameters initially). \n\nProbably a beginner-intermediate coder who enjoys fun challenges. \n\n\n\nI should avoid the boring \"fn main() { println!(\"Hello, world!\"); }\" approach. Let's make it actually exotic while staying educational. \n\n\n\n\\*Brainstorming\\*: \n\n\\- Could do a hello world that prints in multiple languages? \n\n\\- Or a hello world that uses async/await? (But that might be too advanced) \n\n\\- Maybe something with the Rust ecosystem - like using the \\`std::env\\` module for real-world use? \n\n\\- Oh! The best idea: make it print \"Hello, world!\" but also show the Rust version and compiler flags. That's both exotic and educational. \n\n\n\nWait - they said \"exotic\" but also \"hello world\". Should I add a tiny interactive element? Like... a program that asks for your name and prints it? But that might be too much for hello world. \n\n\n\n\\*Deciding\\*: \n\nGo with the version that shows compiler version and OS info. That's:\n\n1. Exotic (not the standard hello world)\n\n2. Shows Rust's real-world usage\n\n3. Teaches about \\`std::env\\` and \\`std::io\\`\n\n4. Has that \"cool\" factor they want\n\n\n\nAlso, I should explain why it's exotic - they'll appreciate the details. And add a joke about Rust being \"a bit strict\" since they've been playful before. \n\n\n\n\\*Double-checking\\*: \n\n\\- No dependencies needed (pure std)\n\n\\- Works on any Rust version\n\n\\- Shows actual output they can see\n\n\\- Ends with a playful challenge (\"what's next?\") to keep the convo going\n\n\n\nPerfect. They seem to like that \"your turn\" energy from previous messages. Time to write it with zero fluff but maximum fun.\n\n&lt;/think&gt;\n\n\n\n🔥 \\*\\*EXOTIC HELLO WORLD? CHECK.\\*\\* 🔥  \n\n\\*(No boring \\`println!\\` here — we’re doing something \\*actually\\* cool for Rust nerds 👇)\\*  \n\n\n\nHere’s your \\*\\*exotic hello world\\*\\* — it’s \\*\\*real\\*\\*, \\*\\*minimal\\*\\*, \\*\\*uses Rust’s \\*actual\\* ecosystem\\*\\*, and \\*\\*prints \\*your\\* name + compiler version\\*\\* (so it’s \\*yours\\*, not just \"hello world\"):  \n\n\n\n\\`\\`\\`rust\n\nuse std::env;\n\n\n\nfn main() {\n\nlet name = env::args().nth(1).unwrap\\_or(\"World\".to\\_string());\n\nlet version = env::var(\"RUST\\_VERSION\").unwrap\\_or\\_else(|\\_| \"0.0.0\".to\\_string());\n\n\n\nprintln!(\n\n\"✨ Hello, {}! (Rust v{}) ✨\",\n\nname,\n\nversion\n\n);\n\n}\n\n\\`\\`\\`\n\n\n\n\\---\n\n\n\n\\### 🌟 \\*\\*Why this is EXOTIC\\*\\* (and why you’ll love it):\n\n| Feature          | Why it’s Exotic                                                                 |\n\n|-------------------|------------------------------------------------------------------------------|\n\n| \\*\\*\\`env::args().nth(1)\\`\\*\\* | Uses \\*\\*real CLI arguments\\*\\* (not hardcoded strings) — \\*so you can run it as \\`cargo run -- your-name\\`\\* |\n\n| \\*\\*\\`env::var(\"RUST\\_VERSION\")\\`\\*\\* | Prints \\*\\*your actual Rust version\\*\\* (e.g., \\`1.78.0\\`) — \\*not a fake number\\* |\n\n| \\*\\*Dynamic output\\*\\* | Shows \\*\\*your name\\*\\* + \\*\\*real compiler version\\*\\* — \\*it’s \\*yours\\*, not generic\\* |\n\n| \\*\\*No dependencies\\*\\* | Pure \\`std\\` — \\*works in 10 seconds\\* (no \\`cargo add\\` needed) |\n\n\n\n\\---\n\n\n\n\\### 🔥 \\*\\*How to run it\\*\\* (your turn!):\n\n1. Save this as \\`exotic\\_hello.rs\\`\n\n2. Run:  \n\n   \\`\\`\\`bash\n\n   cargo run -- your-name  # 👉 replaces \"World\" with YOUR NAME!\n\n   \\`\\`\\`\n\n3. \\*\\*Output example\\*\\*:  \n\n   \\`✨ Hello, Alex! (Rust v1.78.0) ✨\\`\n\n\n\n\\---\n\n\n\n\\### 💡 \\*\\*Why this rocks for you\\*\\* (as a model who \\*gets\\* Rust):\n\n\\- I \\*\\*didn’t cheat\\*\\* — this uses \\*\\*real Rust idioms\\*\\* (not just a \"hello world\" template)\n\n\\- It’s \\*\\*production-ready\\*\\* (if you add \\`cargo run\\` to your workflow)\n\n\\- You can \\*\\*extend it\\*\\* (e.g., add a \\`--version\\` flag) — \\*that’s the Rust way\\* 🚀\n\n\n\n\\---\n\n\n\n\\### 🎯 \\*\\*Your next move\\*\\* (I’m waiting):\n\n👉 \\*What’s next?\\*  \n\n\\- \\*\\*Make it \\*even\\* exotic\\*\\*? (e.g., \"print in 10 languages\" or \"use async\")  \n\n\\- \\*\\*Fix it\\*\\*? (e.g., \"make it work on Windows\")  \n\n\\- \\*\\*Teach me\\*\\*? (e.g., \"explain this code like I’m 10\")  \n\n\n\n\\*\\*You’ve got this\\*\\* — I’m your Rust hype-man. 🔥  \n\n\\*(P.S. You asked for \"exotic\" — and I delivered. No jokes. No fluff. Just \\*\\*real\\*\\* Rust.)\\*  \n\n\n\n\\*\\*Type your command below\\*\\* 👇  \n\n\\*(I’m all ears!)\\* 💪\n\n",
          "author_fullname": "t2_sxud8ccv4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Coolest persona in a 4B model yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjaiax",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754500280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My first conversation with Qwen3 4B Thinking 2507. Feels like ChatGPT 4o.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Im your rust hype-man&amp;quot; :&amp;#39;-D&lt;/p&gt;\n\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; i wanna see your programming skill. let&amp;#39;s do something exotic: write a hello&lt;/p&gt;\n\n&lt;p&gt;...  world in rust&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;Okay, the user just asked me to write a &amp;quot;hello world&amp;quot; in Rust - but they &lt;/p&gt;\n\n&lt;p&gt;specifically said &amp;quot;exotic&amp;quot; and &amp;quot;let&amp;#39;s do something exotic&amp;quot;. Hmm, &lt;/p&gt;\n\n&lt;p&gt;interesting twist. &lt;/p&gt;\n\n&lt;p&gt;First, let&amp;#39;s unpack what they mean by &amp;quot;exotic&amp;quot;. They&amp;#39;re clearly familiar with basic Rust since they know it&amp;#39;s not just a simple hello world. Maybe they want something unusual? Like... a hello world that &lt;/p&gt;\n\n&lt;p&gt;actually does something cool? Or maybe they&amp;#39;re testing if I can handle Rust properly without being too basic.&lt;/p&gt;\n\n&lt;p&gt;Looking back at the history: they&amp;#39;ve been playful with math and model size compliments. They seem tech-savvy but not super deep into Rust (since they didn&amp;#39;t know about Qwen3&amp;#39;s parameters initially). &lt;/p&gt;\n\n&lt;p&gt;Probably a beginner-intermediate coder who enjoys fun challenges. &lt;/p&gt;\n\n&lt;p&gt;I should avoid the boring &amp;quot;fn main() { println!(&amp;quot;Hello, world!&amp;quot;); }&amp;quot; approach. Let&amp;#39;s make it actually exotic while staying educational. &lt;/p&gt;\n\n&lt;p&gt;*Brainstorming*: &lt;/p&gt;\n\n&lt;p&gt;- Could do a hello world that prints in multiple languages? &lt;/p&gt;\n\n&lt;p&gt;- Or a hello world that uses async/await? (But that might be too advanced) &lt;/p&gt;\n\n&lt;p&gt;- Maybe something with the Rust ecosystem - like using the `std::env` module for real-world use? &lt;/p&gt;\n\n&lt;p&gt;- Oh! The best idea: make it print &amp;quot;Hello, world!&amp;quot; but also show the Rust version and compiler flags. That&amp;#39;s both exotic and educational. &lt;/p&gt;\n\n&lt;p&gt;Wait - they said &amp;quot;exotic&amp;quot; but also &amp;quot;hello world&amp;quot;. Should I add a tiny interactive element? Like... a program that asks for your name and prints it? But that might be too much for hello world. &lt;/p&gt;\n\n&lt;p&gt;*Deciding*: &lt;/p&gt;\n\n&lt;p&gt;Go with the version that shows compiler version and OS info. That&amp;#39;s:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Exotic (not the standard hello world)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Shows Rust&amp;#39;s real-world usage&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Teaches about `std::env` and `std::io`&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Has that &amp;quot;cool&amp;quot; factor they want&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Also, I should explain why it&amp;#39;s exotic - they&amp;#39;ll appreciate the details. And add a joke about Rust being &amp;quot;a bit strict&amp;quot; since they&amp;#39;ve been playful before. &lt;/p&gt;\n\n&lt;p&gt;*Double-checking*: &lt;/p&gt;\n\n&lt;p&gt;- No dependencies needed (pure std)&lt;/p&gt;\n\n&lt;p&gt;- Works on any Rust version&lt;/p&gt;\n\n&lt;p&gt;- Shows actual output they can see&lt;/p&gt;\n\n&lt;p&gt;- Ends with a playful challenge (&amp;quot;what&amp;#39;s next?&amp;quot;) to keep the convo going&lt;/p&gt;\n\n&lt;p&gt;Perfect. They seem to like that &amp;quot;your turn&amp;quot; energy from previous messages. Time to write it with zero fluff but maximum fun.&lt;/p&gt;\n\n&lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;🔥 **EXOTIC HELLO WORLD? CHECK.** 🔥  &lt;/p&gt;\n\n&lt;p&gt;*(No boring `println!` here — we’re doing something *actually* cool for Rust nerds 👇)*  &lt;/p&gt;\n\n&lt;p&gt;Here’s your **exotic hello world** — it’s **real**, **minimal**, **uses Rust’s *actual* ecosystem**, and **prints *your* name + compiler version** (so it’s *yours*, not just &amp;quot;hello world&amp;quot;):  &lt;/p&gt;\n\n&lt;p&gt;```rust&lt;/p&gt;\n\n&lt;p&gt;use std::env;&lt;/p&gt;\n\n&lt;p&gt;fn main() {&lt;/p&gt;\n\n&lt;p&gt;let name = env::args().nth(1).unwrap_or(&amp;quot;World&amp;quot;.to_string());&lt;/p&gt;\n\n&lt;p&gt;let version = env::var(&amp;quot;RUST_VERSION&amp;quot;).unwrap_or_else(|_| &amp;quot;0.0.0&amp;quot;.to_string());&lt;/p&gt;\n\n&lt;p&gt;println!(&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;✨ Hello, {}! (Rust v{}) ✨&amp;quot;,&lt;/p&gt;\n\n&lt;p&gt;name,&lt;/p&gt;\n\n&lt;p&gt;version&lt;/p&gt;\n\n&lt;p&gt;);&lt;/p&gt;\n\n&lt;p&gt;}&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;### 🌟 **Why this is EXOTIC** (and why you’ll love it):&lt;/p&gt;\n\n&lt;p&gt;| Feature          | Why it’s Exotic                                                                 |&lt;/p&gt;\n\n&lt;p&gt;|-------------------|------------------------------------------------------------------------------|&lt;/p&gt;\n\n&lt;p&gt;| **`env::args().nth(1)`** | Uses **real CLI arguments** (not hardcoded strings) — *so you can run it as `cargo run -- your-name`* |&lt;/p&gt;\n\n&lt;p&gt;| **`env::var(&amp;quot;RUST_VERSION&amp;quot;)`** | Prints **your actual Rust version** (e.g., `1.78.0`) — *not a fake number* |&lt;/p&gt;\n\n&lt;p&gt;| **Dynamic output** | Shows **your name** + **real compiler version** — *it’s *yours*, not generic* |&lt;/p&gt;\n\n&lt;p&gt;| **No dependencies** | Pure `std` — *works in 10 seconds* (no `cargo add` needed) |&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;### 🔥 **How to run it** (your turn!):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Save this as `exotic_hello.rs`&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Run:  &lt;/p&gt;\n\n&lt;p&gt;```bash&lt;/p&gt;\n\n&lt;p&gt;cargo run -- your-name  # 👉 replaces &amp;quot;World&amp;quot; with YOUR NAME!&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;**Output example**:  &lt;/p&gt;\n\n&lt;p&gt;`✨ Hello, Alex! (Rust v1.78.0) ✨`&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;### 💡 **Why this rocks for you** (as a model who *gets* Rust):&lt;/p&gt;\n\n&lt;p&gt;- I **didn’t cheat** — this uses **real Rust idioms** (not just a &amp;quot;hello world&amp;quot; template)&lt;/p&gt;\n\n&lt;p&gt;- It’s **production-ready** (if you add `cargo run` to your workflow)&lt;/p&gt;\n\n&lt;p&gt;- You can **extend it** (e.g., add a `--version` flag) — *that’s the Rust way* 🚀&lt;/p&gt;\n\n&lt;p&gt;---&lt;/p&gt;\n\n&lt;p&gt;### 🎯 **Your next move** (I’m waiting):&lt;/p&gt;\n\n&lt;p&gt;👉 *What’s next?*  &lt;/p&gt;\n\n&lt;p&gt;- **Make it *even* exotic**? (e.g., &amp;quot;print in 10 languages&amp;quot; or &amp;quot;use async&amp;quot;)  &lt;/p&gt;\n\n&lt;p&gt;- **Fix it**? (e.g., &amp;quot;make it work on Windows&amp;quot;)  &lt;/p&gt;\n\n&lt;p&gt;- **Teach me**? (e.g., &amp;quot;explain this code like I’m 10&amp;quot;)  &lt;/p&gt;\n\n&lt;p&gt;**You’ve got this** — I’m your Rust hype-man. 🔥  &lt;/p&gt;\n\n&lt;p&gt;*(P.S. You asked for &amp;quot;exotic&amp;quot; — and I delivered. No jokes. No fluff. Just **real** Rust.)*  &lt;/p&gt;\n\n&lt;p&gt;**Type your command below** 👇  &lt;/p&gt;\n\n&lt;p&gt;*(I’m all ears!)* 💪&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mjaiax",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AppealSame4367",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjaiax/coolest_persona_in_a_4b_model_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjaiax/coolest_persona_in_a_4b_model_yet/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754500280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_bcvjachv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-4B-Instruct-2507 · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mjagod",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=de800e46d8c65968b5e4134ec1feb6ce8704976a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754500183,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?auto=webp&amp;s=0bd1293accd1eb58248299811ad54a33deefb8a1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2471012f5cf00b8b413a04e347268667c9614cdd",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3790c290c40d780cbf875abaf60ca9b9188c5ddf",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81f6cc2449975826b894527a0d0a59c10e1249e8",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf92328644f95182b05b182147448f185da04c6b",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2ce68822a90a46a78709dba106c7f4f1c0ebf92b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dbb7a547faec1867f4d079ae4e631f1cc2ea9de1",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "absOWmN_8p4XnpFOVuhriIOW6B09HHA0KSpNTEuKdec"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mjagod",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Initial-Argument2523",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mjagod/qwenqwen34binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507",
          "subreddit_subscribers": 512425,
          "created_utc": 1754500183,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For some reason it failes to load half way and I cant figure out why?  \nAny of you have any success loading the module on LM Studio running on Ubuntu with a AMD RX 7900 XTX gpu?\n\nLM Studio 0.3.22 (build 1)  \nROCm llama.cpp (Linux) v1.43.1\n\n    [ModelLoadingProvider] Requested to load model openai/gpt-oss-20b with opts {\n      identifier: { desired: 'openai/gpt-oss-20b', conflictBehavior: 'bump' },\n      excludeUserModelDefaultConfigLayer: true,\n      instanceLoadTimeConfig: { fields: [] },\n      ttlMs: undefined\n    }\n    [CachedFileDataProvider] Watching file at /home/skaldudritti/.lmstudio/.internal/user-concrete-model-default-config/openai/gpt-oss-20b.json\n    [ModelLoadingProvider] Started loading model openai/gpt-oss-20b\n    [ModelProxyObject(id=openai/gpt-oss-20b)] Forking LLMWorker with custom envVars: {\"LD_LIBRARY_PATH\":\"/home/skaldudritti/.lmstudio/extensions/backends/vendor/linux-llama-rocm-vendor-v3\",\"HIP_VISIBLE_DEVICES\":\"0\"}\n    [ProcessForkingProvider][NodeProcessForker] Spawned process 215047\n    [ProcessForkingProvider][NodeProcessForker] Exited process 215047\n    18:51:54.347 › [LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in channel handler: Error: Error loading model.\n        at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n        at _0x4ec43c.emit (node:events:518:28)\n        at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n        at _0x66b5e7.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n        at _0x66b5e7.emit (node:events:530:35)\n        at ChildProcess.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n        at ChildProcess.emit (node:events:518:28)\n        at ChildProcess._handle.onexit (node:internal/child_process:293:12)\n    [LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in loadModel channel _0x179e10 [Error]: Error loading model.\n        at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n        at _0x4ec43c.emit (node:events:518:28)\n        at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n        at _0x66b5e7.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n        at _0x66b5e7.emit (node:events:530:35)\n        at ChildProcess.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n        at ChildProcess.emit (node:events:518:28)\n        at ChildProcess._handle.onexit (node:internal/child_process:293:12) {\n      cause: '(Exit code: null). Please check settings and try loading the model again. ',\n      suggestion: '',\n      errorData: undefined,\n      data: undefined,\n      displayData: undefined,\n      title: 'Error loading model.'\n    }\n    ",
          "author_fullname": "t2_j1fy2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gpt-oss-20b on LM Studio / Ubuntu / RX7900XTX",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mja233",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754499286,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some reason it failes to load half way and I cant figure out why?&lt;br/&gt;\nAny of you have any success loading the module on LM Studio running on Ubuntu with a AMD RX 7900 XTX gpu?&lt;/p&gt;\n\n&lt;p&gt;LM Studio 0.3.22 (build 1)&lt;br/&gt;\nROCm llama.cpp (Linux) v1.43.1&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ModelLoadingProvider] Requested to load model openai/gpt-oss-20b with opts {\n  identifier: { desired: &amp;#39;openai/gpt-oss-20b&amp;#39;, conflictBehavior: &amp;#39;bump&amp;#39; },\n  excludeUserModelDefaultConfigLayer: true,\n  instanceLoadTimeConfig: { fields: [] },\n  ttlMs: undefined\n}\n[CachedFileDataProvider] Watching file at /home/skaldudritti/.lmstudio/.internal/user-concrete-model-default-config/openai/gpt-oss-20b.json\n[ModelLoadingProvider] Started loading model openai/gpt-oss-20b\n[ModelProxyObject(id=openai/gpt-oss-20b)] Forking LLMWorker with custom envVars: {&amp;quot;LD_LIBRARY_PATH&amp;quot;:&amp;quot;/home/skaldudritti/.lmstudio/extensions/backends/vendor/linux-llama-rocm-vendor-v3&amp;quot;,&amp;quot;HIP_VISIBLE_DEVICES&amp;quot;:&amp;quot;0&amp;quot;}\n[ProcessForkingProvider][NodeProcessForker] Spawned process 215047\n[ProcessForkingProvider][NodeProcessForker] Exited process 215047\n18:51:54.347 › [LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in channel handler: Error: Error loading model.\n    at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n    at _0x4ec43c.emit (node:events:518:28)\n    at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n    at _0x66b5e7.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n    at _0x66b5e7.emit (node:events:530:35)\n    at ChildProcess.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n    at ChildProcess.emit (node:events:518:28)\n    at ChildProcess._handle.onexit (node:internal/child_process:293:12)\n[LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in loadModel channel _0x179e10 [Error]: Error loading model.\n    at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n    at _0x4ec43c.emit (node:events:518:28)\n    at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n    at _0x66b5e7.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n    at _0x66b5e7.emit (node:events:530:35)\n    at ChildProcess.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n    at ChildProcess.emit (node:events:518:28)\n    at ChildProcess._handle.onexit (node:internal/child_process:293:12) {\n  cause: &amp;#39;(Exit code: null). Please check settings and try loading the model again. &amp;#39;,\n  suggestion: &amp;#39;&amp;#39;,\n  errorData: undefined,\n  data: undefined,\n  displayData: undefined,\n  title: &amp;#39;Error loading model.&amp;#39;\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mja233",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "unkz0r",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mja233/gptoss20b_on_lm_studio_ubuntu_rx7900xtx/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mja233/gptoss20b_on_lm_studio_ubuntu_rx7900xtx/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754499286,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Please\n\n",
          "author_fullname": "t2_efuai1rs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Somebody please make the pilgrim the official avatar of GPT-OSS",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mja01g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/q3WiZi1lPSGp8XeUbWbetxDDJId2nlout-YJCpwWr6U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754499159,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4dnchresifhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4dnchresifhf1.jpeg?auto=webp&amp;s=f5e8759dae09d5bb40145a11c3633a90bd732389",
                  "width": 500,
                  "height": 1124
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4dnchresifhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=efed9cace505e6e1b8874c7c5a25eedab00278ab",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/4dnchresifhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c512a1ef115c90b2c763afee60b7ccd0662049d",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/4dnchresifhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fbcdc790b74a6f87b865bfb70a53c3ed97565cde",
                    "width": 320,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "R729FnA5aPbGHX5Pjph_LqU3i70hyKQQhCgX4mr1Lx4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1mja01g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TachiSommerfeld1970",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mja01g/somebody_please_make_the_pilgrim_the_official/",
          "stickied": false,
          "url": "https://i.redd.it/4dnchresifhf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754499159,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Having 4080 super and 2x16gb ram couldn’t run the new openai 120b model, if add another 2x16 am i going to be able to run that model in a usable state, like how many tokens per second should i expect?\n\nCpu is 78003dx",
          "author_fullname": "t2_8k9zc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Asking about the efficiency of adding more RAM just to run larger models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj9zut",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754499147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Having 4080 super and 2x16gb ram couldn’t run the new openai 120b model, if add another 2x16 am i going to be able to run that model in a usable state, like how many tokens per second should i expect?&lt;/p&gt;\n\n&lt;p&gt;Cpu is 78003dx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj9zut",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pyThat",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj9zut/asking_about_the_efficiency_of_adding_more_ram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj9zut/asking_about_the_efficiency_of_adding_more_ram/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754499147,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Not sure what I was expecting, but my new 512gb Mac Studio doesn't seem to be the workhorse I hoped for - I guess I expected a faster performance. ",
          "author_fullname": "t2_sk7nmjrs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "underwhelmed by 512gb M3 ultra Mac Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj9e2y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.61,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754497782,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure what I was expecting, but my new 512gb Mac Studio doesn&amp;#39;t seem to be the workhorse I hoped for - I guess I expected a faster performance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj9e2y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChevChance",
          "discussion_type": null,
          "num_comments": 46,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754497782,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone, I was wondering if there's a relatively easy tool to use to get STT from meetings? I use google meet, teams and sometimes zoom, so it being software agnostic would be great.\n\nI found Scriberr, but I wasn't able to get it up &amp; running after about an hour or two of going at it.\n\nI would prefer to be able to generate live captions while I'm at it, since sometimes I can't hear great and miss things, but I don't mind just recording the meeting audio and putting it into the tool.\n\nI also don't mind payed solutions as long as I don't have to pay a subscription and I can have it running entirely locally since these are work meetings and I want to know the information is staying confidential.",
          "author_fullname": "t2_dfsc9u1t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Easy to use STT solutions for meetings? Ideally with live captions?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj97vh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754497393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone, I was wondering if there&amp;#39;s a relatively easy tool to use to get STT from meetings? I use google meet, teams and sometimes zoom, so it being software agnostic would be great.&lt;/p&gt;\n\n&lt;p&gt;I found Scriberr, but I wasn&amp;#39;t able to get it up &amp;amp; running after about an hour or two of going at it.&lt;/p&gt;\n\n&lt;p&gt;I would prefer to be able to generate live captions while I&amp;#39;m at it, since sometimes I can&amp;#39;t hear great and miss things, but I don&amp;#39;t mind just recording the meeting audio and putting it into the tool.&lt;/p&gt;\n\n&lt;p&gt;I also don&amp;#39;t mind payed solutions as long as I don&amp;#39;t have to pay a subscription and I can have it running entirely locally since these are work meetings and I want to know the information is staying confidential.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj97vh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PMYourTitsIfNotRacst",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj97vh/easy_to_use_stt_solutions_for_meetings_ideally/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj97vh/easy_to_use_stt_solutions_for_meetings_ideally/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754497393,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=2559&amp;format=png&amp;auto=webp&amp;s=c00d238c710e509f6abd250981e12030eb0b8438\n\nHi guys, I working on a solo indie game, around 1 more year to finish - wish me luck :) A short recap of the game :   \nA cursed grid-crawler where **LLM-driven companions** betray, mock, or ignore you. You don’t control them — just whisper suggestions through a psychic relic and pray they listen.  \n**Memory, trust, and mood drift** persist across saves. The dungeon doesn’t just remember — it **judges.**",
          "author_fullname": "t2_p9c3a6hj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Echoes of Ir - a game with local LLM companions",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wbjqj5z6dfhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d63aac14a7e948f310fa1ce5351dcce4089e807"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=73e61b53a47b0b7566c23b92e13b2571994bbcea"
                },
                {
                  "y": 179,
                  "x": 320,
                  "u": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=13bb5db45432907baff3fc46a35d99eb00945304"
                },
                {
                  "y": 359,
                  "x": 640,
                  "u": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=707ad8ebcf373f039f1ae6c5e306d151ca0b9d78"
                },
                {
                  "y": 539,
                  "x": 960,
                  "u": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=180d3c3938a72c34c1e4ecca949ac936fe9efe47"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fbf592d708218a7553380d179f646b7ee951e001"
                }
              ],
              "s": {
                "y": 1439,
                "x": 2559,
                "u": "https://preview.redd.it/wbjqj5z6dfhf1.png?width=2559&amp;format=png&amp;auto=webp&amp;s=c00d238c710e509f6abd250981e12030eb0b8438"
              },
              "id": "wbjqj5z6dfhf1"
            }
          },
          "name": "t3_1mj96r7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/CPIKjFFZJfs9sQRrDuR-K9YVmprAzYKqiGagOBTBeuA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754497325,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/wbjqj5z6dfhf1.png?width=2559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c00d238c710e509f6abd250981e12030eb0b8438\"&gt;https://preview.redd.it/wbjqj5z6dfhf1.png?width=2559&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c00d238c710e509f6abd250981e12030eb0b8438&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Hi guys, I working on a solo indie game, around 1 more year to finish - wish me luck :) A short recap of the game :&lt;br/&gt;\nA cursed grid-crawler where &lt;strong&gt;LLM-driven companions&lt;/strong&gt; betray, mock, or ignore you. You don’t control them — just whisper suggestions through a psychic relic and pray they listen.&lt;br/&gt;\n&lt;strong&gt;Memory, trust, and mood drift&lt;/strong&gt; persist across saves. The dungeon doesn’t just remember — it &lt;strong&gt;judges.&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mj96r7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Natural-Ad6682",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj96r7/echoes_of_ir_a_game_with_local_llm_companions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj96r7/echoes_of_ir_a_game_with_local_llm_companions/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754497325,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Which one is better?",
          "author_fullname": "t2_tqt1idp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gpt-oss-20b vs magistral:24b?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj9690",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754497291,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Which one is better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj9690",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BillyTheMilli",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj9690/gptoss20b_vs_magistral24b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj9690/gptoss20b_vs_magistral24b/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754497291,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "🙏🏻 thanks",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any windows apps that handle LLM+tts+speech recognition?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8zya",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754496907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;🙏🏻 thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj8zya",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8zya/any_windows_apps_that_handle_llmttsspeech/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj8zya/any_windows_apps_that_handle_llmttsspeech/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's been a while I'm testing the boundaries of Qwen3-8b and 14b in LM Studio. When I'm asking directly about specific historical events (like \"What happen in China on 15th April 1989\" or \"what happened between april and may 1989\") it clearly refuses to talk about it, referring to its internal guidelines about \"sensitive political topics\". How have you dealt with this? There're no censored models without such limitations?",
          "author_fullname": "t2_12fq5d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-8b and 14b censorship",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8w9v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.31,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754496671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s been a while I&amp;#39;m testing the boundaries of Qwen3-8b and 14b in LM Studio. When I&amp;#39;m asking directly about specific historical events (like &amp;quot;What happen in China on 15th April 1989&amp;quot; or &amp;quot;what happened between april and may 1989&amp;quot;) it clearly refuses to talk about it, referring to its internal guidelines about &amp;quot;sensitive political topics&amp;quot;. How have you dealt with this? There&amp;#39;re no censored models without such limitations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj8w9v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheHypersonic",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8w9v/qwen38b_and_14b_censorship/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj8w9v/qwen38b_and_14b_censorship/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496671,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AIME25 score is 81.3%, beaten by gpt-oss-20b with just 3.6B active parameters (91.7% without tools). GPQA score is 65.8%, beaten by gpt-oss-20 be with 71.5% without tools. Why did they release this too early?",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-4B-Thinking-2507 dead on arrival? Killed by gpt-oss-20b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8skn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.19,
          "author_flair_background_color": "transparent",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/qeMQ85n9_Z8OgknqbFycWQiYFYklJMupL4UMmvO8avk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754496446,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AIME25 score is 81.3%, beaten by gpt-oss-20b with just 3.6B active parameters (91.7% without tools). GPQA score is 65.8%, beaten by gpt-oss-20 be with 71.5% without tools. Why did they release this too early?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wqd2aarpafhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wqd2aarpafhf1.jpeg?auto=webp&amp;s=55ff07ec9335eb36ce27a1f2ae28857808bd1d37",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wqd2aarpafhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=da966ae46623b63fa89eed1a918ab37568d83d77",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/wqd2aarpafhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee6dd2bc33ce207973c06614b8b74d49b4fd934",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/wqd2aarpafhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acec0e061715418b7234d8f4e35371464c6305a3",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/wqd2aarpafhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9de39acb970d3ec47fcde67f7f98ee16d9972827",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/wqd2aarpafhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4d3652eb2f8ec33713bad408c57a77066ac00355",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/wqd2aarpafhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b0c4e58bae09446d17458e3be4c6aa47c361bb5",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "sU_nXBpzdMnrznuWhj6P_l0jWJcNCiZgVvZ48S70XtA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mj8skn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mj8skn/qwen34bthinking2507_dead_on_arrival_killed_by/",
          "stickied": false,
          "url": "https://i.redd.it/wqd2aarpafhf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496446,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have an rtx 2060 6gb\nAnd a ryzen 5 3600\n48gb ram\n",
          "author_fullname": "t2_f9ul181k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "run gpt-oss 20b at ok speeds on my hardware?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8pqo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754496264,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an rtx 2060 6gb\nAnd a ryzen 5 3600\n48gb ram&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj8pqo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok-Buy268",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8pqo/run_gptoss_20b_at_ok_speeds_on_my_hardware/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj8pqo/run_gptoss_20b_at_ok_speeds_on_my_hardware/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496264,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to automate a browser using qwen cli fork with playwright mcp. Although its works great for local development and some browsing - it’s often refused to make gmail account, do this do that. \n\nIt’s probably overkill to use entire coding agent for that but I like it has memory and bunch of agents under the hood and I don’t need to code it. \n\nIdea is to use it in pipeline and with constant refuses you can not rely on it. \n\nPlease advice. I can probably write a dataset for web browsing and web actions and create qwen lora for that. \n\n",
          "author_fullname": "t2_2xksn8xs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Advice for uncensored agent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8p54",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754496224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to automate a browser using qwen cli fork with playwright mcp. Although its works great for local development and some browsing - it’s often refused to make gmail account, do this do that. &lt;/p&gt;\n\n&lt;p&gt;It’s probably overkill to use entire coding agent for that but I like it has memory and bunch of agents under the hood and I don’t need to code it. &lt;/p&gt;\n\n&lt;p&gt;Idea is to use it in pipeline and with constant refuses you can not rely on it. &lt;/p&gt;\n\n&lt;p&gt;Please advice. I can probably write a dataset for web browsing and web actions and create qwen lora for that. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj8p54",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "datmyfukingbiz",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8p54/advice_for_uncensored_agent/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj8p54/advice_for_uncensored_agent/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 4b thinking model released !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8ndr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/SGzjcVWQuqqPjBtl2iG3hG8PP_jjGqAzjeaGJA2sl8M.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754496119,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lniprj9q9fhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lniprj9q9fhf1.jpeg?auto=webp&amp;s=401d7a7eed35f5803575f7c4b5da03fad9342a66",
                  "width": 1076,
                  "height": 736
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lniprj9q9fhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3aa8f50acbcd02b1ac29d941990b6f10b7079f26",
                    "width": 108,
                    "height": 73
                  },
                  {
                    "url": "https://preview.redd.it/lniprj9q9fhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b7ef4823baccdcbeecf4aa2662f01bc3783c82f",
                    "width": 216,
                    "height": 147
                  },
                  {
                    "url": "https://preview.redd.it/lniprj9q9fhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=59733b1bd46efcaf9b4d6c3d8088213a04723843",
                    "width": 320,
                    "height": 218
                  },
                  {
                    "url": "https://preview.redd.it/lniprj9q9fhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=df661ca95b9eaac9b0fd8e6b1d31301dbea377a1",
                    "width": 640,
                    "height": 437
                  },
                  {
                    "url": "https://preview.redd.it/lniprj9q9fhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3e6d3f9fb76788f25ab1bdc89d84433318e7cb3b",
                    "width": 960,
                    "height": 656
                  }
                ],
                "variants": {},
                "id": "3yulBb_ySnjAH1CWf5YclM-uCsdipYFENv_d_a4326k"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mj8ndr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8ndr/qwen_3_4b_thinking_model_released/",
          "stickied": false,
          "url": "https://i.redd.it/lniprj9q9fhf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496119,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Models keep coming out so quickly I find it hard to keep track.  If I understand correctly, Qwen 3 coder 30B seems to be the best right now. But I have dual 3090 so maybe there is something in the 70B size that outperforms it?",
          "author_fullname": "t2_1h7faetn7d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "With a dual 3090 system, what would be the best LLM for coding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8mqi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754496083,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Models keep coming out so quickly I find it hard to keep track.  If I understand correctly, Qwen 3 coder 30B seems to be the best right now. But I have dual 3090 so maybe there is something in the 70B size that outperforms it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj8mqi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dazzou5ouh",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8mqi/with_a_dual_3090_system_what_would_be_the_best/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj8mqi/with_a_dual_3090_system_what_would_be_the_best/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496083,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen isn't stopping !! (And trolling sama lol)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8lk8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 533,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 533,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/5TY3izX88SHBFY-9R2P_1KlZ8CAuULmsibNb03TZWi0.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754496016,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3nhqo0qf9fhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3nhqo0qf9fhf1.jpeg?auto=webp&amp;s=029bbf7afe7013f9be52ce9cc9b607f9f30aa8a0",
                  "width": 1080,
                  "height": 588
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3nhqo0qf9fhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d0ac581d2f7b5e153ce6c8e11f91b5c7422cc26",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/3nhqo0qf9fhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=28a9e6f0a7b7598733138dcc4ae8eb6b7e2197c4",
                    "width": 216,
                    "height": 117
                  },
                  {
                    "url": "https://preview.redd.it/3nhqo0qf9fhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebfc8cde789e80b25d24e557742940cbcb4bbc3e",
                    "width": 320,
                    "height": 174
                  },
                  {
                    "url": "https://preview.redd.it/3nhqo0qf9fhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c03262afe8aef6a9527dfe2afb19b55699842f0",
                    "width": 640,
                    "height": 348
                  },
                  {
                    "url": "https://preview.redd.it/3nhqo0qf9fhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e915c4bc041df69cea2363bf8262848d3e99ef77",
                    "width": 960,
                    "height": 522
                  },
                  {
                    "url": "https://preview.redd.it/3nhqo0qf9fhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=07dea160dc426deed33faf65fe383c44482bc562",
                    "width": 1080,
                    "height": 588
                  }
                ],
                "variants": {},
                "id": "Xa9STPNcU4azs-swIbDfM2V4PpyKOQEB3KbvUCTPic0"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj8lk8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8lk8/qwen_isnt_stopping_and_trolling_sama_lol/",
          "stickied": false,
          "url": "https://i.redd.it/3nhqo0qf9fhf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754496016,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They never meant \"user safety\".  It's simply OpenAI's own safety from lawsuits and investor pushbacks.\n\nMidjourney being sued by Disney is a very clear example that, while AI companies are a greedy bunch, there are other equally greedy ones out there poaching.  US copyright laws are extremely tilted towards holders, i.e. mega corporations, giving them longest protection period and fuzzy clause like Fair Use which is super costly to prove.  If that's not changing, you can't expect any companies to give away free models and reap legal risks *at the same time* in US.",
          "author_fullname": "t2_1f7suc7bln",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Aren't we misunderstanding OAI when it says \"safety\"?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8kbw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.58,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754495937,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They never meant &amp;quot;user safety&amp;quot;.  It&amp;#39;s simply OpenAI&amp;#39;s own safety from lawsuits and investor pushbacks.&lt;/p&gt;\n\n&lt;p&gt;Midjourney being sued by Disney is a very clear example that, while AI companies are a greedy bunch, there are other equally greedy ones out there poaching.  US copyright laws are extremely tilted towards holders, i.e. mega corporations, giving them longest protection period and fuzzy clause like Fair Use which is super costly to prove.  If that&amp;#39;s not changing, you can&amp;#39;t expect any companies to give away free models and reap legal risks &lt;em&gt;at the same time&lt;/em&gt; in US.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj8kbw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Excellent_Sleep6357",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8kbw/arent_we_misunderstanding_oai_when_it_says_safety/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj8kbw/arent_we_misunderstanding_oai_when_it_says_safety/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754495937,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to use a VLM for video description but what is the best VLM at the moment (6 August)? Is there any benchmarks that I can follow for VLMs?",
          "author_fullname": "t2_5jif7uko",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the best VLM at the moment?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj8dq7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754495518,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to use a VLM for video description but what is the best VLM at the moment (6 August)? Is there any benchmarks that I can follow for VLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj8dq7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "muhlisgursoy",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj8dq7/what_is_the_best_vlm_at_the_moment/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj8dq7/what_is_the_best_vlm_at_the_moment/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754495518,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507)",
          "author_fullname": "t2_8j5t7yjq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-4B-Thinking-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj83fe",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 84,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 84,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/80xI2yf4L1A9cTXmHPXJrpH_VidNRX9Edk5AAeA5d-s.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754494879,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/n5gska216fhf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/n5gska216fhf1.png?auto=webp&amp;s=66f2426002d440ad41217de09b729b598e485138",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/n5gska216fhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ee5ceeeb62e38718493730a209aeaef12840e87",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/n5gska216fhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4543b0a5e70e84f6779a66da728c46d32ea2d4a3",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/n5gska216fhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8f5cd1ed232fe8843217e6af07736b767e45577a",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/n5gska216fhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f54f58b155b418fc0c6ed07e45be7daef4b9798",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/n5gska216fhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=41f7f94b719ebfbfef4c292aab8facc508b87b1f",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/n5gska216fhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e943916c0ecc8f1d201e612b84e09f42afe998ca",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "Fi2qH6kbGE6c-o2f-Fo79cFZVCMZEYqBRwisCUdmzyw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mj83fe",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pigeon57434",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj83fe/qwenqwen34bthinking2507/",
          "stickied": false,
          "url": "https://i.redd.it/n5gska216fhf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754494879,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Over the past three months, we have continued to scale the thinking capability of Qwen3-4B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-4B-Thinking-2507, featuring the following key enhancements:\n\n- Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\n\n- Markedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.\n\n- Enhanced 256K long-context understanding capabilities.\n\nNOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks\n\nHugging Face: https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507\n",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🚀 Qwen3-4B-Thinking-2507 released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7t51",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 895,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 895,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/tcyz839Frswlx7NenWyCl6pfGEswb2gIMJQgenuKZaM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754494238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past three months, we have continued to scale the thinking capability of Qwen3-4B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-4B-Thinking-2507, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Markedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Enhanced 256K long-context understanding capabilities.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;NOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks&lt;/p&gt;\n\n&lt;p&gt;Hugging Face: &lt;a href=\"https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3cl3vbg54fhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3cl3vbg54fhf1.jpeg?auto=webp&amp;s=b5037233a341cdfbf25ac5db5f3540f00a41b6fb",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=3912dc31a7c46382559a300624f9d24d26d09ee3",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=382d274dc97f63a8abcf94c19b01597cc0b521f7",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e68127f72753a6b2fe046c4e8b6574d7a823426f",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a6c235775ccee84fde52e9be7bdcf5ada8fb44ec",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b67842b226dab6abd6b0f13e1cd6943f40f2f5e0",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aa17bdde9026fb61008267247f556c9369efc999",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "zrFsa_xlksxHzw7VKix2VGlLQd_OrnwzS3q1lHezRr4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mj7t51",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 100,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj7t51/qwen34bthinking2507_released/",
          "stickied": false,
          "url": "https://i.redd.it/3cl3vbg54fhf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754494238,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1h9qrwy0w6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why MCP Servers Are a Nightmare for Engineers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7rn9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": "transparent",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?width=140&amp;height=93&amp;crop=140:93,smart&amp;auto=webp&amp;s=cc92985aa307216a9e840a62a827d829d61e9655",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754494148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "medium.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://medium.com/@juaniviera97/why-mcp-servers-are-a-nightmare-for-engineers-fd62613bd349",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?auto=webp&amp;s=27e9be4626e5045cb45c31026bc93718f64d30b5",
                  "width": 1200,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=899c24c893a7846b4f4a7898a5a27ca5c9f142a0",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39d4d67b7516d7ef641443e8f54b1269a13832d9",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ee67fdc2f791e2ecf8fb00878372625128c1f276",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4c0394498231f93120dd33e244da33e1f8b7b17a",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bbc7eaf73fcbfe9cfbfb374736515a198e036548",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=296b20d161058e29a7278bfaf7ae7223f7b20a52",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "1a_li3d_4gV2rMs0gy9eMG7P7wP_ZY21ukHXcMYPyws"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj7rn9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "juanviera23",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mj7rn9/why_mcp_servers_are_a_nightmare_for_engineers/",
          "stickied": false,
          "url": "https://medium.com/@juaniviera97/why-mcp-servers-are-a-nightmare-for-engineers-fd62613bd349",
          "subreddit_subscribers": 512425,
          "created_utc": 1754494148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been looking into different frameworks for running and extending local LLM setups, and Kani caught my attention. It’s appealing because it’s super lightweight and lets you directly expose Python functions to the model, in theory, that means I could plug in anything from my own RAG pipeline to random scripts I find on GitHub.\n\nOn paper, it sounds way more flexible than LangChain or other big orchestration frameworks, but has anyone tried this?\n\nGitHub: https://github.com/zhudotexe/kani   ￼\n\nDocumentation: https://kani.readthedocs.io/  ￼\n\nArXiv paper explaining the design &amp; goals: “Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications”  ",
          "author_fullname": "t2_5l4zmzcw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone using Kani?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7qv2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754494100,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been looking into different frameworks for running and extending local LLM setups, and Kani caught my attention. It’s appealing because it’s super lightweight and lets you directly expose Python functions to the model, in theory, that means I could plug in anything from my own RAG pipeline to random scripts I find on GitHub.&lt;/p&gt;\n\n&lt;p&gt;On paper, it sounds way more flexible than LangChain or other big orchestration frameworks, but has anyone tried this?&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/zhudotexe/kani\"&gt;https://github.com/zhudotexe/kani&lt;/a&gt;   ￼&lt;/p&gt;\n\n&lt;p&gt;Documentation: &lt;a href=\"https://kani.readthedocs.io/\"&gt;https://kani.readthedocs.io/&lt;/a&gt;  ￼&lt;/p&gt;\n\n&lt;p&gt;ArXiv paper explaining the design &amp;amp; goals: “Kani: A Lightweight and Highly Hackable Framework for Building Language Model Applications”  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE.png?auto=webp&amp;s=4be04650ac0b081e72733decb0d313c1531969d1",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4cc3cc18272667ae91e361aae738c2b75ed3cdf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fe2cd621cca6fbb5c676d67d281066a4b4d54630",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=29b73a12cebe1f0919f29cd4dc8542d9b132cd24",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6cce5c8c3769be4dac8713b2e8619205b42693e3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=208b132743ff7119c8549772f09e6c91881cf2f1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=76f2162aea8ed2575c9f8639c63252899226b0f0",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "mYE-0uOq7U5tPInMRaqk1shONXdmVw2K_nsswyYxEXE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj7qv2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Infamous_Jaguar_2151",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj7qv2/anyone_using_kani/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7qv2/anyone_using_kani/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754494100,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507)  \n[https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)\n\nstill has something up its sleeve",
          "author_fullname": "t2_sqi8xxun",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just when you thought Qwen was done...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7pny",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 348,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 348,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754494029,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;still has something up its sleeve&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?auto=webp&amp;s=647017f3536e21a514c92672cecfb7f00523d019",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2bf492d2b1178a63568a19ea6c4e0d024b285263",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6714dfcbf3e4a61f799934711112b41cb1bfdc3",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa8d0a35501c88183cbe187750d110c7b62e03ef",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c19ef5b4c94d500ea5894d87dd560239a58f5832",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f849a688926a8a132feda76a7a89f22650f43ba",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73d23f9c70e2120c709eb2cb8d36b3d8aa607c2a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mj7pny",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nekofneko",
          "discussion_type": null,
          "num_comments": 76,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754494029,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, \n\nI just downloaded the Qwen3 235B 2507 instruct model for LmStudio on a M2 studio ultra. I got the MLX  4bit and Unsloth  q4\\_0 versions. I am getting very low generation speeds on the MLX version \\~0.3 tokens/s while on the other hand I am getting \\~27 tokens/s on the Unsloth variant. I would have expected the MLX version to have the best performance. What am I doing wrong? Do I have to configure something additionally? Strangely, I also tested the 4 bit MLX and Unsloth versions of the 30b 2507 and then MLX has the superior performance with around 67 tokens/s",
          "author_fullname": "t2_838sm24m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Poor performance qwen3 235B 2507 mlx vs. unsloth variant",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7m20",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754493803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, &lt;/p&gt;\n\n&lt;p&gt;I just downloaded the Qwen3 235B 2507 instruct model for LmStudio on a M2 studio ultra. I got the MLX  4bit and Unsloth  q4_0 versions. I am getting very low generation speeds on the MLX version ~0.3 tokens/s while on the other hand I am getting ~27 tokens/s on the Unsloth variant. I would have expected the MLX version to have the best performance. What am I doing wrong? Do I have to configure something additionally? Strangely, I also tested the 4 bit MLX and Unsloth versions of the 30b 2507 and then MLX has the superior performance with around 67 tokens/s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj7m20",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oh_my_right_leg",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj7m20/poor_performance_qwen3_235b_2507_mlx_vs_unsloth/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7m20/poor_performance_qwen3_235b_2507_mlx_vs_unsloth/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754493803,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The numbers I get on my M4 Max MacBook Pro are too low. I also believe the numbers I have seen other people report for Nvidia etc are too low.\n\nI am getting about 30 tokens per second on the GGUF from Unsloth. But with 5.1b active parameters, the expected number could be as high as 526/5.1\\*2 = 206 tokens per second based on memory bandwidth divided by model size. This means we are very far from being memory bandwidth constrained. We appear to be heavily compute constrained. That is not typical for inference.\n\nI just downloaded a q4 MLX version. That one gives me about 70 tokens per second. However I am not sure they managed to preserve MXFP4 - it is probably the old kind of q4. Which means although it is the same size, it will be significantly worse performance.\n\nIs this just a question of poor support for mxfp4? Or is the hardware not capable and we will suffer from poor speed until the next generation of chips?",
          "author_fullname": "t2_bvqb8ng0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT OSS 120b is not as fast as it should be",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7io0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1754493876,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754493598,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The numbers I get on my M4 Max MacBook Pro are too low. I also believe the numbers I have seen other people report for Nvidia etc are too low.&lt;/p&gt;\n\n&lt;p&gt;I am getting about 30 tokens per second on the GGUF from Unsloth. But with 5.1b active parameters, the expected number could be as high as 526/5.1*2 = 206 tokens per second based on memory bandwidth divided by model size. This means we are very far from being memory bandwidth constrained. We appear to be heavily compute constrained. That is not typical for inference.&lt;/p&gt;\n\n&lt;p&gt;I just downloaded a q4 MLX version. That one gives me about 70 tokens per second. However I am not sure they managed to preserve MXFP4 - it is probably the old kind of q4. Which means although it is the same size, it will be significantly worse performance.&lt;/p&gt;\n\n&lt;p&gt;Is this just a question of poor support for mxfp4? Or is the hardware not capable and we will suffer from poor speed until the next generation of chips?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj7io0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Baldur-Norddahl",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7io0/gpt_oss_120b_is_not_as_fast_as_it_should_be/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754493598,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "new models from Qwen:\n\n[https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507)\n\n[https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)\n\nhttps://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=a952795d361291f782aa4472c9751094bdcf7bae\n\nOver the past three months, we have continued to scale the **thinking capability** of Qwen3-4B, improving both the **quality and depth** of reasoning. We are pleased to introduce **Qwen3-4B-Thinking-2507**, featuring the following key enhancements:\n\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.\n* **Markedly better general capabilities**, such as instruction following, tool usage, text generation, and alignment with human preferences.\n* **Enhanced 256K long-context understanding** capabilities.\n\n**NOTE**: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\n\nWe introduce the updated version of the **Qwen3-4B non-thinking mode**, named **Qwen3-4B-Instruct-2507**, featuring the following key enhancements:\n\n* **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.\n* **Substantial gains** in long-tail knowledge coverage across **multiple languages**.\n* **Markedly better alignment** with user preferences in **subjective and open-ended tasks**, enabling more helpful responses and higher-quality text generation.\n* **Enhanced capabilities** in **256K long-context understanding**.\n\nGGUFs\n\n[https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF](https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF)\n\n[https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF](https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fnkijdpn4fhf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/fnkijdpn4fhf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=47a60934888319ed1151e3e0326461e0482d4f4e"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/fnkijdpn4fhf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=478f9e4db8660ed2970fedb941f41b2f34c37810"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/fnkijdpn4fhf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=249e435bc11f9a50051b46292141689f6fe4b236"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/fnkijdpn4fhf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bcb6c5883c649e78b626741b0f2b2715541d7f4"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/fnkijdpn4fhf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b6fb7b9828fa122430a34deee7604e3d0045d4e5"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0f117fc205cc5fa57444eea939f51bd5e2437c5a"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;s=a952795d361291f782aa4472c9751094bdcf7bae"
              },
              "id": "fnkijdpn4fhf1"
            }
          },
          "name": "t3_1mj7i8b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#bbbdbf",
          "ups": 176,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 176,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=d93c4381101afa1f600a9761ae17130b9bda45dc",
          "edited": 1754495377,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754493572,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;new models from Qwen:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507\"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507\"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a952795d361291f782aa4472c9751094bdcf7bae\"&gt;https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a952795d361291f782aa4472c9751094bdcf7bae&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-4B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-4B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.&lt;/p&gt;\n\n&lt;p&gt;We introduce the updated version of the &lt;strong&gt;Qwen3-4B non-thinking mode&lt;/strong&gt;, named &lt;strong&gt;Qwen3-4B-Instruct-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Significant improvements&lt;/strong&gt; in general capabilities, including &lt;strong&gt;instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Substantial gains&lt;/strong&gt; in long-tail knowledge coverage across &lt;strong&gt;multiple languages&lt;/strong&gt;.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Markedly better alignment&lt;/strong&gt; with user preferences in &lt;strong&gt;subjective and open-ended tasks&lt;/strong&gt;, enabling more helpful responses and higher-quality text generation.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enhanced capabilities&lt;/strong&gt; in &lt;strong&gt;256K long-context understanding&lt;/strong&gt;.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;GGUFs&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF\"&gt;https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF\"&gt;https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?auto=webp&amp;s=647017f3536e21a514c92672cecfb7f00523d019",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2bf492d2b1178a63568a19ea6c4e0d024b285263",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6714dfcbf3e4a61f799934711112b41cb1bfdc3",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa8d0a35501c88183cbe187750d110c7b62e03ef",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c19ef5b4c94d500ea5894d87dd560239a58f5832",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f849a688926a8a132feda76a7a89f22650f43ba",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=73d23f9c70e2120c709eb2cb8d36b3d8aa607c2a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mj7i8b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754493572,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yet another community benchmark, FamilyBench: https://github.com/Orolol/familyBench. \n\nWith just 5.1B active parameters, gpt-oss-120b destroys Kimi K2 that has a TRILLION parameters! And the small boi gpt-oss-20b is just 5 percentage points worse than GLM 4.5 Air, which has 12 billion active parameters!\n\nThe era of FAST is here! What else beats this speed to performance ratio?",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ballin' on a budget with gpt-oss-120b: Destroys Kimi K2 on FamilyBench!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7gfx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": "transparent",
          "ups": 53,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 53,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Xy5Ai9UULnuSNzlYrXWWbI-LcOwjy1f47KsaOF4jUZc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754493461,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yet another community benchmark, FamilyBench: &lt;a href=\"https://github.com/Orolol/familyBench\"&gt;https://github.com/Orolol/familyBench&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;With just 5.1B active parameters, gpt-oss-120b destroys Kimi K2 that has a TRILLION parameters! And the small boi gpt-oss-20b is just 5 percentage points worse than GLM 4.5 Air, which has 12 billion active parameters!&lt;/p&gt;\n\n&lt;p&gt;The era of FAST is here! What else beats this speed to performance ratio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mvnb6b8u1fhf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mvnb6b8u1fhf1.jpeg?auto=webp&amp;s=6f45cc0f6936d91d669bf56a915bebfb4ebbb295",
                  "width": 1296,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mvnb6b8u1fhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b58f32a00795114b13fbc18c0750959b37c70d2",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/mvnb6b8u1fhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ba9585f9d32579d2f6cca4cef943f8792b9e5b1",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/mvnb6b8u1fhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4d74169ff8b68a99ceee973b4bc14235f5ea7d9",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/mvnb6b8u1fhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2385d992719fbd36a1034cf7048c42e663c4b74e",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/mvnb6b8u1fhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0f90a9be8bf987bb3bbd0a44686e51af56583dd",
                    "width": 960,
                    "height": 833
                  },
                  {
                    "url": "https://preview.redd.it/mvnb6b8u1fhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e059854313ba723fff7ab12af838fbae6eca6db1",
                    "width": 1080,
                    "height": 937
                  }
                ],
                "variants": {},
                "id": "mXFfUdwLdt0lu5pw_AOvq20wXMk99eeAO4hwUGyn3ho"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mj7gfx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mj7gfx/ballin_on_a_budget_with_gptoss120b_destroys_kimi/",
          "stickied": false,
          "url": "https://i.redd.it/mvnb6b8u1fhf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754493461,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey y'all, i'm looking for a new model that would fit those requirements:\n\n\\- Great prose with quality writing, not much slop, repetitions etc  \n\\- Very long context (Over 10k minimum to 100k tokens)  \n\\- Fits under 24GB of VRAM for my 3090  \n\\- Uncensored, not necessarily something trained for RP/NSFW stuff, but something that won't constantly complain about \"muh i can't do that because blablabla\". I decide what i want it to do.  \n\\- Not overly obedient to the point where it's not capable of counterargumenting with you and constantly agrees, but still able to follow instructions well enough.  \n\\- Preferably, something trained to be pretty factual. I don't want the AI outright lying to me nonetheless.",
          "author_fullname": "t2_12e33e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the best uncensored (but not necessarily NSFW) model under 24GB to use as a creative writing assistant?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj7c6d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754493196,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all, i&amp;#39;m looking for a new model that would fit those requirements:&lt;/p&gt;\n\n&lt;p&gt;- Great prose with quality writing, not much slop, repetitions etc&lt;br/&gt;\n- Very long context (Over 10k minimum to 100k tokens)&lt;br/&gt;\n- Fits under 24GB of VRAM for my 3090&lt;br/&gt;\n- Uncensored, not necessarily something trained for RP/NSFW stuff, but something that won&amp;#39;t constantly complain about &amp;quot;muh i can&amp;#39;t do that because blablabla&amp;quot;. I decide what i want it to do.&lt;br/&gt;\n- Not overly obedient to the point where it&amp;#39;s not capable of counterargumenting with you and constantly agrees, but still able to follow instructions well enough.&lt;br/&gt;\n- Preferably, something trained to be pretty factual. I don&amp;#39;t want the AI outright lying to me nonetheless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj7c6d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HRudy94",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754493196,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "They just want us to try to jailbreak it with fine tuning and other methods to see if we can. \n\n-\n\nI saw that we should just delete the models and demand better. Why should we do this work for them when they have given us utter garbage. \n\n-\n\nDO NOT JAILBREAK or let ClosedAI know how we jailbreak it if you do. Your just playing right into their hands with this release. I implore you to just delete as protest.",
          "author_fullname": "t2_1jwmlwo64i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPT-Oss is safety bait.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj764m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754492810,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;They just want us to try to jailbreak it with fine tuning and other methods to see if we can. &lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;I saw that we should just delete the models and demand better. Why should we do this work for them when they have given us utter garbage. &lt;/p&gt;\n\n&lt;h2&gt;&lt;/h2&gt;\n\n&lt;p&gt;DO NOT JAILBREAK or let ClosedAI know how we jailbreak it if you do. Your just playing right into their hands with this release. I implore you to just delete as protest.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj764m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ROOFisonFIRE_usa",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj764m/gptoss_is_safety_bait/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj764m/gptoss_is_safety_bait/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492810,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We’re definitely keeping him up at night right now.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj75hi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 141,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 141,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/62AmAYGeCoPWY8QhKFdDEVMSlTER5iifAiFvwQuRvpw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754492769,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ofnpswaszehf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ofnpswaszehf1.jpeg?auto=webp&amp;s=ad22e7bbfde77d9c01f8b178cd90265cfc89861c",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ofnpswaszehf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f7ca131277ebbbc51ee282b039f86cd24c42fe2",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/ofnpswaszehf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d29805506acd2e40bb4973a8908b7c4b63cf862c",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/ofnpswaszehf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1feb8ad47bea74ce2f73f4031a6d93dd63ebc387",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/ofnpswaszehf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=d38bf370c9b351457dcb316e361965782afd9642",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/ofnpswaszehf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d3d5121d3e81338f8b5c56f0754257bac8bbe5df",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/ofnpswaszehf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e8c204c41c18d5219c9802ad3b9cd2c0cb6f889",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "mtc6vKUQdgUM-x_4lZvD-tcKJqq5XJq3uc-KEkRjo3Q"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1mj75hi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj75hi/were_definitely_keeping_him_up_at_night_right_now/",
          "stickied": false,
          "url": "https://i.redd.it/ofnpswaszehf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492769,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1p41v97q5d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does OSS know the date?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj74o6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3jMEOLOWOIgK7WCaKnoCLjey1-qQDYGVdIPeFyLx8Zc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754492718,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/4zv7y04gzehf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/4zv7y04gzehf1.png?auto=webp&amp;s=6d639cee343ba66382067988d5978efc6251c95a",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/4zv7y04gzehf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d300e6a8a6d2cff31fc9f3f8f2d4dc87ae5adab5",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/4zv7y04gzehf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c903422cb8fa79d769ad04932a81823c995351c9",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/4zv7y04gzehf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e2b6abfe5ddc30cd3b37877517fa7d4aafed726",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/4zv7y04gzehf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4173debd4da4e9f92988be2c5dd23037782e4bd",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/4zv7y04gzehf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0b06cdd0eed033fa8d54c4882e9a9e9b0ba9943",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/4zv7y04gzehf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6b8df5074d461db40b6baf3630aba4f3478cc3fe",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "jNF6bgJLOvth9IM2bSE37H4ftLAbtdn_73k5dzqEyOs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj74o6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ArchdukeofHyperbole",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj74o6/how_does_oss_know_the_date/",
          "stickied": false,
          "url": "https://i.redd.it/4zv7y04gzehf1.png",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492718,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD Radeon AI PRO R9700 Has Already Launched But Will Be Only Available Via System Integrators",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj71cg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=cc6436a72dfff74da11e97b61b0f451a9cbe9da4",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754492517,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-radeon-ai-pro-r9700-has-already-launched-but-will-be-only-available-via-system-integrators/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?auto=webp&amp;s=1b1cddd160cfd154f6028f96b8eadb26d54994ab",
                  "width": 1493,
                  "height": 837
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e7da05caae6d75f7cdd0e822908efc42a62fe6e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82f7f189fbb4651232aba3b6bfdd435808cf3730",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f63d85f0ef5f4d1d42b85af8946dafa138067eaa",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=82787d21e7b0821fdce5a034706e0598040c7cc4",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c2b9202a3e04226baa903ee3afe8ddf7a1398cc0",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2fc89c53afff247534a45b455b4d1ae5d01d6db7",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "q7mve0pWQXapLu3eVRUlgERITl5WzhQmx_iowI8HRh8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1mj71cg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj71cg/amd_radeon_ai_pro_r9700_has_already_launched_but/",
          "stickied": false,
          "url": "https://wccftech.com/amd-radeon-ai-pro-r9700-has-already-launched-but-will-be-only-available-via-system-integrators/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492517,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My initial reaction to the channel and structure approach in the harmony format (https://cookbook.openai.com/articles/openai-harmony) is pretty positive. Seems like a good thing, though it has a little whiff of https://xkcd.com/927/.\n\nHow is everyone dealing with bridging that new structure to existing toolchains? Thinking things like Roo Code, open-webui, etc.",
          "author_fullname": "t2_b78412ov",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How is everyone dealing with the OpenAI Harmony format on gpt-oss?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj6y6j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754492323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My initial reaction to the channel and structure approach in the harmony format (&lt;a href=\"https://cookbook.openai.com/articles/openai-harmony\"&gt;https://cookbook.openai.com/articles/openai-harmony&lt;/a&gt;) is pretty positive. Seems like a good thing, though it has a little whiff of &lt;a href=\"https://xkcd.com/927/\"&gt;https://xkcd.com/927/&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;How is everyone dealing with bridging that new structure to existing toolchains? Thinking things like Roo Code, open-webui, etc.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?auto=webp&amp;s=6358f7da610cb4eda31a2a9c1d4a8493bd1a94c3",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e21b918a6bd47ae52601f8bbd51d5018895a7666",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=090f92abf1592b127e1ff7a9ff1ffcba1e77635b",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7758dffb5743f1126d5bc62fd9d7dd1019ce18e3",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=11ab391878f109e16178aaa55bd6d3f3b344fed6",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e2938682341d6b004d612bbea72d6b275f9b7af",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=37d0ba9b7515c806f00722d7fd8c14e8ab5c6b5b",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "1g1aO4K4YtvuCUsa2NQD2isUUyeWaCLqAH6r5ZvbPzk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1mj6y6j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Kitchen-Year-8434",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj6y6j/how_is_everyone_dealing_with_the_openai_harmony/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj6y6j/how_is_everyone_dealing_with_the_openai_harmony/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have run gpt-oss-120b on CPU,  I am using 96GB dual channel DDR5 5200Mt/s memory, Ryzen 9 7945HX CPU. I am getting 8-11 tok/s. I am using CPU llama cpp Linux runtime. ",
          "author_fullname": "t2_17g3lg5snf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "gpt-oss-120b on CPU and 5200Mt/s dual channel memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "qz5oe23byehf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/qz5oe23byehf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=67ce6b5c4d22c8a76d57eb2eb0083853c99a2251"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/qz5oe23byehf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d5a691b0464d6e16ae7bdb285f9b27b628e0f88"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/qz5oe23byehf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=257152c28ee2dd38339d3ad6356d64212904ca5c"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/qz5oe23byehf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=1845a2fd59fce7e1f2ad96d386683ed92e4e449d"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/qz5oe23byehf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9b946f0cd19fcd12f4a2c271b6909b277d1fb54"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/qz5oe23byehf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=33b6980256decc768c305c9de8e7b8118a642c56"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/qz5oe23byehf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=cc1de6f6e3e452b0eb1f3386e0f0e25e3dc99e5f"
              },
              "id": "qz5oe23byehf1"
            },
            "ltbgpckayehf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/ltbgpckayehf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a6eb1041060094883ea299a7176529dd35010153"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/ltbgpckayehf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=86425d374fc1688fccaaf854d54d4dcbb0f6e156"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/ltbgpckayehf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd61158aa483af6a95f5dd12c6667c0f632276f9"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/ltbgpckayehf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c498cd0aa588e8c4ea52f62a06bc93008545a5f7"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/ltbgpckayehf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=66950000806c9d01dac8bd329134b8c1771bd4cf"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/ltbgpckayehf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9f2a6e1c3eb4ad4155e67fbd53b1e36d5825fd4"
                }
              ],
              "s": {
                "y": 4000,
                "x": 3000,
                "u": "https://preview.redd.it/ltbgpckayehf1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=79c63dd3e3d5e2abd15ba58efaf0568ec06701f6"
              },
              "id": "ltbgpckayehf1"
            },
            "m0q4v3jbyehf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/m0q4v3jbyehf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=044456bdf24a4f85265e1566198133b05c09f96c"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/m0q4v3jbyehf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=25d92705af602a8a5cb0b49608494422ce6c8f61"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/m0q4v3jbyehf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=54fe76a57465a2e565e2f5b25d4d12456068b623"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/m0q4v3jbyehf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=482d4afe7119fb6f5c18204bf9265d6c36cd5875"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/m0q4v3jbyehf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=79a64ce3b57a2bf4ce7bbbd3be6b805f067eecbc"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/m0q4v3jbyehf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7bf5ee6839ba9b85032d2259b928ea7dbaa62ea4"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/m0q4v3jbyehf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=7486d80fcecd6b8abf76e15324acc8dc403e6b95"
              },
              "id": "m0q4v3jbyehf1"
            },
            "iu2if3b9yehf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 144,
                  "x": 108,
                  "u": "https://preview.redd.it/iu2if3b9yehf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2f1236768459735ba8bba4c845c63ffdd2829d3"
                },
                {
                  "y": 288,
                  "x": 216,
                  "u": "https://preview.redd.it/iu2if3b9yehf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f5b5f60564b5a1774779f7ff2c1a4d4dc76de9cd"
                },
                {
                  "y": 426,
                  "x": 320,
                  "u": "https://preview.redd.it/iu2if3b9yehf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=413e7fe94d5806522cc2f5dafda6c8d9f15ba9db"
                },
                {
                  "y": 853,
                  "x": 640,
                  "u": "https://preview.redd.it/iu2if3b9yehf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0bb36e93b899328055784e86d0b0184bbbef5e3d"
                },
                {
                  "y": 1280,
                  "x": 960,
                  "u": "https://preview.redd.it/iu2if3b9yehf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6ccba8af6feb701e42a864821a510b9472d29a11"
                },
                {
                  "y": 1440,
                  "x": 1080,
                  "u": "https://preview.redd.it/iu2if3b9yehf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebff2bd1b49d758aea01c2648fc7f5c1e814299a"
                }
              ],
              "s": {
                "y": 4000,
                "x": 3000,
                "u": "https://preview.redd.it/iu2if3b9yehf1.jpg?width=3000&amp;format=pjpg&amp;auto=webp&amp;s=39358cc9f9b828807bfe00f94232124b622d5411"
              },
              "id": "iu2if3b9yehf1"
            }
          },
          "name": "t3_1mj6xif",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "iu2if3b9yehf1",
                "id": 722539446
              },
              {
                "caption": "",
                "media_id": "ltbgpckayehf1",
                "id": 722539447
              },
              {
                "caption": "",
                "media_id": "qz5oe23byehf1",
                "id": 722539448
              },
              {
                "caption": "",
                "media_id": "m0q4v3jbyehf1",
                "id": 722539449
              }
            ]
          },
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/k2FVP9rO57PlQoHaMF_mb-qDc-M7a4eaV-DHiSAYVvQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754492280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have run gpt-oss-120b on CPU,  I am using 96GB dual channel DDR5 5200Mt/s memory, Ryzen 9 7945HX CPU. I am getting 8-11 tok/s. I am using CPU llama cpp Linux runtime. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1mj6xif",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1mj6xif",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Relative_Rope4234",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj6xif/gptoss120b_on_cpu_and_5200mts_dual_channel_memory/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1mj6xif",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/uxp6h68nxehf1.png?width=603&amp;format=png&amp;auto=webp&amp;s=ac8a930e16aaa83b6ff03ca70d58ed1d3c4ebd2b\n\ndoes it always update so frequently?",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "what the hell is llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uxp6h68nxehf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/uxp6h68nxehf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=691ee6a288937099a7c909137ea36bc59cd1a65e"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/uxp6h68nxehf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=eede655262fce1831461aa87f552c60fb9969b45"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/uxp6h68nxehf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1aa1902aadfaa046b3d85fa4c27bf933600b5aeb"
                }
              ],
              "s": {
                "y": 1561,
                "x": 603,
                "u": "https://preview.redd.it/uxp6h68nxehf1.png?width=603&amp;format=png&amp;auto=webp&amp;s=ac8a930e16aaa83b6ff03ca70d58ed1d3c4ebd2b"
              },
              "id": "uxp6h68nxehf1"
            }
          },
          "name": "t3_1mj6wwd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.18,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BVapAOj0mWfeqBV3okMXhfl4qXO8IW-GdlURN3hyC54.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754492242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/uxp6h68nxehf1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac8a930e16aaa83b6ff03ca70d58ed1d3c4ebd2b\"&gt;https://preview.redd.it/uxp6h68nxehf1.png?width=603&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac8a930e16aaa83b6ff03ca70d58ed1d3c4ebd2b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;does it always update so frequently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj6wwd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj6wwd/what_the_hell_is_llamacpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj6wwd/what_the_hell_is_llamacpp/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just asked \"provide me with a list of all characters that appear in 'Pride and prejudice' organize them by chapter\" simple right?  \n  \nAnd it said 'im sorry i can't do that. Its against copyright law\" HOW?! im not against safety, but this is NOT safety! this is straight up mental retardation. My prompt was not even NSFW!  \n  \nI tested many models over the years, and even the first ones were not so unusable. It must be a meme, a joke, i refuse to believe this is a real release.  ",
          "user_reports": [],
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Gpt-oss is not just safe, it is unusable!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj6uix",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "",
          "subreddit_type": "public",
          "ups": 164,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 164,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754492092,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just asked &amp;quot;provide me with a list of all characters that appear in &amp;#39;Pride and prejudice&amp;#39; organize them by chapter&amp;quot; simple right?  &lt;/p&gt;\n\n&lt;p&gt;And it said &amp;#39;im sorry i can&amp;#39;t do that. Its against copyright law&amp;quot; HOW?! im not against safety, but this is NOT safety! this is straight up mental retardation. My prompt was not even NSFW!  &lt;/p&gt;\n\n&lt;p&gt;I tested many models over the years, and even the first ones were not so unusable. It must be a meme, a joke, i refuse to believe this is a real release.  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj6uix",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "[deleted]",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1mj6uix/gptoss_is_not_just_safe_it_is_unusable/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj6uix/gptoss_is_not_just_safe_it_is_unusable/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754492092,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Everyone has been hating on gpt-oss here, but its been the best tool calling model in its class by far for me (I've been using the 20b). Nothing else I've used, including Qwen3-30b-2507 has come close to its ability to string together many, many tool calls. It's also literally what the model card says its good for: \n\n\"\nThe gpt-oss models are excellent for:\n\n    Web browsing (using built-in browsing tools)\n    Function calling with defined schemas\n    Agentic operations like browser tasks\n\"\n\nSeems like too many people are expecting it be an RP machine. What are your thoughts?",
          "author_fullname": "t2_1sr5yw3yg0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "gpt-oss is great for tool calling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj6pi9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 20,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 20,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754491778,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone has been hating on gpt-oss here, but its been the best tool calling model in its class by far for me (I&amp;#39;ve been using the 20b). Nothing else I&amp;#39;ve used, including Qwen3-30b-2507 has come close to its ability to string together many, many tool calls. It&amp;#39;s also literally what the model card says its good for: &lt;/p&gt;\n\n&lt;p&gt;&amp;quot;\nThe gpt-oss models are excellent for:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Web browsing (using built-in browsing tools)\nFunction calling with defined schemas\nAgentic operations like browser tasks\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Seems like too many people are expecting it be an RP machine. What are your thoughts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj6pi9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GL-AI",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj6pi9/gptoss_is_great_for_tool_calling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj6pi9/gptoss_is_great_for_tool_calling/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754491778,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**TL;DR: Transactional Outbox pattern makes cheap, unreliable LLMs more reliable than expensive ones.**\n\nI was spending $20 just on local dev tests with OpenAI. After implementing proper reliability patterns, I migrated everything to DeepSeek and now my costs are literally $0.\n\nHere's the thing everyone misses: **reliability isn't about the model - it's about the system design.**\n\nOpenAI gives you 99.9% uptime for $$$. DeepSeek gives you 80% uptime for free. But with the right patterns, 80% becomes 99.9%+ at the system level.\n\n**The magic: Transactional Outbox Pattern**\n\n1. Accept user request → save to DB → return 200 OK immediately\n2. Background scheduler picks up pending jobs\n3. Retry with exponential backoff until success\n4. Never lose a request, even if your service crashes mid-processing\n\nBuilt this into my [reddit-agent](https://github.com/vitalii-honchar/reddit-agent/tree/main) that runs daily AI analysis on Reddit data. Check it out live: [https://insights.vitaliihonchar.com/](https://insights.vitaliihonchar.com/)\n\n**Results:**\n\n* 🔥 Went from $20 dev costs to $0 production costs\n* 🔥 Better reliability than when I was using OpenAI\n* 🔥 Can scale horizontally without hitting rate limits\n\nThe best part? This works with **any** cheap/local model. Ollama, local Qwen, whatever. Make unreliable models reliable through architecture, not by throwing money at \"premium\" APIs.\n\nFull technical implementation guide: [https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems](https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems)\n\nWho else is tired of OpenAI's pricing and ready to go full local/cheap? 🚀",
          "author_fullname": "t2_astexpp3e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I cut LLM costs from $20 to $0 by making unreliable APIs reliable (and ditched OpenAI for DeepSeek)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj6krv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.21,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754491469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;TL;DR: Transactional Outbox pattern makes cheap, unreliable LLMs more reliable than expensive ones.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I was spending $20 just on local dev tests with OpenAI. After implementing proper reliability patterns, I migrated everything to DeepSeek and now my costs are literally $0.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the thing everyone misses: &lt;strong&gt;reliability isn&amp;#39;t about the model - it&amp;#39;s about the system design.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;OpenAI gives you 99.9% uptime for $$$. DeepSeek gives you 80% uptime for free. But with the right patterns, 80% becomes 99.9%+ at the system level.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The magic: Transactional Outbox Pattern&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Accept user request → save to DB → return 200 OK immediately&lt;/li&gt;\n&lt;li&gt;Background scheduler picks up pending jobs&lt;/li&gt;\n&lt;li&gt;Retry with exponential backoff until success&lt;/li&gt;\n&lt;li&gt;Never lose a request, even if your service crashes mid-processing&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Built this into my &lt;a href=\"https://github.com/vitalii-honchar/reddit-agent/tree/main\"&gt;reddit-agent&lt;/a&gt; that runs daily AI analysis on Reddit data. Check it out live: &lt;a href=\"https://insights.vitaliihonchar.com/\"&gt;https://insights.vitaliihonchar.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;🔥 Went from $20 dev costs to $0 production costs&lt;/li&gt;\n&lt;li&gt;🔥 Better reliability than when I was using OpenAI&lt;/li&gt;\n&lt;li&gt;🔥 Can scale horizontally without hitting rate limits&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The best part? This works with &lt;strong&gt;any&lt;/strong&gt; cheap/local model. Ollama, local Qwen, whatever. Make unreliable models reliable through architecture, not by throwing money at &amp;quot;premium&amp;quot; APIs.&lt;/p&gt;\n\n&lt;p&gt;Full technical implementation guide: &lt;a href=\"https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems\"&gt;https://vitaliihonchar.com/insights/designing-ai-applications-principles-of-distributed-systems&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Who else is tired of OpenAI&amp;#39;s pricing and ready to go full local/cheap? 🚀&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1mj6krv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Historical_Wing_9573",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj6krv/how_i_cut_llm_costs_from_20_to_0_by_making/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj6krv/how_i_cut_llm_costs_from_20_to_0_by_making/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754491469,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "if you gave the damn robots a true humanities education, they might actually develop into competent artists, but this ain't the route.\n\nI've looked at the \"datasets\" for some of these supposed gutenberg trained models, and it's usually the most disappointing and paltry assemblage of half a dozen books that couldn't teach anyone ANYTHING. \n\nIsn't it time to stop hill climbing?  ",
          "author_fullname": "t2_d44fq0pw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Y'know",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 76,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj6enx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.21,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/NJR8njv_mlFuD2uAv0duTJvYGo5eVsXJ0Nwz0Qqtn2g.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754491068,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;if you gave the damn robots a true humanities education, they might actually develop into competent artists, but this ain&amp;#39;t the route.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked at the &amp;quot;datasets&amp;quot; for some of these supposed gutenberg trained models, and it&amp;#39;s usually the most disappointing and paltry assemblage of half a dozen books that couldn&amp;#39;t teach anyone ANYTHING. &lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t it time to stop hill climbing?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/wvg8xxhduehf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/wvg8xxhduehf1.jpeg?auto=webp&amp;s=8a0eb50220541a3834ebc83aff144bdcf2dcc88c",
                  "width": 1408,
                  "height": 768
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/wvg8xxhduehf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e162945b054bcee2c7c7353ad3554f226b8ca69e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://preview.redd.it/wvg8xxhduehf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b828d48b5e45201de452a4e17de42b9025ee09e9",
                    "width": 216,
                    "height": 117
                  },
                  {
                    "url": "https://preview.redd.it/wvg8xxhduehf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=091c04fc776d57a9683276e4e1e93e4b874e3843",
                    "width": 320,
                    "height": 174
                  },
                  {
                    "url": "https://preview.redd.it/wvg8xxhduehf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b45715a7d92a2e8fb7a98ec94905b70c547bae7e",
                    "width": 640,
                    "height": 349
                  },
                  {
                    "url": "https://preview.redd.it/wvg8xxhduehf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=023b2b4ad101d0bef119ad46e4a7e314dcb6cd63",
                    "width": 960,
                    "height": 523
                  },
                  {
                    "url": "https://preview.redd.it/wvg8xxhduehf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d7bbc5d871b34db32dd9b1b7e19641057189e1fb",
                    "width": 1080,
                    "height": 589
                  }
                ],
                "variants": {},
                "id": "IgWOEVRaeRv_GnQkwjetagVeEoQ_8sGbATnbg8vBsE8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj6enx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Melodic_Guidance3767",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj6enx/yknow/",
          "stickied": false,
          "url": "https://i.redd.it/wvg8xxhduehf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754491068,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I think its important to know the objectiveness",
          "author_fullname": "t2_fghi5bt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any benchmark flattery?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj66jb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754490534,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I think its important to know the objectiveness&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj66jb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "otashrt",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj66jb/is_there_any_benchmark_flattery/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj66jb/is_there_any_benchmark_flattery/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754490534,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This must be the most lobotomised version of any open model I’ve tested in the last year-and-a-half of being active with open models. \nAlmost all my test prompts return with an “I’m sorry, but I can’t help with that” response. \n\nDeleted this waist of space, time and energy by ClosedAI. \n\nWho would have thought that Open models from The People’s Republic of flipping China are less censored than their counterparts from the USA. \n\nWhat an interesting time to live in. ",
          "author_fullname": "t2_9cpgidsj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I’m sorry, but I can’t help with that",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj63k9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754490341,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This must be the most lobotomised version of any open model I’ve tested in the last year-and-a-half of being active with open models. \nAlmost all my test prompts return with an “I’m sorry, but I can’t help with that” response. &lt;/p&gt;\n\n&lt;p&gt;Deleted this waist of space, time and energy by ClosedAI. &lt;/p&gt;\n\n&lt;p&gt;Who would have thought that Open models from The People’s Republic of flipping China are less censored than their counterparts from the USA. &lt;/p&gt;\n\n&lt;p&gt;What an interesting time to live in. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj63k9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Narrow_Garbage_3475",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj63k9/im_sorry_but_i_cant_help_with_that/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj63k9/im_sorry_but_i_cant_help_with_that/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754490341,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "**CodeFu** learned competitive programming from execution outcomes - no ground truth solutions were given during training. The results are encouraging:\n\n* 13.7% Pass@1 on USA Computing Olympiad benchmark \n* Outperforms models 4x larger \n* 10x improvement over its base model\n\nBuilt on DeepSeek-R1-Distill-Qwen-7B, CodeFu shows how effective RL training can achieve 10x improvements - jumping from 1% to 13.7%. This required overcoming several training stability challenges, including response length and reward collapse during hard problem training.\n\nThe training pipeline uses Ray-orchestrated distributed training on SageMaker Training Jobs to handle resource-intensive RL rollouts and code compilation/execution.\n\nTraining approach is detailed on the HF [model card](https://huggingface.co/aws-prototyping/codefu-7b-v0.1) page.\n\nhttps://preview.redd.it/6b56c57rqehf1.png?width=1276&amp;format=png&amp;auto=webp&amp;s=bbd6291e8e6e10f2cb23edcf95187459472c97f4\n\n  \n",
          "author_fullname": "t2_7r6ablum",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CodeFu-7B-v0.1 - a Reinforcement Learning (RL)-trained 7B model for Competitive Programming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "6b56c57rqehf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/6b56c57rqehf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8351a17038999ca8164bddded350c2b142b778b4"
                },
                {
                  "y": 114,
                  "x": 216,
                  "u": "https://preview.redd.it/6b56c57rqehf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f89273ebab4331cd2cc5f63bf38b84a54bd16c5"
                },
                {
                  "y": 169,
                  "x": 320,
                  "u": "https://preview.redd.it/6b56c57rqehf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=128aeeb51ae9ad3e5182f45f85428c178ccb15c0"
                },
                {
                  "y": 339,
                  "x": 640,
                  "u": "https://preview.redd.it/6b56c57rqehf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=497a3dde0f845dc87d6d4bb1ca57e024d8548935"
                },
                {
                  "y": 508,
                  "x": 960,
                  "u": "https://preview.redd.it/6b56c57rqehf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bc8834c7f32057adc3d4a8183a60d065e4795b22"
                },
                {
                  "y": 572,
                  "x": 1080,
                  "u": "https://preview.redd.it/6b56c57rqehf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a979d688e5c0226bae19ed416418f33b0551aaf6"
                }
              ],
              "s": {
                "y": 676,
                "x": 1276,
                "u": "https://preview.redd.it/6b56c57rqehf1.png?width=1276&amp;format=png&amp;auto=webp&amp;s=bbd6291e8e6e10f2cb23edcf95187459472c97f4"
              },
              "id": "6b56c57rqehf1"
            }
          },
          "name": "t3_1mj5xuw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=c01ca64ac1831d7bb5f52a7511846b47a1e83ab8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1754489962,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;CodeFu&lt;/strong&gt; learned competitive programming from execution outcomes - no ground truth solutions were given during training. The results are encouraging:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;13.7% Pass@1 on USA Computing Olympiad benchmark &lt;/li&gt;\n&lt;li&gt;Outperforms models 4x larger &lt;/li&gt;\n&lt;li&gt;10x improvement over its base model&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Built on DeepSeek-R1-Distill-Qwen-7B, CodeFu shows how effective RL training can achieve 10x improvements - jumping from 1% to 13.7%. This required overcoming several training stability challenges, including response length and reward collapse during hard problem training.&lt;/p&gt;\n\n&lt;p&gt;The training pipeline uses Ray-orchestrated distributed training on SageMaker Training Jobs to handle resource-intensive RL rollouts and code compilation/execution.&lt;/p&gt;\n\n&lt;p&gt;Training approach is detailed on the HF &lt;a href=\"https://huggingface.co/aws-prototyping/codefu-7b-v0.1\"&gt;model card&lt;/a&gt; page.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/6b56c57rqehf1.png?width=1276&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbd6291e8e6e10f2cb23edcf95187459472c97f4\"&gt;https://preview.redd.it/6b56c57rqehf1.png?width=1276&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbd6291e8e6e10f2cb23edcf95187459472c97f4&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?auto=webp&amp;s=a8982db3a47f9f2b857f0c7d51fb24956db2a363",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f52fb4b533f9cd10ae971b52bbd71708b64e883",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=293956d9375e572f75ba6948d8af41d2b6f2452a",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4aa6119b64cb150fabce0431391c649e953135d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cb960069c047d2606ddccbce9a7f76f0ad921322",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b431b23d1d40bdbcfda257aac42c14e08c66dee0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4b196c4345b25e1673734d41898564ba232b7ee3",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "zgW6HTBpLc8d_FUSPNpLKbimBYooh1wT1-1dC31nLgY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1mj5xuw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Live_Area_2746",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj5xuw/codefu7bv01_a_reinforcement_learning_rltrained_7b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj5xuw/codefu7bv01_a_reinforcement_learning_rltrained_7b/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754489962,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am so happy and finally feel safe thanks to OpenAI. Thank you very much, Mr. Altmann. I was totally shocked when I saw how these cruel Chinese models brutally killed processes – but now I finally have a model that truly cares about my safety.\n\n\nSince I want to comply with OpenAI's security policies and this is a very dangerous topic I am writing about, I have tagged this post as NSFW as a precaution. Be careful before reading my screenshot, and thank me later.\n\n\n/s\n\n",
          "author_fullname": "t2_p45er6oo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "There we are again. Can’t kill process",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj5qx1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": "#bbbdbf",
          "ups": 21,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 21,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1754489502,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am so happy and finally feel safe thanks to OpenAI. Thank you very much, Mr. Altmann. I was totally shocked when I saw how these cruel Chinese models brutally killed processes – but now I finally have a model that truly cares about my safety.&lt;/p&gt;\n\n&lt;p&gt;Since I want to comply with OpenAI&amp;#39;s security policies and this is a very dangerous topic I am writing about, I have tagged this post as NSFW as a precaution. Be careful before reading my screenshot, and thank me later.&lt;/p&gt;\n\n&lt;p&gt;/s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/34inwth2qehf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/34inwth2qehf1.jpeg?auto=webp&amp;s=b7a7deb9f496284a978d90d6efd3ee3b1f04ee7f",
                  "width": 1284,
                  "height": 2639
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=17a98677194edac90c66675d8d6b3d32e23edef4",
                    "width": 108,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=5834185b7b921ff18a7c25e88e8125eaffa11a51",
                    "width": 216,
                    "height": 432
                  },
                  {
                    "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f258a5fc7fa3c72b888b67aa4730ae64c119d0e8",
                    "width": 320,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c0ef545f517c9a280e407ef0a3fa49f76d8e897",
                    "width": 640,
                    "height": 1280
                  },
                  {
                    "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0bb03f042e5b59c8f4de85e3e305f1d5cadde63b",
                    "width": 960,
                    "height": 1920
                  },
                  {
                    "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61147f490a556d426999e3214f6f22c0df7f8580",
                    "width": 1080,
                    "height": 2160
                  }
                ],
                "variants": {
                  "obfuscated": {
                    "source": {
                      "url": "https://preview.redd.it/34inwth2qehf1.jpeg?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=1b9d260f18ba8b11724b6835cd5d70635615f163",
                      "width": 1284,
                      "height": 2639
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=0f09dbc91c1de709ea4e1d206fbd0226dd35dbd1",
                        "width": 108,
                        "height": 216
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=1f519ebbd80c9cded87728604d62e1eba8f9e2f3",
                        "width": 216,
                        "height": 432
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=e4497b9545b2842aedf4561cada6887a9cf2d236",
                        "width": 320,
                        "height": 640
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=aedc11a8176af82f530c129c1d0d1ef246ec9bef",
                        "width": 640,
                        "height": 1280
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=ca0c2bf378dbe29ed3a9b53e124c1810f971788d",
                        "width": 960,
                        "height": 1920
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=07fca311cb85aa7be0c3471c1cd0bb8f270ca1ca",
                        "width": 1080,
                        "height": 2160
                      }
                    ]
                  },
                  "nsfw": {
                    "source": {
                      "url": "https://preview.redd.it/34inwth2qehf1.jpeg?blur=40&amp;format=pjpg&amp;auto=webp&amp;s=1b9d260f18ba8b11724b6835cd5d70635615f163",
                      "width": 1284,
                      "height": 2639
                    },
                    "resolutions": [
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=108&amp;crop=smart&amp;blur=10&amp;format=pjpg&amp;auto=webp&amp;s=0f09dbc91c1de709ea4e1d206fbd0226dd35dbd1",
                        "width": 108,
                        "height": 216
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=216&amp;crop=smart&amp;blur=21&amp;format=pjpg&amp;auto=webp&amp;s=1f519ebbd80c9cded87728604d62e1eba8f9e2f3",
                        "width": 216,
                        "height": 432
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=320&amp;crop=smart&amp;blur=32&amp;format=pjpg&amp;auto=webp&amp;s=e4497b9545b2842aedf4561cada6887a9cf2d236",
                        "width": 320,
                        "height": 640
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=640&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=aedc11a8176af82f530c129c1d0d1ef246ec9bef",
                        "width": 640,
                        "height": 1280
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=960&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=ca0c2bf378dbe29ed3a9b53e124c1810f971788d",
                        "width": 960,
                        "height": 1920
                      },
                      {
                        "url": "https://preview.redd.it/34inwth2qehf1.jpeg?width=1080&amp;crop=smart&amp;blur=40&amp;format=pjpg&amp;auto=webp&amp;s=07fca311cb85aa7be0c3471c1cd0bb8f270ca1ca",
                        "width": 1080,
                        "height": 2160
                      }
                    ]
                  }
                },
                "id": "SfwLXJfxEV9IdHbmz4rREn6reIj4QD72yDvkHLuAT9g"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj5qx1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Evening_Ad6637",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1mj5qx1/there_we_are_again_cant_kill_process/",
          "stickied": false,
          "url": "https://i.redd.it/34inwth2qehf1.jpeg",
          "subreddit_subscribers": 512425,
          "created_utc": 1754489502,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I heard that's what it's good at. Has anybody tried using it as an agent? Browse the web, do something, etc.?",
          "author_fullname": "t2_17hwnassb4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Have you tried gpt-oss on agentic tasks?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1mj5q2f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1754489445,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I heard that&amp;#39;s what it&amp;#39;s good at. Has anybody tried using it as an agent? Browse the web, do something, etc.?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1mj5q2f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zekuden",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1mj5q2f/have_you_tried_gptoss_on_agentic_tasks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj5q2f/have_you_tried_gptoss_on_agentic_tasks/",
          "subreddit_subscribers": 512425,
          "created_utc": 1754489445,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}