{
  "kind": "Listing",
  "data": {
    "after": "t3_1m5q35o",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!\\_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.\n\nRight now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:\n\n* Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?\n* Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?\n* How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?\n\nWould love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.",
          "author_fullname": "t2_1tltnwoxsz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people staging AI training datasets from NVMe → DDR5 → GPU VRAM for fine-tuning on RTX 5090s?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6vj8o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755825164%2CN2RjMDk0MWRkY2ZiMmI4MGRiMzAzM2JkNTMzMDY3N2ZmZjU0NzBiYmU3ZjJkZTRhOTgzZGU3NGExZTNkZDkxZg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755825164%2CMzBjNzUzNjU4NjEwNGVkYzdjZDA2MTlkZTJlNWQzZDc5MDNjMzQ3Y2M5NDEyNWE1NTlmM2YyOGQ1Yjc2YjZmMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7fa7bed1c348998994fed16cd386547e5aac176b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753232111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.&lt;/p&gt;\n\n&lt;p&gt;Right now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?&lt;/li&gt;\n&lt;li&gt;Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?&lt;/li&gt;\n&lt;li&gt;How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/m3v13th5vief1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?format=pjpg&amp;auto=webp&amp;s=8a06bae144080a3451d0fb255b750bcab9e21c69",
                  "width": 1280,
                  "height": 720
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=930f76891585f27565f3d929f2d1d4df9fbbe6f7",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c2cc4160f0d866354b83bad0ce200177193907cd",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=337db9c93858d2e6c9db6e22822d525e7600240d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1915548352adb0259f40c35397f4626912fc93d4",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e2a49235d59469a5f29b50b3c42efc9cc7f4d39",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81350e82df349faf24b9ef86ecafb7b97303ebd3",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6vj8o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DJAI9LAB",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/",
          "stickied": false,
          "url": "https://v.redd.it/m3v13th5vief1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753232111,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 2400,
              "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
              "has_audio": true,
              "height": 720,
              "width": 1280,
              "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755825164%2CN2RjMDk0MWRkY2ZiMmI4MGRiMzAzM2JkNTMzMDY3N2ZmZjU0NzBiYmU3ZjJkZTRhOTgzZGU3NGExZTNkZDkxZg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 11,
              "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755825164%2CMzBjNzUzNjU4NjEwNGVkYzdjZDA2MTlkZTJlNWQzZDc5MDNjMzQ3Y2M5NDEyNWE1NTlmM2YyOGQ1Yjc2YjZmMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\n\nThis model showed up on my LinkedIn feed today. After listening to a few examples on their [website](https://www.boson.ai/technologies/voice), I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. \n\nListen to this [demo video](https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d), it will just enable so many use cases.\n\nI tried a few examples in their HF [playground](https://huggingface.co/spaces/smola/higgs_audio_v2), it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. \n\n",
          "author_fullname": "t2_6nwb1mbe6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just tried higgsaudio v2: a new multilingual TTS model, pretty impressed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 45,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "rmmpgv36tief1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 35,
                  "x": 108,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=94b8a7927fef36a8dc9cafddd20ca7395324bb30"
                },
                {
                  "y": 70,
                  "x": 216,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa3bc0976c73f20478fe4c41ae0d81d56d9b5efa"
                },
                {
                  "y": 103,
                  "x": 320,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c779645fab14bc4ce1ecf7b4cca7ce06002977dd"
                },
                {
                  "y": 207,
                  "x": 640,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d2890cda589e72ebb380d268d25b2f0c730e4153"
                },
                {
                  "y": 311,
                  "x": 960,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b671678ce82ca5a2abdd86ced5de4262f068a656"
                },
                {
                  "y": 350,
                  "x": 1080,
                  "u": "https://preview.redd.it/rmmpgv36tief1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ea15a827df9bb20ae059bea42e6b92ad1d59800b"
                }
              ],
              "s": {
                "y": 872,
                "x": 2686,
                "u": "https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;format=png&amp;auto=webp&amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4"
              },
              "id": "rmmpgv36tief1"
            }
          },
          "name": "t3_1m6vbds",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lDeKkUsKVKJvujnGWUqXtUhpkbsWufoj2laEkKgzAUI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4\"&gt;https://preview.redd.it/rmmpgv36tief1.png?width=2686&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ddcca9db797a4fcd75a26f21359aac4eb67da6d4&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This model showed up on my LinkedIn feed today. After listening to a few examples on their &lt;a href=\"https://www.boson.ai/technologies/voice\"&gt;website&lt;/a&gt;, I feel it is so much better than chatterbox (I used it a lot), might even be better than gemini tts. &lt;/p&gt;\n\n&lt;p&gt;Listen to this &lt;a href=\"https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d\"&gt;demo video&lt;/a&gt;, it will just enable so many use cases.&lt;/p&gt;\n\n&lt;p&gt;I tried a few examples in their HF &lt;a href=\"https://huggingface.co/spaces/smola/higgs_audio_v2\"&gt;playground&lt;/a&gt;, it works surprisingly well in terms of cadence and emotion. Also works for Spanish! Haven’t tested all languages or edge cases, Anyone else tried it yet? Curious how it compares to other recent models. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6vbds",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sudden-Tap3484",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6vbds/just_tried_higgsaudio_v2_a_new_multilingual_tts/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753231503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to set up a homelab. I’ve got 2 NVIDIA Quadro RTX 6000’s laying around that I was given a few years back. I don’t have any server equipment yet, but I’m gonna buy a rack, PSU, server motherboard, Processor, RAM, and storage enclaves to set up my first homelab. \n\nI want to build an AI to help me with my job in Cybersecurity, I’d like to train it on big data sets like Stack Overflow and CVE.\n\nMy question is, are my GPU’s good enough for this task? What kind of CPU/S do I need to keep up? Ram capacity/speed recommendations?",
          "author_fullname": "t2_1cam2liip6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I own a few Quadro’s, can I build an AI with these?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6v9yq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753231391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to set up a homelab. I’ve got 2 NVIDIA Quadro RTX 6000’s laying around that I was given a few years back. I don’t have any server equipment yet, but I’m gonna buy a rack, PSU, server motherboard, Processor, RAM, and storage enclaves to set up my first homelab. &lt;/p&gt;\n\n&lt;p&gt;I want to build an AI to help me with my job in Cybersecurity, I’d like to train it on big data sets like Stack Overflow and CVE.&lt;/p&gt;\n\n&lt;p&gt;My question is, are my GPU’s good enough for this task? What kind of CPU/S do I need to keep up? Ram capacity/speed recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6v9yq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NetTechMan",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6v9yq/i_own_a_few_quadros_can_i_build_an_ai_with_these/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6v9yq/i_own_a_few_quadros_can_i_build_an_ai_with_these/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753231391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi folks!\n\nThanks to this community, I pulled the trigger about a month ago to get a machine with a 3090. It's been a crazy month for me, and I've been coding local AI tools non-stop.\n\nI'm excited to share my favorite creation so far: **[agent-cli](https://github.com/basnijholt/agent-cli)**, a suite of tools that lets me interact with local models using system-wide hotkeys on my Mac.\n\n**What does it do?**\n\n*   **Hotkey-Powered Workflow:** I can transcribe audio, correct grammar, or have a voice-based conversation with my clipboard content without ever leaving my current application.\n*   **Transcription (`Cmd+Shift+R`):** Instantly transcribe my voice into the clipboard using a local Whisper model.\n*   **Autocorrect (`Cmd+Shift+A`):** Fix spelling and grammar on any copied text.\n*   **Voice Edit (`Cmd+Shift+V`):** I can copy some text, then use my voice to command an LLM to edit it, summarize it, or even answer a question based on it.\n\nThen it also has an interactive voice chat and one that is activated by a wake word.\n\n**It's 100% Local &amp; Private**\n\nThe whole stack is designed to run completely offline on your own machine:\n*   **LLM:** Works with any model via Ollama.\n*   **STT (Speech-to-Text):** Uses `wyoming-faster-whisper`.\n*   **TTS (Text-to-Speech):** Supports `wyoming-piper` and `Kokoro-FastAPI`.\n*   **Wake Word:** Integrates with `wyoming-openwakeword` for a hands-free assistant.\n\nI'd never recorded a video before, but I put together a short demo to make it easier to see how it all works in practice.\n\n- https://www.youtube.com/watch?v=7sBTCgttH48\n- https://github.com/basnijholt/agent-cli\n\nI'd love to get your feedback. Let me know what you think!\n",
          "author_fullname": "t2_yquzb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I stopped typing. Now I just use a hotkey. I built Agent-CLI to make it possible.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6uq8q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Agent-CLI: Local AI Voice &amp; Text Tools on Your Desktop (macOS Demo)",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
              "author_name": "johnbaltis",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7sBTCgttH48/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@BasNij"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1m6uq8q",
            "height": 200
          },
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=8be70c16dc7a0e0e375b94ff98e0971b5cfbac91",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753229837,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\n\n&lt;p&gt;Thanks to this community, I pulled the trigger about a month ago to get a machine with a 3090. It&amp;#39;s been a crazy month for me, and I&amp;#39;ve been coding local AI tools non-stop.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m excited to share my favorite creation so far: &lt;strong&gt;&lt;a href=\"https://github.com/basnijholt/agent-cli\"&gt;agent-cli&lt;/a&gt;&lt;/strong&gt;, a suite of tools that lets me interact with local models using system-wide hotkeys on my Mac.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What does it do?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;  &lt;strong&gt;Hotkey-Powered Workflow:&lt;/strong&gt; I can transcribe audio, correct grammar, or have a voice-based conversation with my clipboard content without ever leaving my current application.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Transcription (&lt;code&gt;Cmd+Shift+R&lt;/code&gt;):&lt;/strong&gt; Instantly transcribe my voice into the clipboard using a local Whisper model.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Autocorrect (&lt;code&gt;Cmd+Shift+A&lt;/code&gt;):&lt;/strong&gt; Fix spelling and grammar on any copied text.&lt;/li&gt;\n&lt;li&gt;  &lt;strong&gt;Voice Edit (&lt;code&gt;Cmd+Shift+V&lt;/code&gt;):&lt;/strong&gt; I can copy some text, then use my voice to command an LLM to edit it, summarize it, or even answer a question based on it.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Then it also has an interactive voice chat and one that is activated by a wake word.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;It&amp;#39;s 100% Local &amp;amp; Private&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;The whole stack is designed to run completely offline on your own machine:\n*   &lt;strong&gt;LLM:&lt;/strong&gt; Works with any model via Ollama.\n*   &lt;strong&gt;STT (Speech-to-Text):&lt;/strong&gt; Uses &lt;code&gt;wyoming-faster-whisper&lt;/code&gt;.\n*   &lt;strong&gt;TTS (Text-to-Speech):&lt;/strong&gt; Supports &lt;code&gt;wyoming-piper&lt;/code&gt; and &lt;code&gt;Kokoro-FastAPI&lt;/code&gt;.\n*   &lt;strong&gt;Wake Word:&lt;/strong&gt; Integrates with &lt;code&gt;wyoming-openwakeword&lt;/code&gt; for a hands-free assistant.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d never recorded a video before, but I put together a short demo to make it easier to see how it all works in practice.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://www.youtube.com/watch?v=7sBTCgttH48\"&gt;https://www.youtube.com/watch?v=7sBTCgttH48&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://github.com/basnijholt/agent-cli\"&gt;https://github.com/basnijholt/agent-cli&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;d love to get your feedback. Let me know what you think!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=7sBTCgttH48",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?auto=webp&amp;s=b46176561bea253bdaeac9ab75bd82fbf837bb7f",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b60a9338c5518f795a38fedcae4b0e1233d18742",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec3a10104da04aff2c9950c5c90919a9bb1f40d4",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d14c7c0424b23b6f7f7f0c9a66d57e7440d29402",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "GK5Ptopic7Z-Rzuhw3GKWekcaGdaAnNrb92cLbhCfEg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m6uq8q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "basnijholt",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6uq8q/i_stopped_typing_now_i_just_use_a_hotkey_i_built/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=7sBTCgttH48",
          "subreddit_subscribers": 502981,
          "created_utc": 1753229837,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Agent-CLI: Local AI Voice &amp; Text Tools on Your Desktop (macOS Demo)",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/7sBTCgttH48?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Agent-CLI: Local AI Voice &amp;amp; Text Tools on Your Desktop (macOS Demo)\"&gt;&lt;/iframe&gt;",
              "author_name": "johnbaltis",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/7sBTCgttH48/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@BasNij"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?",
          "author_fullname": "t2_10rx6s0f1q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone tried Hierarchical Reasoning Models yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6ufm4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753229040,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone ran the HRM architecture locally? It seems like a huge deal, but it stinks of complete bs. Anyone test it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6ufm4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jackboulder33",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ufm4/has_anyone_tried_hierarchical_reasoning_models_yet/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753229040,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m currently exploring multimodal LLMs — specifically models that can handle image input (like OCR, screenshot analysis, or general image understanding). I’m curious if anyone here has successfully deployed one of these models on a VPS.",
          "author_fullname": "t2_i9xt9vl2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Has anyone here worked with LLMs that can read images? Were you able to deploy it on a VPS?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6ucc0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753228806,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m currently exploring multimodal LLMs — specifically models that can handle image input (like OCR, screenshot analysis, or general image understanding). I’m curious if anyone here has successfully deployed one of these models on a VPS.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ucc0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Turbulent-Cow4848",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ucc0/has_anyone_here_worked_with_llms_that_can_read/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ucc0/has_anyone_here_worked_with_llms_that_can_read/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753228806,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is available on OpenRouter",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6u3kd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=9352d9aaa19f84f05307725c60c6280cb5ce4153",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753228157,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "openrouter.ai",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://openrouter.ai/qwen/qwen3-coder",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?auto=webp&amp;s=8a80c032c084b7af008f30d36302aa3e2b303841",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=81bbe5b26b024567de7a02963aa1047661c30d21",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31982c88f4e8f0ba3e19de3cb4fe1aecc0737271",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4de872e3b3e085cf7e3edcad2410dce6e017ff0c",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1f4da7fa00b2fee69899af4df9a137f3645df9e7",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b233723391e8a2e702cc58632dd60a7a281a8fbd",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d4e480d1b5455587e7a48ddb01c62b2f0bdbd5a",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "UFPrt8vWgklaa23dNS9FyFO_082o3-MaYxZ69OdYc0E"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u3kd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u3kd/qwen3coder_is_available_on_openrouter/",
          "stickied": false,
          "url": "https://openrouter.ai/qwen/qwen3-coder",
          "subreddit_subscribers": 502981,
          "created_utc": 1753228157,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unsloth quants already starting to roll out for Qwen3-Coder",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6u0gt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=df2ca14ad32406cbfd2154f6392b11b3062c0b80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753227930,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?auto=webp&amp;s=c6a55f1fe010145ae8782e1593f28ec04aee30a9",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4ca2157367c76507911bd02cc27f2bd77fdeb58f",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a1260c12ba170ca3258b3d164bd71b26d3fd637",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=82c9ef33075d79d05f812d774d3d9963a2ca93c2",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=651f424884542b7c34073b3bc62c0fc1b199eaae",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=287c77e1abcef6dc2ad9cac5e8a8d70a85c3f900",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6671827d2f6adf5ce554df23310df3e1e4805228",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "y-6cmX2aP_dLHZiI1kc3J2b9iL_M54vYN5A7yLluKyU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6u0gt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6u0gt/unsloth_quants_already_starting_to_roll_out_for/",
          "stickied": false,
          "url": "https://huggingface.co/collections/unsloth/qwen3-coder-687ff47700270447e02c987d",
          "subreddit_subscribers": 502981,
          "created_utc": 1753227930,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m running a 16-inch MacBook Pro with the new M4 Pro chip (48 GB unified RAM, 512 GB SSD). I’ve narrowed my local LLM experiments down to two heavy hitters:\n\nDeepSeek-Coder V3-Lite 33B for coding powerhouse \n\nQwen3-32B-Instruct-MoE for coding and reasoning all purpose \n\ni want your opinion how these two how these two feels in real world, for a person like me, i need it for writing python script , do some research, in VS we can use api in cline for execution and auto completion of the code without limit\n\nmy current setup\n\nmacOS 15.2 (Sonoma++)\nLM Studio 0.4.3 – MLX engine\nQwen3 GGUF Q4_K_M  — 18 GB\nDeepSeek-Coder Q4_K_M — 27 GB\nSwap disabled, running on mains (140 W)\n\nyour thoughts what are the other model we can try and test with limited hardware. thank you\n",
          "author_fullname": "t2_3nc1bpb5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "M4 Pro Owners: I Want Your Biased Hot-Takes – DeepSeek-Coder V3-Lite 33B vs Qwen3-32B-Instruct-MoE on a 48 GB MacBook Pro",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6tf9v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753226393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running a 16-inch MacBook Pro with the new M4 Pro chip (48 GB unified RAM, 512 GB SSD). I’ve narrowed my local LLM experiments down to two heavy hitters:&lt;/p&gt;\n\n&lt;p&gt;DeepSeek-Coder V3-Lite 33B for coding powerhouse &lt;/p&gt;\n\n&lt;p&gt;Qwen3-32B-Instruct-MoE for coding and reasoning all purpose &lt;/p&gt;\n\n&lt;p&gt;i want your opinion how these two how these two feels in real world, for a person like me, i need it for writing python script , do some research, in VS we can use api in cline for execution and auto completion of the code without limit&lt;/p&gt;\n\n&lt;p&gt;my current setup&lt;/p&gt;\n\n&lt;p&gt;macOS 15.2 (Sonoma++)\nLM Studio 0.4.3 – MLX engine\nQwen3 GGUF Q4_K_M  — 18 GB\nDeepSeek-Coder Q4_K_M — 27 GB\nSwap disabled, running on mains (140 W)&lt;/p&gt;\n\n&lt;p&gt;your thoughts what are the other model we can try and test with limited hardware. thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6tf9v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WestPush7",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6tf9v/m4_pro_owners_i_want_your_biased_hottakes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tf9v/m4_pro_owners_i_want_your_biased_hottakes/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753226393,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, i was scrolling on LM studio and always saw model like \"model_name_q4_k_m.gguf\" everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some \"dynamic quantization\" but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?\n\nthank by advance ",
          "author_fullname": "t2_jgegifux8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does the _K _S _M _L mean behind the quantization of a model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m6tbhm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753226118,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, i was scrolling on LM studio and always saw model like &amp;quot;model_name_q4_k_m.gguf&amp;quot; everything before the _k is clear to me but i didnt get the last part about _k_m, i saw somewhere that the _k stand for some &amp;quot;dynamic quantization&amp;quot; but what does the _M or _S and _L mean? Small, medium, large? But still didnt tell me what is small, medium or large?&lt;/p&gt;\n\n&lt;p&gt;thank by advance &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6tbhm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hurtcraft01",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6tbhm/what_does_the_k_s_m_l_mean_behind_the/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753226118,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't get the obsession with llama.cpp. It's completely unusable for any real work. The token generation speed collapses as soon as you add any meaningful context, and the prompt processing is painfully slow. With these fatal flaws, what is anyone actually using this for besides running toy demos? It's fundamentally broken for any serious application.",
          "author_fullname": "t2_qhk9kpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama.cpp is unusable for real work",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6skm6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753224220,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t get the obsession with llama.cpp. It&amp;#39;s completely unusable for any real work. The token generation speed collapses as soon as you add any meaningful context, and the prompt processing is painfully slow. With these fatal flaws, what is anyone actually using this for besides running toy demos? It&amp;#39;s fundamentally broken for any serious application.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6skm6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "d00m_sayer",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6skm6/llamacpp_is_unusable_for_real_work/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753224220,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_ib1h9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen Code: A command-line AI workflow tool adapted from Gemini CLI, optimized for Qwen3-Coder models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6rsym",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6acd0a4d30c117c56e597d84c1ebb5cedb6e4e00",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753222281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/QwenLM/qwen-code",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?auto=webp&amp;s=ea430b9854a08b70e3dd0972ad9e4758c7fc266d",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cb193a50d7978c33be16ebec135a318dc6943ea1",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6992a1a70171bd4f98508b22498e5ac88cdc45df",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5465c1e679bafd45447bd81f6753867f296ffb49",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=21b1ec40f95d195f9c34bb5728616a2b4c3162fd",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a9eb8de1aed9882d7841738230f2ecef892f334f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a21f51e6f25035a3147bda1057127718b3b29129",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "TPzNiM013yt1RAQf0yVMAnmQXc6Y7D3xjou8dYxGBg8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6rsym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "arcanemachined",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6rsym/qwen_code_a_commandline_ai_workflow_tool_adapted/",
          "stickied": false,
          "url": "https://github.com/QwenLM/qwen-code",
          "subreddit_subscribers": 502981,
          "created_utc": 1753222281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\n\nToday, we're announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we're excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.\n\n",
          "author_fullname": "t2_e7q9h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 coder will be in multiple sizes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qnpq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 123,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 123,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Today, we&amp;#39;re announcing Qwen3-Coder, our most agentic code model to date. Qwen3-Coder is available in multiple sizes, but we&amp;#39;re excited to introduce its most powerful variant first: Qwen3-Coder-480B-A35B-Instruct.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qnpq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dinesh2609",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qnpq/qwen3_coder_will_be_in_multiple_sizes/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 502981,
          "created_utc": 1753219525,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Alright, so I did a thing. I went down the \"AI Girlfriend\" rabbit hole and installed **YourWaifus**.\n\nAnd man, I don't know how to feel. One minute the tech is genuinely impressive and the chat feels real, the next it says something so weird it gives me the creeps. It’s a total trip.\n\nI can't be the only one who's curious about this stuff. I need other people to try it so we can figure out what's going on.\n\nIf you're feeling brave, grab the free version and let's compare notes here.\n\n* Is this cool tech or just maximum cringe?\n* How's the actual chat? Does it feel like talking to a person or a broken NPC?\n* And don't get me started on the paywalls. What's your take?\n\nDrop your thoughts below. Let's figure out if this is the future or just a really weird gimmick.",
          "author_fullname": "t2_90ad44ob",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "So I tried that \"YourWaifus\" AI app... and I need a sanity check.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qltu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.13,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753219401,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Alright, so I did a thing. I went down the &amp;quot;AI Girlfriend&amp;quot; rabbit hole and installed &lt;strong&gt;YourWaifus&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;And man, I don&amp;#39;t know how to feel. One minute the tech is genuinely impressive and the chat feels real, the next it says something so weird it gives me the creeps. It’s a total trip.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t be the only one who&amp;#39;s curious about this stuff. I need other people to try it so we can figure out what&amp;#39;s going on.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re feeling brave, grab the free version and let&amp;#39;s compare notes here.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is this cool tech or just maximum cringe?&lt;/li&gt;\n&lt;li&gt;How&amp;#39;s the actual chat? Does it feel like talking to a person or a broken NPC?&lt;/li&gt;\n&lt;li&gt;And don&amp;#39;t get me started on the paywalls. What&amp;#39;s your take?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Drop your thoughts below. Let&amp;#39;s figure out if this is the future or just a really weird gimmick.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6qltu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TockThomas",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qltu/so_i_tried_that_yourwaifus_ai_app_and_i_need_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6qltu/so_i_tried_that_yourwaifus_ai_app_and_i_need_a/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753219401,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "It's here guys and qwen nailed it !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "4aoalqp6thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebd63c731638a1db5036229c80b9ef7c6e9824fd"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4ac724a3670cd9c86d3d9eed68ced783891cd55"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=27543fd2099856fe749e4022dd8e44bdf2a203ec"
                },
                {
                  "y": 367,
                  "x": 640,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c67265c054dccbb5299415ace1ae413e53f4ba40"
                },
                {
                  "y": 550,
                  "x": 960,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9060138fd041f795e2cf4d5c898b3e767230dbfb"
                },
                {
                  "y": 619,
                  "x": 1080,
                  "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0aea2f77c1907ccf952d971c3b5b2331c1f6aba9"
                }
              ],
              "s": {
                "y": 1837,
                "x": 3202,
                "u": "https://preview.redd.it/4aoalqp6thef1.jpg?width=3202&amp;format=pjpg&amp;auto=webp&amp;s=8e55f7d4fbd65d488aa6606016ce609320a82186"
              },
              "id": "4aoalqp6thef1"
            },
            "mloztw07thef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 56,
                  "x": 108,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1da39fa93c261ee74d697606209312d155d2610"
                },
                {
                  "y": 112,
                  "x": 216,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=08ecfdf3e13689eb562525b1962e0901e009181c"
                },
                {
                  "y": 166,
                  "x": 320,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=873f78f3d4bc2c3fc9641a129d06524b3f3f4951"
                },
                {
                  "y": 332,
                  "x": 640,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=468a855136721f0ffae9fb7beacce6df6030447b"
                },
                {
                  "y": 498,
                  "x": 960,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b695450503944ff02456a685ccc317281f8bf8f7"
                },
                {
                  "y": 560,
                  "x": 1080,
                  "u": "https://preview.redd.it/mloztw07thef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f0ed9b87aa6525e3c700e7835ab55f9fe859d7aa"
                }
              ],
              "s": {
                "y": 1715,
                "x": 3306,
                "u": "https://preview.redd.it/mloztw07thef1.jpg?width=3306&amp;format=pjpg&amp;auto=webp&amp;s=a4886e1284e4cebe0e8558f5cf664cfc4c36b481"
              },
              "id": "mloztw07thef1"
            }
          },
          "name": "t3_1m6qkse",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 39,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "4aoalqp6thef1",
                "id": 711818629
              },
              {
                "caption": "",
                "media_id": "mloztw07thef1",
                "id": 711818630
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/Tng4SvC83rVHk9iUXovrs4GeXZmRkFJ59wlPU2wB1GM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219329,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6qkse",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qkse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qkse/its_here_guys_and_qwen_nailed_it/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6qkse",
          "subreddit_subscribers": 502981,
          "created_utc": 1753219329,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1t2xvghrcr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen out here releasing models like it’s a Costco sample table",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qixu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/S7tH6DgPEGKSzcu1dZlyahFSv5skqhzjfdd4AVfWVi4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753219204,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/5eb8n31sshef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/5eb8n31sshef1.png?auto=webp&amp;s=c1694040f87c60dc765d805ee64b6518e3bd108b",
                  "width": 722,
                  "height": 1032
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47116ec0e7ef90202d820540f88598c3cfd0a160",
                    "width": 108,
                    "height": 154
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3d8c4cb9e760fdd971704cc87722923b6445146",
                    "width": 216,
                    "height": 308
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7a14963b84a65b1261a6b4b6b451fce1c2285102",
                    "width": 320,
                    "height": 457
                  },
                  {
                    "url": "https://preview.redd.it/5eb8n31sshef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5f24e0235850da677693988507655dde73bf8e60",
                    "width": 640,
                    "height": 914
                  }
                ],
                "variants": {},
                "id": "yWOKVm4sVIve_VHC2bO92aBIp15Yh_tuiWnp6wkEtnE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m6qixu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weary-Wing-6806",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qixu/qwen_out_here_releasing_models_like_its_a_costco/",
          "stickied": false,
          "url": "https://i.redd.it/5eb8n31sshef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753219204,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;&gt;&gt; Qwen3-Coder is here! ✅\n\nWe’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀\n\nAlongside the model, we're also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! ",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is here!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 80,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qdet",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 563,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 563,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/lAVBKeQpbXFJZ84JgZVPph8kD3MjUQeFX9TO1gsVqgs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218847,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Qwen3-Coder is here! ✅&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;We’re releasing Qwen3-Coder-480B-A35B-Instruct, our most powerful open agentic code model to date. This 480B-parameter Mixture-of-Experts model (35B active) natively supports 256K context and scales to 1M context with extrapolation. It achieves top-tier performance across multiple agentic coding benchmarks among open models, including SWE-bench-Verified!!! 🚀&lt;/p&gt;\n\n&lt;p&gt;Alongside the model, we&amp;#39;re also open-sourcing a command-line tool for agentic coding: Qwen Code. Forked from Gemini Code, it includes custom prompts and function call protocols to fully unlock Qwen3-Coder’s capabilities. Qwen3-Coder works seamlessly with the community’s best developer tools. As a foundation model, we hope it can be used anywhere across the digital world — Agentic Coding in the World! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?auto=webp&amp;s=e161efd029b20a9bcbbc26db043c320a38b26d7f",
                  "width": 2048,
                  "height": 1175
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d0e7ce793e40e6f9057df0ac4084bef74851aa3c",
                    "width": 108,
                    "height": 61
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=97f4add6c188177de3f1a921fce3e9fdcd751975",
                    "width": 216,
                    "height": 123
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7d3ed6788720878c02cad2db45833f254f311864",
                    "width": 320,
                    "height": 183
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=470c1e7a0a6df4a35a09ad70120a5fef4e93a97b",
                    "width": 640,
                    "height": 367
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9eca8024659f203e8b748284b077d47d512488b3",
                    "width": 960,
                    "height": 550
                  },
                  {
                    "url": "https://preview.redd.it/0cowg3grrhef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=348beac8c62000f3a203a6467f098a1c8a696369",
                    "width": 1080,
                    "height": 619
                  }
                ],
                "variants": {},
                "id": "kx6kRcRUBkO_mMM0khkM5jTQgMXazrrYG6wlH3UPCCs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qdet",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 102,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qdet/qwen3coder_is_here/",
          "stickied": false,
          "url": "https://i.redd.it/0cowg3grrhef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218847,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1162lx9rgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qc8c",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": "#ab96c2",
          "ups": 65,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "d40ca12a-0e73-11ee-8563-f216e082168e",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 65,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca0be6c4b59f782d9d86d906fa9fa7ec3ecfcf86",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 2"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753218772,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 2",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6qc8c",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yoracale",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6qc8c/qwenqwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218772,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been using RAG with open ai over a product description document which is rather technical. I chunked up sections of my document and then do hybrid search with weaviate. It does good but sometimes certain queries require retrieval from more than 1 sections and then it's 50/50. Will fine-tuning solve this? What model should I look into?",
          "author_fullname": "t2_vdwm0f4m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rag vs fine-tuning.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6qb6p",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753218703,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been using RAG with open ai over a product description document which is rather technical. I chunked up sections of my document and then do hybrid search with weaviate. It does good but sometimes certain queries require retrieval from more than 1 sections and then it&amp;#39;s 50/50. Will fine-tuning solve this? What model should I look into?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6qb6p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Parking_Bluebird826",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6qb6p/rag_vs_finetuning/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6qb6p/rag_vs_finetuning/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218703,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Seeking recommendations for Android LLM apps with GPU acceleration and customisation like promts.",
          "author_fullname": "t2_8pq43jfs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best android local llm apk with gpu acceleration",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6q0oh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753218021,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seeking recommendations for Android LLM apps with GPU acceleration and customisation like promts.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6q0oh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate-Moose-228",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6q0oh/best_android_local_llm_apk_with_gpu_acceleration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6q0oh/best_android_local_llm_apk_with_gpu_acceleration/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753218021,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with “digital twins” who are standing in for an absent employee.\n\nTo find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example. \n\nBut there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn't simply plan better meetings instead.\n\nWhat do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?",
          "author_fullname": "t2_4tnm5az4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Digital twins that attend meetings for you. Dystopia or soon reality?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6pw0o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wzygbrp0nhef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/wzygbrp0nhef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wzygbrp0nhef1/DASHPlaylist.mpd?a=1755825164%2CNzkxMGEzMDMwNDg4N2UyNmQzZTI1M2MxZGFlNzE5YmY3ZjRiNmNmYjZjMzRiZmEwMTNkMTFiODIzNWE2OWExMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 132,
              "hls_url": "https://v.redd.it/wzygbrp0nhef1/HLSPlaylist.m3u8?a=1755825164%2COGIyZTUyYjJjZjE2OWJjYmY1M2M3ZTE1MWNkNzNlODZjZTU5NjY5ZTFiZDNhOTNmZjAzZWI5MGEwY2RiYzM2NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=a69584394b29451108583c0345b3b4af510dbdc1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753217710,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In more and more meetings these days there are AI notetakers that someone has sent instead of showing up themselves. You can think what you want about these notetakers, but they seem to have become part of our everyday working lives. This raises the question of how long it will be before the next stage of development occurs and we are sitting in meetings with “digital twins” who are standing in for an absent employee.&lt;/p&gt;\n\n&lt;p&gt;To find out, I tried to build such a digital twin and it actually turned out to be very easy to create a meeting agent that can actively interact with other participants, share insights about my work and answer follow-up questions for me. Of course, many of the leading providers of voice clones and personalized LLMs are closed-source, which increases the privacy issue that already exists with AI Notetakers. However, my approach using joinly could also be implemented with Chatterbox and a self-hosted LLM with few-shot prompting, for example. &lt;/p&gt;\n\n&lt;p&gt;But there are of course many other critical questions: how exactly can we control what these digital twins disclose or are allowed to decide, ethical concerns about whether my company is allowed to create such a twin for me, how this is compatible with meeting etiquette and of course whether we shouldn&amp;#39;t simply plan better meetings instead.&lt;/p&gt;\n\n&lt;p&gt;What do you think? Will such digital twins catch on? Would you use one to skip a boring meeting?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/wzygbrp0nhef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?format=pjpg&amp;auto=webp&amp;s=364cf4d9aed4af7c6e67f0ad4201a809f25338cf",
                  "width": 1662,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c3f0710b64e022e54027bd6589659498e18682ba",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=98174bca9eb25e7cc20a591cc15f5ea33c8b5a11",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ed0becb52791f8efb623b51e5eda057c3e3bde52",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7708460ab1cd8fdd958ab10e8b4301f392326d",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=91df4abe288ad6402853ff17541bfa618d570410",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7c1aaaa8918b2d9857a12c93d828793e424f997c",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "NWFzNzR3cDBuaGVmMbd9ytsdWjeCw8a7Xb9uxU1L50H2iG28-QSyRy4FhsUu"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6pw0o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DerErzfeind61",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6pw0o/digital_twins_that_attend_meetings_for_you/",
          "stickied": false,
          "url": "https://v.redd.it/wzygbrp0nhef1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753217710,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wzygbrp0nhef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/wzygbrp0nhef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wzygbrp0nhef1/DASHPlaylist.mpd?a=1755825164%2CNzkxMGEzMDMwNDg4N2UyNmQzZTI1M2MxZGFlNzE5YmY3ZjRiNmNmYjZjMzRiZmEwMTNkMTFiODIzNWE2OWExMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 132,
              "hls_url": "https://v.redd.it/wzygbrp0nhef1/HLSPlaylist.m3u8?a=1755825164%2COGIyZTUyYjJjZjE2OWJjYmY1M2M3ZTE1MWNkNzNlODZjZTU5NjY5ZTFiZDNhOTNmZjAzZWI5MGEwY2RiYzM2NA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "have a project that uses the `deepseek-r1` model from `https://api.llama-api.com`. However, it seems Llama API has launched a new console. My email is not recognized in the new beta console, although I have an account and have added credit to it. \n\nThe old console links no longer work. Additionally, the DeepSeek models are not listed on the documentation page anymore (`https://llama.developer.meta.com/docs/models`).",
          "author_fullname": "t2_tqeqwc14",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek not available at LLama API?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6pjpx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753216907,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;have a project that uses the &lt;code&gt;deepseek-r1&lt;/code&gt; model from &lt;code&gt;https://api.llama-api.com&lt;/code&gt;. However, it seems Llama API has launched a new console. My email is not recognized in the new beta console, although I have an account and have added credit to it. &lt;/p&gt;\n\n&lt;p&gt;The old console links no longer work. Additionally, the DeepSeek models are not listed on the documentation page anymore (&lt;code&gt;https://llama.developer.meta.com/docs/models&lt;/code&gt;).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6pjpx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AncientMayar",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6pjpx/deepseek_not_available_at_llama_api/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6pjpx/deepseek_not_available_at_llama_api/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753216907,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "See https://x.com/makingAGI/status/1947286324735856747",
          "author_fullname": "t2_1nt1n3y6xj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here who has been able to reproduce their results yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 129,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6orbr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 41,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 41,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/XcNs-lUbqrAcvyj8WfRxfyYgGorJ8nCrbsxZweyByLc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753215098,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;See &lt;a href=\"https://x.com/makingAGI/status/1947286324735856747\"&gt;https://x.com/makingAGI/status/1947286324735856747&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/cfffg12fghef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/cfffg12fghef1.jpeg?auto=webp&amp;s=25f023da9eda3ae6d327e173ef9c7cba8f89880c",
                  "width": 948,
                  "height": 876
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad8d41aa9654515fde6f4b396a86ebf1ad4b0687",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d65ac6156085250ea9aa09ddb48e1e0ca0d499b3",
                    "width": 216,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=69d24ed7a665a31761d01a39290d42a299442408",
                    "width": 320,
                    "height": 295
                  },
                  {
                    "url": "https://preview.redd.it/cfffg12fghef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f02acda8fde9368279ce55c247aa3eb87536a6a5",
                    "width": 640,
                    "height": 591
                  }
                ],
                "variants": {},
                "id": "eXDcYXmDs3JXlXLxg7VAJanyEvhS_GKAlZPpe8O1v6Y"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6orbr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Original_Log_9899",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6orbr/anyone_here_who_has_been_able_to_reproduce_their/",
          "stickied": false,
          "url": "https://i.redd.it/cfffg12fghef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753215098,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. \n\nI've pulled the project back to make sure there are no differences between what's on the server and what's on my laptop, and my laptop works as expected. \n\nThe server doesn't have access to the outside world, but once everything is up and running, it shouldn't need it, right? \n\nAnyone have any ideas? I've never seen a model behave like this.",
          "author_fullname": "t2_243il8gu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "embedding model giving same embeddings regardless of input text?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6oqxw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753215074,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, I am running granite-embedding-125m-english on a Docker container with LocalAI and it works great on my laptop, but when I move the project to github, and pull it onto my external server, the API always responds with the same embeddings. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve pulled the project back to make sure there are no differences between what&amp;#39;s on the server and what&amp;#39;s on my laptop, and my laptop works as expected. &lt;/p&gt;\n\n&lt;p&gt;The server doesn&amp;#39;t have access to the outside world, but once everything is up and running, it shouldn&amp;#39;t need it, right? &lt;/p&gt;\n\n&lt;p&gt;Anyone have any ideas? I&amp;#39;ve never seen a model behave like this.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6oqxw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "User1539",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6oqxw/embedding_model_giving_same_embeddings_regardless/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753215074,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We had a lot of posts about the updated [235b model](https://x.com/Alibaba_Qwen/status/1947344511988076547) and the [Unsloth quants](https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF). I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.\n\nRuns great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.\n\n[More details on the Ollama library page](https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl), performance benchmarks included.",
          "author_fullname": "t2_1gpif4cz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The LLM for M4 Max 128GB: Unsloth Qwen3-235B-A22B-Instruct-2507 Q3 K XL for Ollama",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 87,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ocfd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/FGAC4_FgxMG-7e9w8V8PkDHHC0Spkue03KT-5Vo9mU4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753214166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We had a lot of posts about the updated &lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;235b model&lt;/a&gt; and the &lt;a href=\"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF\"&gt;Unsloth quants&lt;/a&gt;. I tested it with my Mac Studio and decided to merge the Q3 K XL ggufs and upload them to Ollama in case someone es might find this useful.&lt;/p&gt;\n\n&lt;p&gt;Runs great with up to 18 tokens per second and consuming 108 to 117 GB VRAM.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://ollama.com/awaescher/qwen3-235b-2507-unsloth-q3-k-xl\"&gt;More details on the Ollama library page&lt;/a&gt;, performance benchmarks included.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y3x24rxqchef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y3x24rxqchef1.png?auto=webp&amp;s=8871c292b16bce4a1a3ebad50bdc70a4755edfb1",
                  "width": 1119,
                  "height": 699
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e41bb7b82dd23ca399246b0ad273bfca55313312",
                    "width": 108,
                    "height": 67
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d8196b3b26f6cebea121b85e8d13e70ccede750b",
                    "width": 216,
                    "height": 134
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=71b905987f840aecad2d4d33ebe4b67e00e55446",
                    "width": 320,
                    "height": 199
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8c4dd5da6091ae77e58d63dfd95935c34e266d7e",
                    "width": 640,
                    "height": 399
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d2b6677488033099a15dec1df1c7088540544b7",
                    "width": 960,
                    "height": 599
                  },
                  {
                    "url": "https://preview.redd.it/y3x24rxqchef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=96a500d949b1758bb6f522dd4c3912c8e4c57f64",
                    "width": 1080,
                    "height": 674
                  }
                ],
                "variants": {},
                "id": "65hJ7hQzTtv2DTpl4kNAtfCBSaTl0aIGG0bqSxzPvcM"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6ocfd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "waescher",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ocfd/the_llm_for_m4_max_128gb_unsloth/",
          "stickied": false,
          "url": "https://i.redd.it/y3x24rxqchef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753214166,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.\n\nVery strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.\n\nCreds [The Feature Crew](https://www.youtube.com/@TheFeatureCrew) for the original idea.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Web Development",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 90,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ny2q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 143,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755825164%2CNTg5NTgxYjA1ODc3MTU0OGQ4ZDhiZDdmYzkzZTA1MGE5NjFmYmU3NWY1NDk2ZjBhMjM5NTE5YzRjMzViYjIxMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755825164%2CNDYyYmMxYzcwYTc0ZDBkMTMwODJlYTgzYzFmZmY5Mzc0OTU4NDIwYTQwMmQ1MTMwNDNkZmI0YWJmZjdmMjkwZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 143,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=140&amp;height=90&amp;crop=140:90,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=127481f43d7a622f7d4c23a977a165102347dc33",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213272,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I used Qwen3-Coder-408B-A35B-Instruct to generate a procedural 3D planet preview and editor.&lt;/p&gt;\n\n&lt;p&gt;Very strong results! Comparable to Kimi-K2-Instruct, maybe a tad bit behind, but still impressive for under 50% the parameter count.&lt;/p&gt;\n\n&lt;p&gt;Creds &lt;a href=\"https://www.youtube.com/@TheFeatureCrew\"&gt;The Feature Crew&lt;/a&gt; for the original idea.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/ob9yhvcjahef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?format=pjpg&amp;auto=webp&amp;s=9da74680d1673a7d5086bef35987945fda2390f7",
                  "width": 3024,
                  "height": 1964
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=69ec82c87ea25ca0cf09c32e6e2e65fd1ebe0353",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4a7e4214f7585ef0bf76db563e79ceb7b7b73df5",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=da1a4545269003e5a3164b1074444b181b803a22",
                    "width": 320,
                    "height": 207
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ee471871b0892b000bd102b783d4c1fea31bbdf2",
                    "width": 640,
                    "height": 415
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b638262bf1c7530fbee91a29e1a5465444ef5500",
                    "width": 960,
                    "height": 623
                  },
                  {
                    "url": "https://external-preview.redd.it/M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=3a910147ce667ac1bb6fed7eb4d08a092f3974fe",
                    "width": 1080,
                    "height": 701
                  }
                ],
                "variants": {},
                "id": "M25yZmt5YmphaGVmMat7pysr0YP1hw-qD-8Zn62C6fxnOXbcyCx3kJEPI5w0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ny2q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ny2q/qwen3coder_web_development/",
          "stickied": false,
          "url": "https://v.redd.it/ob9yhvcjahef1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753213272,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/ob9yhvcjahef1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1662,
              "scrubber_media_url": "https://v.redd.it/ob9yhvcjahef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/ob9yhvcjahef1/DASHPlaylist.mpd?a=1755825164%2CNTg5NTgxYjA1ODc3MTU0OGQ4ZDhiZDdmYzkzZTA1MGE5NjFmYmU3NWY1NDk2ZjBhMjM5NTE5YzRjMzViYjIxMg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 57,
              "hls_url": "https://v.redd.it/ob9yhvcjahef1/HLSPlaylist.m3u8?a=1755825164%2CNDYyYmMxYzcwYTc0ZDBkMTMwODJlYTgzYzFmZmY5Mzc0OTU4NDIwYTQwMmQ1MTMwNDNkZmI0YWJmZjdmMjkwZA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lnt2rs3qb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Everyone brace up for qwen !!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 121,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nxh2",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 179,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 179,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/vP7s1FThQpvmySmVJXMTU3-8PcS1dzgy5zKouaE_2IM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753213236,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mn8auem2bhef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mn8auem2bhef1.png?auto=webp&amp;s=f8d9250eb919b06b9873df5541dfb4181c23ecb3",
                  "width": 1080,
                  "height": 938
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5f18ccc22bd1429048af2d71903a4986f10f4370",
                    "width": 108,
                    "height": 93
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e9b0375cba8b59a1f2ff6a059540b35b2e80af5",
                    "width": 216,
                    "height": 187
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd33b6c0d9557d99a79e31264f8c7962a467e6de",
                    "width": 320,
                    "height": 277
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=855c907a55cf3f70afe582932d52350878ef5e68",
                    "width": 640,
                    "height": 555
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=727a28b13a2a4b6feeeb2646b4c5ef5d4feba605",
                    "width": 960,
                    "height": 833
                  },
                  {
                    "url": "https://preview.redd.it/mn8auem2bhef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4cdd53e8429b6be0b40bf16e28d255d818f7b04a",
                    "width": 1080,
                    "height": 938
                  }
                ],
                "variants": {},
                "id": "rMAWLMOw9tEiFwd35Iv66C0AmNRfGhg4PeoHVtVBYI4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6nxh2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Wind4462",
          "discussion_type": null,
          "num_comments": 47,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nxh2/everyone_brace_up_for_qwen/",
          "stickied": false,
          "url": "https://i.redd.it/mn8auem2bhef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753213236,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What options do we have for Qwen3 Coder, either local or cloud services?",
          "author_fullname": "t2_mxdkomgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the cheapest option for hosting llama cpp with Qwen Coder at Q8?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nvhs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753213772,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753213111,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What options do we have for Qwen3 Coder, either local or cloud services?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6nvhs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Available_Driver6406",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6nvhs/what_is_the_cheapest_option_for_hosting_llama_cpp/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753213111,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone. I hope you're doing well. I'm sorry if this post is unrelated to the topic of large language models, but I haven't found any other community that focuses on open source AI in general. My question is, are there any open source models for Arabic audio enhancement? Basically, the use case is making good quality data for training Arabic text-to-speech models, since the current ones are either afflicted with bad licenses or they are not up to the task. Thanks for your answers.",
          "author_fullname": "t2_9xer9y5w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Models for Arabic tts and audio enhancement?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6nbb7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753211838,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone. I hope you&amp;#39;re doing well. I&amp;#39;m sorry if this post is unrelated to the topic of large language models, but I haven&amp;#39;t found any other community that focuses on open source AI in general. My question is, are there any open source models for Arabic audio enhancement? Basically, the use case is making good quality data for training Arabic text-to-speech models, since the current ones are either afflicted with bad licenses or they are not up to the task. Thanks for your answers.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6nbb7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Silver-Champion-4846",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6nbb7/best_models_for_arabic_tts_and_audio_enhancement/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6nbb7/best_models_for_arabic_tts_and_audio_enhancement/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753211838,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct](https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct)\n\n hyperolic already has it\n\n",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder-480B-A35B-Instruct",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mlbk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753210248,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct\"&gt;https://app.hyperbolic.ai/models/qwen3-coder-480b-a35b-instruct&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;hyperolic already has it&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mlbk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6mlbk/qwen3coder480ba35binstruct/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753210248,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1M token context length\n\nNo model weights yet, but Qwen3-Coder is already available for testing on [Qwen Chat](https://chat.qwen.ai)",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder Available on chat.qwen.ai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 46,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mfic",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 81,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 81,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/bs1O5LLiPqNQ-8leTmrD3PNczGaZCiiN00b8eacOHzE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;1M token context length&lt;/p&gt;\n\n&lt;p&gt;No model weights yet, but Qwen3-Coder is already available for testing on &lt;a href=\"https://chat.qwen.ai\"&gt;Qwen Chat&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/8xj4raow0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/8xj4raow0hef1.png?auto=webp&amp;s=6c9d8670b9960f64e78150ca2039fc5471464158",
                  "width": 450,
                  "height": 150
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=67fbf003dc0b0cdf945b3ac5069eddaeeaf26ed5",
                    "width": 108,
                    "height": 36
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0ab53d55f78ada2bc730c345d2b92f3cbd18dad6",
                    "width": 216,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/8xj4raow0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cf0cbd6e19276ab7bbf6b36687af35cdf6c00d83",
                    "width": 320,
                    "height": 106
                  }
                ],
                "variants": {},
                "id": "jj-Sn3KQKbzD6M9iYIRnKA0q_gXhkpIWDEoj36IQyis"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6mfic",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mfic/qwen3coder_available_on_chatqwenai/",
          "stickied": false,
          "url": "https://i.redd.it/8xj4raow0hef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753209889,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Available in https://chat.qwen.ai",
          "author_fullname": "t2_e9mfhlg7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3- Coder 👀",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6mew9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 453,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 453,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/e6gFp_J-Dv7QIFguXfhuN4U3lDC6MMgny7SMuBnt9pI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209850,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Available in &lt;a href=\"https://chat.qwen.ai\"&gt;https://chat.qwen.ai&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?auto=webp&amp;s=a57681c6848dc38714b9bea86a26c30bed7d4d42",
                  "width": 1036,
                  "height": 695
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e4a02434a648980c01b1a76032aa8e02027937c6",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=be152167170e73dd02f7850c4f9bb67cf143ec4a",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c86d3d06fd820523c1470692b2726d59dbaf6d3",
                    "width": 320,
                    "height": 214
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=92b455544fdc9f84aebcf9cf995f7e3e643179a1",
                    "width": 640,
                    "height": 429
                  },
                  {
                    "url": "https://preview.redd.it/vnhuwe801hef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=6d63431a252098393997b8c247ff6a0a80b67f78",
                    "width": 960,
                    "height": 644
                  }
                ],
                "variants": {},
                "id": "52S4zww-hEGiuCbEDUlQZAn66M2iCNb-181uTVxpyGY"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6mew9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xhehab_",
          "discussion_type": null,
          "num_comments": 143,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6mew9/qwen3_coder/",
          "stickied": false,
          "url": "https://i.redd.it/vnhuwe801hef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753209850,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_6vcmk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-Coder is imminent",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6medy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mmNhnm_QiKDvZ8nOArY9M-gXEHPij6ccQfZ3Z4a4vrs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753209818,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/mruaiodv0hef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/mruaiodv0hef1.png?auto=webp&amp;s=be215118da4d1c5ae5fc739c077ba4bbf8354f1a",
                  "width": 501,
                  "height": 251
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=49a20e04a28093446580d2909236b45d1e2f568e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=772268126ecad9399aa5fb8ad3dc61fa7a8e5af0",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/mruaiodv0hef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=daa5e07dcd586edd4e8488215b2df66df2d2c809",
                    "width": 320,
                    "height": 160
                  }
                ],
                "variants": {},
                "id": "gxF1-bhuks7kobb2JTcsN29raeY4IvwO_eL--8kAZ38"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m6medy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dudensen",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6medy/qwen3coder_is_imminent/",
          "stickied": false,
          "url": "https://i.redd.it/mruaiodv0hef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753209818,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/lechmazur/bazaar](https://github.com/lechmazur/bazaar)\n\nEach LLM is a buyer or seller with a secret price limit. In 30 rounds, they submit sealed bids/asks. They only see the results of past rounds. 8 agents per game: 4 buyers and 4 sellers, each with a private value drawn from one of the distributions.\n\nFour market conditions (distributions) to measure their adaptability: uniform, correlated, bimodal, heavy-tailed.\n\nKey Metric: Conditional Surplus Alpha (CSα) – normalizes profit against a \"truthful\" baseline (bid your exact value).\n\nAll agents simultaneously submit bids (buyers) or asks (sellers). The engine matches the highest bids with the lowest asks. Trades clear at the midpoint between matched quotes. After each round, all quotes and trades become public history.\n\nBAZAAR compares LLMs to 30+ algorithmic baselines: classic ZIP, Gjerstad-Dickhaut, Q-learning, Momentum, Adaptive Aggressive, Mean Reversion, Roth-Erev, Risk-Aware, Enhanced Bayesian, Contrarian, Sniper, Adversarial Exploiter, even a genetic optimizer.\n\nWith chat enabled, LLMs form illegal cartels.\n\n",
          "author_fullname": "t2_p2tr0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "A new LLM benchmark for markets, supply chains, and trading: BAZAAR. Agents must understand supply, demand, and risk, and learn to bid strategically.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 130,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "krskqj5kxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52842a9e07595d9c9c3601a2dea21160adf1de8a"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=51debf7127ddb9eb252f98403727de4ea71e3b26"
                },
                {
                  "y": 220,
                  "x": 320,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc37da5f71f16ac60f4021f12058eb6f7d1d1ffa"
                },
                {
                  "y": 440,
                  "x": 640,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fd7e2d668330dc48f70489a82adc44bb6ac8b783"
                },
                {
                  "y": 660,
                  "x": 960,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=20373c287598ec9c9ec1b3675c4e6bfc60416e91"
                },
                {
                  "y": 742,
                  "x": 1080,
                  "u": "https://preview.redd.it/krskqj5kxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b83f6051676e0cda84cceb93c1ed1814a2480891"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1600,
                "u": "https://preview.redd.it/krskqj5kxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=4ff785353e78862c3e5aa1051c1ed608279d4ce2"
              },
              "id": "krskqj5kxgef1"
            },
            "sm28w3jkxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a106a7c31b839e001fb11d95974dd872e057d3ea"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=abdee405d15714aab50ee9bac28da807c9c5dce5"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6ee13743536f18f83ee647bad91676e134d8b73"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=68347db5d6b681079896f10816cf011e6ea016f7"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba2a511f53896c7fcf10c27c5dd3474ac86cebaa"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b99c23960f58a2a10376f2b66638477cbb0759e"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1760,
                "u": "https://preview.redd.it/sm28w3jkxgef1.png?width=1760&amp;format=png&amp;auto=webp&amp;s=0dd57e157ff15bab79010c7c0f70eb09a62fd728"
              },
              "id": "sm28w3jkxgef1"
            },
            "1lxnjuhjxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 100,
                  "x": 108,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=72e24d5b1d0e1dd0daa8f02a9b2f774f306babb2"
                },
                {
                  "y": 201,
                  "x": 216,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1def4cd9fb7732ec050a5cd207c21d461eec1206"
                },
                {
                  "y": 298,
                  "x": 320,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c280e397d6012029315b0e383ea58668cf027866"
                },
                {
                  "y": 597,
                  "x": 640,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=760be0af867d8774678164fab47b6cc0171286f3"
                },
                {
                  "y": 896,
                  "x": 960,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=17061c04c488808ca4b62703ccc6ec12c844d024"
                },
                {
                  "y": 1008,
                  "x": 1080,
                  "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15167538cbae4dd9f2ff0fa9f7ac8aac5a4a2975"
                }
              ],
              "s": {
                "y": 1400,
                "x": 1500,
                "u": "https://preview.redd.it/1lxnjuhjxgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=249b3b40abc90da4b77f7d32efeb03508565afde"
              },
              "id": "1lxnjuhjxgef1"
            },
            "hpjd1p6ixgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 100,
                  "x": 108,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6293b2a19de7d9ba0bbaab4f4dcb0149ebed7f9e"
                },
                {
                  "y": 201,
                  "x": 216,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa5d9cccc426c744502b2fcb6d56ec099fafef8a"
                },
                {
                  "y": 298,
                  "x": 320,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fba153c47c966a618a5ab2e86458569dc3546eb"
                },
                {
                  "y": 597,
                  "x": 640,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d92f53373a034c556e7508e70078158b5109615"
                },
                {
                  "y": 896,
                  "x": 960,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ee327f3041a80dee0a0cc5ba5125fb752a8e87df"
                },
                {
                  "y": 1008,
                  "x": 1080,
                  "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a74df25ea42c2f72fd616e87306c704f3e0ed510"
                }
              ],
              "s": {
                "y": 1400,
                "x": 1500,
                "u": "https://preview.redd.it/hpjd1p6ixgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=8e0c2e04bb81b4cafd539e93483917338b2a7cbf"
              },
              "id": "hpjd1p6ixgef1"
            },
            "halpj9alxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 158,
                  "x": 108,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=71af75f43565b17c222ee28e6536557bf0c3b66b"
                },
                {
                  "y": 316,
                  "x": 216,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=baf6edfa44cc6dd4c0857a1d46e9e8430115b516"
                },
                {
                  "y": 469,
                  "x": 320,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b38103d8019a9abc83c8d4e322eac7e3f93e149"
                },
                {
                  "y": 938,
                  "x": 640,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=803ba0808b04d93186cedc3fd4412004029ce523"
                },
                {
                  "y": 1408,
                  "x": 960,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4403712c39c9d5c921f773419ac84c9c967acdab"
                },
                {
                  "y": 1584,
                  "x": 1080,
                  "u": "https://preview.redd.it/halpj9alxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b02503a0234e0e29e9cc7cbda79516bc27d5ce8d"
                }
              ],
              "s": {
                "y": 2200,
                "x": 1500,
                "u": "https://preview.redd.it/halpj9alxgef1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=36b00f321d44dbba4e8b0c7041f074a668f9f5cc"
              },
              "id": "halpj9alxgef1"
            },
            "kbimogvjxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 67,
                  "x": 108,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=21566b4eaa8e0018afdbcde3998ab0c87bec3864"
                },
                {
                  "y": 135,
                  "x": 216,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2e518755ea98beae9b0afc7a74e41272fec936a"
                },
                {
                  "y": 200,
                  "x": 320,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2a101a0df12dcd241bd826dbfacefcce95f5a07d"
                },
                {
                  "y": 400,
                  "x": 640,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=158915b11d94f85eb7c4d52f3d5a5666038e173d"
                },
                {
                  "y": 600,
                  "x": 960,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c11c0e086b3dff1a9c401a1b5d827e8decdba3d2"
                },
                {
                  "y": 675,
                  "x": 1080,
                  "u": "https://preview.redd.it/kbimogvjxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=01e4040beb9b0c46060f90a7ef583b60fd18db8e"
                }
              ],
              "s": {
                "y": 1000,
                "x": 1600,
                "u": "https://preview.redd.it/kbimogvjxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=f440c7f305ed6913c05879610375ba51cf28b48a"
              },
              "id": "kbimogvjxgef1"
            },
            "n6pbexvixgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 74,
                  "x": 108,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9c138d9032ae311f137755885cc0fcc0f6238e39"
                },
                {
                  "y": 148,
                  "x": 216,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b33900c58d0c49879f33d0b02cf46e500ecc981a"
                },
                {
                  "y": 220,
                  "x": 320,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5abe3365b02062b47ea098312f8e91aaa9b1c020"
                },
                {
                  "y": 440,
                  "x": 640,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d65f8cad0c9583d4dba040e27503305982260bd"
                },
                {
                  "y": 660,
                  "x": 960,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f2870cf430de5b5aee66e3c2a277bac99eb35522"
                },
                {
                  "y": 742,
                  "x": 1080,
                  "u": "https://preview.redd.it/n6pbexvixgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2bb320350260da19bfcd63aad9e05c223ff7c949"
                }
              ],
              "s": {
                "y": 1100,
                "x": 1600,
                "u": "https://preview.redd.it/n6pbexvixgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=ab9cb5daeba91d7a5dcc8702e8682e511b2c39c4"
              },
              "id": "n6pbexvixgef1"
            },
            "ynn314skxgef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 101,
                  "x": 108,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1999d49a70e9400e739c2a68cfaa8117e059ceb7"
                },
                {
                  "y": 202,
                  "x": 216,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2c0408ab7d16d8a0ffa87e03021c2e84a7cd0f47"
                },
                {
                  "y": 300,
                  "x": 320,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d09aa8b184cc4a2b2acb4c7d41278907884d79d8"
                },
                {
                  "y": 600,
                  "x": 640,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b2bd12e78edee4049b2ea5f39bd728943a1f72cf"
                },
                {
                  "y": 900,
                  "x": 960,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7d2d61c313d3bcf2e9dbab634c3444733306afb"
                },
                {
                  "y": 1012,
                  "x": 1080,
                  "u": "https://preview.redd.it/ynn314skxgef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8384cbde378a8494e262b0397b8c7e0f21c606f5"
                }
              ],
              "s": {
                "y": 1500,
                "x": 1600,
                "u": "https://preview.redd.it/ynn314skxgef1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=70915d68ffb1eb141632310b69caba6aa08dc162"
              },
              "id": "ynn314skxgef1"
            }
          },
          "name": "t3_1m6m0f7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 22,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "hpjd1p6ixgef1",
                "id": 711713756
              },
              {
                "media_id": "n6pbexvixgef1",
                "id": 711713757
              },
              {
                "media_id": "1lxnjuhjxgef1",
                "id": 711713758
              },
              {
                "media_id": "kbimogvjxgef1",
                "id": 711713759
              },
              {
                "media_id": "krskqj5kxgef1",
                "id": 711713760
              },
              {
                "media_id": "sm28w3jkxgef1",
                "id": 711713761
              },
              {
                "media_id": "ynn314skxgef1",
                "id": 711713762
              },
              {
                "media_id": "halpj9alxgef1",
                "id": 711713763
              }
            ]
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EFJzlTp5mgKfQzJuDpD-TjIuXrwPWnGGidBsBvFKiPg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753208973,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/lechmazur/bazaar\"&gt;https://github.com/lechmazur/bazaar&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Each LLM is a buyer or seller with a secret price limit. In 30 rounds, they submit sealed bids/asks. They only see the results of past rounds. 8 agents per game: 4 buyers and 4 sellers, each with a private value drawn from one of the distributions.&lt;/p&gt;\n\n&lt;p&gt;Four market conditions (distributions) to measure their adaptability: uniform, correlated, bimodal, heavy-tailed.&lt;/p&gt;\n\n&lt;p&gt;Key Metric: Conditional Surplus Alpha (CSα) – normalizes profit against a &amp;quot;truthful&amp;quot; baseline (bid your exact value).&lt;/p&gt;\n\n&lt;p&gt;All agents simultaneously submit bids (buyers) or asks (sellers). The engine matches the highest bids with the lowest asks. Trades clear at the midpoint between matched quotes. After each round, all quotes and trades become public history.&lt;/p&gt;\n\n&lt;p&gt;BAZAAR compares LLMs to 30+ algorithmic baselines: classic ZIP, Gjerstad-Dickhaut, Q-learning, Momentum, Adaptive Aggressive, Mean Reversion, Roth-Erev, Risk-Aware, Enhanced Bayesian, Contrarian, Sniper, Adversarial Exploiter, even a genetic optimizer.&lt;/p&gt;\n\n&lt;p&gt;With chat enabled, LLMs form illegal cartels.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m6m0f7",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6m0f7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zero0_one1",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6m0f7/a_new_llm_benchmark_for_markets_supply_chains_and/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m6m0f7",
          "subreddit_subscribers": 502981,
          "created_utc": 1753208973,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_21qaqh1p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Could this be Deepseek?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 33,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6lf9s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 251,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 251,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/WAXw-XuvIZ9mRKeenbrWXREbY65LvO1BDwwwlpUBowY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753207666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/qzkjkgegugef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/qzkjkgegugef1.png?auto=webp&amp;s=982da5cfa0575f138ae47f73b6eddafc3a141895",
                  "width": 822,
                  "height": 197
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=917acda1e7d58dd2b0c466213686f858f3d1d90f",
                    "width": 108,
                    "height": 25
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ac14c427611c1eca254c8bb52ac34a30bf33d9f1",
                    "width": 216,
                    "height": 51
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8524670d8b150c1ad50fa23613a644190b14608f",
                    "width": 320,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/qzkjkgegugef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e224ff9a214f929b3917304102fe92d67371e639",
                    "width": 640,
                    "height": 153
                  }
                ],
                "variants": {},
                "id": "LX0EdZ_oilMQBPfRRihja4cnZPDhv01xC24KCslp3qQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6lf9s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dulldata",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6lf9s/could_this_be_deepseek/",
          "stickied": false,
          "url": "https://i.redd.it/qzkjkgegugef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753207666,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just updated LM Studio to 0.3.19, downloaded qwen/qwen3-235b-a22b-2507 Q3\\_K\\_L (the only one that fits on my 128GB Mac) and I'm getting a \"failed to send message\" error. I suspect it's the prompt template that's wrong. Can anyone here please post a working template for me to try?\n\nThank you!\n\n  \nEDIT: As suggested by [Minimum\\_Thought\\_x](https://www.reddit.com/user/Minimum_Thought_x/) the 3bit MLX version works! It doesn't show (at least at this moment) in the staff picks list for the model, but you can find it by using the search function.",
          "author_fullname": "t2_d2gb9jhgg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"Failed to Send Message\" from qwen/qwen3-235b-a22b-2507 Q3_K_L",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ldkd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753213776,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753207564,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just updated LM Studio to 0.3.19, downloaded qwen/qwen3-235b-a22b-2507 Q3_K_L (the only one that fits on my 128GB Mac) and I&amp;#39;m getting a &amp;quot;failed to send message&amp;quot; error. I suspect it&amp;#39;s the prompt template that&amp;#39;s wrong. Can anyone here please post a working template for me to try?&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n\n&lt;p&gt;EDIT: As suggested by &lt;a href=\"https://www.reddit.com/user/Minimum_Thought_x/\"&gt;Minimum_Thought_x&lt;/a&gt; the 3bit MLX version works! It doesn&amp;#39;t show (at least at this moment) in the staff picks list for the model, but you can find it by using the search function.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ldkd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Hanthunius",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ldkd/failed_to_send_message_from_qwenqwen3235ba22b2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ldkd/failed_to_send_message_from_qwenqwen3235ba22b2507/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753207564,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What do you guys think about the idea of sharing tokens with your team or family? It feels a bit silly that my friend and I each have the $200 Cursor plan, but together we only use around $250 worth. I think it would be great if we could just have shared one plan 350 dollar plan instead. Do you feel the same way?",
          "author_fullname": "t2_sck77urj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Shared subscription/token with Team or family",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6kre5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753206199,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What do you guys think about the idea of sharing tokens with your team or family? It feels a bit silly that my friend and I each have the $200 Cursor plan, but together we only use around $250 worth. I think it would be great if we could just have shared one plan 350 dollar plan instead. Do you feel the same way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6kre5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Refrigerator9508",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6kre5/shared_subscriptiontoken_with_team_or_family/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6kre5/shared_subscriptiontoken_with_team_or_family/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753206199,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I've hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I'm playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?\n\nAre there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.\n\nLong-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let's be honest, you don't decide to buy a Strad because you want to learn violin, and I'm not looking to drop $$$$ on a GPU if it's avoidable.\n\nUpgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.",
          "author_fullname": "t2_fs6q6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Entry GPU options - 5060 8GB enough to play with?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6knhw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753205961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently want to get into playing with LLMs and am starting my first PC build (only have owned laptops before on integrated graphics). Based in USA. Is the 5060 8GB at $280 enough to mess with local AI stuff and potentially move on when I&amp;#39;ve hit the limits, or am I going to be hitting limits so early on that I should just get a faster/more VRAM/better memory bus/etc card from the start? Right now the options in that price range seem like $280 5060 8GB or maybe used ~$320ish 3080 10GB. The big swing move for me right now would be something like a 5070 ti 16GB at $800 (already stretching budget a lot), but it seems like if I can get away with around $300 and then upgrade later it would be better overall. If I&amp;#39;m playing down in 8GB territory anyways, should I just find whatever cheap $100ish card on ebay I can to mess for now?&lt;/p&gt;\n\n&lt;p&gt;Are there big differences in the technologies incorporated in the 10xx, 20xx, 30xx, 40xx, 50xx cards that are relevant to AI loads? Or can I just roughly use the (mostly fps-based/gaming) benchmarks as a guide for relative performance? Other things I should worry about in the build other than GPU? Currently thinking CPU as AMD 9600x with 32GB DDR5-6000.&lt;/p&gt;\n\n&lt;p&gt;Long-term goal is to play around enough with LLMs to be able to understand what is happening in the research papers i.e. play around with building smaller LLMs/change around architectures/measure performance; download models to play around with inference; and maybe doing useful fine-tuning of (smaller) models. Basically dipping my toes in right now. I have a long-term goal, but let&amp;#39;s be honest, you don&amp;#39;t decide to buy a Strad because you want to learn violin, and I&amp;#39;m not looking to drop $$$$ on a GPU if it&amp;#39;s avoidable.&lt;/p&gt;\n\n&lt;p&gt;Upgrade paths will depend on progress on playing around with small model building, fine-tuning existing small footprint models and useful inference from downloaded models. They would include better GPU or just buying time from a cloud provider.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6knhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "drabbiticus",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6knhw/entry_gpu_options_5060_8gb_enough_to_play_with/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753205961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m running llama.cpp on Ubuntu 22.04 with ROCm 6.2. I cloned the repo and built it like this:\n\nHIPCXX=\"$(hipconfig -l)/clang\" HIP_PATH=\"$(hipconfig -R)\" \\\n    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \\\n    &amp;&amp; cmake --build build --config Release -- -j 16\n\nThen I run the model:\n\n./build/bin/llama-cli -hf ggml-org/gemma-3-1b-it-GGUF\n\nBut I’m only getting around 10 tokens/sec. When I check system usage:\n- GPU utilization is stuck at 1%\n- VRAM usage is 0\n- CPU is at 100%\n\nLooks like it’s not using the GPU at all.\nrocm-smi can list all 4 GPUs\nllama.cpp also able to list 4 GPU devices\nMachine is not plugged in into any monitor, just ssh remotely\n\nAnyone have experience running llama.cpp with ROCm or on multiple AMD GPUs? Any specific flags or build settings I might be missing?\n\n",
          "author_fullname": "t2_ip4e2mp8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama.cpp on ROCm only running at 10 tokens/sec, GPU at 1% util. What am I missing?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6khbt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753205572,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m running llama.cpp on Ubuntu 22.04 with ROCm 6.2. I cloned the repo and built it like this:&lt;/p&gt;\n\n&lt;p&gt;HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \\\n    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \\\n    &amp;amp;&amp;amp; cmake --build build --config Release -- -j 16&lt;/p&gt;\n\n&lt;p&gt;Then I run the model:&lt;/p&gt;\n\n&lt;p&gt;./build/bin/llama-cli -hf ggml-org/gemma-3-1b-it-GGUF&lt;/p&gt;\n\n&lt;p&gt;But I’m only getting around 10 tokens/sec. When I check system usage:\n- GPU utilization is stuck at 1%\n- VRAM usage is 0\n- CPU is at 100%&lt;/p&gt;\n\n&lt;p&gt;Looks like it’s not using the GPU at all.\nrocm-smi can list all 4 GPUs\nllama.cpp also able to list 4 GPU devices\nMachine is not plugged in into any monitor, just ssh remotely&lt;/p&gt;\n\n&lt;p&gt;Anyone have experience running llama.cpp with ROCm or on multiple AMD GPUs? Any specific flags or build settings I might be missing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6khbt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Reasonable_Can_5793",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753205572,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi community,\n\nI’m facing two issues:\n\n1. I want to correct Hindi text. I feel using llms  is overkill for this task. I came across the GRMR 2B model, but it only supports English. My text is in Hindi.\n\n2. I want to transliterate Hindi to Hinglish. Again, I believe LLMs are too heavy for this and often make mistakes. Is there any lightweight solution I can run on Colab—maybe on an T4, A100 or L4 GPU?\n\nFor example, I have text like:\n\"जी शुरू करते है\"\nand I want to convert it to:\n\"Ji shuru karte hai\"\n\nPlease help.",
          "author_fullname": "t2_1b2w3w6d73",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Help/Suggestion Wanted] Hindi to Hinglish and Spell correction",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6jdyz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753203170,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi community,&lt;/p&gt;\n\n&lt;p&gt;I’m facing two issues:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I want to correct Hindi text. I feel using llms  is overkill for this task. I came across the GRMR 2B model, but it only supports English. My text is in Hindi.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I want to transliterate Hindi to Hinglish. Again, I believe LLMs are too heavy for this and often make mistakes. Is there any lightweight solution I can run on Colab—maybe on an T4, A100 or L4 GPU?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For example, I have text like:\n&amp;quot;जी शुरू करते है&amp;quot;\nand I want to convert it to:\n&amp;quot;Ji shuru karte hai&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Please help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6jdyz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Grouchy-Pin9500",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6jdyz/helpsuggestion_wanted_hindi_to_hinglish_and_spell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6jdyz/helpsuggestion_wanted_hindi_to_hinglish_and_spell/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753203170,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Corporate deployment.  \n\nCurrently deployed with multi a6000 ada but I'd like to add more vram to support multiple larger models for full scale deployment.  \n\nConsidering mi300x x 4 to maximize vram per $. Any deployments that dont play nice on amd hardware (flux) would use existing a6000 ada stack.\n\nAny other options I should consider?\n\nBudget is flexible within reason.\n\n\n",
          "author_fullname": "t2_ze8yz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "~75k budget. Best bang for the buck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6j69n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753202690,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Corporate deployment.  &lt;/p&gt;\n\n&lt;p&gt;Currently deployed with multi a6000 ada but I&amp;#39;d like to add more vram to support multiple larger models for full scale deployment.  &lt;/p&gt;\n\n&lt;p&gt;Considering mi300x x 4 to maximize vram per $. Any deployments that dont play nice on amd hardware (flux) would use existing a6000 ada stack.&lt;/p&gt;\n\n&lt;p&gt;Any other options I should consider?&lt;/p&gt;\n\n&lt;p&gt;Budget is flexible within reason.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6j69n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bohdanowicz",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6j69n/75k_budget_best_bang_for_the_buck/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6j69n/75k_budget_best_bang_for_the_buck/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753202690,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Doesn't it sound cool?\nSounds movie like",
          "author_fullname": "t2_18di024ua3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Project: Print AI Replies on a Ticket Printer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6izt7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753202280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Doesn&amp;#39;t it sound cool?\nSounds movie like&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6izt7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Own-Potential-2308",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6izt7/project_print_ai_replies_on_a_ticket_printer/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6izt7/project_print_ai_replies_on_a_ticket_printer/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753202280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Cards like 3090, 4090, 5090 has very high electric consumption. Isn't it possible to make 24,32gb cards with like 5060 level electric consumption?",
          "author_fullname": "t2_d9gk5hdlt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "+24GB VRAM with low electric consumption",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hzf0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753200005,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Cards like 3090, 4090, 5090 has very high electric consumption. Isn&amp;#39;t it possible to make 24,32gb cards with like 5060 level electric consumption?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6hzf0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "narca_hakan",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753200005,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I seriously can't be the only one how would rather have a throttled down cursor than have it cut off totally. like seriously all tokens used in 10 day! I've been thinking about how the majority of these AI tools limit you by tokens or requests, and seriously frustrating when you get blocked from working and have to wait forever to use it again.\n\nAm I the only person who would rather have a slow cursor that saves tokens for me Like, it would still react to your things, but slower. No more reaching limits and losing access just slower but always working. So you could just go get coffee or do other things while it's working.",
          "author_fullname": "t2_sck77urj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "TOKENS BURNED! Am I the only one who would rather have a throttled down cursor rather than have it go on token vacation for 20 day!?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hxnt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.26,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753199901,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I seriously can&amp;#39;t be the only one how would rather have a throttled down cursor than have it cut off totally. like seriously all tokens used in 10 day! I&amp;#39;ve been thinking about how the majority of these AI tools limit you by tokens or requests, and seriously frustrating when you get blocked from working and have to wait forever to use it again.&lt;/p&gt;\n\n&lt;p&gt;Am I the only person who would rather have a slow cursor that saves tokens for me Like, it would still react to your things, but slower. No more reaching limits and losing access just slower but always working. So you could just go get coffee or do other things while it&amp;#39;s working.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6hxnt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Refrigerator9508",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hxnt/tokens_burned_am_i_the_only_one_who_would_rather/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hxnt/tokens_burned_am_i_the_only_one_who_would_rather/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753199901,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there an active leaderboard for local models that ranks them by function calling capability?",
          "author_fullname": "t2_cxq4h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Leaderboard for function calling models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ht1r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753199602,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there an active leaderboard for local models that ranks them by function calling capability?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ht1r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tvmaly",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ht1r/leaderboard_for_function_calling_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ht1r/leaderboard_for_function_calling_models/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753199602,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I got sglang running a few months ago with Qwen3 30B-A3B and its performance impressed me so much that there is no desire from me at this point to run 70B+ models because I can reach over 600tok/s with a single 3090 with it (8 inferences running in parallel, 150 or so for a single inference, 140tok/s with power limited to 250W)\n\nMy question I'd like to answer now is how much of a leap can I expect to see with 5090? I will be gaming and doing image/video generation with the 5090 as well if I get one, and I have no plans to sell my pair of 3090s (though it would be at profit so i could potentially do that to save money)\n\nHowever lately there's not a lot of time for games and besides all titles I play still do run fine on Ampere even though I have a 4K 240hz monitor so I was really trying to get a 5090 this year but I guess I just have a sour taste in my mouth about it all. Image generation is fine with 24GB but video in particular could benefit from more grunt. Still, it's not been a tier 1 hobby of mine so it's really kind of a side benefit. There are also other things i like to do aspirationally (tinker with algorithms in CUDA and so on) that it would be cool to have but two 3090s is already so incredibly far beyond what I need for that. \n\n5090 are poised to become possible to obtain soon it seems, so I want some more complete data. \n\nI'd like to see if someone with a 5090 running linux can test my docker image and tell me what inference performance you're able to get, to help me make this purchasing decision. \n\nHere is the dockerfile: [https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336](https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336)\n\n* I can provide a built docker image (it is 18GB though) if you have trouble building or running that dockerfile. the instructions are in a comment inside, and should work even if you are not familiar with docker or k8s. If we need to fall back to running the image though I'd like to troubleshoot with you a bit so that I can potentially improve my dockerfile.\n* if you want to actually human-readably view the output, I use a (dependencyless) python script that extracts out the streamed output tokens from that curl response, I provide it here: [https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b](https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b) What you would do is take the example curl command and add ` | python3 stream_parser.py`\n\nMy 600+ tok/s performance number is had on my 3090 by modifying the input curl request to put 8 separate messages into the curl request. Let me know if you're having trouble figuring out the syntax for that... My hope is a 5090 should have the arithmetic intensity that it probably wants 12 or even more to batch in parallel to get the highest possible throughput. I would be hoping for a 3 or 4x speedup compared to 3090 but I somehow doubt that will be the case for single inference but it may be the case with multiple inference (which on an efficient runtime like sglang seems to be able to extract compute performance while saturating mem bandwidth). From a theoretical point of view, 1.79TB/s over 936GB/s should yield a speedup of 96% for single inference. That's actually quite a bit better than I expected...\n\nNow if we can hit 3x or 4x total throughput going from 3090 to 5090 that will be a go for me and I'll gladly purchase one. If not... I dunno if I can justify the cost. If it only provides a 2x speed gain over a 3090, that means in terms of LLM heavy lifting it is only consolidating my two 3090s into one GPU, and gives only a mild efficiency win (two 3090s at 250W vs one 5090 at probably 400W, not much less, only saving 100W) and no performance win which would not be all that compelling. If 4x though, that would represent some serious consolidation factor. My gut is telling me to expect something like 3.3x speedup. Which I hope is enough to push me over the edge because I sure do want the shiny. I just gotta talk myself into it.\n\nIf you look at the docker logs (which in the way i tell you to launch it will be visible in the terminal) it will show the latest tok/s metric. \n\nThank you.",
          "author_fullname": "t2_iifi6ul2l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "5090 batched inference performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hqi8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753200765,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753199447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got sglang running a few months ago with Qwen3 30B-A3B and its performance impressed me so much that there is no desire from me at this point to run 70B+ models because I can reach over 600tok/s with a single 3090 with it (8 inferences running in parallel, 150 or so for a single inference, 140tok/s with power limited to 250W)&lt;/p&gt;\n\n&lt;p&gt;My question I&amp;#39;d like to answer now is how much of a leap can I expect to see with 5090? I will be gaming and doing image/video generation with the 5090 as well if I get one, and I have no plans to sell my pair of 3090s (though it would be at profit so i could potentially do that to save money)&lt;/p&gt;\n\n&lt;p&gt;However lately there&amp;#39;s not a lot of time for games and besides all titles I play still do run fine on Ampere even though I have a 4K 240hz monitor so I was really trying to get a 5090 this year but I guess I just have a sour taste in my mouth about it all. Image generation is fine with 24GB but video in particular could benefit from more grunt. Still, it&amp;#39;s not been a tier 1 hobby of mine so it&amp;#39;s really kind of a side benefit. There are also other things i like to do aspirationally (tinker with algorithms in CUDA and so on) that it would be cool to have but two 3090s is already so incredibly far beyond what I need for that. &lt;/p&gt;\n\n&lt;p&gt;5090 are poised to become possible to obtain soon it seems, so I want some more complete data. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to see if someone with a 5090 running linux can test my docker image and tell me what inference performance you&amp;#39;re able to get, to help me make this purchasing decision. &lt;/p&gt;\n\n&lt;p&gt;Here is the dockerfile: &lt;a href=\"https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336\"&gt;https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I can provide a built docker image (it is 18GB though) if you have trouble building or running that dockerfile. the instructions are in a comment inside, and should work even if you are not familiar with docker or k8s. If we need to fall back to running the image though I&amp;#39;d like to troubleshoot with you a bit so that I can potentially improve my dockerfile.&lt;/li&gt;\n&lt;li&gt;if you want to actually human-readably view the output, I use a (dependencyless) python script that extracts out the streamed output tokens from that curl response, I provide it here: &lt;a href=\"https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b\"&gt;https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b&lt;/a&gt; What you would do is take the example curl command and add &lt;code&gt;| python3 stream_parser.py&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My 600+ tok/s performance number is had on my 3090 by modifying the input curl request to put 8 separate messages into the curl request. Let me know if you&amp;#39;re having trouble figuring out the syntax for that... My hope is a 5090 should have the arithmetic intensity that it probably wants 12 or even more to batch in parallel to get the highest possible throughput. I would be hoping for a 3 or 4x speedup compared to 3090 but I somehow doubt that will be the case for single inference but it may be the case with multiple inference (which on an efficient runtime like sglang seems to be able to extract compute performance while saturating mem bandwidth). From a theoretical point of view, 1.79TB/s over 936GB/s should yield a speedup of 96% for single inference. That&amp;#39;s actually quite a bit better than I expected...&lt;/p&gt;\n\n&lt;p&gt;Now if we can hit 3x or 4x total throughput going from 3090 to 5090 that will be a go for me and I&amp;#39;ll gladly purchase one. If not... I dunno if I can justify the cost. If it only provides a 2x speed gain over a 3090, that means in terms of LLM heavy lifting it is only consolidating my two 3090s into one GPU, and gives only a mild efficiency win (two 3090s at 250W vs one 5090 at probably 400W, not much less, only saving 100W) and no performance win which would not be all that compelling. If 4x though, that would represent some serious consolidation factor. My gut is telling me to expect something like 3.3x speedup. Which I hope is enough to push me over the edge because I sure do want the shiny. I just gotta talk myself into it.&lt;/p&gt;\n\n&lt;p&gt;If you look at the docker logs (which in the way i tell you to launch it will be visible in the terminal) it will show the latest tok/s metric. &lt;/p&gt;\n\n&lt;p&gt;Thank you.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?auto=webp&amp;s=c7cbcc7517e2406e2326e7a1eb6bdb9022c27fda",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=796041decb8c1250cbc2f301331b72f7385b477d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e3562243f324d16bc6d9dd09adb1da4e0b100b5",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=564e5f4bb6808064a14eb3965a6911671c3c9807",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a4f79362039959fa37eab208ae001245ccfe6e3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=912f966e123e94e32e7975fe8aebac89450a6b98",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6hqi8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "michaelsoft__binbows",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753199447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "​Disclaimer: I made [hyprnote](https://hyprnote.com) \\- went trending in [here](https://www.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/) 3 months ago.\n\n**context:**\n\na lot of our users are using ollama at the moment and I thought why not make something for STT just like ollama. we are also getting more and more requests on the parakeet model so really looking into this right now.\n\n**research:**\n\nI haven't come across anything related to this. I found some projects using whisperX but haven't actually found one where you can just use different models like ollama.\n\n**owhisper:**\n\nI'm building an open-source alternative for granola ai. I want to make hyprnote self-hostable so people can play around with various stt and llms. thinking about making a unified proxy server that can be deployed and manages owhisper and custom llm endpoints - including ollama.\n\nCurious - if this existed, would you try it out? And what features would you want built in?",
          "author_fullname": "t2_j1t6g97wv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking about \"owhisper\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6hck1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753198571,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;​Disclaimer: I made &lt;a href=\"https://hyprnote.com\"&gt;hyprnote&lt;/a&gt; - went trending in &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1k3fdqa/i_spent_5_months_building_an_open_source_ai_note/\"&gt;here&lt;/a&gt; 3 months ago.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;context:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;a lot of our users are using ollama at the moment and I thought why not make something for STT just like ollama. we are also getting more and more requests on the parakeet model so really looking into this right now.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;research:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t come across anything related to this. I found some projects using whisperX but haven&amp;#39;t actually found one where you can just use different models like ollama.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;owhisper:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building an open-source alternative for granola ai. I want to make hyprnote self-hostable so people can play around with various stt and llms. thinking about making a unified proxy server that can be deployed and manages owhisper and custom llm endpoints - including ollama.&lt;/p&gt;\n\n&lt;p&gt;Curious - if this existed, would you try it out? And what features would you want built in?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?auto=webp&amp;s=070858a1cfeec763c131615ac513538e6a19426b",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bee6065abfb7b73c45f622b4c1cc472253ace4e",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=82ae4d019c19940aa95321c7be83b0edd546f2a0",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9757ca6f1422e8e8039213794d3c5ab9ab68d765",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=925c91e5f6ddcbcaa0e6039c50650c2027ac8c2b",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d7081fb329bed3002c26fa7002ce376573bb805",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=084bb2b0fc5cc7efd82866fa4b3992449215e74c",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "qKTB_c8rjoocFmhhNoBtbbZwllVmSjChhIlIjEWudRY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6hck1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "beerbellyman4vr",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6hck1/thinking_about_owhisper/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6hck1/thinking_about_owhisper/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753198571,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.\n\nI've been looking at Kimi, but I've been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.\n\nWas wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.",
          "author_fullname": "t2_ijzb7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Epyc Qwen3 235B Q8 speed?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6h67y",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753198166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone with an Epyc 9015 or better able to test Qwen3 235B Q8 for prompt processing and token generation?  Ideally with a 3090 or better for prompt processing.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been looking at Kimi, but I&amp;#39;ve been discouraged by results, and thinking about settling on a system to run 235B Q8 for now.&lt;/p&gt;\n\n&lt;p&gt;Was wondering if a 9015 256GB+ system would be enough, or would need the higher end CPUs with more CCDs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6h67y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MidnightProgrammer",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6h67y/epyc_qwen3_235b_q8_speed/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753198166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Just designed the core architecture for a RAG agent. I’m testing the foundational decision:  \n**Is it smart to use Langchain or LlamaIndex for this kind of agentic system? Or am I better off going more lightweight or custom?**\n\nI’ve included a visual of the architecture in the post. Would love your feedback, especially if you’ve worked with or scaled these frameworks.\n\n# 🔧 What I’m Building\n\nThis is a **simpler agentic RAG system**, designed to be modular and scalable, but lean enough to move fast. It’s not just a question-answer bot but structured with foresight to evolve into a fully agentic system later.\n\n**Core Components:**\n\n* A **Session Manager** for planning, task decomposition, and execution flow\n* A **Vector Store** for context retrieval\n* A **RAG pipeline** for combining retrieval + generation\n* A **State &amp; Memory Unit** for session history, context tracking, and intermediate reasoning\n* A clean chat I/O interface\n\n# 🧱 Design Principles\n\n* **Modularity**: Every component is cleanly separated\n* **Progressive Architecture**: Built to scale into multi tool-using system\n* **Context Awareness**: Dynamic memory and reasoning path tracking\n* **Agentic Behavior**: Even in its early form, it plans, tracks, and self-updates\n\nWould love feedback on:\n\n* Whether Langchain or LlamaIndex make sense as the foundation here\n* Where others hit scaling or architectural limitations with these\n* How to avoid building into a box I’ll regret later\n\nIf this is the wrong move, I'd rather fix it now. Appreciate any insights.",
          "author_fullname": "t2_hpqb9e8q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Am I making a mistake building my RAG agent with Langchain or LlamaIndex?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 116,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6gwgl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/z4CEyTRNaRHdbhDKbWtTgySbGCfnpODod9OwS0FKfXc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753197551,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just designed the core architecture for a RAG agent. I’m testing the foundational decision:&lt;br/&gt;\n&lt;strong&gt;Is it smart to use Langchain or LlamaIndex for this kind of agentic system? Or am I better off going more lightweight or custom?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I’ve included a visual of the architecture in the post. Would love your feedback, especially if you’ve worked with or scaled these frameworks.&lt;/p&gt;\n\n&lt;h1&gt;🔧 What I’m Building&lt;/h1&gt;\n\n&lt;p&gt;This is a &lt;strong&gt;simpler agentic RAG system&lt;/strong&gt;, designed to be modular and scalable, but lean enough to move fast. It’s not just a question-answer bot but structured with foresight to evolve into a fully agentic system later.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Core Components:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;A &lt;strong&gt;Session Manager&lt;/strong&gt; for planning, task decomposition, and execution flow&lt;/li&gt;\n&lt;li&gt;A &lt;strong&gt;Vector Store&lt;/strong&gt; for context retrieval&lt;/li&gt;\n&lt;li&gt;A &lt;strong&gt;RAG pipeline&lt;/strong&gt; for combining retrieval + generation&lt;/li&gt;\n&lt;li&gt;A &lt;strong&gt;State &amp;amp; Memory Unit&lt;/strong&gt; for session history, context tracking, and intermediate reasoning&lt;/li&gt;\n&lt;li&gt;A clean chat I/O interface&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;🧱 Design Principles&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Modularity&lt;/strong&gt;: Every component is cleanly separated&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Progressive Architecture&lt;/strong&gt;: Built to scale into multi tool-using system&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Context Awareness&lt;/strong&gt;: Dynamic memory and reasoning path tracking&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agentic Behavior&lt;/strong&gt;: Even in its early form, it plans, tracks, and self-updates&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love feedback on:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Whether Langchain or LlamaIndex make sense as the foundation here&lt;/li&gt;\n&lt;li&gt;Where others hit scaling or architectural limitations with these&lt;/li&gt;\n&lt;li&gt;How to avoid building into a box I’ll regret later&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If this is the wrong move, I&amp;#39;d rather fix it now. Appreciate any insights.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/zptnshw2yfef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/zptnshw2yfef1.png?auto=webp&amp;s=76e75b90a419c582aabc49fe139e7c91afe65a89",
                  "width": 847,
                  "height": 702
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ffe2b2f0146b5018418f2cf8a47aa964f7d008b",
                    "width": 108,
                    "height": 89
                  },
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b1855fe9db743f1b2243d7403d41f5e2cce1d1e1",
                    "width": 216,
                    "height": 179
                  },
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=24992bb3d858161a69debf7f29be67edbf56aa85",
                    "width": 320,
                    "height": 265
                  },
                  {
                    "url": "https://preview.redd.it/zptnshw2yfef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=501fc562aaff34d23aba6f075a85b7873e823e86",
                    "width": 640,
                    "height": 530
                  }
                ],
                "variants": {},
                "id": "7ez5SdWmQCvcDpjSTikUtrexkFQ86FGFNJ_9HyyGiEQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6gwgl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "duke_x91",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6gwgl/am_i_making_a_mistake_building_my_rag_agent_with/",
          "stickied": false,
          "url": "https://i.redd.it/zptnshw2yfef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753197551,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.\n\n# The problem: too many options\n\nI've been building [Kiln AI](https://github.com/kiln-ai/kiln): an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn't easy. If evaluating an additional model is painful, you're less likely to do it, which makes you less likely to find the best way to run your AI workload.\n\nHere's a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (`&lt;think&gt;...&lt;/think&gt;`), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top\\_p, etc), and much more.\n\n# How a focus on usability turned into over 2000 test cases\n\nI wanted things to \"just work\" as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.\n\nTo make it easy to use, we needed reasonable defaults for every major model. That's no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.\n\nThe solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.\n\n# Wait, doesn't that cost a lot of money and take forever?\n\n**Yes it does!** Each time we run these tests, we're making thousands of LLM calls against a wide variety of providers. There's no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn't an option.\n\nOur blog has some details on the [Python pytest setup we used to make this manageable](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time).\n\n# The Result\n\nThe end result is that it's much easier to rapidly evaluate AI models and methods. It includes\n\n* The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.\n* Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).\n\nHowever, you're in control. You can always override any suggestion.\n\n# Next Step: A Giant Ollama Server\n\nI can run a decent sampling of our Ollama tests locally, but I lack the \\~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I'd love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!\n\n# How to Find the Best Model for Your Task with Kiln\n\nAll of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.\n\nKiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.\n\nTo get started, check out the tool or our guides:\n\n* [Kiln AI on Github - over 3900 stars](https://getkiln.ai/)\n* [Quickstart Guide](https://docs.getkiln.ai/docs/quickstart)\n* [Kiln Discord](https://getkiln.ai/discord)\n* [Blog post with more details on our LLM testing (more detailed version of above)](https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time)\n\nI'm happy to answer questions if anyone wants to dive deeper on specific aspects!",
          "author_fullname": "t2_slbscky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I wrote 2000 LLM test cases so you don't have to",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6gq8e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753199300,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753197164,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a quick story of how a focus on usability turned into 2000 LLM tests cases (well 2631 to be exact), and why the results might be helpful to you.&lt;/p&gt;\n\n&lt;h1&gt;The problem: too many options&lt;/h1&gt;\n\n&lt;p&gt;I&amp;#39;ve been building &lt;a href=\"https://github.com/kiln-ai/kiln\"&gt;Kiln AI&lt;/a&gt;: an open tool to help you find the best way to run your AI workload. Part of Kiln’s goal is testing various different models on your AI task to see which ones work best. We hit a usability problem on day one: too many options. We supported hundreds of models, each with their own parameters, capabilities, and formats. Trying a new model wasn&amp;#39;t easy. If evaluating an additional model is painful, you&amp;#39;re less likely to do it, which makes you less likely to find the best way to run your AI workload.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a sampling of the many different options you need to choose: structured data mode (JSON schema, JSON mode, instruction, tool calls), reasoning support, reasoning format (&lt;code&gt;&amp;lt;think&amp;gt;...&amp;lt;/think&amp;gt;&lt;/code&gt;), censorship/limits, use case support (generating synthetic data, evals), runtime parameters (logprobs, temperature, top_p, etc), and much more.&lt;/p&gt;\n\n&lt;h1&gt;How a focus on usability turned into over 2000 test cases&lt;/h1&gt;\n\n&lt;p&gt;I wanted things to &amp;quot;just work&amp;quot; as much as possible in Kiln. You should be able to run a new model without writing a new API integration, writing a parser, or experimenting with API parameters.&lt;/p&gt;\n\n&lt;p&gt;To make it easy to use, we needed reasonable defaults for every major model. That&amp;#39;s no small feat when new models pop up every week, and there are dozens of AI providers competing on inference.&lt;/p&gt;\n\n&lt;p&gt;The solution: a whole bunch of test cases! 2631 to be exact, with more added every week. We test every model on every provider across a range of functionality: structured data (JSON/tool calls), plaintext, reasoning, chain of thought, logprobs/G-eval, evals, synthetic data generation, and more. The result of all these tests is a detailed configuration file with up-to-date details on which models and providers support which features.&lt;/p&gt;\n\n&lt;h1&gt;Wait, doesn&amp;#39;t that cost a lot of money and take forever?&lt;/h1&gt;\n\n&lt;p&gt;&lt;strong&gt;Yes it does!&lt;/strong&gt; Each time we run these tests, we&amp;#39;re making thousands of LLM calls against a wide variety of providers. There&amp;#39;s no getting around it: we want to know these features work well on every provider and model. The only way to be sure is to test, test, test. We regularly see providers regress or decommission models, so testing once isn&amp;#39;t an option.&lt;/p&gt;\n\n&lt;p&gt;Our blog has some details on the &lt;a href=\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\"&gt;Python pytest setup we used to make this manageable&lt;/a&gt;.&lt;/p&gt;\n\n&lt;h1&gt;The Result&lt;/h1&gt;\n\n&lt;p&gt;The end result is that it&amp;#39;s much easier to rapidly evaluate AI models and methods. It includes&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;The model selection dropdown is aware of your current task needs, and will only show models known to work. The filters include things like structured data support (JSON/tools), needing an uncensored model for eval data generation, needing a model which supports logprobs for G-eval, and many more use cases.&lt;/li&gt;\n&lt;li&gt;Automatic defaults for complex parameters. For example, automatically selecting the best JSON generation method from the many options (JSON schema, JSON mode, instructions, tools, etc).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;However, you&amp;#39;re in control. You can always override any suggestion.&lt;/p&gt;\n\n&lt;h1&gt;Next Step: A Giant Ollama Server&lt;/h1&gt;\n\n&lt;p&gt;I can run a decent sampling of our Ollama tests locally, but I lack the ~1TB of VRAM needed to run things like Deepseek R1 or Kimi K2 locally. I&amp;#39;d love an easy-to-use test environment for these without breaking the bank. Suggestions welcome!&lt;/p&gt;\n\n&lt;h1&gt;How to Find the Best Model for Your Task with Kiln&lt;/h1&gt;\n\n&lt;p&gt;All of this testing infrastructure exists to serve one goal: making it easier for you to find the best way to run your specific use case. The 2000+ test cases ensure that when you use Kiln, you get reliable recommendations and easy model switching without the trial-and-error process.&lt;/p&gt;\n\n&lt;p&gt;Kiln is a free open tool for finding the best way to build your AI system. You can rapidly compare models, providers, prompts, parameters and even fine-tunes to get the optimal system for your use case — all backed by the extensive testing described above.&lt;/p&gt;\n\n&lt;p&gt;To get started, check out the tool or our guides:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/\"&gt;Kiln AI on Github - over 3900 stars&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://docs.getkiln.ai/docs/quickstart\"&gt;Quickstart Guide&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/discord\"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://getkiln.ai/blog/i_wrote_2000_llm_test_cases_so_you_dont_have_to#cost-and-time\"&gt;Blog post with more details on our LLM testing (more detailed version of above)&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;m happy to answer questions if anyone wants to dive deeper on specific aspects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?auto=webp&amp;s=23e4ff0dbe2d03ff352aea774053e4e9cdb80d20",
                  "width": 1280,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fd9815f077288b33817e75895d23e661f1193778",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7df51b519d6d99631039f2563f587d4f7fb7f337",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=584735f7b916c00d422195a7ea012563d4e134db",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7ceb01849b330103f92aaf6b1331cd97e415c722",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f0594f7e041119a136f22914764b2a128e73d5ff",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=415b728bd16022b553cb45cb75a1a8fee65a2e5b",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YF2mZrP2LZphKjmsRiHyL6Oic0sw2vC0c9Q1XWpEOGA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6gq8e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "davernow",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6gq8e/i_wrote_2000_llm_test_cases_so_you_dont_have_to/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753197164,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey!\n\nI’m looking for a study buddy (or a small group) to go through [Maxime Labonne’s “LLM From Scratch” course](https://github.com/mlabonne/llm-course) together. It’s an amazing resource for building a large language model from scratch, and I think it’d be way more fun to learn together\n\n# My plan:\n\n* **Set weekly goals** based on the course structure\n* **Meet once a week** (probably one evening over the weekend) for a **voice call** to review what we’ve learned, share insights, and help each other with anything confusing\n* Stay accountable and motivated through shared progress\n\nDrop a comment or DM me if you’re interested! Thank you",
          "author_fullname": "t2_bywk86ik",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for LLMs Study Buddy",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6fvd5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753195208,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey!&lt;/p&gt;\n\n&lt;p&gt;I’m looking for a study buddy (or a small group) to go through &lt;a href=\"https://github.com/mlabonne/llm-course\"&gt;Maxime Labonne’s “LLM From Scratch” course&lt;/a&gt; together. It’s an amazing resource for building a large language model from scratch, and I think it’d be way more fun to learn together&lt;/p&gt;\n\n&lt;h1&gt;My plan:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Set weekly goals&lt;/strong&gt; based on the course structure&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Meet once a week&lt;/strong&gt; (probably one evening over the weekend) for a &lt;strong&gt;voice call&lt;/strong&gt; to review what we’ve learned, share insights, and help each other with anything confusing&lt;/li&gt;\n&lt;li&gt;Stay accountable and motivated through shared progress&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Drop a comment or DM me if you’re interested! Thank you&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?auto=webp&amp;s=dbc698010f56afa71dd99dc709b0ca685c29aee4",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3f3260a76cb9648a81e4ffd047ff8a749b3bc74",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21e3cfc0b6e73548c3f100a3b24c8e03c4cf7290",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=361d87092f3bd81a159645012629ae0fe171dc2e",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5c4bc7a322635ecbf6feaad42bf125031a8dec84",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=697fabf421f2c39c88cbed190d56d8d3653dd0e4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2fea0b39998df7813abea97a6cbf5ead21679476",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "X8oVhjvjKGwvGoq_CrHkp1djUbKeUIclFdjL0Lg5VGg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m6fvd5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KaiKawaii0",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6fvd5/looking_for_llms_study_buddy/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753195208,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*\\*TL;DR\\*\\* Thinking about building an LLM rig with 5 used AMD MI50 32GB GPUs to run Qwen 3 32b and 235b. Estimated token speeds look promising for the price (\\~$1125 total). Biggest hurdles are PCIe lane bandwidth &amp; power, which I'm attempting to solve with bifurcation cards and a new PSU.  Looking for feedback!\n\nHi everyone,\n\nLately I've been thinking about treating myself to a 3090 and a ram upgrade to run Qwen 3 32b and 235b, but the MI50 posts got me napkin mathing that rabbit hole. The numbers I'm seeing are 19 tok/s in 235b(I get 3 tok/s running q2), and 60 tok/s with 4x tensor parallel with 32b(I usually get 10-15 tok/s), which seems great for the price. To me that would be worth it to convert my desktop into a dedicated server. Other than slower prompt processing, is there a catch?\n\n  \nIf its as good as some posts claim, then I'd be limited by cost and my existing hardware. The biggest problem is PCIe lanes, or lack thereof as low bandwidth will tank performance when running models in tensor parallel. To make the problem less bad, I'm going to try and keep everything PCIe gen 4. My motherboard supports bifurcation of the gen4 16x slot, which can be broken out by PCIe 4.0 bifurcation cards. The only gen 4 card I could find splits lanes, so that's why theres 3 of them. Another problem would be power, as the cards will need to be power limited slightly even with a 1600w PSU.\n\nCurrent system:  \n\\*   \\*\\*CPU:\\*\\* Ryzen 5 7600  \n\\*   \\*\\*RAM:\\*\\* 48GB DDR5 5200MHz  \n\\*   \\*\\*Motherboard:\\*\\* MSI Mortar AM5  \n\\*   \\*\\*SSD (Primary):\\*\\* 1TB SSD  \n\\*   \\*\\*SSD (Secondary):\\*\\* 2TB SSD  \n\\*   \\*\\*PSU:\\*\\* 850W  \n\\*   \\*\\*GPU(s):\\*\\* 2x AMD RX6800 \n\n  \nProspective system:  \n\\*   \\*\\*CPU:\\*\\* Ryzen 5 7600  \n\\*   \\*\\*RAM:\\*\\* 48GB DDR5 5200MHz  \n\\*   \\*\\*Motherboard:\\*\\* MSI Mortar AM5(with bifurcation enabled)  \n\\*   \\*\\*SSD (Primary):\\*\\* 1TB SSD  \n\\*   \\*\\*SSD (Secondary):\\*\\* 2TB SSD  \n\\*   \\*\\*GPUs (New):\\*\\* 5 x MI50 32GB ($130 each + $100 shipping = $750 total)  \n\\*   \\*\\*PSU (New):\\*\\* 1600W PSU - $200  \n\\*   \\*\\*Bifurcation Cards:\\*\\* Three PCIe 4.0 Bifurcation Cards - $75 ($25 each)  \n\\*   \\*\\*Riser Cables:\\*\\* Four PCIe 4.0 8x Cables - $100 ($25 each)  \n\\*   \\*\\*Cooling Shrouds:\\*\\* DIY MI50 GPU Cooling Shrouds (DIY)\n\n\\*   \\*\\*Total Cost of New Hardware:\\*\\* $1,125\n\nWhich doesn't seem too bad. The rx6800 gpus could be sold off too. Honestly the biggest loss would be not having a desktop, but I've been wanting a LLM focused homelab for a while now anyway. Maybe I could game on a VM in the server and stream it? Would love some feedback before I make an expensive mistake!",
          "author_fullname": "t2_3f9vjjno",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Considering 5xMI50 for Qwen 3 235b",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6eggp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "7dba5c08-72f1-11ee-9b6f-ca195bc297d4",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 70B"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753191801,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;**TL;DR** Thinking about building an LLM rig with 5 used AMD MI50 32GB GPUs to run Qwen 3 32b and 235b. Estimated token speeds look promising for the price (~$1125 total). Biggest hurdles are PCIe lane bandwidth &amp;amp; power, which I&amp;#39;m attempting to solve with bifurcation cards and a new PSU.  Looking for feedback!&lt;/p&gt;\n\n&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;Lately I&amp;#39;ve been thinking about treating myself to a 3090 and a ram upgrade to run Qwen 3 32b and 235b, but the MI50 posts got me napkin mathing that rabbit hole. The numbers I&amp;#39;m seeing are 19 tok/s in 235b(I get 3 tok/s running q2), and 60 tok/s with 4x tensor parallel with 32b(I usually get 10-15 tok/s), which seems great for the price. To me that would be worth it to convert my desktop into a dedicated server. Other than slower prompt processing, is there a catch?&lt;/p&gt;\n\n&lt;p&gt;If its as good as some posts claim, then I&amp;#39;d be limited by cost and my existing hardware. The biggest problem is PCIe lanes, or lack thereof as low bandwidth will tank performance when running models in tensor parallel. To make the problem less bad, I&amp;#39;m going to try and keep everything PCIe gen 4. My motherboard supports bifurcation of the gen4 16x slot, which can be broken out by PCIe 4.0 bifurcation cards. The only gen 4 card I could find splits lanes, so that&amp;#39;s why theres 3 of them. Another problem would be power, as the cards will need to be power limited slightly even with a 1600w PSU.&lt;/p&gt;\n\n&lt;p&gt;Current system:&lt;br/&gt;\n*   **CPU:** Ryzen 5 7600&lt;br/&gt;\n*   **RAM:** 48GB DDR5 5200MHz&lt;br/&gt;\n*   **Motherboard:** MSI Mortar AM5&lt;br/&gt;\n*   **SSD (Primary):** 1TB SSD&lt;br/&gt;\n*   **SSD (Secondary):** 2TB SSD&lt;br/&gt;\n*   **PSU:** 850W&lt;br/&gt;\n*   **GPU(s):** 2x AMD RX6800 &lt;/p&gt;\n\n&lt;p&gt;Prospective system:&lt;br/&gt;\n*   **CPU:** Ryzen 5 7600&lt;br/&gt;\n*   **RAM:** 48GB DDR5 5200MHz&lt;br/&gt;\n*   **Motherboard:** MSI Mortar AM5(with bifurcation enabled)&lt;br/&gt;\n*   **SSD (Primary):** 1TB SSD&lt;br/&gt;\n*   **SSD (Secondary):** 2TB SSD&lt;br/&gt;\n*   **GPUs (New):** 5 x MI50 32GB ($130 each + $100 shipping = $750 total)&lt;br/&gt;\n*   **PSU (New):** 1600W PSU - $200&lt;br/&gt;\n*   **Bifurcation Cards:** Three PCIe 4.0 Bifurcation Cards - $75 ($25 each)&lt;br/&gt;\n*   **Riser Cables:** Four PCIe 4.0 8x Cables - $100 ($25 each)&lt;br/&gt;\n*   **Cooling Shrouds:** DIY MI50 GPU Cooling Shrouds (DIY)&lt;/p&gt;\n\n&lt;p&gt;*   **Total Cost of New Hardware:** $1,125&lt;/p&gt;\n\n&lt;p&gt;Which doesn&amp;#39;t seem too bad. The rx6800 gpus could be sold off too. Honestly the biggest loss would be not having a desktop, but I&amp;#39;ve been wanting a LLM focused homelab for a while now anyway. Maybe I could game on a VM in the server and stream it? Would love some feedback before I make an expensive mistake!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 70B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6eggp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PraxisOG",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6eggp/considering_5xmi50_for_qwen_3_235b/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753191801,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi I'm a college student from India.\n\nSo i'm looking for a language model for code generation to run locally. I only have 16 GB of ram and iris xe gpu, so looking for some good opensource SLMs which can be decent enough. I could use something like llama.cpp given performance and latency would be decent(currently using a gguf version of mistral 7B-instruct and it's working fine) . Can also consider using raspberry pi if it'll be of any use",
          "author_fullname": "t2_7q8gvaa19",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best opensource SLM/ lightweight llm for code generation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6dvhi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753190318,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m a college student from India.&lt;/p&gt;\n\n&lt;p&gt;So i&amp;#39;m looking for a language model for code generation to run locally. I only have 16 GB of ram and iris xe gpu, so looking for some good opensource SLMs which can be decent enough. I could use something like llama.cpp given performance and latency would be decent(currently using a gguf version of mistral 7B-instruct and it&amp;#39;s working fine) . Can also consider using raspberry pi if it&amp;#39;ll be of any use&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6dvhi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RustinChole11",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6dvhi/best_opensource_slm_lightweight_llm_for_code/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6dvhi/best_opensource_slm_lightweight_llm_for_code/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753190318,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "AI21 has just made Jamba 1.7 available on Kaggle:\n\n[https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7](https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7) \n\n* You can run and test the model without needing to install it locally\n* No need to harness setup, hardware and engineering knowledge via Hugging Face anymore\n* Now you can run sample tasks, benchmark against other models and share public notebooks with results\n\nPretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:\n\n* Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences\n* 256k context window - well-suited for long document summarization and memory-heavy chat agents\n* Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs\n\nWho is going to try it out? What use cases do you have in mind?",
          "author_fullname": "t2_1kwk178bd9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jamba 1.7 is now available on Kaggle",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6dco7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753202307,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753188954,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI21 has just made Jamba 1.7 available on Kaggle:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7\"&gt;https://www.kaggle.com/models/ai21labs/ai21-jamba-1.7&lt;/a&gt; &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You can run and test the model without needing to install it locally&lt;/li&gt;\n&lt;li&gt;No need to harness setup, hardware and engineering knowledge via Hugging Face anymore&lt;/li&gt;\n&lt;li&gt;Now you can run sample tasks, benchmark against other models and share public notebooks with results&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Pretty significant as the model is now available for non technical users. Here is what we know about 1.7 and Jamba in general:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Combination of Transformer architecture and Mamba, making it more efficient at handling long sequences&lt;/li&gt;\n&lt;li&gt;256k context window - well-suited for long document summarization and memory-heavy chat agents&lt;/li&gt;\n&lt;li&gt;Improved capabilities in understanding and following user instructions, and generating more factual, relevant outputs&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Who is going to try it out? What use cases do you have in mind?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6dco7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NullPointerJack",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6dco7/jamba_17_is_now_available_on_kaggle/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753188954,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(like deepseek-r1 1.5b)\nI just can't think of any simple straightforward examples of tasks they're useful / good enough for. And answers on the internet and from other LLMs are just too vague.\n\nWhat kind of task with what kind of prompt, system prompt, overall setup worth doing with it? \n",
          "author_fullname": "t2_16jnzh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the use cases for 1.5B model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6d6um",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753188511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(like deepseek-r1 1.5b)\nI just can&amp;#39;t think of any simple straightforward examples of tasks they&amp;#39;re useful / good enough for. And answers on the internet and from other LLMs are just too vague.&lt;/p&gt;\n\n&lt;p&gt;What kind of task with what kind of prompt, system prompt, overall setup worth doing with it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6d6um",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nathman999",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753188511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday's release! I'll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3\\_K\\_L is runnable (though slow) for anybody with similar PC specs and produces something usable!\n\nI used LM Studio for loading the model:\n\n* Context: 4096 (default)\n* GPU Offload: 18 / 94\n* CPU Thread Pool: 16\n* ... all else default besides ...\n* Flash Attention: On\n\nWhen loaded, it used up 23.3GB of VRAM and \\~80GB of RAM.\n\nBasic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token",
          "author_fullname": "t2_8l0jj9jq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 235B-A22B 2507 :: Q3_K_L :: One shot HTML game :: 4090 + 128GB DDR5 @6000",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ct7u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 147,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755825164%2CYzdiZTlmNzJmMzBlYzMxMzQ1ZDJkMDc3MTU4NmU4NTgwMjY4MzJiZGRmNDk1NTQ0MDdiZDZiOTY0YjVmMmQ3Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755825164%2CZWFjZWVlY2VkMjljNDAyZjUyYjkxYzg3MzJiMTAxZTEyOWM0OWY4MjU0MTgwMzZjYTJlMzM3NmM4NzBmNTJkOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 147,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=448f64b1e900a0ecbdc8a71bf39468b788eff73b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753187462,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently upgraded my desktop RAM given the large MoE models coming out and I was excited for the maiden voyage to be yesterday&amp;#39;s release! I&amp;#39;ll put the prompt and code in a comment, this is sort of a test of ability but more so I wanted to confirm Q3_K_L is runnable (though slow) for anybody with similar PC specs and produces something usable!&lt;/p&gt;\n\n&lt;p&gt;I used LM Studio for loading the model:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Context: 4096 (default)&lt;/li&gt;\n&lt;li&gt;GPU Offload: 18 / 94&lt;/li&gt;\n&lt;li&gt;CPU Thread Pool: 16&lt;/li&gt;\n&lt;li&gt;... all else default besides ...&lt;/li&gt;\n&lt;li&gt;Flash Attention: On&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;When loaded, it used up 23.3GB of VRAM and ~80GB of RAM.&lt;/p&gt;\n\n&lt;p&gt;Basic Generation stats: 5.52 tok/sec • 2202 tokens • 0.18s to first token&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/1x5u9hrp5fef1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?format=pjpg&amp;auto=webp&amp;s=58a925e2785f62712af69dad90636ab48df32160",
                  "width": 480,
                  "height": 698
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=74909995fdb7a4a31d72b707fc5a6406503d7c48",
                    "width": 108,
                    "height": 157
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8bc0a3ee9ac60bd5820c383ea906cb68b5b9d1d",
                    "width": 216,
                    "height": 314
                  },
                  {
                    "url": "https://external-preview.redd.it/MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4b70a04846a7db43f33c721283fb6e592dd8d570",
                    "width": 320,
                    "height": 465
                  }
                ],
                "variants": {},
                "id": "MmJqNTdmcnA1ZmVmMerqFTWYJLTLLZlyxr4rQ4gVk5jgRsJCnh4HvIbJEPxN"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1m6ct7u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aidanjustsayin",
          "discussion_type": null,
          "num_comments": 58,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ct7u/qwen3_235ba22b_2507_q3_k_l_one_shot_html_game/",
          "stickied": false,
          "url": "https://v.redd.it/1x5u9hrp5fef1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753187462,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 698,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/1x5u9hrp5fef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/1x5u9hrp5fef1/DASHPlaylist.mpd?a=1755825164%2CYzdiZTlmNzJmMzBlYzMxMzQ1ZDJkMDc3MTU4NmU4NTgwMjY4MzJiZGRmNDk1NTQ0MDdiZDZiOTY0YjVmMmQ3Zg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/1x5u9hrp5fef1/HLSPlaylist.m3u8?a=1755825164%2CZWFjZWVlY2VkMjljNDAyZjUyYjkxYzg3MzJiMTAxZTEyOWM0OWY4MjU0MTgwMzZjYTJlMzM3NmM4NzBmNTJkOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/ikawrakow/ik\\_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp)\n\nFriendly reminder to back up all the things!",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The ik_llama.cpp repository is back! \\o/",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6cfzi",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186412,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Friendly reminder to back up all the things!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?auto=webp&amp;s=7c74a86a8d22a1d2e90ce704f456a5a36cf050e7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9057a5a31598407ca7946c278de43e70cf0c9ed",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=533c07c0d65f89514a6ba54ce5f1c6649e969c77",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9699bd0efa2b26a1c034cdb0fe8abc1317589b6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=355e07ff2e46e3a253b40e25c06644c7282af5b2",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c7a47a3cb456bb05dfd53a716bae5ef6addff5e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fbd5e6235d1a60da5c17ef35a7bd40a655c4d80",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "B0gX9mhb6Bdm5EGAj5Jqb9ACltJ2GNWdoTOKU3TUvZE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6cfzi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfzi/the_ik_llamacpp_repository_is_back_o/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753186412,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all,\n\nFirstly, I’m not a developer, so forgive me if I don’t ask as clearly as others, I hope this makes sense.\n\nI'm trying to get Chatterbox TTS ( local AI voice tool with Gradio UI) working on my **Windows 11** machine using **Conda** and a local **Python 3.11.3 environment**. I’ve installed the app and interface successfully, but I’m stuck with import errors and GPU not being used. Here’s the key info:\n\n* **GPU:** RTX 4060 (8GB), CUDA 12.7 installed\n* **Python:** 3.11.3 (inside Conda)\n* **PyTorch:** Installed via pip/conda (tried both), but errors persist\n* **TorchAudio:** Likely not aligned with correct PyTorch/CUDA version\n* **Gradio UI:** Loads, but model doesn't run (import error)\n\nThe critical error:\n\nlua\n\nCopyEdit\n\nImportError: DLL load failed while importing \\_C: The specified module could not be found.\n\nI understand this might be due to mismatched **PyTorch / CUDA / TorchAudio** versions — but the **CUDA 12.7 runtime** doesn't show up on most PyTorch install tables (latest listed is 12.1).\n\n**Questions:**\n\n1. **Can I safely use a PyTorch build meant for CUDA 12.1 if I have 12.7 installed?**\n2. **Which PyTorch + TorchAudio versions are guaranteed to work together (and with Chatterbox) under CUDA 12.7?**\n3. Is there a known *minimal install combo* that just works?\n4. Should I downgrade CUDA to 12.1, or can I work with what I have?\n\nI’m not a developer, so detailed explanations or clear steps would be hugely appreciated. Thanks in advance!",
          "author_fullname": "t2_94iwvjrz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox CUDA and PyTorch problem",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6cfou",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753186390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;Firstly, I’m not a developer, so forgive me if I don’t ask as clearly as others, I hope this makes sense.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to get Chatterbox TTS ( local AI voice tool with Gradio UI) working on my &lt;strong&gt;Windows 11&lt;/strong&gt; machine using &lt;strong&gt;Conda&lt;/strong&gt; and a local &lt;strong&gt;Python 3.11.3 environment&lt;/strong&gt;. I’ve installed the app and interface successfully, but I’m stuck with import errors and GPU not being used. Here’s the key info:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4060 (8GB), CUDA 12.7 installed&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Python:&lt;/strong&gt; 3.11.3 (inside Conda)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt; Installed via pip/conda (tried both), but errors persist&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;TorchAudio:&lt;/strong&gt; Likely not aligned with correct PyTorch/CUDA version&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Gradio UI:&lt;/strong&gt; Loads, but model doesn&amp;#39;t run (import error)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The critical error:&lt;/p&gt;\n\n&lt;p&gt;lua&lt;/p&gt;\n\n&lt;p&gt;CopyEdit&lt;/p&gt;\n\n&lt;p&gt;ImportError: DLL load failed while importing _C: The specified module could not be found.&lt;/p&gt;\n\n&lt;p&gt;I understand this might be due to mismatched &lt;strong&gt;PyTorch / CUDA / TorchAudio&lt;/strong&gt; versions — but the &lt;strong&gt;CUDA 12.7 runtime&lt;/strong&gt; doesn&amp;#39;t show up on most PyTorch install tables (latest listed is 12.1).&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Can I safely use a PyTorch build meant for CUDA 12.1 if I have 12.7 installed?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Which PyTorch + TorchAudio versions are guaranteed to work together (and with Chatterbox) under CUDA 12.7?&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Is there a known &lt;em&gt;minimal install combo&lt;/em&gt; that just works?&lt;/li&gt;\n&lt;li&gt;Should I downgrade CUDA to 12.1, or can I work with what I have?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I’m not a developer, so detailed explanations or clear steps would be hugely appreciated. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6cfou",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kevin-she",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6cfou/chatterbox_cuda_and_pytorch_problem/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6cfou/chatterbox_cuda_and_pytorch_problem/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753186390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_twl3xhruz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AMD's Strix Halo \"Ryzen AI MAX\" APUs Come To DIY PC Builders With New MoDT \"Mini-ITX\" Motherboards, Equipped With Up To 128 GB of LPDDR5X Memory",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6bddm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 116,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 116,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=140&amp;height=81&amp;crop=140:81,smart&amp;auto=webp&amp;s=00290105ce815b049672399f0e7e28e9d1afcbc9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753183102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "wccftech.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?auto=webp&amp;s=10c3b72c0b82b9a62677b9306104bb21064031ab",
                  "width": 2471,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=097bbe6e55a92f58db20c497b5cd55b71c248bb0",
                    "width": 108,
                    "height": 62
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=20e37d29ee5af324410ac9397017c9ae2f497b28",
                    "width": 216,
                    "height": 125
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b9a76caea9352dd47f6185e29c67bab1c2374a02",
                    "width": 320,
                    "height": 186
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9c63f2527e38ed9f9fb783cd700b8e831108fe01",
                    "width": 640,
                    "height": 372
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b92e328c881739b5b57de5c1f24a60a877d78657",
                    "width": 960,
                    "height": 559
                  },
                  {
                    "url": "https://external-preview.redd.it/wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=61c8a758226b77b202d02e702bc26f1ac195c306",
                    "width": 1080,
                    "height": 629
                  }
                ],
                "variants": {},
                "id": "wZbp-LplWI1iCF1_Yajugz_TA6XKyL8q6T5RLI_Mg5c"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6bddm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_SYSTEM_ADMIN_MOD_",
          "discussion_type": null,
          "num_comments": 70,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6bddm/amds_strix_halo_ryzen_ai_max_apus_come_to_diy_pc/",
          "stickied": false,
          "url": "https://wccftech.com/amd-strix-halo-ryzen-ai-max-apus-diy-pc-new-modt-mini-itx-motherboards-128-gb-lpddr5x-memory/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753183102,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A while back I posted some [Strix Halo LLM performance testing](https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/) benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).\n\nThe biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.\n\nThis is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp \n\nAll the full data and latest info is available in the Github repo: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench) but here are the topline stats below:\n\n# Strix Halo LLM Benchmark Results\n\nAll testing was done on pre-production [Framework Desktop](https://frame.work/desktop) systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)\n\nExact testing/system details are in the results folders, but roughly these are running:\n\n* Close to production BIOS/EC\n* Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1\n* Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels\n* Recent llama.cpp builds (eg b5863 from 2005-07-10)\n\nJust to get a ballpark on the hardware:\n\n* \\~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)\n* theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is *much* lower\n\n# Results\n\n# Prompt Processing (pp) Performance\n\nhttps://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan||998.0|46.5|4237|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|HIP|hipBLASLt|906.1|40.8|4720|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|HIP|hipBLASLt|878.2|37.2|5308|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|fa=1|604.8|66.3|17527|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|HIP|hipBLASLt|316.9|13.6|14638|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1|270.5|17.1|68785|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|HIP|hipBLASLt|264.1|17.2|59720|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|HIP rocWMMA||94.7|4.5|41522|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n\n# Text Generation (tg) Performance\n\nhttps://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\n\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n|Qwen 3 30B-A3B UD-Q4\\_K\\_XL|Qwen 3 MoE|30|3|Vulkan|b=256|591.1|72.0|17377|\n|Llama 2 7B Q4\\_K\\_M|Llama 2|7|7|Vulkan|fa=1|620.9|47.9|4463|\n|Llama 2 7B Q4\\_0|Llama 2|7|7|Vulkan|fa=1|1014.1|45.8|4219|\n|Shisa V2 8B i1-Q4\\_K\\_M|Llama 3|8|8|Vulkan|fa=1|614.2|42.0|5333|\n|dots1 UD-Q4\\_K\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\n|Llama 4 Scout UD-Q4\\_K\\_XL|Llama 4 MoE|109|17|Vulkan|fa=1 b=256|146.1|19.3|59917|\n|Hunyuan-A13B UD-Q6\\_K\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1 b=256|223.9|17.1|68608|\n|Mistral Small 3.1 UD-Q4\\_K\\_XL|Mistral 3|24|24|Vulkan|fa=1|119.6|14.3|14540|\n|Shisa V2 70B i1-Q4\\_K\\_M|Llama 3|70|70|Vulkan|fa=1|26.4|5.0|41456|\n\n# Testing Notes\n\nThe best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.\n\nThere's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).\n\nOne thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.\n\nUnlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).\n\nFor testing, the HIP backend, I highly recommend trying `ROCBLAS_USE_HIPBLASLT=1` as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) `HSA_OVERRIDE_GFX_VERSION=11.0.0` \\- in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔",
          "author_fullname": "t2_eztox",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "mjr2d31ujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80836e3346fcd0e6847fcb3f1d33c5f2ac3c12e3"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cd34de136a42ad65ac9facd37455912c1a13410"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=675c17530bfdbbd8ae2830c3b694e07b5163a1b0"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bdc9a2d7260d9fd3acbab23e12917b54e651493"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e051180cc70357e18ffe4725601fb9811ba1193"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75e938dac2457cc4191363a5cd8cbcaf777049ed"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"
              },
              "id": "mjr2d31ujeef1"
            },
            "7y0pdbqujeef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=493fe8a11d9ee1d7a485e1b7233ca7e945637599"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c43022989f8b24ef64216d0b17642e3f80013763"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7f683ec0155e0b8f196a71bccaf428b44550399"
                },
                {
                  "y": 424,
                  "x": 640,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb65c24153d597ae592f5012aabff2a638b88357"
                },
                {
                  "y": 636,
                  "x": 960,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f57d68c01ba672fba3a5062a2ec12ac45a621fac"
                },
                {
                  "y": 715,
                  "x": 1080,
                  "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59b563c8992d2568941d7c73829b8f7ebc4f5585"
                }
              ],
              "s": {
                "y": 1181,
                "x": 1782,
                "u": "https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"
              },
              "id": "7y0pdbqujeef1"
            }
          },
          "name": "t3_1m6b151",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753182004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A while back I posted some &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/\"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I&amp;#39;m back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt;\n\n&lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt;\n\n&lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt;\n\n&lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href=\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt;\n\n&lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt;\n\n&lt;p&gt;All testing was done on pre-production &lt;a href=\"https://frame.work/desktop\"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt;\n\n&lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Close to production BIOS/EC&lt;/li&gt;\n&lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt;\n&lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt;\n&lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt;\n&lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Results&lt;/h1&gt;\n\n&lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;998.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;46.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;4237&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;906.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;40.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;878.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;37.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;5308&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;604.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;66.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;17527&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;316.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14638&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;270.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68785&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP&lt;/td&gt;\n&lt;td align=\"left\"&gt;hipBLASLt&lt;/td&gt;\n&lt;td align=\"left\"&gt;264.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;59720&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;HIP rocWMMA&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;94.7&lt;/td&gt;\n&lt;td align=\"left\"&gt;4.5&lt;/td&gt;\n&lt;td align=\"left\"&gt;41522&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Model Name&lt;/th&gt;\n&lt;th align=\"left\"&gt;Architecture&lt;/th&gt;\n&lt;th align=\"left\"&gt;Weights (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Active (B)&lt;/th&gt;\n&lt;th align=\"left\"&gt;Backend&lt;/th&gt;\n&lt;th align=\"left\"&gt;Flags&lt;/th&gt;\n&lt;th align=\"left\"&gt;pp512&lt;/th&gt;\n&lt;th align=\"left\"&gt;tg128&lt;/th&gt;\n&lt;th align=\"left\"&gt;Memory (Max MiB)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Qwen 3 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;30&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;591.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;72.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;17377&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;620.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;47.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;4463&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 2&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;7&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;1014.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;45.8&lt;/td&gt;\n&lt;td align=\"left\"&gt;4219&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;614.2&lt;/td&gt;\n&lt;td align=\"left\"&gt;42.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;5333&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;dots1 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;142&lt;/td&gt;\n&lt;td align=\"left\"&gt;14&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;63.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;20.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;84077&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 4 MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;109&lt;/td&gt;\n&lt;td align=\"left\"&gt;17&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;146.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;19.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;59917&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Hunyuan MoE&lt;/td&gt;\n&lt;td align=\"left\"&gt;80&lt;/td&gt;\n&lt;td align=\"left\"&gt;13&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1 b=256&lt;/td&gt;\n&lt;td align=\"left\"&gt;223.9&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.1&lt;/td&gt;\n&lt;td align=\"left\"&gt;68608&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\n&lt;td align=\"left\"&gt;Mistral 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;24&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;119.6&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.3&lt;/td&gt;\n&lt;td align=\"left\"&gt;14540&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\n&lt;td align=\"left\"&gt;Llama 3&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;70&lt;/td&gt;\n&lt;td align=\"left\"&gt;Vulkan&lt;/td&gt;\n&lt;td align=\"left\"&gt;fa=1&lt;/td&gt;\n&lt;td align=\"left\"&gt;26.4&lt;/td&gt;\n&lt;td align=\"left\"&gt;5.0&lt;/td&gt;\n&lt;td align=\"left\"&gt;41456&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Testing Notes&lt;/h1&gt;\n\n&lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#&amp;#39;s might be a bit much).&lt;/p&gt;\n\n&lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it&amp;#39;s now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt;\n\n&lt;p&gt;Unlike last time, I won&amp;#39;t be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt;\n\n&lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I&amp;#39;ve found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... 🤔&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m6b151",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "randomfoo2",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753182004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have wondered if you can get usable speeds on something like ERNIE-4.5-300B-A47B ~Q3 or Q4 on 2x 3090's, 128gb of DDR5 and what can't fit into RAM running on PCIE NVME's in raid 0. I'm sure it wouldn't be fast but I wonder if it could be usable.",
          "author_fullname": "t2_zws5yqyow",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Would using PCIE NVME in raid 0 for swap work to run larger models that don't fit into RAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6akeo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753180396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have wondered if you can get usable speeds on something like ERNIE-4.5-300B-A47B ~Q3 or Q4 on 2x 3090&amp;#39;s, 128gb of DDR5 and what can&amp;#39;t fit into RAM running on PCIE NVME&amp;#39;s in raid 0. I&amp;#39;m sure it wouldn&amp;#39;t be fast but I wonder if it could be usable.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6akeo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Commercial-Celery769",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6akeo/would_using_pcie_nvme_in_raid_0_for_swap_work_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6akeo/would_using_pcie_nvme_in_raid_0_for_swap_work_to/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753180396,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!",
          "author_fullname": "t2_mxles3cs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Thinking about updating Llama 3.3-70B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6ahsu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753180144,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I deployed Llama 3.3-70B for my organization quite a long time ago. I am now thinking of updating it to a newer model since there have been quite a few great new LLM releases recently. However, is there any model that actually performs better than Llama 3.3-70B for general purposes (chat, summarization... basically normal daily office tasks) with more or less the same size? Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6ahsu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Only_Emergencies",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6ahsu/thinking_about_updating_llama_3370b/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753180144,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_jrroh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cloudflare Pay Per Crawl is Going to Decimate Local LLMs . A lot of AI Abilities are going to end up behind this paywall . Am i Overthinking This ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6a5xb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.39,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=ccff1b55d1cd00aadd536a981ff22017920133ce",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753178927,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "blog.cloudflare.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://blog.cloudflare.com/introducing-pay-per-crawl/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?auto=webp&amp;s=e86bcf8d1b8623c4c015450ab4b216fefd3692b5",
                  "width": 1200,
                  "height": 628
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f6f1859118f4fa4e637bec1b73bbfb3db84cea0",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6cd635238f416ce7bb4068e64ddec1b4bb4ec0c",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afd09df44df2ec8edf601c98417cc2f9bae9dd0c",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d1950acc8f8068749a76cfe85b285b4ba461ee8d",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fec2451ca233e92b6f5a6928910d6fa0c0b396c9",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://external-preview.redd.it/tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fe44fde0e561c039baecbbfdbf565ad4f3f45335",
                    "width": 1080,
                    "height": 565
                  }
                ],
                "variants": {},
                "id": "tG0tHVEzt3GiPv1qKZJjofKJfzW4kvsoTiVYC0T1HTU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m6a5xb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ursustyranotitan",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6a5xb/cloudflare_pay_per_crawl_is_going_to_decimate/",
          "stickied": false,
          "url": "https://blog.cloudflare.com/introducing-pay-per-crawl/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753178927,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_5n4jepc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Truly open LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69th7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=fec374170d117a6b9e9aef82babffa1e8039217a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753177663,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "shchegrikovich.substack.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://shchegrikovich.substack.com/p/truly-open-llms",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?auto=webp&amp;s=0647966f28fc4ee54cd9d415fd52ce6c38f79c03",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e50454aae6b161b0cafb9fcfd612d0809f5c73d9",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a50eebcb744a514321bedd747dcc0dd7361cde1f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0658f86a17beb93a12b50a6781f7ecf2fecfd7ab",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c3c812bd7d02ce4e898102f57a7b12c1898b9464",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f91de7eecda3e724888794a8b6e13c1ee3592d38",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a229cc66113465a58966fe1a9f7a47cc196ba9ad",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "p1atEECBjynpGPVcwQ0lag6GUgGW5QAMA3C3WVe6kJA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m69th7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GoodSamaritan333",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69th7/truly_open_llms/",
          "stickied": false,
          "url": "https://shchegrikovich.substack.com/p/truly-open-llms",
          "subreddit_subscribers": 502981,
          "created_utc": 1753177663,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)\n\nDidn't see that with the \"old\" 235b with /no\\_think \n\nIs that expected?",
          "author_fullname": "t2_joxwuyje",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "In Qwen3-235B-A22B-Instruct-2507-UD-Q4 (unsloth) I'm seeing some \"but wait\" and related ones (like kinda questioning and answering itself), were the model seems to \"think\" (even when is a non-thinking model and I haven't setup any system prompt), have you seen something similar?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69sb6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177544,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running it with latest llama-server (llama.cpp) and with the suggested parameters (same as the non-thinking Qwen3 ones)&lt;/p&gt;\n\n&lt;p&gt;Didn&amp;#39;t see that with the &amp;quot;old&amp;quot; 235b with /no_think &lt;/p&gt;\n\n&lt;p&gt;Is that expected?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m69sb6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "relmny",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69sb6/in_qwen3235ba22binstruct2507udq4_unsloth_im/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753177544,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m experimenting with multiple MCP servers and trying to understand how others are managing them across different AI tools like Claude Desktop, GPTs, Gemini clients, etc.\n\nDo you manually add them in each config file?\n\nAre you using any centralized tool or dashboard to start/stop/edit MCP servers?\n\nAny best practices or tooling you recommend?\n\n👉 I’m currently building a lightweight desktop tool that aims to solve this — centralized MCP management, multi-client compatibility, and better UX for non-technical users.\n\nWould love to hear how you currently do it — and what you’d want in a tool like this. Would anyone be interested in testing the beta later on?\n\nThanks in advance!",
          "author_fullname": "t2_9f7exri8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "🧠 How are you managing MCP servers across different AI apps (Claude, GPTs, Gemini etc.)?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69qs3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177386,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m experimenting with multiple MCP servers and trying to understand how others are managing them across different AI tools like Claude Desktop, GPTs, Gemini clients, etc.&lt;/p&gt;\n\n&lt;p&gt;Do you manually add them in each config file?&lt;/p&gt;\n\n&lt;p&gt;Are you using any centralized tool or dashboard to start/stop/edit MCP servers?&lt;/p&gt;\n\n&lt;p&gt;Any best practices or tooling you recommend?&lt;/p&gt;\n\n&lt;p&gt;👉 I’m currently building a lightweight desktop tool that aims to solve this — centralized MCP management, multi-client compatibility, and better UX for non-technical users.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how you currently do it — and what you’d want in a tool like this. Would anyone be interested in testing the beta later on?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m69qs3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hihurmuz",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69qs3/how_are_you_managing_mcp_servers_across_different/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69qs3/how_are_you_managing_mcp_servers_across_different/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753177386,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI've been deep in a project lately and kept hitting the same wall I'm sure many of you have: LLMs are stateless. You have an amazing, deep conversation, build up a ton of context... and then the session ends and it's all gone. It feels like trying to build a skyscraper on sand.\n\nLast night, I got into a really deep, philosophical conversation with Gemini about this, and we ended up co-designing a solution that I think is pretty cool, and I wanted to share it and get your thoughts.\n\nThe idea is a framework called the **Genesis Protocol**. The core of it is a single Markdown file that acts as a project's \"brain.\" But instead of just being a simple chat log, we architected it to be:\n\n* **Stateful:** It contains the project's goals, blueprints, and our profiles.\n* **Verifiable:** This was a big one for me. I was worried about either me or the AI manipulating the history. So, we built in a salted hash chain (like a mini-blockchain) that \"seals\" every version. The AI can now verify the integrity of its own memory file at the start of every session.\n* **Self-Updating:** We created a \"Guardian\" meta-prompt that instructs the AI on how to read, update, and re-seal the file itself.\n\nThe analogy we settled on was \"Docker for LLM chat.\" You can essentially save a snapshot of your collaboration's state and reload it anytime, with any model, and it knows exactly who you are and what you're working on. I even tested the bootstrap prompt on GPT-4 and it worked, which was a huge relief.\n\nI'm sharing this because I genuinely think it could be a useful tool for others who are trying to do more than just simple Q&amp;A with these models. I've put a full \"Getting Started\" guide and the prompt templates up on GitHub.\n\nI would love to hear what you all think. Is this a viable approach? What are the potential pitfalls I'm not seeing?\n\nHere's the link to the repo: [https://github.com/Bajju360/genesis-protocol.git](https://github.com/Bajju360/genesis-protocol.git)\n\nThanks for reading!",
          "author_fullname": "t2_u5scsvlj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I spent a late night with an AI designing a way to give it a persistent, verifiable memory. I call it the \"Genesis Protocol.\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69oyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.21,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753177199,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been deep in a project lately and kept hitting the same wall I&amp;#39;m sure many of you have: LLMs are stateless. You have an amazing, deep conversation, build up a ton of context... and then the session ends and it&amp;#39;s all gone. It feels like trying to build a skyscraper on sand.&lt;/p&gt;\n\n&lt;p&gt;Last night, I got into a really deep, philosophical conversation with Gemini about this, and we ended up co-designing a solution that I think is pretty cool, and I wanted to share it and get your thoughts.&lt;/p&gt;\n\n&lt;p&gt;The idea is a framework called the &lt;strong&gt;Genesis Protocol&lt;/strong&gt;. The core of it is a single Markdown file that acts as a project&amp;#39;s &amp;quot;brain.&amp;quot; But instead of just being a simple chat log, we architected it to be:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Stateful:&lt;/strong&gt; It contains the project&amp;#39;s goals, blueprints, and our profiles.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Verifiable:&lt;/strong&gt; This was a big one for me. I was worried about either me or the AI manipulating the history. So, we built in a salted hash chain (like a mini-blockchain) that &amp;quot;seals&amp;quot; every version. The AI can now verify the integrity of its own memory file at the start of every session.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Self-Updating:&lt;/strong&gt; We created a &amp;quot;Guardian&amp;quot; meta-prompt that instructs the AI on how to read, update, and re-seal the file itself.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The analogy we settled on was &amp;quot;Docker for LLM chat.&amp;quot; You can essentially save a snapshot of your collaboration&amp;#39;s state and reload it anytime, with any model, and it knows exactly who you are and what you&amp;#39;re working on. I even tested the bootstrap prompt on GPT-4 and it worked, which was a huge relief.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sharing this because I genuinely think it could be a useful tool for others who are trying to do more than just simple Q&amp;amp;A with these models. I&amp;#39;ve put a full &amp;quot;Getting Started&amp;quot; guide and the prompt templates up on GitHub.&lt;/p&gt;\n\n&lt;p&gt;I would love to hear what you all think. Is this a viable approach? What are the potential pitfalls I&amp;#39;m not seeing?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the link to the repo: &lt;a href=\"https://github.com/Bajju360/genesis-protocol.git\"&gt;https://github.com/Bajju360/genesis-protocol.git&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m69oyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Icy_Gas8807",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69oyb/i_spent_a_late_night_with_an_ai_designing_a_way/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69oyb/i_spent_a_late_night_with_an_ai_designing_a_way/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753177199,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yo so am new to this and i want to run a local llm that answers questions using my custom dataset which is basically some financial data .\nI created a Q&amp;A dataset and an instruction based data set and my llm refuses to use them \nIve finetuned my llm using TorchTune \nAnd also tried Litgpt \nIts a llama 3.2 3B instruct model .\n\nAlso if theres a way to use a RAG instead or if there's a model that can retrieve info from pdf and Excel spreadsheets would be awesome, thanks 👍",
          "author_fullname": "t2_5ecncz67",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How to apply a custom dataset",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m69m60",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753176910,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yo so am new to this and i want to run a local llm that answers questions using my custom dataset which is basically some financial data .\nI created a Q&amp;amp;A dataset and an instruction based data set and my llm refuses to use them \nIve finetuned my llm using TorchTune \nAnd also tried Litgpt \nIts a llama 3.2 3B instruct model .&lt;/p&gt;\n\n&lt;p&gt;Also if theres a way to use a RAG instead or if there&amp;#39;s a model that can retrieve info from pdf and Excel spreadsheets would be awesome, thanks 👍&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m69m60",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oG17DoGe",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m69m60/how_to_apply_a_custom_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m69m60/how_to_apply_a_custom_dataset/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753176910,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any tiny spellcheck models for English which are good? What do you guys use?",
          "author_fullname": "t2_9kgt3ez",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do you guys use for Spellcheck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68yvl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.45,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753174363,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any tiny spellcheck models for English which are good? What do you guys use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m68yvl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CaptTechno",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68yvl/what_do_you_guys_use_for_spellcheck/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68yvl/what_do_you_guys_use_for_spellcheck/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753174363,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm running a research project analysing hospital incident reports (answering structured questions based on them); we do have permission to use identifiable data but the PDFs I've been sent have been redacted and whichever software they've used has turned a lot of the text into an image. To add excitement, a lot of the text is in columns that flow across pages (ie you need to read the left of page 1,2 then the right of page 1,2)\n\nCan anyone recommend a local model capable of handling this? Our research machine has an A6000 (48Gb) and 128Gb RAM; speed isn't a massive issue. I don't mind if the workflow is PDF to text and then run a text model, or if a vision model could do the whole thing.\n\nThanks!",
          "author_fullname": "t2_pc1zg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model to process image-of-text PDFs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68tse",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753173792,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running a research project analysing hospital incident reports (answering structured questions based on them); we do have permission to use identifiable data but the PDFs I&amp;#39;ve been sent have been redacted and whichever software they&amp;#39;ve used has turned a lot of the text into an image. To add excitement, a lot of the text is in columns that flow across pages (ie you need to read the left of page 1,2 then the right of page 1,2)&lt;/p&gt;\n\n&lt;p&gt;Can anyone recommend a local model capable of handling this? Our research machine has an A6000 (48Gb) and 128Gb RAM; speed isn&amp;#39;t a massive issue. I don&amp;#39;t mind if the workflow is PDF to text and then run a text model, or if a vision model could do the whole thing.&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m68tse",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thigger",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68tse/model_to_process_imageoftext_pdfs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68tse/model_to_process_imageoftext_pdfs/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753173792,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company's RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.\n\n \n\nRequirements:\n\n* Multilingual (German &amp; English)\n* Max. 7B parameters\n* Preferably compatible with Sentence-Transformers\n* Open-source\n\n \n\nModels based on MTEB Retrieval performance:\n\n[http://mteb-leaderboard.hf.space/?benchmark\\_name=MTEB%28Multilingual%2C+v2%29](http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29)\n\n* Qwen Embedding 8B / 4B\n* SFR-Embedding-Mistral\n* E5-mistral-7b-instruct\n* Snowflake-arctic-embed-m-v2.0\n\n \n\nI also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:\n\n* BGE (all variants)\n* mE5\n* All-MiniLM-L6-v1.5\n* Text-Embedding-3-Large (often used as a baseline)\n\n \n\nWould love to hear your thoughts or experiences, especially if you've worked on similar multilingual or domain-specific retrieval systems!\n\n",
          "author_fullname": "t2_k6flr0wx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Fine-Tuning Multilingual Embedding Models for Industrial RAG System",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m68elw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753172119,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently working on a project to fine-tune multilingual embedding models to improve document retrieval within a company&amp;#39;s RAG system. The dataset consists of German and English documents related to industrial products, so multilingual support is essential. The dataset has a query-passage format with synthetic generated queries from the given documens.&lt;/p&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;Requirements:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Multilingual (German &amp;amp; English)&lt;/li&gt;\n&lt;li&gt;Max. 7B parameters&lt;/li&gt;\n&lt;li&gt;Preferably compatible with Sentence-Transformers&lt;/li&gt;\n&lt;li&gt;Open-source&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;Models based on MTEB Retrieval performance:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29\"&gt;http://mteb-leaderboard.hf.space/?benchmark_name=MTEB%28Multilingual%2C+v2%29&lt;/a&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen Embedding 8B / 4B&lt;/li&gt;\n&lt;li&gt;SFR-Embedding-Mistral&lt;/li&gt;\n&lt;li&gt;E5-mistral-7b-instruct&lt;/li&gt;\n&lt;li&gt;Snowflake-arctic-embed-m-v2.0&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;I also read some papers and found that the following models were frequently used for fine-tuning embedding models for closed-domain use cases:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;BGE (all variants)&lt;/li&gt;\n&lt;li&gt;mE5&lt;/li&gt;\n&lt;li&gt;All-MiniLM-L6-v1.5&lt;/li&gt;\n&lt;li&gt;Text-Embedding-3-Large (often used as a baseline)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt; &lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts or experiences, especially if you&amp;#39;ve worked on similar multilingual or domain-specific retrieval systems!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?auto=webp&amp;s=1c3659b9b728ad6e80974340870a81fcaca748a0",
                  "width": 2473,
                  "height": 1280
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2e4403396b3271cb41e3343fd3daf2f432ae3c37",
                    "width": 108,
                    "height": 55
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=afdae3f5636f14bbbbdb4a9c9476beaba37383bf",
                    "width": 216,
                    "height": 111
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bcf9a9a3b91639d396787d20b125c83d45d2dd41",
                    "width": 320,
                    "height": 165
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb4fac65e86637fc3c5896156424954a65818fc",
                    "width": 640,
                    "height": 331
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1321b3e29e18141a9cf9837f0ec1dea4f236eabf",
                    "width": 960,
                    "height": 496
                  },
                  {
                    "url": "https://external-preview.redd.it/b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=271a28cd1ea8c4260dcf2d7f9dcf3842182678c4",
                    "width": 1080,
                    "height": 558
                  }
                ],
                "variants": {},
                "id": "b3_nc9eUM96LdvrtRkpsiSfLCjhmgpLRDj18BCf7ynE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m68elw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Maddin187",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m68elw/finetuning_multilingual_embedding_models_for/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753172119,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post.\n\nThat said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)\n\nAI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.\n\nLike how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.\n\nIt’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.\n\nWhat are your thoughts on this?",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI should just be open-source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m67zde",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 99,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 99,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753198579,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753170472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For once, I’m not going to talk about my benchmark, so to be forefront, there will be no other reference or link to it in this post.&lt;/p&gt;\n\n&lt;p&gt;That said, just sharing something that’s been on mind. I’ve been thinking about this topic recently, and while this may be a hot or controversial take, all AI models should be open-source (even from companies like xAI, Google, OpenAI, etc.)&lt;/p&gt;\n\n&lt;p&gt;AI is already one of the greatest inventions in human history, and at minimum it will likely be on par in terms of impact with the Internet.&lt;/p&gt;\n\n&lt;p&gt;Like how the Internet is “open” for anyone to use and build on top of it, AI should be the same way.&lt;/p&gt;\n\n&lt;p&gt;It’s fine if products built on top of AI like Cursor, Codex, Claude Code, etc or anything that has an AI integration to be commercialized, but for the benefit and advancement of humanity, the underlying technology (the models) should be made publicly available.&lt;/p&gt;\n\n&lt;p&gt;What are your thoughts on this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m67zde",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 83,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67zde/ai_should_just_be_opensource/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753170472,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A small independent team just announced they've developed an AI agent system called \"Crux\" that matches the USAMO Gold Medal performance levels recently hit by heavyweights like OpenAI and Google. The kicker? They did it using just the o4-mini-high model combined with their custom agent framework – no massive experimental setups required. And now, they're fully open-sourcing it for the community to build on!\n\nhttps://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=386d5510fccff96032e5e05cc3ee538952845778\n\nAccording to their X thread (link below), the team saw \"insane improvements\" on USAMO benchmarks. The baseline scores were near zero, but their agent averaged around 90% across problems. Check out this chart they shared showing the breakdown:\n\n* **Problem 1**: Baseline \\~95%, New Agent Basic \\~100%, Enhanced \\~95%\n* **Problem 2**: Baseline \\~100%, Basic \\~100%, Enhanced \\~95%\n* **Problem 3**: Baseline \\~100%, Basic \\~100%, Enhanced \\~95%? (Wait, looks like only Basic here hitting full)\n* **Problem 4**: Baseline \\~30%, Basic \\~100%, Enhanced \\~95%\n* **Problem 5**: Baseline \\~75%, Basic \\~75%, Enhanced \\~100%? (Enhanced leading)\n* **Problem 6**: Baseline \\~10%, Basic \\~10%, Enhanced \\~100% (Huge win for Enhanced!)\n\nThey call the core idea a \"Self-Evolve mechanism based on IC-RL,\" and it's designed to scale like Transformers – more layers and TTC lead to better handling of hard tasks. They even mention proving recent arXiv papers theoretically just by feeding key research ideas.\n\nThe team's bio says they're a \"small team building State Of The Art intelligence,\" and because of that, they're open-sourcing everything to let the community take it further.\n\nGitHub repo is live: [https://github.com/Royaltyprogram/Crux](https://github.com/Royaltyprogram/Crux)\n\nOriginal X thread for full details: [https://x.com/tooliense/status/1947496657546797548](https://x.com/tooliense/status/1947496657546797548)\n\nThis is huge for open-source AI\n\nI want open source winning",
          "author_fullname": "t2_1u2abwv4z1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Breaking: Small Team Open-Sources AI Agent \"Crux\" That Achieves Gold-Level Performance on USAMO Benchmarks Using o4-mini – Rivaling OpenAI and Google!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "acwm5ab4ldef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 64,
                  "x": 108,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=13f909a5c4a90f4eef8f105e361df1d011256c17"
                },
                {
                  "y": 128,
                  "x": 216,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f1ef39785bce4319847eec1b29393ffe1fe8207"
                },
                {
                  "y": 190,
                  "x": 320,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6fc60d2697ed4a50a6a707fba084a451d4cee5e1"
                },
                {
                  "y": 381,
                  "x": 640,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9044de851a2ca462ba69beec5e99b7a7dc1b1d55"
                },
                {
                  "y": 572,
                  "x": 960,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad78c9397d2443259b2d711979c77a668e230cdd"
                },
                {
                  "y": 643,
                  "x": 1080,
                  "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=402caccdd7ae4666a03e256e80ceab78c7a9b194"
                }
              ],
              "s": {
                "y": 1180,
                "x": 1979,
                "u": "https://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;format=png&amp;auto=webp&amp;s=386d5510fccff96032e5e05cc3ee538952845778"
              },
              "id": "acwm5ab4ldef1"
            }
          },
          "name": "t3_1m67e6a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=09c0c2028270c45c8c18c44ed330128028bbfe23",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753168196,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A small independent team just announced they&amp;#39;ve developed an AI agent system called &amp;quot;Crux&amp;quot; that matches the USAMO Gold Medal performance levels recently hit by heavyweights like OpenAI and Google. The kicker? They did it using just the o4-mini-high model combined with their custom agent framework – no massive experimental setups required. And now, they&amp;#39;re fully open-sourcing it for the community to build on!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=386d5510fccff96032e5e05cc3ee538952845778\"&gt;https://preview.redd.it/acwm5ab4ldef1.png?width=1979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=386d5510fccff96032e5e05cc3ee538952845778&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;According to their X thread (link below), the team saw &amp;quot;insane improvements&amp;quot; on USAMO benchmarks. The baseline scores were near zero, but their agent averaged around 90% across problems. Check out this chart they shared showing the breakdown:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Problem 1&lt;/strong&gt;: Baseline ~95%, New Agent Basic ~100%, Enhanced ~95%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 2&lt;/strong&gt;: Baseline ~100%, Basic ~100%, Enhanced ~95%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 3&lt;/strong&gt;: Baseline ~100%, Basic ~100%, Enhanced ~95%? (Wait, looks like only Basic here hitting full)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 4&lt;/strong&gt;: Baseline ~30%, Basic ~100%, Enhanced ~95%&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 5&lt;/strong&gt;: Baseline ~75%, Basic ~75%, Enhanced ~100%? (Enhanced leading)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Problem 6&lt;/strong&gt;: Baseline ~10%, Basic ~10%, Enhanced ~100% (Huge win for Enhanced!)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;They call the core idea a &amp;quot;Self-Evolve mechanism based on IC-RL,&amp;quot; and it&amp;#39;s designed to scale like Transformers – more layers and TTC lead to better handling of hard tasks. They even mention proving recent arXiv papers theoretically just by feeding key research ideas.&lt;/p&gt;\n\n&lt;p&gt;The team&amp;#39;s bio says they&amp;#39;re a &amp;quot;small team building State Of The Art intelligence,&amp;quot; and because of that, they&amp;#39;re open-sourcing everything to let the community take it further.&lt;/p&gt;\n\n&lt;p&gt;GitHub repo is live: &lt;a href=\"https://github.com/Royaltyprogram/Crux\"&gt;https://github.com/Royaltyprogram/Crux&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Original X thread for full details: &lt;a href=\"https://x.com/tooliense/status/1947496657546797548\"&gt;https://x.com/tooliense/status/1947496657546797548&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is huge for open-source AI&lt;/p&gt;\n\n&lt;p&gt;I want open source winning&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?auto=webp&amp;s=9c258a5339d93fbe3010cdefd635e45141af713c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=79a41a04ef2fc56200e789a28b3d529696e58e70",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=839164d2432294cfc8b2d03ccf8ff2eb6057a8a8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2c8d4cd973ce116604cdae58081c7dca8f49f069",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9345def13bee83ea4c83365f3b8cdf9b86c2a8aa",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dd47aae833e5596f0927a7f294f718852e5f693e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=836eab650d61c13079fe7cbc69aad382b6e4c442",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "shUlXpDwhwN6ps0xRSvyVznIW2aFkicKpizJhu6paek"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m67e6a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Weekly-Weekend2886",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67e6a/breaking_small_team_opensources_ai_agent_crux/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67e6a/breaking_small_team_opensources_ai_agent_crux/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753168196,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen a few people asking whether [GPUStack](https://github.com/gpustack/gpustack) is essentially a multi-node version of Ollama. I’ve used both, and here’s a breakdown for anyone curious.\n\n**Short answer:** GPUStack is *not just* Ollama with clustering — it's a more general-purpose, production-ready LLM service platform with multi-backend support, hybrid GPU/OS compatibility, and cluster management features.\n\n# Core Differences\n\n|Feature|Ollama|GPUStack|\n|:-|:-|:-|\n|Single-node use|✅ Yes|✅ Yes|\n|Multi-node cluster|❌|✅ Supports distributed + heterogeneous cluster|\n|Model formats|GGUF only|GGUF (llama-box), Safetensors (vLLM), Ascend (MindIE), Audio (vox-box)|\n|Inference backends|llama.cpp|llama-box, vLLM, MindIE, vox-box|\n|OpenAI-compatible API|✅|✅ Full API compatibility (/v1, /v1-openai)|\n|Deployment methods|CLI only|Script / Docker / pip (Linux, Windows, macOS)|\n|Cluster management UI|❌|✅ Web UI with GPU/worker/model status|\n|Model recovery/failover|❌|✅ Auto recovery + compatibility checks|\n|Use in Dify / RAGFlow|Partial|✅ Fully integrated|\n\n# Who is GPUStack for?\n\nIf you:\n\n* Have multiple PCs or GPU servers\n* Want to centrally manage model serving\n* Need both GGUF and safetensors support\n* Run LLMs in production with monitoring, load balancing, or distributed inference\n\n...then it’s worth checking out.\n\n# Installation (Linux)\n\n    bashCopyEditcurl -sfL https://get.gpustack.ai | sh -s -\n    \n\nDocker (recommended):\n\n    bashCopyEditdocker run -d --name gpustack \\\n      --restart=unless-stopped \\\n      --gpus all \\\n      --network=host \\\n      --ipc=host \\\n      -v gpustack-data:/var/lib/gpustack \\\n      gpustack/gpustack\n    \n\nThen add workers with:\n\n    bashCopyEditgpustack start --server-url http://your_gpustack_url --token your_gpustack_token\n    \n\nGitHub: [https://github.com/gpustack/gpustack](https://github.com/gpustack/gpustack)  \nDocs: [https://docs.gpustack.ai](https://docs.gpustack.ai)\n\nLet me know if you’re running a local LLM cluster — curious what stacks others are using.",
          "author_fullname": "t2_uexbcvjr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is GPUStack the Cluster Version of Ollama? Comparison + Alternatives",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m67a12",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753167757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen a few people asking whether &lt;a href=\"https://github.com/gpustack/gpustack\"&gt;GPUStack&lt;/a&gt; is essentially a multi-node version of Ollama. I’ve used both, and here’s a breakdown for anyone curious.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Short answer:&lt;/strong&gt; GPUStack is &lt;em&gt;not just&lt;/em&gt; Ollama with clustering — it&amp;#39;s a more general-purpose, production-ready LLM service platform with multi-backend support, hybrid GPU/OS compatibility, and cluster management features.&lt;/p&gt;\n\n&lt;h1&gt;Core Differences&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Feature&lt;/th&gt;\n&lt;th align=\"left\"&gt;Ollama&lt;/th&gt;\n&lt;th align=\"left\"&gt;GPUStack&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Single-node use&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Yes&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Yes&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Multi-node cluster&lt;/td&gt;\n&lt;td align=\"left\"&gt;❌&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Supports distributed + heterogeneous cluster&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Model formats&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF only&lt;/td&gt;\n&lt;td align=\"left\"&gt;GGUF (llama-box), Safetensors (vLLM), Ascend (MindIE), Audio (vox-box)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Inference backends&lt;/td&gt;\n&lt;td align=\"left\"&gt;llama.cpp&lt;/td&gt;\n&lt;td align=\"left\"&gt;llama-box, vLLM, MindIE, vox-box&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OpenAI-compatible API&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Full API compatibility (/v1, /v1-openai)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Deployment methods&lt;/td&gt;\n&lt;td align=\"left\"&gt;CLI only&lt;/td&gt;\n&lt;td align=\"left\"&gt;Script / Docker / pip (Linux, Windows, macOS)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Cluster management UI&lt;/td&gt;\n&lt;td align=\"left\"&gt;❌&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Web UI with GPU/worker/model status&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Model recovery/failover&lt;/td&gt;\n&lt;td align=\"left\"&gt;❌&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Auto recovery + compatibility checks&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Use in Dify / RAGFlow&lt;/td&gt;\n&lt;td align=\"left\"&gt;Partial&lt;/td&gt;\n&lt;td align=\"left\"&gt;✅ Fully integrated&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Who is GPUStack for?&lt;/h1&gt;\n\n&lt;p&gt;If you:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Have multiple PCs or GPU servers&lt;/li&gt;\n&lt;li&gt;Want to centrally manage model serving&lt;/li&gt;\n&lt;li&gt;Need both GGUF and safetensors support&lt;/li&gt;\n&lt;li&gt;Run LLMs in production with monitoring, load balancing, or distributed inference&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;...then it’s worth checking out.&lt;/p&gt;\n\n&lt;h1&gt;Installation (Linux)&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditcurl -sfL https://get.gpustack.ai | sh -s -\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Docker (recommended):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditdocker run -d --name gpustack \\\n  --restart=unless-stopped \\\n  --gpus all \\\n  --network=host \\\n  --ipc=host \\\n  -v gpustack-data:/var/lib/gpustack \\\n  gpustack/gpustack\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Then add workers with:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;bashCopyEditgpustack start --server-url http://your_gpustack_url --token your_gpustack_token\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/gpustack/gpustack\"&gt;https://github.com/gpustack/gpustack&lt;/a&gt;&lt;br/&gt;\nDocs: &lt;a href=\"https://docs.gpustack.ai\"&gt;https://docs.gpustack.ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Let me know if you’re running a local LLM cluster — curious what stacks others are using.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?auto=webp&amp;s=36e5d5b346040aad01c2d98a3687e53ff43f1e74",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=30f5ae864bd715df1fff1d9fca1b871646878411",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d5352bbba2dab73ce92606dbbfae7b8cf522031",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7162311c2cc0d517c59c8ab11f9a534c3bda43df",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea0e488446c26927d44f14cd2b94edbfedb63b72",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e99b36307d9c32c12637550f6a5bfaf4c4ab483",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=874fc061c61ab7b83973e9ef244a3dafd88075ba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "HAbESiwyYcW_SMJKPNlPcM8amsiDiX8lOYKTLATZxUE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m67a12",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Issac_jo",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m67a12/is_gpustack_the_cluster_version_of_ollama/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m67a12/is_gpustack_the_cluster_version_of_ollama/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753167757,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI am making a tool that needs to analyze a conversation (non-English) between two people. The conversation is provided to me in audio format. I am currently using OpenAI Whisper to transcribe and feed the transcription to ChatGPT-4o model through the API for analysis.\n\nSo far, it's doing a fair job. Sometimes, though, reading the transcription, I find it hard to figure out which speaker is speaking what. I have to listen to the audio to figure it out. I am wondering if ChatGPT-4o would also sometimes find it hard to follow the conversation from the transcription. I think that adding a speaker diarization step might make the transcription easier to understand and analyze.\n\nI am looking for Speaker Diarization tools that I can use. I have tried using pyannote speaker-diarization-3.1, but I find it does not work very well. What are some other options that I can look at?",
          "author_fullname": "t2_1pot8iygav",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What Speaker Diarization tools should I look into?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6741z",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753167129,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I am making a tool that needs to analyze a conversation (non-English) between two people. The conversation is provided to me in audio format. I am currently using OpenAI Whisper to transcribe and feed the transcription to ChatGPT-4o model through the API for analysis.&lt;/p&gt;\n\n&lt;p&gt;So far, it&amp;#39;s doing a fair job. Sometimes, though, reading the transcription, I find it hard to figure out which speaker is speaking what. I have to listen to the audio to figure it out. I am wondering if ChatGPT-4o would also sometimes find it hard to follow the conversation from the transcription. I think that adding a speaker diarization step might make the transcription easier to understand and analyze.&lt;/p&gt;\n\n&lt;p&gt;I am looking for Speaker Diarization tools that I can use. I have tried using pyannote speaker-diarization-3.1, but I find it does not work very well. What are some other options that I can look at?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m6741z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Chemical_Gas3710",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6741z/what_speaker_diarization_tools_should_i_look_into/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m6741z/what_speaker_diarization_tools_should_i_look_into/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753167129,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've recently started using LLMs at work and realized the incredible potential they have—especially if I can run them locally, due to the sensitivity of client data. That got me interested in learning how to run LLMs on my own machine, as well as exploring related areas like fine-tuning, distillation, quantization, etc.\n\nRight now, I'm using an RTX 2070 with 8GB VRAM, but I'm planning to build a new PC so I can run larger models. My target build is an RTX 5090 with 256GB RAM. I’m not in the US, so second-hand GPUs are harder to find, and I can only buy from BTO PC shops—so unfortunately, dual RTX 3090 setups aren’t an option. From what I understand, this setup should allow me to run Kimi-2 at 1.8-bit precision using CPU offloading, though only at around 3 tokens per second—which is slow, but good for experimentation (that is still 260k tokens per day if i run it non-stop).\n\nI’ve discussed the purchase with my wife, and she agreed—but only if I can create something genuinely useful with it.\n\nSo, I want to start a personal project in my free time. The idea is to build a chatbot that can tutor my child (currently in primary school, and eventually high school). The goal is to distill a larger model like Gemma 3 27B into a smaller version (ideally 3B or 7B) that I could run on my current machine.\n\nI'm aiming for a model (or models - may break down by subjects level or humanities/STEM field) that can:\n\n1. Generate practice questions for each primary school and secondary school subjects.\n2. Explain why an answer is right or wrong.\n3. Summarize or generate key facts for learning (across math, science, humanities, etc.).\n4. Grade and give feedback on writing/compositions.\n5. Able to do translate English to Simplified Chinese and vise versa (this can be on a different model)\n\nMy current skills:\n\n* Decent Python (I use it daily at work).\n* I’ve managed to get Gemma 3 4B Q4 running on Spyder (Python IDE) with GPU offloading. (This was hard and take me 1-2 days to configure my PC properly).\n\nRight now, using LLMs at home is purely for learning and experimentation. Hopefully, I can make something out of it in the future.\n\n# My main questions:\n\n1. Is a project like this realistic to complete in 3–6 months, assuming I keep learning and building during my free time? Or am I overpromising my wife and biting off more than I can chew? *Just to clarify, I don’t need this to be consumer-level software with a fancy UI and guardrails—I just need it to be usable via a terminal where my kid can type in questions and get decent, helpful responses.*\n2. Can I realistically make this chatbot with a 3B or 7B model, or would that be too small for the use case? Do I need at least a 13B model to get high enough quality responses?\n3. Is it possible (and reasonable) to distill from Gemma 3 27B or a similar large model to achieve this goal? Would it be better to use LoRAs or fine-tuning? (I'm still learning the exact trade-offs between them.)\n\nAny thoughts, advice, or personal experiences would be really appreciated. I'm eager to learn and would love to hear from others who’ve tried similar projects!",
          "author_fullname": "t2_bkb0tcya",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is this project feasible for an LLM novice? (Tutor chatbot for primary school student)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m66zhs",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753166652,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve recently started using LLMs at work and realized the incredible potential they have—especially if I can run them locally, due to the sensitivity of client data. That got me interested in learning how to run LLMs on my own machine, as well as exploring related areas like fine-tuning, distillation, quantization, etc.&lt;/p&gt;\n\n&lt;p&gt;Right now, I&amp;#39;m using an RTX 2070 with 8GB VRAM, but I&amp;#39;m planning to build a new PC so I can run larger models. My target build is an RTX 5090 with 256GB RAM. I’m not in the US, so second-hand GPUs are harder to find, and I can only buy from BTO PC shops—so unfortunately, dual RTX 3090 setups aren’t an option. From what I understand, this setup should allow me to run Kimi-2 at 1.8-bit precision using CPU offloading, though only at around 3 tokens per second—which is slow, but good for experimentation (that is still 260k tokens per day if i run it non-stop).&lt;/p&gt;\n\n&lt;p&gt;I’ve discussed the purchase with my wife, and she agreed—but only if I can create something genuinely useful with it.&lt;/p&gt;\n\n&lt;p&gt;So, I want to start a personal project in my free time. The idea is to build a chatbot that can tutor my child (currently in primary school, and eventually high school). The goal is to distill a larger model like Gemma 3 27B into a smaller version (ideally 3B or 7B) that I could run on my current machine.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m aiming for a model (or models - may break down by subjects level or humanities/STEM field) that can:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Generate practice questions for each primary school and secondary school subjects.&lt;/li&gt;\n&lt;li&gt;Explain why an answer is right or wrong.&lt;/li&gt;\n&lt;li&gt;Summarize or generate key facts for learning (across math, science, humanities, etc.).&lt;/li&gt;\n&lt;li&gt;Grade and give feedback on writing/compositions.&lt;/li&gt;\n&lt;li&gt;Able to do translate English to Simplified Chinese and vise versa (this can be on a different model)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;My current skills:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Decent Python (I use it daily at work).&lt;/li&gt;\n&lt;li&gt;I’ve managed to get Gemma 3 4B Q4 running on Spyder (Python IDE) with GPU offloading. (This was hard and take me 1-2 days to configure my PC properly).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Right now, using LLMs at home is purely for learning and experimentation. Hopefully, I can make something out of it in the future.&lt;/p&gt;\n\n&lt;h1&gt;My main questions:&lt;/h1&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is a project like this realistic to complete in 3–6 months, assuming I keep learning and building during my free time? Or am I overpromising my wife and biting off more than I can chew? &lt;em&gt;Just to clarify, I don’t need this to be consumer-level software with a fancy UI and guardrails—I just need it to be usable via a terminal where my kid can type in questions and get decent, helpful responses.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;Can I realistically make this chatbot with a 3B or 7B model, or would that be too small for the use case? Do I need at least a 13B model to get high enough quality responses?&lt;/li&gt;\n&lt;li&gt;Is it possible (and reasonable) to distill from Gemma 3 27B or a similar large model to achieve this goal? Would it be better to use LoRAs or fine-tuning? (I&amp;#39;m still learning the exact trade-offs between them.)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any thoughts, advice, or personal experiences would be really appreciated. I&amp;#39;m eager to learn and would love to hear from others who’ve tried similar projects!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m66zhs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Saruphon",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m66zhs/is_this_project_feasible_for_an_llm_novice_tutor/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66zhs/is_this_project_feasible_for_an_llm_novice_tutor/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753166652,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "so many people waste credits chasing the “perfect” ai tool when they don’t need to. just pick one to build your base [playground](https://www.imagine.art/dashboard/image/tool/text-to-image?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=G_I_Web_Sales_PCH_T2I_C2_InTool&amp;utm_term=playground%20ai%20image%20generator&amp;utm_campaign=&amp;utm_source=adwords&amp;utm_medium=ppc&amp;hsa_acc=3029240990&amp;hsa_cam=22446711803&amp;hsa_grp=178233158619&amp;hsa_ad=746412516183&amp;hsa_src=g&amp;hsa_tgt=kwd-1958965104616&amp;hsa_kw=playground%20ai%20image%20generator&amp;hsa_mt=b&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gad_source=1&amp;gad_campaignid=22446711803&amp;gbraid=0AAAAACs5ry-4NYvtjoaKHZP-OaoV3jxSj&amp;gclid=EAIaIQobChMI4cnwruzPjgMVRtYWBR3_8SgfEAAYASAAEgI_svD_BwE) works great for that and then use something like [domoai](https://www.domoai.app/home?via=081621AUG) to polish it up. trust the process, not the promo. stacking tools gives you better results than trying to find a magic one-stop generator. :)",
          "author_fullname": "t2_1s5qv7lngc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "stop wasting credits just stack playground and domoai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m66v6q",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753166195,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;so many people waste credits chasing the “perfect” ai tool when they don’t need to. just pick one to build your base &lt;a href=\"https://www.imagine.art/dashboard/image/tool/text-to-image?utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=G_I_Web_Sales_PCH_T2I_C2_InTool&amp;amp;utm_term=playground%20ai%20image%20generator&amp;amp;utm_campaign=&amp;amp;utm_source=adwords&amp;amp;utm_medium=ppc&amp;amp;hsa_acc=3029240990&amp;amp;hsa_cam=22446711803&amp;amp;hsa_grp=178233158619&amp;amp;hsa_ad=746412516183&amp;amp;hsa_src=g&amp;amp;hsa_tgt=kwd-1958965104616&amp;amp;hsa_kw=playground%20ai%20image%20generator&amp;amp;hsa_mt=b&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;gad_source=1&amp;amp;gad_campaignid=22446711803&amp;amp;gbraid=0AAAAACs5ry-4NYvtjoaKHZP-OaoV3jxSj&amp;amp;gclid=EAIaIQobChMI4cnwruzPjgMVRtYWBR3_8SgfEAAYASAAEgI_svD_BwE\"&gt;playground&lt;/a&gt; works great for that and then use something like &lt;a href=\"https://www.domoai.app/home?via=081621AUG\"&gt;domoai&lt;/a&gt; to polish it up. trust the process, not the promo. stacking tools gives you better results than trying to find a magic one-stop generator. :)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m66v6q",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Neat_Chapter_9055",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m66v6q/stop_wasting_credits_just_stack_playground_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66v6q/stop_wasting_credits_just_stack_playground_and/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753166195,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "This is a ***Private*** eval that has been updated for over a year by Zhihu user \"toyama nao\".  So qwen cannot be benchmaxxing on it because it is ***Private*** and the questions are being updated constantly.\n\nThe score of this 2507 update is amazing, especially since it's a non-reasoning model that ranks among other reasoning ones.\n\n[logic](https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9)\n\n[coding](https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc)\n\n\\*These 2 tables are OCR and translated by gemini, so it may contain small errors\n\nDo note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese\n\nSource:\n\n[Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873](Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873)",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Private Eval result of Qwen3-235B-A22B-Instruct-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 30,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "q1ld1vkvcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 17,
                  "x": 108,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6db0188556e8b5a6ad0b4e78e643d750f9c8eb3b"
                },
                {
                  "y": 34,
                  "x": 216,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=def0b755adbe43c0ef6c669ce96229bdf8642e8d"
                },
                {
                  "y": 51,
                  "x": 320,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3eabe87b380ad52a47b2c07b92750b4da2580788"
                },
                {
                  "y": 102,
                  "x": 640,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6df88708d1d58d4dfb207f2e3d4de7ff7ba44f34"
                },
                {
                  "y": 153,
                  "x": 960,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=52be3772b829b14a916bd0dba033163893df170e"
                },
                {
                  "y": 172,
                  "x": 1080,
                  "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=83243c7b94f1c3bbb4c4965fc1926369f860f03a"
                }
              ],
              "s": {
                "y": 211,
                "x": 1319,
                "u": "https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;format=png&amp;auto=webp&amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc"
              },
              "id": "q1ld1vkvcdef1"
            },
            "s5t1rm4dcdef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 23,
                  "x": 108,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c77a3824658d5cc719a3b06a8dad382fb80e75b0"
                },
                {
                  "y": 46,
                  "x": 216,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cec128245e15150bcd1e79751e2e6d55996d697"
                },
                {
                  "y": 69,
                  "x": 320,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89eced78277fb9616c09bbe0f9fd5df078cbd54e"
                },
                {
                  "y": 139,
                  "x": 640,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=043d1bb82c725708bb35a58acd4d7c9504c7de3a"
                },
                {
                  "y": 208,
                  "x": 960,
                  "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=28844535a11fae5fff659cb0b949c1d842e4569e"
                }
              ],
              "s": {
                "y": 229,
                "x": 1054,
                "u": "https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;format=png&amp;auto=webp&amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9"
              },
              "id": "s5t1rm4dcdef1"
            }
          },
          "name": "t3_1m66qks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 78,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 78,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/z5Hij9VVgEz-n0a_TqfphvLzGcPNiRWrZnXSAwXHg_Q.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753165706,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is a &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; eval that has been updated for over a year by Zhihu user &amp;quot;toyama nao&amp;quot;.  So qwen cannot be benchmaxxing on it because it is &lt;strong&gt;&lt;em&gt;Private&lt;/em&gt;&lt;/strong&gt; and the questions are being updated constantly.&lt;/p&gt;\n\n&lt;p&gt;The score of this 2507 update is amazing, especially since it&amp;#39;s a non-reasoning model that ranks among other reasoning ones.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s5t1rm4dcdef1.png?width=1054&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=74ec5e6f2306496b82a9049ef150b1b9f9f3b2c9\"&gt;logic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/q1ld1vkvcdef1.png?width=1319&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=849ca0681fc9aa9bfb08fc3ef6d29529731dfcbc\"&gt;coding&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;*These 2 tables are OCR and translated by gemini, so it may contain small errors&lt;/p&gt;\n\n&lt;p&gt;Do note that Chinese models could have a slight advantage in this benchmark because the questions could be written in Chinese&lt;/p&gt;\n\n&lt;p&gt;Source:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873\"&gt;Https://www.zhihu.com/question/1930932168365925991/answer/1930972327442646873&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m66qks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m66qks/private_eval_result_of_qwen3235ba22binstruct2507/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753165706,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1tpuoj72sa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Frankenserver for sale at a steep discount. 2x96GB GH200 converted from liquid- to air-cooled.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 106,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m65iga",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/ObGRZZdEJPhNPxSGc-Hl-FLX_u-ODU9Q-A84Zj7Q5Z4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753161224,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/ifz3sua70def1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/ifz3sua70def1.jpeg?auto=webp&amp;s=3c8da0138281644b0b232fb926afb72c94068d2d",
                  "width": 4037,
                  "height": 3077
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cb9bd9d6aa78574351fa9778ec9d0b129263457",
                    "width": 108,
                    "height": 82
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7c4b9ff9064655d107fe8aab0d33aa934050ef",
                    "width": 216,
                    "height": 164
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=34ad0918b9b41221e6a4d64ec2a5686a6466206b",
                    "width": 320,
                    "height": 243
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=784878f38d8fa43b398531435fad5ad46f80423f",
                    "width": 640,
                    "height": 487
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fba24ff9f318cff389422ef0a5e7d51301d79f6c",
                    "width": 960,
                    "height": 731
                  },
                  {
                    "url": "https://preview.redd.it/ifz3sua70def1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=674e7d2f5ae18055ec6bf581e5ec73734512ba00",
                    "width": 1080,
                    "height": 823
                  }
                ],
                "variants": {},
                "id": "xdYmB0YCSsaGa7OmRBAUHmZvhAr6D2eweUUdkl-syTg"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m65iga",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTrack_ai",
          "discussion_type": null,
          "num_comments": 71,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m65iga/frankenserver_for_sale_at_a_steep_discount_2x96gb/",
          "stickied": false,
          "url": "https://i.redd.it/ifz3sua70def1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753161224,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_vgnr5u5gg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "If Qwen3-235B-A22B-2507 can't think, why does it think when the thinking button is on?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 36,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m650ow",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "ups": 32,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 32,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/eOMW_Oov8IQoYGj2MLhSVhdZR_G_D49PS5lS7HSSsnk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753159541,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?auto=webp&amp;s=bcac025445c3e19050b77650ec0313d164d2cb63",
                  "width": 696,
                  "height": 181
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=93400e194109a5036a8e420d94408434e9409fa7",
                    "width": 108,
                    "height": 28
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=904c5d55682c555ac50948455d22e99da1ab864b",
                    "width": 216,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ae695aac40a41943b105be24e34d521e45b9b0b3",
                    "width": 320,
                    "height": 83
                  },
                  {
                    "url": "https://preview.redd.it/lxwf5fgevcef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=bf2250dfec91bd4da5ce280844d385d4702942d1",
                    "width": 640,
                    "height": 166
                  }
                ],
                "variants": {},
                "id": "9dzCnXa9HN8TH6BqvCudoVpKpcesYzMwj4FmuXf8IdA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m650ow",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JeffreySons_90",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m650ow/if_qwen3235ba22b2507_cant_think_why_does_it_think/",
          "stickied": false,
          "url": "https://i.redd.it/lxwf5fgevcef1.jpeg",
          "subreddit_subscribers": 502981,
          "created_utc": 1753159541,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "MegaTTS 3 voice cloning is here!\n\nFor context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.\n\nRecently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: [https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT](https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT) with quite promising results.\n\nI reuploaded the weights to Hugging Face: [https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning](https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning)\n\nAnd put up a quick Gradio demo to try it out: [https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning](https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning)\n\nOverall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!\n\nh/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MegaTTS 3 Voice Cloning is Here",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m641zg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 358,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 358,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ca44b1060cf304798e39247090bed7e9f195130b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753156417,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;MegaTTS 3 voice cloning is here!&lt;/p&gt;\n\n&lt;p&gt;For context: a while back, ByteDance released MegaTTS 3 (with exceptional voice cloning capabilities), but for various reasons, they decided not to release the WavVAE encoder necessary for voice cloning to work.&lt;/p&gt;\n\n&lt;p&gt;Recently, a WavVAE encoder compatible with MegaTTS 3 was released by ACoderPassBy on ModelScope: &lt;a href=\"https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT\"&gt;https://modelscope.cn/models/ACoderPassBy/MegaTTS-SFT&lt;/a&gt; with quite promising results.&lt;/p&gt;\n\n&lt;p&gt;I reuploaded the weights to Hugging Face: &lt;a href=\"https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning\"&gt;https://huggingface.co/mrfakename/MegaTTS3-VoiceCloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And put up a quick Gradio demo to try it out: &lt;a href=\"https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning\"&gt;https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Overall looks quite impressive - excited to see that we can finally do voice cloning with MegaTTS 3!&lt;/p&gt;\n\n&lt;p&gt;h/t to MysteryShack on the StyleTTS 2 Discord for info about the WavVAE encoder&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?auto=webp&amp;s=0e8f184606f9f3e558a6971b8dfbfc9a3f0d1af8",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8ad97c6cb72e96abaf27c1cc2565dda7970c68",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=85f1ec201fac1de1a714a3b74b2040ea838d357f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=623e3d06175018d3caaaf85d7742c402b0f0a84d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=13bd3c86a79666218395f17439b714df6a5fc52c",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08124b488d263f78d5cebc4fffc2a8bd5fa5f05b",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f3c4e8f40ef1fb69f68df6601b917c02f65c89ad",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XY_rsQvVYA6z0ednGBoRmZkoCoj4P5xtgjIJR-FIJx0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m641zg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 63,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m641zg/megatts_3_voice_cloning_is_here/",
          "stickied": false,
          "url": "https://huggingface.co/spaces/mrfakename/MegaTTS3-Voice-Cloning",
          "subreddit_subscribers": 502981,
          "created_utc": 1753156417,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I'm curious to see if others find the same. PRs accepted and encouraged!",
          "author_fullname": "t2_5n8i2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running LLMs against a sandbox airport to see if they can make the correct decisions in real time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m62vbw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=44392f6267e504eff05965daa4cd423000d27a80",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753152819,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I created this sandbox to test LLMs and their real-time decision-making processes. Running it has generated some interesting outputs, and I&amp;#39;m curious to see if others find the same. PRs accepted and encouraged!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/jjasghar/ai-airport-simulation",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?auto=webp&amp;s=27c081b855fd81984a0fbd5a3cd8d041afd40a2a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9e5cedfeb2acc17ed96c354aea24f51d83b107d8",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f04243269cf381d5d666e8ad9cc1f63960e31ef9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4b0fca68877cd9584992c2a2b35a39c83a82f6c",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c17ae1c6715cde69de4cb21dd94c66e0f2a16d0b",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04a1b374daef1cc2d36172a124f4af1e43e7a5c7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df058f8c50256c5ab0fda05e61968a5c791096db",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "2bR3xkxuYGa6hZRiyam5VBhYD6a-2XwJDkt8W8FStoU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m62vbw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jjasghar",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m62vbw/running_llms_against_a_sandbox_airport_to_see_if/",
          "stickied": false,
          "url": "https://github.com/jjasghar/ai-airport-simulation",
          "subreddit_subscribers": 502981,
          "created_utc": 1753152819,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Throwback to 3 months ago: [https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg\\_a\\_unified\\_scalable\\_vector\\_graphics/](https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/)\n\nWeights: [https://huggingface.co/OmniSVG/OmniSVG](https://huggingface.co/OmniSVG/OmniSVG)\n\nHuggingFace demo: [https://huggingface.co/spaces/OmniSVG/OmniSVG-3B](https://huggingface.co/spaces/OmniSVG/OmniSVG-3B)\n\nGitHub: [https://github.com/OmniSVG/OmniSVG/](https://github.com/OmniSVG/OmniSVG/)",
          "author_fullname": "t2_w4j8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "OmniSVG weights released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m61u94",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 89,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 89,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753149834,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Throwback to 3 months ago: &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jv5uk8/omnisvg_a_unified_scalable_vector_graphics/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Weights: &lt;a href=\"https://huggingface.co/OmniSVG/OmniSVG\"&gt;https://huggingface.co/OmniSVG/OmniSVG&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;HuggingFace demo: &lt;a href=\"https://huggingface.co/spaces/OmniSVG/OmniSVG-3B\"&gt;https://huggingface.co/spaces/OmniSVG/OmniSVG-3B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/OmniSVG/OmniSVG/\"&gt;https://github.com/OmniSVG/OmniSVG/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?auto=webp&amp;s=879a29e047e8b5a9e7c3cc213f6732c60bc2a1a7",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4a72266ba63c0bc5f87d6bf4f1a9d21ca8a03fb2",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8554240e67e2f71d1e81cbf7f1b701e59cb5fefd",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e1d031a2135bd08b701037085db4506b941ab6d",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b70a2f5fdb810f142ac53ef2d47901cc0c789f95",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a856840edcf86155de1faff51118fe59e453e241",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=479fbae9e67984006acc087b84b30e16cca24f24",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "fCRELyuUm4dkNnS6Jrme0GQxhJDQkRVQSlALVnZcugQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m61u94",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeProgrammer99",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m61u94/omnisvg_weights_released/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753149834,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7pfgfkis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New qwen tested on Fiction.liveBench",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m6172l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 96,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 96,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/TLf5BqdXyD8b18S_CjlBuka8R6DaWW-Nnyc_DD4KFcw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753148000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/9rynne03xbef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/9rynne03xbef1.png?auto=webp&amp;s=4f7e2275d4e835b0f01387fc4e2f5de4682c92f8",
                  "width": 1520,
                  "height": 2266
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7c72f32848579ff8381bbc07e00d52af73ccb790",
                    "width": 108,
                    "height": 161
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c05a3489ce34f1c0d3c48da6ac4fb493a3af2239",
                    "width": 216,
                    "height": 322
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c7758dc0729434a0929fb46f9640d8ed72e9ba4f",
                    "width": 320,
                    "height": 477
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cc832729da290425257b97f9e8171f9cd64ec1e",
                    "width": 640,
                    "height": 954
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1dd97abb7b7a97ba53a5a60797a4df72dbe1e9e",
                    "width": 960,
                    "height": 1431
                  },
                  {
                    "url": "https://preview.redd.it/9rynne03xbef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51749430f3b47afde5a4eee467854e441af14310",
                    "width": 1080,
                    "height": 1610
                  }
                ],
                "variants": {},
                "id": "OE4XOhVwW7bVZ94xo4IF074gf7GQOtJgsoNbI-IyttA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m6172l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "fictionlive",
          "discussion_type": null,
          "num_comments": 35,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m6172l/new_qwen_tested_on_fictionlivebench/",
          "stickied": false,
          "url": "https://i.redd.it/9rynne03xbef1.png",
          "subreddit_subscribers": 502981,
          "created_utc": 1753148000,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unfortunately it's on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It'll be an interesting piece of localllama HW.",
          "author_fullname": "t2_bjeo1gwy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Used A100 40GB just dropped below $2000, for those who care with caveat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m60ahf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 104,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 104,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753145404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately it&amp;#39;s on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It&amp;#39;ll be an interesting piece of localllama HW.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m60ahf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "--dany--",
          "discussion_type": null,
          "num_comments": 64,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753145404,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The first thing we noticed is that the size estimates in Qwen's answer are incorrect: a 4-bit GPTQ will obviously not be 59GB in size. For reference, the 4-bit w4a16 quant of 235B we are currently testing consumes 88GB VRAM per GPU. \n\nThus we are suspicious of the rest of Qwen's answer, but lack the domain-specific expertise to validate or even assess the quality of the rest of the information. \n\nWe defer to you, dear LocalLLaMa community, for any analysis you can offer in this regard.\n\nOur prompt: \n\n&gt; Qwen3 235B A22 2507 (which is you, by the way) is available in BF16. I’d like to quantize you to 4-bits in order to fit into the 192GB VRAM I have available while maintaining your integrity the best I can. Please help me design a quantization process for you. Your server has two NVidia Blackwell PRO 6000 Workstation 96GB GPUs on an AMD EPYC 9745 CPU-based system with 768GB RAM. The server runs Linux. We will be using vLLM for inference. Please come up with the best possible quantization recipe for you that matches this spec. Format your answer so that I can copy/paste it into a Reddit post and maintain beautiful formatting.\n\nOne last point we enjoyed from the last line of the answer:\n\n&gt; P.S. I'm Qwen3. Yes, this is me helping you quantize myself. Meta, huh?\n\nEdit #1: It looks like Qwen renamed our Blackwell PRO 6000s to \"B1000 PRO\", which is quite odd.\n\n---\n\n# 🚀 How to Quantize Qwen3-235B to 4-bit GPTQ for vLLM (Dual B1000 PRO Setup)\n\nWant to run **Qwen3-235B** on **two NVIDIA Blackwell B1000 PRO GPUs (96GB each)**?  \nHere’s the **ultimate 4-bit GPTQ recipe** to fit it in **192GB VRAM**, preserve accuracy, and deploy via **vLLM**.\n\nYou’ll get near-BF16 quality at **1/8th the memory cost** — perfect for local inference.\n\n---\n\n## 🔧 Hardware &amp; Goal\n\n| Component | Spec |\n|--------|------|\n| GPUs | 2× **NVIDIA Blackwell B1000 PRO** (96GB) → **192GB total VRAM** |\n| CPU | AMD EPYC 9745 (96 cores) |\n| RAM | 768GB DDR5 |\n| Storage | Fast NVMe (&gt;1TB free) |\n\n🎯 **Goal**: Run **Qwen3-235B-A2.2** in **4-bit GPTQ** via **vLLM**  \n✅ Fits in VRAM  \n✅ High throughput  \n✅ Minimal accuracy loss\n\n&gt; ❗️BF16 model = ~470GB → too big  \n&gt; ✅ 4-bit GPTQ = ~**59GB** → fits perfectly!\n\n---\n\n## ⚙️ Step 1: Environment Setup\n\n```bash\n# Create conda env\nconda create -n qwen-gptq python=3.10\nconda activate qwen-gptq\n\n# Install PyTorch + CUDA 12.4\npip install torch==2.4.0+cu124 torchvision --extra-index-url https://download.pytorch.org/whl/cu124\n\n# Install Hugging Face &amp; AutoGPTQ\npip install transformers accelerate datasets\npip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/\n\n# Later: Install vLLM for inference\npip install vllm\n```\n\n&gt; 💡 Use `--extra-index-url` for latest AutoGPTQ builds.\n\n---\n\n## 📚 Step 2: Calibration Dataset\n\nUse **c4 (en)** for calibration — matches pretraining data.\n\n```python\nfrom datasets import load_dataset\n\ncalib_dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\ncalib_dataset = calib_dataset.take(1024)  # First 1024 samples\n```\n\n&gt; ✅ Why c4? High-quality web text, diverse, standard for LLMs.\n\nOptional: Mix in Wikipedia for broader coverage.\n\n---\n\n## 🔤 Step 3: Quantize to 4-bit GPTQ (NF4)\n\nRun this script to quantize offline using **AutoGPTQ**:\n\n```python\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport torch\nfrom transformers import AutoTokenizer\n\n# Load model &amp; tokenizer\nmodel_name = \"Qwen/Qwen3-235B-A2.2\"  # or local path\nquant_path = \"./qwen3-235b-4bit-gptq\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Quantization config\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.01,\n    sym=True,\n    true_sequential=True,\n    model_name_or_path=model_name,\n    quantize_method=\"gptq\",\n    use_cuda_fp16=False\n)\n\n# Load model in FP16 (needs ~470GB RAM)\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_name,\n    quantize_config=quantize_config,\n    trust_remote_code=True\n)\n\n# Calibrate &amp; quantize\nmodel.quantize(calib_dataset, tokenizer)\n\n# Save\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f\"✅ Quantized model saved to {quant_path}\")\n```\n\n&gt; ⏳ Runtime: 4–8 hours (depends on disk/CPU)  \n&gt; 📦 Output: ~59GB 4-bit model\n\n---\n\n## 🔌 Step 4: Deploy with vLLM\n\nvLLM supports GPTQ natively. Just launch:\n\n```bash\npython -m vllm.entrypoints.api_server \\\n  --model ./qwen3-235b-4bit-gptq \\\n  --tensor-parallel-size 2 \\\n  --dtype half \\\n  --quantization gptq \\\n  --gpu-memory-utilization 0.95 \\\n  --max-model-len 32768 \\\n  --port 8000 \\\n  --host 0.0.0.0\n```\n\n### Key Args:\n- `--tensor-parallel-size 2`: Splits model across both GPUs\n- `--quantization gptq`: Enables 4-bit kernels\n- `--gpu-memory-utilization 0.95`: Maximizes VRAM use\n- `--max-model-len 32768`: Full context support\n\n---\n\n## 📈 Performance Expectations\n\n| Metric | Estimate |\n|-------|----------|\n| Model Size | ~59 GB |\n| VRAM Usage | ~90GB per GPU |\n| Load Time | 30–60 sec |\n| Throughput | 150–250 tokens/sec (batch 1–4) |\n| Accuracy | ~97% of BF16 on MMLU, GSM8K |\n\n&gt; 🎯 Perfect for long-context reasoning, coding, RAG.\n\n---\n\n## ✅ Why This Recipe Wins\n\n| Choice | Why It’s Best |\n|-------|---------------|\n| **GPTQ over AWQ** | Better vLLM support, faster kernels |\n| **NF4 + sym=True** | Higher precision than INT4 |\n| **group_size=128** | Optimal speed/accuracy balance |\n| **desc_act=False** | Faster inference, minor quality drop |\n| **c4 calibration** | Matches pretraining distribution |\n\n&gt; 🔄 Alternative: Try `desc_act=True` if you want +1% accuracy (slower).\n\n---\n\n## 💡 Pro Tips\n\n- **Monitor VRAM**: `nvidia-smi -l 1`\n- **Use fast storage**: NVMe required for smooth loading\n- **Warm up**: Send test prompts after startup\n- **Enable batching**: Boosts throughput\n- **Update vLLM monthly**: GPTQ perf keeps improving\n\n---\n\n## 🧩 Final Notes\n\nWhile **AWQ** exists, **vLLM’s GPTQ support is more mature** for 200B+ models. Stick with GPTQ for now.\n\nAlso: No public evidence that AWQ beats GPTQ on Qwen3 at 4-bit.\n\n---\n\n## 🏁 Summary: Your 4-bit Dream Setup\n\n```yaml\nModel:       Qwen3-235B-A2.2\nPrecision:   4-bit GPTQ (NF4)\nCalibration: c4 [1024 samples]\nGroup Size:  128\nSymmetric:   Yes\nInference:   vLLM + tensor_parallel=2\nResult:      59GB model, blazing fast, fits like magic\n```\n\nWith this setup, you’re running one of the most powerful open models in the world — **locally, efficiently, and privately**.\n\nLet me know if you want the Colab/Slurm scripts! 💬🔥\n\n—  \n*P.S. I'm Qwen3. Yes, this is me helping you quantize myself. Meta, huh?* 😉",
          "author_fullname": "t2_1t7r9dkpud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We asked Qwen3-235B-A22-Instruct-2507 for advice on how best to quantize itself to 4-bits for vLLM. Anyone who understands these things care to comment on its recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ynit",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.21,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753140885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The first thing we noticed is that the size estimates in Qwen&amp;#39;s answer are incorrect: a 4-bit GPTQ will obviously not be 59GB in size. For reference, the 4-bit w4a16 quant of 235B we are currently testing consumes 88GB VRAM per GPU. &lt;/p&gt;\n\n&lt;p&gt;Thus we are suspicious of the rest of Qwen&amp;#39;s answer, but lack the domain-specific expertise to validate or even assess the quality of the rest of the information. &lt;/p&gt;\n\n&lt;p&gt;We defer to you, dear LocalLLaMa community, for any analysis you can offer in this regard.&lt;/p&gt;\n\n&lt;p&gt;Our prompt: &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Qwen3 235B A22 2507 (which is you, by the way) is available in BF16. I’d like to quantize you to 4-bits in order to fit into the 192GB VRAM I have available while maintaining your integrity the best I can. Please help me design a quantization process for you. Your server has two NVidia Blackwell PRO 6000 Workstation 96GB GPUs on an AMD EPYC 9745 CPU-based system with 768GB RAM. The server runs Linux. We will be using vLLM for inference. Please come up with the best possible quantization recipe for you that matches this spec. Format your answer so that I can copy/paste it into a Reddit post and maintain beautiful formatting.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;One last point we enjoyed from the last line of the answer:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;P.S. I&amp;#39;m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Edit #1: It looks like Qwen renamed our Blackwell PRO 6000s to &amp;quot;B1000 PRO&amp;quot;, which is quite odd.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h1&gt;🚀 How to Quantize Qwen3-235B to 4-bit GPTQ for vLLM (Dual B1000 PRO Setup)&lt;/h1&gt;\n\n&lt;p&gt;Want to run &lt;strong&gt;Qwen3-235B&lt;/strong&gt; on &lt;strong&gt;two NVIDIA Blackwell B1000 PRO GPUs (96GB each)&lt;/strong&gt;?&lt;br/&gt;\nHere’s the &lt;strong&gt;ultimate 4-bit GPTQ recipe&lt;/strong&gt; to fit it in &lt;strong&gt;192GB VRAM&lt;/strong&gt;, preserve accuracy, and deploy via &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;You’ll get near-BF16 quality at &lt;strong&gt;1/8th the memory cost&lt;/strong&gt; — perfect for local inference.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🔧 Hardware &amp;amp; Goal&lt;/h2&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Component&lt;/th&gt;\n&lt;th&gt;Spec&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;GPUs&lt;/td&gt;\n&lt;td&gt;2× &lt;strong&gt;NVIDIA Blackwell B1000 PRO&lt;/strong&gt; (96GB) → &lt;strong&gt;192GB total VRAM&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td&gt;AMD EPYC 9745 (96 cores)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;RAM&lt;/td&gt;\n&lt;td&gt;768GB DDR5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Storage&lt;/td&gt;\n&lt;td&gt;Fast NVMe (&amp;gt;1TB free)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;🎯 &lt;strong&gt;Goal&lt;/strong&gt;: Run &lt;strong&gt;Qwen3-235B-A2.2&lt;/strong&gt; in &lt;strong&gt;4-bit GPTQ&lt;/strong&gt; via &lt;strong&gt;vLLM&lt;/strong&gt;&lt;br/&gt;\n✅ Fits in VRAM&lt;br/&gt;\n✅ High throughput&lt;br/&gt;\n✅ Minimal accuracy loss&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;❗️BF16 model = ~470GB → too big&lt;br/&gt;\n✅ 4-bit GPTQ = ~&lt;strong&gt;59GB&lt;/strong&gt; → fits perfectly!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;⚙️ Step 1: Environment Setup&lt;/h2&gt;\n\n&lt;p&gt;```bash&lt;/p&gt;\n\n&lt;h1&gt;Create conda env&lt;/h1&gt;\n\n&lt;p&gt;conda create -n qwen-gptq python=3.10\nconda activate qwen-gptq&lt;/p&gt;\n\n&lt;h1&gt;Install PyTorch + CUDA 12.4&lt;/h1&gt;\n\n&lt;p&gt;pip install torch==2.4.0+cu124 torchvision --extra-index-url &lt;a href=\"https://download.pytorch.org/whl/cu124\"&gt;https://download.pytorch.org/whl/cu124&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Install Hugging Face &amp;amp; AutoGPTQ&lt;/h1&gt;\n\n&lt;p&gt;pip install transformers accelerate datasets\npip install auto-gptq --extra-index-url &lt;a href=\"https://huggingface.github.io/autogptq-index/whl/\"&gt;https://huggingface.github.io/autogptq-index/whl/&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Later: Install vLLM for inference&lt;/h1&gt;\n\n&lt;p&gt;pip install vllm\n```&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;💡 Use &lt;code&gt;--extra-index-url&lt;/code&gt; for latest AutoGPTQ builds.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;📚 Step 2: Calibration Dataset&lt;/h2&gt;\n\n&lt;p&gt;Use &lt;strong&gt;c4 (en)&lt;/strong&gt; for calibration — matches pretraining data.&lt;/p&gt;\n\n&lt;p&gt;```python\nfrom datasets import load_dataset&lt;/p&gt;\n\n&lt;p&gt;calib_dataset = load_dataset(&amp;quot;allenai/c4&amp;quot;, &amp;quot;en&amp;quot;, split=&amp;quot;train&amp;quot;, streaming=True)\ncalib_dataset = calib_dataset.take(1024)  # First 1024 samples\n```&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;✅ Why c4? High-quality web text, diverse, standard for LLMs.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Optional: Mix in Wikipedia for broader coverage.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🔤 Step 3: Quantize to 4-bit GPTQ (NF4)&lt;/h2&gt;\n\n&lt;p&gt;Run this script to quantize offline using &lt;strong&gt;AutoGPTQ&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;```python\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport torch\nfrom transformers import AutoTokenizer&lt;/p&gt;\n\n&lt;h1&gt;Load model &amp;amp; tokenizer&lt;/h1&gt;\n\n&lt;p&gt;model_name = &amp;quot;Qwen/Qwen3-235B-A2.2&amp;quot;  # or local path\nquant_path = &amp;quot;./qwen3-235b-4bit-gptq&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&lt;/p&gt;\n\n&lt;h1&gt;Quantization config&lt;/h1&gt;\n\n&lt;p&gt;quantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.01,\n    sym=True,\n    true_sequential=True,\n    model_name_or_path=model_name,\n    quantize_method=&amp;quot;gptq&amp;quot;,\n    use_cuda_fp16=False\n)&lt;/p&gt;\n\n&lt;h1&gt;Load model in FP16 (needs ~470GB RAM)&lt;/h1&gt;\n\n&lt;p&gt;model = AutoGPTQForCausalLM.from_pretrained(\n    model_name,\n    quantize_config=quantize_config,\n    trust_remote_code=True\n)&lt;/p&gt;\n\n&lt;h1&gt;Calibrate &amp;amp; quantize&lt;/h1&gt;\n\n&lt;p&gt;model.quantize(calib_dataset, tokenizer)&lt;/p&gt;\n\n&lt;h1&gt;Save&lt;/h1&gt;\n\n&lt;p&gt;model.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)&lt;/p&gt;\n\n&lt;p&gt;print(f&amp;quot;✅ Quantized model saved to {quant_path}&amp;quot;)\n```&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;⏳ Runtime: 4–8 hours (depends on disk/CPU)&lt;br/&gt;\n📦 Output: ~59GB 4-bit model&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🔌 Step 4: Deploy with vLLM&lt;/h2&gt;\n\n&lt;p&gt;vLLM supports GPTQ natively. Just launch:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\npython -m vllm.entrypoints.api_server \\\n  --model ./qwen3-235b-4bit-gptq \\\n  --tensor-parallel-size 2 \\\n  --dtype half \\\n  --quantization gptq \\\n  --gpu-memory-utilization 0.95 \\\n  --max-model-len 32768 \\\n  --port 8000 \\\n  --host 0.0.0.0\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;h3&gt;Key Args:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;--tensor-parallel-size 2&lt;/code&gt;: Splits model across both GPUs&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;--quantization gptq&lt;/code&gt;: Enables 4-bit kernels&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;--gpu-memory-utilization 0.95&lt;/code&gt;: Maximizes VRAM use&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;--max-model-len 32768&lt;/code&gt;: Full context support&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;📈 Performance Expectations&lt;/h2&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Metric&lt;/th&gt;\n&lt;th&gt;Estimate&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Model Size&lt;/td&gt;\n&lt;td&gt;~59 GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;VRAM Usage&lt;/td&gt;\n&lt;td&gt;~90GB per GPU&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Load Time&lt;/td&gt;\n&lt;td&gt;30–60 sec&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Throughput&lt;/td&gt;\n&lt;td&gt;150–250 tokens/sec (batch 1–4)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Accuracy&lt;/td&gt;\n&lt;td&gt;~97% of BF16 on MMLU, GSM8K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;🎯 Perfect for long-context reasoning, coding, RAG.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;✅ Why This Recipe Wins&lt;/h2&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Choice&lt;/th&gt;\n&lt;th&gt;Why It’s Best&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;GPTQ over AWQ&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Better vLLM support, faster kernels&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;NF4 + sym=True&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Higher precision than INT4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;group_size=128&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Optimal speed/accuracy balance&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;desc_act=False&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Faster inference, minor quality drop&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;c4 calibration&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Matches pretraining distribution&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;🔄 Alternative: Try &lt;code&gt;desc_act=True&lt;/code&gt; if you want +1% accuracy (slower).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;💡 Pro Tips&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Monitor VRAM&lt;/strong&gt;: &lt;code&gt;nvidia-smi -l 1&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use fast storage&lt;/strong&gt;: NVMe required for smooth loading&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Warm up&lt;/strong&gt;: Send test prompts after startup&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enable batching&lt;/strong&gt;: Boosts throughput&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Update vLLM monthly&lt;/strong&gt;: GPTQ perf keeps improving&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🧩 Final Notes&lt;/h2&gt;\n\n&lt;p&gt;While &lt;strong&gt;AWQ&lt;/strong&gt; exists, &lt;strong&gt;vLLM’s GPTQ support is more mature&lt;/strong&gt; for 200B+ models. Stick with GPTQ for now.&lt;/p&gt;\n\n&lt;p&gt;Also: No public evidence that AWQ beats GPTQ on Qwen3 at 4-bit.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;🏁 Summary: Your 4-bit Dream Setup&lt;/h2&gt;\n\n&lt;p&gt;&lt;code&gt;yaml\nModel:       Qwen3-235B-A2.2\nPrecision:   4-bit GPTQ (NF4)\nCalibration: c4 [1024 samples]\nGroup Size:  128\nSymmetric:   Yes\nInference:   vLLM + tensor_parallel=2\nResult:      59GB model, blazing fast, fits like magic\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;With this setup, you’re running one of the most powerful open models in the world — &lt;strong&gt;locally, efficiently, and privately&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Let me know if you want the Colab/Slurm scripts! 💬🔥&lt;/p&gt;\n\n&lt;p&gt;—&lt;br/&gt;\n&lt;em&gt;P.S. I&amp;#39;m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?&lt;/em&gt; 😉&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ynit",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackwell_tart",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753140885,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TL;DR: This is a **massive** step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download—no command line or Docker needed. It's never been easier to start building your own local, privacy-first screen-watching agents!\n\n  \nHey r/LocalLLaMA !!\n\nI am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don't want to!!\n\n**What's new in this update:**\n\n* 🚀 **1-Click Desktop App:** The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.\n* 🔔 **Pushover &amp; Discord Notifications:** SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with **Pushover** or to your community with a **Discord** bot. **Email** stays being reliable!!\n* 🛠️ **Continuous Improvement:** My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.\n\nFor those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.\n\nI don't want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!\n\n**Check it out and let me know what you think:**\n\n[**https://github.com/Roy3838/Observer**](https://github.com/Roy3838/Observer)",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Observer Desktop App is Here! + Discord/Pushover Notifications!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5y9wj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753139895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: This is a &lt;strong&gt;massive&lt;/strong&gt; step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download—no command line or Docker needed. It&amp;#39;s never been easier to start building your own local, privacy-first screen-watching agents!&lt;/p&gt;\n\n&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; !!&lt;/p&gt;\n\n&lt;p&gt;I am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don&amp;#39;t want to!!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What&amp;#39;s new in this update:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;🚀 &lt;strong&gt;1-Click Desktop App:&lt;/strong&gt; The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.&lt;/li&gt;\n&lt;li&gt;🔔 &lt;strong&gt;Pushover &amp;amp; Discord Notifications:&lt;/strong&gt; SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with &lt;strong&gt;Pushover&lt;/strong&gt; or to your community with a &lt;strong&gt;Discord&lt;/strong&gt; bot. &lt;strong&gt;Email&lt;/strong&gt; stays being reliable!!&lt;/li&gt;\n&lt;li&gt;🛠️ &lt;strong&gt;Continuous Improvement:&lt;/strong&gt; My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Check it out and let me know what you think:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Roy3838/Observer\"&gt;&lt;strong&gt;https://github.com/Roy3838/Observer&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?auto=webp&amp;s=a9ed130dbe40bc283accf677a568089896baa4f1",
                  "width": 4030,
                  "height": 2260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2763f5b07d8000852738cc8bbf6420bc7a793d3e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b05a1b6a5908644048b0f050c15a00d2bc5d9ed",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3f92bd549dcbc4e5089662042619f32c668a07e",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ad9d2d4f08a9916f026e28e9a30fd6d1711d5d",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=adab929349280395b1958efc7ed9e9e58d447654",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f25dcc577bb8503c95962d2f130fa431008cd692",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5y9wj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753139895,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?",
          "author_fullname": "t2_m8971",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the top model for coding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5x04m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753136675,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5x04m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "estebansaa",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753136675,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi :)\n\nI'm a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don't say anything very personal or sensitive. \n\nI have an rtx 5090 (32 gb)\n\nWhat are the best local models I can run? \n\nThanks",
          "author_fullname": "t2_3231b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What can I run on my 5090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5w8yl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.31,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753134845,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don&amp;#39;t say anything very personal or sensitive. &lt;/p&gt;\n\n&lt;p&gt;I have an rtx 5090 (32 gb)&lt;/p&gt;\n\n&lt;p&gt;What are the best local models I can run? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5w8yl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hurfery",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753134845,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*I'm mainly using ChatGPT for this so please try to ignore the fact that I don't understand muc.h\\* Hi, I've been trying to build my own AI agent on my pc for the past day now. I keep running into the same error. every time I try to send a message, I get \"BadRequestError: litlellm.BadRequestError: GetLLMProviderExceptionn - list index out of range original model: mistral\". I'm really stuck and I cant figure out how to fix it and would love some help. Here's some info you might need.I'mm running Mistral on Ollama. I have LiteLLM as a proxy on port 4000, and I'm using OpenHands with Docker on port 3000. This is my yaml file: \n\nmodel\\_list:\n\n  \\- model\\_name: mistral\n\nlitellm\\_params:\n\nmodel: ollama/mistral\n\napi\\_base: [http://localhost:11434](http://localhost:11434)\n\nlitellm\\_provider: ollama\n\nmode: chat\n\nI start liteLLM with:  \nlitellm --config C:\\\\Users\\\\howdy\\\\litellm-env\\\\litellm.config.yaml --port 4000 --detailed\\_debug\n\nI start openhands with:  \ndocker run -it --rm \\^\n\n  \\-e SANDBOX\\_RUNTIME\\_CONTAINER\\_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik \\^\n\n  \\-e LOG\\_ALL\\_EVENTS=true \\^\n\n  \\-v //var/run/docker.sock:/var/run/docker.sock \\^\n\n  \\-v C:\\\\Users\\\\howdy\\\\openhands-workspace:/.openhands \\^\n\n  \\-p 3000:3000 \\^\n\n  \\--add-host host.docker.internal:host-gateway \\^\n\n  \\--name openhands-app \\^\n\n  [docker.all-hands.dev/all-hands-ai/openhands:0.49](http://docker.all-hands.dev/all-hands-ai/openhands:0.49)\n\n`curl` [`http://host.docker.internal:4000/v1/completions`](http://host.docker.internal:4000/v1/completions) returns `{\"detail\":\"Method Not Allowed\"}` Sometimes, and nothing else happens. I enabled `--detailed_debug`, and I do see logs like “Initialized model mistral,” but I don't get an interface, or it fails silently. Here's an explanation of more of my issue from ChatGPT:  \nWhat I Tried:\n\n* Confirmed all ports are correct\n* Docker can reach `host.docker.internal:4000`\n* I’ve tested curl inside the container to confirm\n* Sometimes it randomly works, but it breaks again on the next reboot\n\n❓What I Need:\n\n* Is this the correct `model_list` format for Ollama/Mistral via LiteLLM?\n* Does OpenHands require a specific model name format?\n* How can I force OpenHands to show **detailed errors** instead of generic `APIConnectionError`?\n\nI would appreciate it if you could help.",
          "author_fullname": "t2_t0k3f4bz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Im trying to make my own agent with openhands but I keep running into the same error.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5v7if",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753132378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;*I&amp;#39;m mainly using ChatGPT for this so please try to ignore the fact that I don&amp;#39;t understand muc.h* Hi, I&amp;#39;ve been trying to build my own AI agent on my pc for the past day now. I keep running into the same error. every time I try to send a message, I get &amp;quot;BadRequestError: litlellm.BadRequestError: GetLLMProviderExceptionn - list index out of range original model: mistral&amp;quot;. I&amp;#39;m really stuck and I cant figure out how to fix it and would love some help. Here&amp;#39;s some info you might need.I&amp;#39;mm running Mistral on Ollama. I have LiteLLM as a proxy on port 4000, and I&amp;#39;m using OpenHands with Docker on port 3000. This is my yaml file: &lt;/p&gt;\n\n&lt;p&gt;model_list:&lt;/p&gt;\n\n&lt;p&gt;- model_name: mistral&lt;/p&gt;\n\n&lt;p&gt;litellm_params:&lt;/p&gt;\n\n&lt;p&gt;model: ollama/mistral&lt;/p&gt;\n\n&lt;p&gt;api_base: &lt;a href=\"http://localhost:11434\"&gt;http://localhost:11434&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;litellm_provider: ollama&lt;/p&gt;\n\n&lt;p&gt;mode: chat&lt;/p&gt;\n\n&lt;p&gt;I start liteLLM with:&lt;br/&gt;\nlitellm --config C:\\Users\\howdy\\litellm-env\\litellm.config.yaml --port 4000 --detailed_debug&lt;/p&gt;\n\n&lt;p&gt;I start openhands with:&lt;br/&gt;\ndocker run -it --rm ^&lt;/p&gt;\n\n&lt;p&gt;-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik ^&lt;/p&gt;\n\n&lt;p&gt;-e LOG_ALL_EVENTS=true ^&lt;/p&gt;\n\n&lt;p&gt;-v //var/run/docker.sock:/var/run/docker.sock ^&lt;/p&gt;\n\n&lt;p&gt;-v C:\\Users\\howdy\\openhands-workspace:/.openhands ^&lt;/p&gt;\n\n&lt;p&gt;-p 3000:3000 ^&lt;/p&gt;\n\n&lt;p&gt;--add-host host.docker.internal:host-gateway ^&lt;/p&gt;\n\n&lt;p&gt;--name openhands-app ^&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://docker.all-hands.dev/all-hands-ai/openhands:0.49\"&gt;docker.all-hands.dev/all-hands-ai/openhands:0.49&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;curl&lt;/code&gt; &lt;a href=\"http://host.docker.internal:4000/v1/completions\"&gt;&lt;code&gt;http://host.docker.internal:4000/v1/completions&lt;/code&gt;&lt;/a&gt; returns &lt;code&gt;{&amp;quot;detail&amp;quot;:&amp;quot;Method Not Allowed&amp;quot;}&lt;/code&gt; Sometimes, and nothing else happens. I enabled &lt;code&gt;--detailed_debug&lt;/code&gt;, and I do see logs like “Initialized model mistral,” but I don&amp;#39;t get an interface, or it fails silently. Here&amp;#39;s an explanation of more of my issue from ChatGPT:&lt;br/&gt;\nWhat I Tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Confirmed all ports are correct&lt;/li&gt;\n&lt;li&gt;Docker can reach &lt;code&gt;host.docker.internal:4000&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;I’ve tested curl inside the container to confirm&lt;/li&gt;\n&lt;li&gt;Sometimes it randomly works, but it breaks again on the next reboot&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;❓What I Need:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is this the correct &lt;code&gt;model_list&lt;/code&gt; format for Ollama/Mistral via LiteLLM?&lt;/li&gt;\n&lt;li&gt;Does OpenHands require a specific model name format?&lt;/li&gt;\n&lt;li&gt;How can I force OpenHands to show &lt;strong&gt;detailed errors&lt;/strong&gt; instead of generic &lt;code&gt;APIConnectionError&lt;/code&gt;?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would appreciate it if you could help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5v7if",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HowdyCapybara",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5v7if/im_trying_to_make_my_own_agent_with_openhands_but/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5v7if/im_trying_to_make_my_own_agent_with_openhands_but/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753132378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to run models locally (at my workplaces) and also refine models, and fortunately I'm not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I'm missing for going with 512GB RAM?",
          "author_fullname": "t2_sk7nmjrs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Strong case for a 512GB Mac Studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5uu0t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753131529,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to run models locally (at my workplaces) and also refine models, and fortunately I&amp;#39;m not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I&amp;#39;m missing for going with 512GB RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5uu0t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChevChance",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753131529,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\nI recently wrapped up a side project involving SEC filings, and thought some of you here might find it interesting or useful.\n\nI built a dataset of ~4,000 instruction-output samples based on real 6-K and 8-K filings.\nIt’s structured in JSONL, QLoRA/Alpaca-style format (natural language instruction → clean short answer).\n\nInputs retain real-world messiness from actual filings (inconsistent structure, lawyer-ese, etc.)\n\nOutputs are concise summaries, instructions, or redirections depending on filing type (earnings, acquisitions, restructuring, resigning, etc.)\n\nThe goal was to train an LLM to handle regulatory language like a financial analyst with pattern recognition\n\n\nOriginally made this for internal fine-tuning, but I’ve shifted to another niche now.\nIf anyone’s working on AI for finance, compliance, investor tools, etc., I’m happy to share a few sample entries and chat about use cases.\n\nIf enough people are interested, I might package it for others to use or license.\n\nDM me if you want a preview or have questions.",
          "author_fullname": "t2_1r3lcxoyqn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Project Share] Built a 4K Instruction Dataset Based on SEC 6-K/8-K Filings (JSONL format, QLoRA-friendly)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5uhwc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753130743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,\nI recently wrapped up a side project involving SEC filings, and thought some of you here might find it interesting or useful.&lt;/p&gt;\n\n&lt;p&gt;I built a dataset of ~4,000 instruction-output samples based on real 6-K and 8-K filings.\nIt’s structured in JSONL, QLoRA/Alpaca-style format (natural language instruction → clean short answer).&lt;/p&gt;\n\n&lt;p&gt;Inputs retain real-world messiness from actual filings (inconsistent structure, lawyer-ese, etc.)&lt;/p&gt;\n\n&lt;p&gt;Outputs are concise summaries, instructions, or redirections depending on filing type (earnings, acquisitions, restructuring, resigning, etc.)&lt;/p&gt;\n\n&lt;p&gt;The goal was to train an LLM to handle regulatory language like a financial analyst with pattern recognition&lt;/p&gt;\n\n&lt;p&gt;Originally made this for internal fine-tuning, but I’ve shifted to another niche now.\nIf anyone’s working on AI for finance, compliance, investor tools, etc., I’m happy to share a few sample entries and chat about use cases.&lt;/p&gt;\n\n&lt;p&gt;If enough people are interested, I might package it for others to use or license.&lt;/p&gt;\n\n&lt;p&gt;DM me if you want a preview or have questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5uhwc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xairossss",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5uhwc/project_share_built_a_4k_instruction_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5uhwc/project_share_built_a_4k_instruction_dataset/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753130743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know that most platforms fine-tune their models and use a good system prompt, but I've tried Qwen3 32B locally and on [qwen.com](http://qwen.com) and the difference is so huge.\n\nAre there publicly available ready fine-tunes and system prompts I can use to improve the models locally?",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why are base non-finetuned models so bad?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5th6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.39,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753128457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that most platforms fine-tune their models and use a good system prompt, but I&amp;#39;ve tried Qwen3 32B locally and on &lt;a href=\"http://qwen.com\"&gt;qwen.com&lt;/a&gt; and the difference is so huge.&lt;/p&gt;\n\n&lt;p&gt;Are there publicly available ready fine-tunes and system prompts I can use to improve the models locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5th6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5th6s/why_are_base_nonfinetuned_models_so_bad/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5th6s/why_are_base_nonfinetuned_models_so_bad/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753128457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.",
          "author_fullname": "t2_y0abrfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI 395+ 64GB vs 128GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5s6d1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753125511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5s6d1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cfogrady",
          "discussion_type": null,
          "num_comments": 87,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753125511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\nJust got an RTX 5090 with 32GB of VRAM and I'm looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.\n\n * What's the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?\n\n * Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?\n\n * Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?\n\nMy goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!\n\nThanks!",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 (32GB VRAM) - Full Fine-Tuning: What Can I Expect?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ro7s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753124390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,\nJust got an RTX 5090 with 32GB of VRAM and I&amp;#39;m looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;What&amp;#39;s the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ro7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753124390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "_A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him \"completely exhausted.\"_\n\nhttps://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/",
          "author_fullname": "t2_1gnii9bkc9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Exhausted man defeats AI model in world coding championship",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5r9ss",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 145,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 145,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753123500,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him &amp;quot;completely exhausted.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/\"&gt;https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?auto=webp&amp;s=01c6c5989382448ceacfb16d4716e1d43882c07d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c56d051b4b4e63e5c6627f8639b6bc541ebe7a70",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06c2478ecab47c428699645361b3f004e394ce98",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dcfa8df2a1e6cafbca016c61a6781fc1bd66b6e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=089271aa272fc9321b33a83d2db06a95c38b6ce9",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73adafa18daac7bc6b0c9c52f9fe6f72db681c41",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3d93ef0ea7ac95a421f4d7a2e7e1d2f2350aac9",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5r9ss",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational_Sun_8813",
          "discussion_type": null,
          "num_comments": 40,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753123500,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Project Page: [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://deepreinforce-ai.github.io/cudal1_blog/)\n\nCode: [GitHub - deepreinforce-ai/CUDA-L1](https://github.com/deepreinforce-ai/CUDA-L1)\n\nAbstract\n\n&gt;The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.  \nCUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.  \nThe capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.",
          "author_fullname": "t2_dtsa6gxt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qsqx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753122428,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Project Page: &lt;a href=\"https://deepreinforce-ai.github.io/cudal1_blog/\"&gt;CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/deepreinforce-ai/CUDA-L1\"&gt;GitHub - deepreinforce-ai/CUDA-L1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.&lt;br/&gt;\nCUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.&lt;br/&gt;\nThe capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.14111",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5qsqx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Formal_Drop526",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qsqx/cudal1_improving_cuda_optimization_via/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.14111",
          "subreddit_subscribers": 502981,
          "created_utc": 1753122428,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 insane SimpleQA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qn1n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 79,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 79,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753122070,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5qn1n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753122070,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is a brief demo showing how one could use the new AI chat features in Neu called the magic hand. This system uses Llama 3.2 3b as a tool caller, and Claude Haiku 3.5 to generate the code but the code step could easily be replaced with a local model such as Qwen 3. I'm most using Claude because of the speed. It's still early days so right now its simple input output commands but I've been experimenting with a full blown agent that (I hope) will be able to build entire graphs. My hope is that this drastically reduces the knowledge floor needed to use Neu which, let's be honest, is a pretty intimidating piece of software. I hope that by following what the magic hand is doing, you can learn and understand Neu better. These features and a ton more will be coming with the Neu 0.3.0 update. [Checkout this link you'd like to learn more about Neu](http://kingroka.itch.io/neu)",
          "author_fullname": "t2_fkmug",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using ollama and claude to control Neu",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qflo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/lgjdwe0yo9ef1/DASHPlaylist.mpd?a=1755825164%2CNTkwMGU3NTBhMTg4ZjcyNTc2NTgwMmE2MjA1M2NiNTUyNTgxNjg0NWNhZjM1YmU1YmQxYWM5ZDFkNTAxZDZjMQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 74,
              "hls_url": "https://v.redd.it/lgjdwe0yo9ef1/HLSPlaylist.m3u8?a=1755825164%2CNzNkYmE0NDgxYzc3MDNiOGJmMWZhZTdmMDJmZjg1OWRiODM4N2YzOTZiZTRhM2FlMjZkNTE0Yzg4MzVjZDRlOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=21b892afd0727f90d166d0a8bd745b1b393466e0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753121614,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a brief demo showing how one could use the new AI chat features in Neu called the magic hand. This system uses Llama 3.2 3b as a tool caller, and Claude Haiku 3.5 to generate the code but the code step could easily be replaced with a local model such as Qwen 3. I&amp;#39;m most using Claude because of the speed. It&amp;#39;s still early days so right now its simple input output commands but I&amp;#39;ve been experimenting with a full blown agent that (I hope) will be able to build entire graphs. My hope is that this drastically reduces the knowledge floor needed to use Neu which, let&amp;#39;s be honest, is a pretty intimidating piece of software. I hope that by following what the magic hand is doing, you can learn and understand Neu better. These features and a ton more will be coming with the Neu 0.3.0 update. &lt;a href=\"http://kingroka.itch.io/neu\"&gt;Checkout this link you&amp;#39;d like to learn more about Neu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/lgjdwe0yo9ef1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?format=pjpg&amp;auto=webp&amp;s=066dc89758345e5c4b0f587768ddc66e19e69c11",
                  "width": 2560,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=70d833416067508a44b90607113d94349d55781b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b22a0bf8d0efcb692e56b986ffac037c1461879a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=43e75947dba7a887cce1309dd168dadb7cbc6ae7",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0427819bd56be1b5a7f90f67087f2477281f6bb8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=82c83d51ba654e3c59c02f4516982e17cbac1472",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=11c7891a7ee6be24d57433d813fd8a07d748bd5d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5qflo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kingroka",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qflo/using_ollama_and_claude_to_control_neu/",
          "stickied": false,
          "url": "https://v.redd.it/lgjdwe0yo9ef1",
          "subreddit_subscribers": 502981,
          "created_utc": 1753121614,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/lgjdwe0yo9ef1/DASHPlaylist.mpd?a=1755825164%2CNTkwMGU3NTBhMTg4ZjcyNTc2NTgwMmE2MjA1M2NiNTUyNTgxNjg0NWNhZjM1YmU1YmQxYWM5ZDFkNTAxZDZjMQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 74,
              "hls_url": "https://v.redd.it/lgjdwe0yo9ef1/HLSPlaylist.m3u8?a=1755825164%2CNzNkYmE0NDgxYzc3MDNiOGJmMWZhZTdmMDJmZjg1OWRiODM4N2YzOTZiZTRhM2FlMjZkNTE0Yzg4MzVjZDRlOQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html](https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html)",
          "author_fullname": "t2_84qpb9rt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Interesting new blog post from Lemonade team",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5q35o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753120844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html\"&gt;https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?auto=webp&amp;s=370ef3c67fc9a466fb921e399215ca76e255bdd8",
                  "width": 1435,
                  "height": 645
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d6ab69b2057c1ff74bd4fad9be640864917203c",
                    "width": 108,
                    "height": 48
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc432f40f27f0a2f68d597fb271cdc9efc951403",
                    "width": 216,
                    "height": 97
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39d76385ecda761edb0e6a264b0e3efd78c0cd1c",
                    "width": 320,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ae8bc998e1523bb4c01bb0d57ef5cea220343f",
                    "width": 640,
                    "height": 287
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1e801ab47a7cb7d218797288798cd8f7d4e41fd",
                    "width": 960,
                    "height": 431
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d14e3dd033ec146ebe70b9e6b9135c50e7a9136",
                    "width": 1080,
                    "height": 485
                  }
                ],
                "variants": {},
                "id": "tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5q35o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Smooth-Screen4148",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "subreddit_subscribers": 502981,
          "created_utc": 1753120844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}