{
  "kind": "Listing",
  "data": {
    "after": "t3_1lpoqlu",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7\n\n# \n\n  \n*On Day 8, we looked at what Rotary Positional Embeddings (RoPE) are and why they are important in transformers.*\n\n*Today, on Day 9, we’re going to code RoPE and see how it’s implemented in the DeepSeek Children’s Stories model, a transformer architecture optimized for generating engaging stories for kids.*\n\n*Quick Recap: What is RoPE?*\n\n*RoPE is a method for injecting positional information into transformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism.*\n\n*This provides several advantages:*\n\n* ***Relative Position Awareness****: Understands the distance between tokens*\n* ***Extrapolation****: Handles sequences longer than seen during training*\n* ***Efficiency****: Doesn’t require additional embeddings — just math inside attention*\n\n#  Code Walkthrough\n\n*Let’s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model* [*https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model*](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model) *codebase.*\n\n# 1: Implementation: RoPEPositionalEncoding\n\n*In the file src/model/deepseek.py, you’ll find the class RoPEPositionalEncoding.*\n\n*This class:*\n\n* *Precomputes rotation frequencies*\n* *Provides an apply\\_rope method*\n* *Applies RoPE to input tensors, usually the query and key vectors*\n\n&amp;#8203;\n\n    # deepseek.py\n    class RoPEPositionalEncoding(nn.Module):\n        def __init__(self, dim, max_len=2048):\n            super().__init__()\n            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n            t = torch.arange(max_len, dtype=torch.float)\n            freqs = torch.einsum(\"i,j-&gt;ij\", t, inv_freq)\n            emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n            self.register_buffer(\"positional_encoding\", emb)\n\n        def apply_rope(self, x, position_ids):\n            rope = self.positional_encoding[position_ids]\n            x1, x2 = x[..., ::2], x[..., 1::2]\n            rope1, rope2 = rope[..., ::2], rope[..., 1::2]\n            return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1)\n\n&gt;***Note****: The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies.*\n\n# 2: Usage: Integrating RoPE into Attention\n\n*The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). Here’s how RoPE is integrated:*\n\n    # deepseek.py\n    q = self.q_proj(x)\n    k = self.k_proj(x)\n\n    q = self.rope.apply_rope(q, position_ids)\n    k = self.rope.apply_rope(k, position_ids)\n\n*What’s happening?*\n\n* `x` *is projected into query (*`q`*) and key (*`k`*) vectors.*\n* *RoPE is applied to both using apply\\_rope, injecting position awareness.*\n* *Attention proceeds as usual — except now the queries and keys are aware of their relative positions.*\n\n# 3: Where RoPE is Used\n\n* ***Every Transformer Block****: Each block in the DeepSeek model uses MLA and applies RoPE.*\n* ***During Both Training and Inference****: RoPE is always on, helping the model understand the token sequence no matter the mode.*\n\n# Why RoPE is Perfect for Story Generation\n\n*In story generation, especially for children’s stories, context is everything.*\n\n*RoPE enables the model to:*\n\n* *Track who did what across paragraphs*\n* *Maintain chronological consistency*\n* *Preserve narrative flow even in long outputs*\n\n*This is crucial when the model must remember that “the dragon flew over the mountain” five paragraphs ago.*\n\n# Conclusion\n\n*Rotary Positional Embeddings (RoPE) are not just a theoretical improvement; they offer practical performance and generalization benefits.*\n\n*If you’re working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, you should absolutely consider using RoPE.*\n\n*Next Up (Day 10): We’ll dive into one of my favorite topics , model distillation: what it is, how it works, and why it’s so powerful.*\n\n*Codebase:* [*https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model*](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model)",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 9/50: Building a Small Language Model from Scratch — Coding Rotary Positional Embeddings (RoPE)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": true,
          "media_metadata": {
            "5wzktz5ijoaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ee71cc035bbb52ca909828163bda5b700206248"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e24bae63fa7b1f4370b715903e1f5a396a9d526"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a128849d5dd8f60b1fd8797320b64ca086a52958"
                },
                {
                  "y": 640,
                  "x": 640,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3a06d4af88d05b20c654cb11c9bab89e85592f4"
                },
                {
                  "y": 960,
                  "x": 960,
                  "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9de890d9da4436870159e3dd2fee03f2b049f914"
                }
              ],
              "s": {
                "y": 1024,
                "x": 1024,
                "u": "https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7"
              },
              "id": "5wzktz5ijoaf1"
            }
          },
          "name": "t3_1lqsvmf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=d67d337b55965c2b5d71b5eef82e922385ee69a1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751557433,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7\"&gt;https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;On Day 8, we looked at what Rotary Positional Embeddings (RoPE) are and why they are important in transformers.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Today, on Day 9, we’re going to code RoPE and see how it’s implemented in the DeepSeek Children’s Stories model, a transformer architecture optimized for generating engaging stories for kids.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Quick Recap: What is RoPE?&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;RoPE is a method for injecting positional information into transformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;This provides several advantages:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Relative Position Awareness&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Understands the distance between tokens&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Extrapolation&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Handles sequences longer than seen during training&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Efficiency&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Doesn’t require additional embeddings — just math inside attention&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt; Code Walkthrough&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Let’s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model&lt;/em&gt; &lt;a href=\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt; &lt;em&gt;codebase.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;1: Implementation: RoPEPositionalEncoding&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;In the file src/model/deepseek.py, you’ll find the class RoPEPositionalEncoding.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;This class:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Precomputes rotation frequencies&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Provides an apply_rope method&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Applies RoPE to input tensors, usually the query and key vectors&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# deepseek.py\nclass RoPEPositionalEncoding(nn.Module):\n    def __init__(self, dim, max_len=2048):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(max_len, dtype=torch.float)\n        freqs = torch.einsum(&amp;quot;i,j-&amp;gt;ij&amp;quot;, t, inv_freq)\n        emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n        self.register_buffer(&amp;quot;positional_encoding&amp;quot;, emb)\n\n    def apply_rope(self, x, position_ids):\n        rope = self.positional_encoding[position_ids]\n        x1, x2 = x[..., ::2], x[..., 1::2]\n        rope1, rope2 = rope[..., ::2], rope[..., 1::2]\n        return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies.&lt;/em&gt;&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;h1&gt;2: Usage: Integrating RoPE into Attention&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). Here’s how RoPE is integrated:&lt;/em&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;# deepseek.py\nq = self.q_proj(x)\nk = self.k_proj(x)\n\nq = self.rope.apply_rope(q, position_ids)\nk = self.rope.apply_rope(k, position_ids)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;em&gt;What’s happening?&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;x&lt;/code&gt; &lt;em&gt;is projected into query (&lt;/em&gt;&lt;code&gt;q&lt;/code&gt;&lt;em&gt;) and key (&lt;/em&gt;&lt;code&gt;k&lt;/code&gt;&lt;em&gt;) vectors.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;RoPE is applied to both using apply_rope, injecting position awareness.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Attention proceeds as usual — except now the queries and keys are aware of their relative positions.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;3: Where RoPE is Used&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Every Transformer Block&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Each block in the DeepSeek model uses MLA and applies RoPE.&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;During Both Training and Inference&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: RoPE is always on, helping the model understand the token sequence no matter the mode.&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Why RoPE is Perfect for Story Generation&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;In story generation, especially for children’s stories, context is everything.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;RoPE enables the model to:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;em&gt;Track who did what across paragraphs&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Maintain chronological consistency&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Preserve narrative flow even in long outputs&lt;/em&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;em&gt;This is crucial when the model must remember that “the dragon flew over the mountain” five paragraphs ago.&lt;/em&gt;&lt;/p&gt;\n\n&lt;h1&gt;Conclusion&lt;/h1&gt;\n\n&lt;p&gt;&lt;em&gt;Rotary Positional Embeddings (RoPE) are not just a theoretical improvement; they offer practical performance and generalization benefits.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If you’re working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, you should absolutely consider using RoPE.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Next Up (Day 10): We’ll dive into one of my favorite topics , model distillation: what it is, how it works, and why it’s so powerful.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Codebase:&lt;/em&gt; &lt;a href=\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?auto=webp&amp;s=0c27ab9b5764679c50e0adc71710dfe3f9448763",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e02674ee596709cdf5dd29ebf3417f363bd55eda",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7c566f65ff67005168e511f30aefa29a58f5d6b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9345b7ed424e9f4076418fc4b52f3cd0080f8083",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=438b67c1d9deb5dd0f97a1c609b743a0ba02da9a",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=da546090177ad66dad893ac1b08290d8c72a16af",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=140c7aa532bc0b58e2d5708f61190e9112bc36e0",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqsvmf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751557433,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\nUnfortunately it doesn’t work.\nCorrect endpoint \nAPI key as Vertex Admin\nModel : gemini-2.5-pro\n\nAlways get error 404 no body…\n\nThx\n",
          "author_fullname": "t2_mq7at82i",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AnythingLLM Vertex Ai",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lqsvf6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751557420,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nUnfortunately it doesn’t work.\nCorrect endpoint \nAPI key as Vertex Admin\nModel : gemini-2.5-pro&lt;/p&gt;\n\n&lt;p&gt;Always get error 404 no body…&lt;/p&gt;\n\n&lt;p&gt;Thx&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqsvf6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OkReference5581",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqsvf6/anythingllm_vertex_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqsvf6/anythingllm_vertex_ai/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751557420,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1siqb5ohs4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI2 releases OLMo 32B - Truly open source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lqsrim",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=78588b5017fe46f038f5e9169204c535d5258584",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751557168,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.imgur.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.imgur.com/2zGShZY.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?auto=webp&amp;s=224f789169d4e3a3693eb284d2c7de00055a55d9",
                  "width": 1080,
                  "height": 1349
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cba3f2c96187e9404c85908939d4bab5ba4adde",
                    "width": 108,
                    "height": 134
                  },
                  {
                    "url": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1cd573dd652ea03a538214617f1b4bcf9ba753d4",
                    "width": 216,
                    "height": 269
                  },
                  {
                    "url": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e56519351b86f12323143ac41999e750a787fcf3",
                    "width": 320,
                    "height": 399
                  },
                  {
                    "url": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b6776200b9360b78b464838388591b605dbec8cd",
                    "width": 640,
                    "height": 799
                  },
                  {
                    "url": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3b418229c71e2cc460d2942f342712b5ca91d962",
                    "width": 960,
                    "height": 1199
                  },
                  {
                    "url": "https://external-preview.redd.it/3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60fe347417047890348b274df321126ec8bf13f9",
                    "width": 1080,
                    "height": 1349
                  }
                ],
                "variants": {},
                "id": "3Vd_xhZIHH3nsDqdxTxbcGJMXR35bC9rlb6_fbhajXE"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqsrim",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "biggflingbollar",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqsrim/ai2_releases_olmo_32b_truly_open_source/",
          "stickied": false,
          "url": "https://i.imgur.com/2zGShZY.png",
          "subreddit_subscribers": 494001,
          "created_utc": 1751557168,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello I was going back and forth with ChatGPT and other models to try and find a research gap involving a two-step approach to LLM reasoning and clarity for users. This is essentially the question i came up with:\n\n  \nCan fine-tuning an MLLM with dual-purpose instruction pairs—combining explicit refusals with grounded reinterpretations—reduce hallucinations while improving user trust and perceived helpfulness in ambiguous or misleading prompts?\n\nGPT says that it's a new approach compared to existing studies and methods out there, but I find that hard to believe. This approach would explicitly refuse the given prompt given that it is false/unreasonable/ unfeasible, etc. Then it would give its own reasoning, clarifying and reinterpreting the prompt by itself, then give the answer to this new prompt. If anyone has any information if this has been implemented or if this is truly new, I would appreciate the help.",
          "author_fullname": "t2_anfoy9hg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Potential for Research?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lqsod4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751556959,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello I was going back and forth with ChatGPT and other models to try and find a research gap involving a two-step approach to LLM reasoning and clarity for users. This is essentially the question i came up with:&lt;/p&gt;\n\n&lt;p&gt;Can fine-tuning an MLLM with dual-purpose instruction pairs—combining explicit refusals with grounded reinterpretations—reduce hallucinations while improving user trust and perceived helpfulness in ambiguous or misleading prompts?&lt;/p&gt;\n\n&lt;p&gt;GPT says that it&amp;#39;s a new approach compared to existing studies and methods out there, but I find that hard to believe. This approach would explicitly refuse the given prompt given that it is false/unreasonable/ unfeasible, etc. Then it would give its own reasoning, clarifying and reinterpreting the prompt by itself, then give the answer to this new prompt. If anyone has any information if this has been implemented or if this is truly new, I would appreciate the help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqsod4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RockNo8451",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqsod4/potential_for_research/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqsod4/potential_for_research/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751556959,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source \n\nYou can get it here\n\nhttps://huggingface.co/moelanoby/phi-3-M3-coder",
          "author_fullname": "t2_1mkofsrzlo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have made a True Reasoning LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lqqxhq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 39,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 39,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751552742,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I have created an LLM with my own custom architecture. My architecture uses self correction and Long term memory in vector states which makes it more stable and perform a bit better. And I used phi-3-mini for this project and after finetuning the model with the custom architecture it acheived 98.17% on HumanEval benchmark (you could recommend me other lightweight benchmarks for me) and I have made thee model open source &lt;/p&gt;\n\n&lt;p&gt;You can get it here&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/moelanoby/phi-3-M3-coder\"&gt;https://huggingface.co/moelanoby/phi-3-M3-coder&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?auto=webp&amp;s=3967b1160a2f4b5eece9500196937b642c5fe942",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ed293b664cabbdfa93509e2abe75b146f835a45a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d16a7767822b4be25c36802a580daece5b9a107",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac0aa0b1b942392082fa109aea6da886d484c322",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=044beab733ac2218558b35002b4cdf80f7045286",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1669a37b0cbffb2cc0784e19bd6b8fdadb1767fb",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c973e8ca983e0955e0f31bffd264559a73eb0e24",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "kKe9_1jTFhdqIiq4KwRPU-IXIZSPmcBkiqgJnTL3j8k"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqqxhq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "moilanopyzedev",
          "discussion_type": null,
          "num_comments": 88,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqqxhq/i_have_made_a_true_reasoning_llm/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751552742,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unmute github: [https://github.com/kyutai-labs/unmute](https://github.com/kyutai-labs/unmute)\n\nUnmute blog: [https://kyutai.org/next/unmute](https://kyutai.org/next/unmute)\n\nTTS blog with a demo: [https://kyutai.org/next/tts](https://kyutai.org/next/tts)\n\nTTS weights: [https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29](https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29)\n\nSTT was released earlier so the whole component stack is now out.",
          "author_fullname": "t2_12aeph",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kyutai Unmute (incl. TTS) released",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lqqx16",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 30,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 30,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751553064,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751552711,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unmute github: &lt;a href=\"https://github.com/kyutai-labs/unmute\"&gt;https://github.com/kyutai-labs/unmute&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Unmute blog: &lt;a href=\"https://kyutai.org/next/unmute\"&gt;https://kyutai.org/next/unmute&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TTS blog with a demo: &lt;a href=\"https://kyutai.org/next/tts\"&gt;https://kyutai.org/next/tts&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;TTS weights: &lt;a href=\"https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29\"&gt;https://huggingface.co/collections/kyutai/text-to-speech-6866192e7e004ed04fd39e29&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;STT was released earlier so the whole component stack is now out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?auto=webp&amp;s=2ba392f5b24e4c7b242b4a97febc3fed4017a1c9",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bfc17e6601cf3dc507ecfc1402a53a0c14022fb2",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=912da36d80c961dbf899ba910b78e9485c383f20",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=988d8643ffdcabc636d17c5cd78fa26714beff1a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=88cd8fece80ec2c5b2f542e2803a0e1a1ea6143f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0857293dafc2b875d0fb1e209bea06ea05e1157f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=794a9576782e128543b07fbc773ab216e0e149da",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "_LMiFlTaq2TQieU_1mtU0mAeO3Zj4Y5uoHEf7nTG6Z8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqqx16",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rerri",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqqx16/kyutai_unmute_incl_tts_released/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751552711,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've been liking Gemma3 but the text extraction performance is far, far behind any of the \"chat\" offerings. Can one do better?",
          "author_fullname": "t2_bnssb5gv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best local TEXT EXTRACTION model 24GB/48GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqpvcb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751550001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been liking Gemma3 but the text extraction performance is far, far behind any of the &amp;quot;chat&amp;quot; offerings. Can one do better?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqpvcb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Otherwise-Tiger3359",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqpvcb/best_local_text_extraction_model_24gb48gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqpvcb/best_local_text_extraction_model_24gb48gb/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751550001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys,\n\nWe're the startup team behind some of the projects you might be familiar with, including **PowerInfer (https://github.com/SJTU-IPADS/PowerInfer)** and **SmallThinker (https://huggingface.co/PowerInfer/SmallThinker-3B-Preview)**. The feedback from this community has been crucial, and we're excited to give you a heads-up on our next open-source release coming in **late July**.\n\nWe're releasing two new MoE models, both of which we have **pre-trained from scratch** with a structure specifically optimized for efficient inference on edge devices:\n\n* **A new 4B Reasoning Model:** An evolution of SmallThinker with significantly improved logic capabilities.\n* **A 20B Model:** Designed for high performance in a local-first environment.\n\nWe'll be releasing the **full weights, a technical report, and parts of the training dataset** for both.\n\nOur core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we've also been developing a dedicated edge device. It's a small, self-contained unit (**around 10x7x1.5 cm**) capable of running the 20B model completely offline with a power draw of **around 30W**.\n\nThis is still a work in progress, but it proves what's possible with full-stack optimization. We'd love to get your feedback on this direction:\n\n1. For a compact, private device like this, what are the most compelling use cases you can imagine?\n2. For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?\n3. Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?\n\nWe'll be in the comments to answer questions. We're incredibly excited to share our work and believe local AI is the future we're all building together",
          "author_fullname": "t2_1qznwxvu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Upcoming Release &amp; Feedback] A new 4B &amp; 20B model, building on our SmallThinker work. Plus, a new hardware device to run them locally.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqpm60",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751549323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re the startup team behind some of the projects you might be familiar with, including &lt;strong&gt;PowerInfer (&lt;a href=\"https://github.com/SJTU-IPADS/PowerInfer\"&gt;https://github.com/SJTU-IPADS/PowerInfer&lt;/a&gt;)&lt;/strong&gt; and &lt;strong&gt;SmallThinker (&lt;a href=\"https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\"&gt;https://huggingface.co/PowerInfer/SmallThinker-3B-Preview&lt;/a&gt;)&lt;/strong&gt;. The feedback from this community has been crucial, and we&amp;#39;re excited to give you a heads-up on our next open-source release coming in &lt;strong&gt;late July&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re releasing two new MoE models, both of which we have &lt;strong&gt;pre-trained from scratch&lt;/strong&gt; with a structure specifically optimized for efficient inference on edge devices:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;A new 4B Reasoning Model:&lt;/strong&gt; An evolution of SmallThinker with significantly improved logic capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;A 20B Model:&lt;/strong&gt; Designed for high performance in a local-first environment.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;ll be releasing the &lt;strong&gt;full weights, a technical report, and parts of the training dataset&lt;/strong&gt; for both.&lt;/p&gt;\n\n&lt;p&gt;Our core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we&amp;#39;ve also been developing a dedicated edge device. It&amp;#39;s a small, self-contained unit (&lt;strong&gt;around 10x7x1.5 cm&lt;/strong&gt;) capable of running the 20B model completely offline with a power draw of &lt;strong&gt;around 30W&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;This is still a work in progress, but it proves what&amp;#39;s possible with full-stack optimization. We&amp;#39;d love to get your feedback on this direction:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;For a compact, private device like this, what are the most compelling use cases you can imagine?&lt;/li&gt;\n&lt;li&gt;For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?&lt;/li&gt;\n&lt;li&gt;Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;We&amp;#39;ll be in the comments to answer questions. We&amp;#39;re incredibly excited to share our work and believe local AI is the future we&amp;#39;re all building together&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?auto=webp&amp;s=1aec8de7771a3a6bf2fb7294273c490dc06dcabd",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=035b7f53a7b18dd7b7ecb29766539170f3263cdd",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=886c0dc66edad038eb63775c7c89f33b970808be",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6893b211bf4a8cff73e02050ba60d32bb76b4e7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=45900a8c745f7b4a70676f45ecc603e4d0d5b769",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d93656512791ac056fda1d75e5b42132e5505b99",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=043adc578549f1f078c89c4ba97448d3830f71d3",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqpm60",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yzmizeyu",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751549323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm writing a program that compares two text sections. Sometimes the OCR screws up so I can't just do a A==B comparison.\n\nFor instance, I'd like the LLM to compare\n\n\"Further\" == \"Father\" and say \"Same\".\n\nBut \"15\" == \"30\" and say \"Different\"\n\nI know the beefier ChatGPT models can do this, but I need to run this locally.\n\nMy plan is to run the prompt ~3-5 times, using ~3 different models, and if a consensus is met, using that consensus output. \n\nHistorically and currently, I've had trouble getting ~7B models to follow instructions like this. I may be able to get up to ~70B models, and maybe maybe 400B models if I can get cost approval. But for now, I'm mostly looking for 'prompt engineering'.",
          "author_fullname": "t2_u49aibv3e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What kind of prompts *Always* give a 1 word response?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqphqd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751548996,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m writing a program that compares two text sections. Sometimes the OCR screws up so I can&amp;#39;t just do a A==B comparison.&lt;/p&gt;\n\n&lt;p&gt;For instance, I&amp;#39;d like the LLM to compare&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Further&amp;quot; == &amp;quot;Father&amp;quot; and say &amp;quot;Same&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;But &amp;quot;15&amp;quot; == &amp;quot;30&amp;quot; and say &amp;quot;Different&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I know the beefier ChatGPT models can do this, but I need to run this locally.&lt;/p&gt;\n\n&lt;p&gt;My plan is to run the prompt ~3-5 times, using ~3 different models, and if a consensus is met, using that consensus output. &lt;/p&gt;\n\n&lt;p&gt;Historically and currently, I&amp;#39;ve had trouble getting ~7B models to follow instructions like this. I may be able to get up to ~70B models, and maybe maybe 400B models if I can get cost approval. But for now, I&amp;#39;m mostly looking for &amp;#39;prompt engineering&amp;#39;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqphqd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Waterbottles_solve",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqphqd/what_kind_of_prompts_always_give_a_1_word_response/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqphqd/what_kind_of_prompts_always_give_a_1_word_response/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751548996,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Speed Comparison Reference: https://youtu.be/VGyKwi9Rfhk\n\nDo you guys know if there's an workaround for pushing the RTX 3060 12GB faster with a ~32b model?\n\nCan it handle light text-to-speech + image generation within ~14b models?\n\nWhat's the most common issues you've ran with this GPU in AI stuff?\n\nNote: CPU is Ryzen 5 4600g/20GB Ram with me possibly upgrading to 36GB soon.",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "About RTX 3060 12GB running AI models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqpggb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751548896,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Speed Comparison Reference: &lt;a href=\"https://youtu.be/VGyKwi9Rfhk\"&gt;https://youtu.be/VGyKwi9Rfhk&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Do you guys know if there&amp;#39;s an workaround for pushing the RTX 3060 12GB faster with a ~32b model?&lt;/p&gt;\n\n&lt;p&gt;Can it handle light text-to-speech + image generation within ~14b models?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the most common issues you&amp;#39;ve ran with this GPU in AI stuff?&lt;/p&gt;\n\n&lt;p&gt;Note: CPU is Ryzen 5 4600g/20GB Ram with me possibly upgrading to 36GB soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?auto=webp&amp;s=b73b668679f72aff163660e18641c686769ef8df",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5524f2540197439b406429b48aeee3f676b290d0",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e37d546bfd3a6d7491dbfd693e0cb9147708538a",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b1c3f8e0a242937dd6bfbab1fe4b1442568aa8ea",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "1l9TvGsnHr-xVtadKook-5Kyhctt1Qy7XDW2ZTdFZDM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqpggb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqpggb/about_rtx_3060_12gb_running_ai_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqpggb/about_rtx_3060_12gb_running_ai_models/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751548896,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3f345",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I ran llama.cpp on a Raspberry Pi",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqo9lk",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running AI on a Raspberry Pi",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
              "author_name": "Krisseck",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/TNxIIDkP2Zg/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Krisseck"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lqo9lk",
            "height": 200
          },
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=72a6bcf9ec3fbde58d5f907ae2d129e55ced5423",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751545561,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.youtube.com/watch?v=TNxIIDkP2Zg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?auto=webp&amp;s=2ddb116bd3193d0f3249d7b78ed1b09faf3478bf",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=953fe4c8b71ee7f0e2e63b136f684205c3a95f5d",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1b1d9a5dfb33e01fcda36c3f991481f5b430bb78",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df2abe3d648a732348a80314b8b3b98f9cf5f098",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "ZWQha1gxtezTow-pIhOuvJt_MLt9uakB-VthPyt0xWs"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lqo9lk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Risse",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqo9lk/i_ran_llamacpp_on_a_raspberry_pi/",
          "stickied": false,
          "url": "https://www.youtube.com/watch?v=TNxIIDkP2Zg",
          "subreddit_subscribers": 494001,
          "created_utc": 1751545561,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Running AI on a Raspberry Pi",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/TNxIIDkP2Zg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Running AI on a Raspberry Pi\"&gt;&lt;/iframe&gt;",
              "author_name": "Krisseck",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/TNxIIDkP2Zg/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@Krisseck"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi Everyone \n\nI am silent follower of all you wonderful folks. I have learnt to play around Ollama and tie it up with my application make AI Application \n\nNow, I am planning to move to Llama.cpp can someone suggest how should I approach it and what should be learning path\n\nTIA\n",
          "author_fullname": "t2_eabbhyzu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Llama.cpp after Ollama for industry grade softwares",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqo8q0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751545491,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Everyone &lt;/p&gt;\n\n&lt;p&gt;I am silent follower of all you wonderful folks. I have learnt to play around Ollama and tie it up with my application make AI Application &lt;/p&gt;\n\n&lt;p&gt;Now, I am planning to move to Llama.cpp can someone suggest how should I approach it and what should be learning path&lt;/p&gt;\n\n&lt;p&gt;TIA&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqo8q0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "bull_bear25",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751545491,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm working on a privacy sensitive usecase that needs a LLM. Instead of relaying the entire prompt to the server, I want to run a few layers in the client and then send the intermediate state to the server to be run until completion.  \nWhile I understand this doesn't exactly solve the privacy issue, this level of information loss is enough for my usecase.\n\nMy questions:  \n1. Is something like this even possible? Has anybody done something like this before?  \n2. If this is possible, will the resulting clients-side model be runnable with limited hardware (rephrase: Does running a partial model going to require enough hardware power as much as running a full model?)",
          "author_fullname": "t2_2ue366bm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to split a model to run a portion of it on client and run the remaining layers on server. Is that possible?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqo1bt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751544887,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a privacy sensitive usecase that needs a LLM. Instead of relaying the entire prompt to the server, I want to run a few layers in the client and then send the intermediate state to the server to be run until completion.&lt;br/&gt;\nWhile I understand this doesn&amp;#39;t exactly solve the privacy issue, this level of information loss is enough for my usecase.&lt;/p&gt;\n\n&lt;p&gt;My questions:&lt;br/&gt;\n1. Is something like this even possible? Has anybody done something like this before?&lt;br/&gt;\n2. If this is possible, will the resulting clients-side model be runnable with limited hardware (rephrase: Does running a partial model going to require enough hardware power as much as running a full model?)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqo1bt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "crazycodemonkey",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqo1bt/i_want_to_split_a_model_to_run_a_portion_of_it_on/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751544887,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Inspired by this post:\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running\\_qwen3\\_235b\\_on\\_a\\_single\\_3060\\_12gb\\_6\\_ts/](https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/)\n\nI decided to try my luck with Qwen 235b so downloaded Unsloth's Q2XL. I've got 96GB of cheap RAM (DDR5 5600) and a 4080 Super (16GB).\n\n  \nMy runtime args:\n\nllama-cli -m Qwen3-235B-A22B-UD-Q2\\_K\\_XL-00001-of-00002.gguf -ot \".ffn\\_.\\*\\_exps.=CPU\" -c 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa\n\nSuper simple user prompt because I wasn't expecting miracles:\n\ntell me a joke\n\nResult:  \n8t/s ingestion, 5t/s generation. Actually kinda shocked. Perhaps I can use this as my backup. Haven't tried any actual work on it yet.\n\n\n\ncli output blurb:\n\nllama\\_perf\\_sampler\\_print:    sampling time =      24.81 ms /   476 runs   (    0.05 ms per token, 19183.49 tokens per second)\n\nllama\\_perf\\_context\\_print:        load time =   16979.96 ms\n\nllama\\_perf\\_context\\_print: prompt eval time =    1497.01 ms /    12 tokens (  124.75 ms per token,     8.02 tokens per second)\n\nllama\\_perf\\_context\\_print:        eval time =   85040.21 ms /   463 runs   (  183.67 ms per token,     5.44 tokens per second)\n\nllama\\_perf\\_context\\_print:       total time =  100251.11 ms /   475 tokens\n\n  \nQuestion:\n\nIt looks like I'm only using 11.1GB @ 32k. What other cheeky offloads can I do to use up that extra VRAM, if any?",
          "author_fullname": "t2_by77ogdhr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I can't believe it actually runs - Qwen 235b @ 16GB VRAM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqnwih",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 92,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 92,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751544478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by this post:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki3sze/running_qwen3_235b_on_a_single_3060_12gb_6_ts/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I decided to try my luck with Qwen 235b so downloaded Unsloth&amp;#39;s Q2XL. I&amp;#39;ve got 96GB of cheap RAM (DDR5 5600) and a 4080 Super (16GB).&lt;/p&gt;\n\n&lt;p&gt;My runtime args:&lt;/p&gt;\n\n&lt;p&gt;llama-cli -m Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -c 32768 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 --color -if -ngl 99 -fa&lt;/p&gt;\n\n&lt;p&gt;Super simple user prompt because I wasn&amp;#39;t expecting miracles:&lt;/p&gt;\n\n&lt;p&gt;tell me a joke&lt;/p&gt;\n\n&lt;p&gt;Result:&lt;br/&gt;\n8t/s ingestion, 5t/s generation. Actually kinda shocked. Perhaps I can use this as my backup. Haven&amp;#39;t tried any actual work on it yet.&lt;/p&gt;\n\n&lt;p&gt;cli output blurb:&lt;/p&gt;\n\n&lt;p&gt;llama_perf_sampler_print:    sampling time =      24.81 ms /   476 runs   (    0.05 ms per token, 19183.49 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:        load time =   16979.96 ms&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print: prompt eval time =    1497.01 ms /    12 tokens (  124.75 ms per token,     8.02 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:        eval time =   85040.21 ms /   463 runs   (  183.67 ms per token,     5.44 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:       total time =  100251.11 ms /   475 tokens&lt;/p&gt;\n\n&lt;p&gt;Question:&lt;/p&gt;\n\n&lt;p&gt;It looks like I&amp;#39;m only using 11.1GB @ 32k. What other cheeky offloads can I do to use up that extra VRAM, if any?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqnwih",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Secure_Reflection409",
          "discussion_type": null,
          "num_comments": 53,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqnwih/i_cant_believe_it_actually_runs_qwen_235b_16gb/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751544478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nI'm currently on my master's thesis and with my supervisor we figured that a real-time user-rule-based hallucination prevention framework is something interesting to work on.\n\nFor now, I built a custom RegexLogitsProcessor class that takes a Regex pattern as an input and sets the logits to infinity and therefore are not chosen, which match the Regex pattern. To illustrate this, the most simple use-case is that no digits are allowed in the output and the Regex is set to \"\\\\d\".\n\n[https://github.com/lebe1/LettucePrevent/blob/main/logits\\_processor\\_detector.py](https://github.com/lebe1/LettucePrevent/blob/main/logits_processor_detector.py)\n\nAnother idea wsa stick within the Huggingface framework and therefore the LogitsProcessor was chosen over the StoppingCriteria.\n\n[https://huggingface.co/docs/transformers.js/main/en/api/generation/logits\\_process](https://huggingface.co/docs/transformers.js/main/en/api/generation/logits_process)\n\nIn my next attempt, I'm trying to extend this class to input a custom python class so that the user can also work with the input and have a case suitable for RAG cases for example like \"no other numbers than mentioned in the input\".\n\nCurrently I like the approach with Regex due to its transparency but I would be really interested what your thoughts are on this. The only alternative I see could be an NER approach. Could you recommend something like that? What critics do you have in mind with this whole idea or what other features could you see with such a framework?",
          "author_fullname": "t2_dy1jw7m8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hallucination prevention framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqnvfr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751544391,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently on my master&amp;#39;s thesis and with my supervisor we figured that a real-time user-rule-based hallucination prevention framework is something interesting to work on.&lt;/p&gt;\n\n&lt;p&gt;For now, I built a custom RegexLogitsProcessor class that takes a Regex pattern as an input and sets the logits to infinity and therefore are not chosen, which match the Regex pattern. To illustrate this, the most simple use-case is that no digits are allowed in the output and the Regex is set to &amp;quot;\\d&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/lebe1/LettucePrevent/blob/main/logits_processor_detector.py\"&gt;https://github.com/lebe1/LettucePrevent/blob/main/logits_processor_detector.py&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Another idea wsa stick within the Huggingface framework and therefore the LogitsProcessor was chosen over the StoppingCriteria.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/docs/transformers.js/main/en/api/generation/logits_process\"&gt;https://huggingface.co/docs/transformers.js/main/en/api/generation/logits_process&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In my next attempt, I&amp;#39;m trying to extend this class to input a custom python class so that the user can also work with the input and have a case suitable for RAG cases for example like &amp;quot;no other numbers than mentioned in the input&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Currently I like the approach with Regex due to its transparency but I would be really interested what your thoughts are on this. The only alternative I see could be an NER approach. Could you recommend something like that? What critics do you have in mind with this whole idea or what other features could you see with such a framework?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?auto=webp&amp;s=97cffbab2a2683ead2d609c30a55166e8f7dc2bf",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87d8479c331c36beeb3bfedb2bdc6a12c0645526",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b88cd4f0edfe81f9d10c2c597d117eac1abb7de4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa4ed0560e5970b0d9143ecc1c771773742acdfa",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f5511c86c6822362408285424603198c70978b3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=43ad9da53b859a94f24b9a1af5236ed5e5c4a520",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=76d69410bfbd870b1f975e18c5a1a7ab707eec59",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "J6mi4u4VW-V8D0Kj94jzflHiQaSrM_BdcCxJ4Xlh4RU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqnvfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lebe1",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqnvfr/hallucination_prevention_framework/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqnvfr/hallucination_prevention_framework/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751544391,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Over the past year, we’ve learned a lot from this community while exploring model merging. Now we’re giving back with **Mergenetic**, an open-source library that makes *evolutionary* merging practical without needing big hardware.\n\nWhat it does:\n\n* Evolves high-quality LLM merges using evolutionary algorithms\n* Supports SLERP, TIES, DARE, Task Arithmetic, and more\n* Efficient: search happens in parameter space, not gradient needed\n* Modular, hackable, and built on familiar tools (`mergekit`, `pymoo`, `lm-eval-harness`)\n\nRun it via Python, CLI, or GUI — and try some wild merge experiments on your own GPU.\n\nFor details, check out our papers:\n\n* ACL 2025 Demo: [arxiv.org/abs/2505.11427](https://arxiv.org/pdf/2505.11427)\n* ICML 2025: [arxiv.org/abs/2502.10436](https://arxiv.org/pdf/2502.10436)\n\n🔗 [GitHub: tommasomncttn/mergenetic](https://github.com/tommasomncttn/mergenetic)\n\nWould love feedback or contributions — hope it’s useful to some of you!",
          "author_fullname": "t2_81hdual0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hey r/LocalLLaMA! We made evolutionary model merging feasible on consumer GPUs – meet Mergenetic 🧬",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqndyy",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751542836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Over the past year, we’ve learned a lot from this community while exploring model merging. Now we’re giving back with &lt;strong&gt;Mergenetic&lt;/strong&gt;, an open-source library that makes &lt;em&gt;evolutionary&lt;/em&gt; merging practical without needing big hardware.&lt;/p&gt;\n\n&lt;p&gt;What it does:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Evolves high-quality LLM merges using evolutionary algorithms&lt;/li&gt;\n&lt;li&gt;Supports SLERP, TIES, DARE, Task Arithmetic, and more&lt;/li&gt;\n&lt;li&gt;Efficient: search happens in parameter space, not gradient needed&lt;/li&gt;\n&lt;li&gt;Modular, hackable, and built on familiar tools (&lt;code&gt;mergekit&lt;/code&gt;, &lt;code&gt;pymoo&lt;/code&gt;, &lt;code&gt;lm-eval-harness&lt;/code&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Run it via Python, CLI, or GUI — and try some wild merge experiments on your own GPU.&lt;/p&gt;\n\n&lt;p&gt;For details, check out our papers:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;ACL 2025 Demo: &lt;a href=\"https://arxiv.org/pdf/2505.11427\"&gt;arxiv.org/abs/2505.11427&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;ICML 2025: &lt;a href=\"https://arxiv.org/pdf/2502.10436\"&gt;arxiv.org/abs/2502.10436&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;🔗 &lt;a href=\"https://github.com/tommasomncttn/mergenetic\"&gt;GitHub: tommasomncttn/mergenetic&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Would love feedback or contributions — hope it’s useful to some of you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqndyy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "leviatan0",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751542836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_aq4j0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AIDC-AI/Ovis-U1-3B: unified model integrating multimodal understanding, text-to-image generation, and image editing in a single framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqnczx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=ed3bce847dca1ffb246b2e4a76b4dfbd83826764",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751542748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/AIDC-AI/Ovis-U1-3B",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?auto=webp&amp;s=b9296215e82ba455412506ddeeca9fff813a9e05",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0dbe24b46aef7931d46e3c6ae47f1acbde8eede0",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e84b5b96ed8596b3ccbfcc88d1c992ba56a96324",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5a49894c6b3a41b86514167d98bb24020d7fb049",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b7cac54a1763a205a826aebda0fe3bb69d0bd91",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e9af4c55bd61fd6f8b0d3bbcacb73b3ddd7db0fc",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2e924ac77adc7ff845a2a7cf9100edcc0e1add8b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "HVzGeHB569N7L3pa-jqHJvQIdQGHXW4_YiWgKAjHt18"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqnczx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "nullmove",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqnczx/aidcaiovisu13b_unified_model_integrating/",
          "stickied": false,
          "url": "https://huggingface.co/AIDC-AI/Ovis-U1-3B",
          "subreddit_subscribers": 494001,
          "created_utc": 1751542748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Down a deep rabbit hole of prompt eng, fine tuning w Unsloth, but not getting any great results.\n\nMy use case: Creating social content which sounds like me, not AI slop.\n\nWhat's the best way to do this nowadays? Would appreciate any direction\n\nEdit for more context: Right now I'm generating content with a powerful model, then I'm aiming to do the 'styling' in a final call.",
          "author_fullname": "t2_165nf2mrsb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best way to get an LLM to sound like me? Prompt eng or Finetune?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqmmv2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751543339,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751540252,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Down a deep rabbit hole of prompt eng, fine tuning w Unsloth, but not getting any great results.&lt;/p&gt;\n\n&lt;p&gt;My use case: Creating social content which sounds like me, not AI slop.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best way to do this nowadays? Would appreciate any direction&lt;/p&gt;\n\n&lt;p&gt;Edit for more context: Right now I&amp;#39;m generating content with a powerful model, then I&amp;#39;m aiming to do the &amp;#39;styling&amp;#39; in a final call.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqmmv2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RelevantPractice2074",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqmmv2/best_way_to_get_an_llm_to_sound_like_me_prompt/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751540252,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "### Anyone here run llama4 with 1 million to 10 million context?\n\nJust curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.\n\nWhat are vram/ram requirements for 1m context? 10m context?",
          "author_fullname": "t2_3h2irqtz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone here run llama4 scout/Maverick with 1 million to 10 million context?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqmbh3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751540265,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751539102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h3&gt;Anyone here run llama4 with 1 million to 10 million context?&lt;/h3&gt;\n\n&lt;p&gt;Just curious if anyone has. If yes please list your software platform (i.e. vLLM, Ollama, llama.cpp, etc), your GPU count and make models.&lt;/p&gt;\n\n&lt;p&gt;What are vram/ram requirements for 1m context? 10m context?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqmbh3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "night0x63",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqmbh3/anyone_here_run_llama4_scoutmaverick_with_1/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751539102,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I wanted to share my experience and hear about yours.\n\nHardware : \n\nGPU : 3060 12GB\nCPU : i5-3060\nRAM : 32GB\n\nFront-end : Koboldcpp + open-webui\n\nUse cases : General Q&amp;A, Long context RAG, Humanities, Summarization, Translation, code. \n\nI've been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. \n\nGEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they're quite fast, and have a good ability to stick to the prompt. \n\nGemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it's variants. \n\nWhat are your experiences? Do you use other models of the same range? \n\nGood day everyone! \n\n\n",
          "author_fullname": "t2_cpgzcud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Yappp - Yet Another Poor Peasent Post",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqlsyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751537200,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I wanted to share my experience and hear about yours.&lt;/p&gt;\n\n&lt;p&gt;Hardware : &lt;/p&gt;\n\n&lt;p&gt;GPU : 3060 12GB\nCPU : i5-3060\nRAM : 32GB&lt;/p&gt;\n\n&lt;p&gt;Front-end : Koboldcpp + open-webui&lt;/p&gt;\n\n&lt;p&gt;Use cases : General Q&amp;amp;A, Long context RAG, Humanities, Summarization, Translation, code. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. &lt;/p&gt;\n\n&lt;p&gt;GEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they&amp;#39;re quite fast, and have a good ability to stick to the prompt. &lt;/p&gt;\n\n&lt;p&gt;Gemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it&amp;#39;s variants. &lt;/p&gt;\n\n&lt;p&gt;What are your experiences? Do you use other models of the same range? &lt;/p&gt;\n\n&lt;p&gt;Good day everyone! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqlsyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "needthosepylons",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751537200,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I host deepseek and other models locally, but I am limited to the speed of my machine.\n\nAnyone subscribed to cloud providers where deepseek and other models are hosted, and they'll just give you an api key to use it or something?",
          "author_fullname": "t2_kh59ca8x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which cloud compute are you using?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqlcbu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751535396,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I host deepseek and other models locally, but I am limited to the speed of my machine.&lt;/p&gt;\n\n&lt;p&gt;Anyone subscribed to cloud providers where deepseek and other models are hosted, and they&amp;#39;ll just give you an api key to use it or something?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqlcbu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rushblyatiful",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqlcbu/which_cloud_compute_are_you_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqlcbu/which_cloud_compute_are_you_using/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751535396,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, this is Emre from the Jan team.   \n  \nWe've been testing MCP servers in Jan Beta, and last week we promoted the feature to the stable with v0.6.2 build as an experimental feature, and ditched Jan Beta. So Jan is now experimenting with MCP Servers.\n\nHow to try MCP in Jan:\n\n* Settings -&gt; General -&gt; toggle \"Experimental Features\"\n* A new \"MCP Servers\" tab appears -&gt; add or enable your server\n\nQuick tip: To use MCP servers, make sure the model's Tools capability is enabled.\n\nFull doc with screenshots: [https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan](https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan)\n\nQuick note, this is still an experimental feature, please expect bugs, and flagging bugs would be super helpful for us to improve the capabilities.\n\nPlus, since then we've pushed a few hot-fixes to smooth out model loading and MCP performance.\n\nOther recent fixes &amp; tweaks:\n\n* CORS bypass for localhost providers (Ollama :11434, LM Studio :1234).\n* We fixed a bug that caused some GGUF models to get stuck while loading.\n* Lighter UI polish and clearer error messages.\n\nWith this update, Jan now supports [Jan-nano 4B ](https://huggingface.co/Menlo/Jan-nano-gguf)as well, it's available in Jan Hub. For the best experience, we suggest using the model for web searches and the 128K variant for deep-research tasks.\n\nFor the latest build, please update your Jan or [download the latest](https://jan.ai/).",
          "author_fullname": "t2_g6cmmsdd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Jan now supports MCP servers as an experimental feature",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 81,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqkknh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 70,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1844,
              "scrubber_media_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8sdnjxd6emaf1/DASHPlaylist.mpd?a=1754149686%2CYTBlNGQ5ZDQyNjAyNGM2MzdlNTZiNmU5NmY4YmIwYmY2N2YyY2QyZjY5NWNmZjlhYWFiNGNiMWY3MWI2ZjY5MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/8sdnjxd6emaf1/HLSPlaylist.m3u8?a=1754149686%2CODU0MTRkZjJmNzdkZTFlY2M4MzQ4MzliY2E4NWZhNDBjNTI5M2QwNWVlZTQ4YWFiZTZmM2UyNTMzNzM2ODkzYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 70,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=140&amp;height=81&amp;crop=140:81,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=09b4f8b875ef9edc6171e54b553d10bb25844acb",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751532281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, this is Emre from the Jan team.   &lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve been testing MCP servers in Jan Beta, and last week we promoted the feature to the stable with v0.6.2 build as an experimental feature, and ditched Jan Beta. So Jan is now experimenting with MCP Servers.&lt;/p&gt;\n\n&lt;p&gt;How to try MCP in Jan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Settings -&amp;gt; General -&amp;gt; toggle &amp;quot;Experimental Features&amp;quot;&lt;/li&gt;\n&lt;li&gt;A new &amp;quot;MCP Servers&amp;quot; tab appears -&amp;gt; add or enable your server&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Quick tip: To use MCP servers, make sure the model&amp;#39;s Tools capability is enabled.&lt;/p&gt;\n\n&lt;p&gt;Full doc with screenshots: &lt;a href=\"https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan\"&gt;https://jan.ai/docs/mcp#configure-and-use-mcps-within-jan&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Quick note, this is still an experimental feature, please expect bugs, and flagging bugs would be super helpful for us to improve the capabilities.&lt;/p&gt;\n\n&lt;p&gt;Plus, since then we&amp;#39;ve pushed a few hot-fixes to smooth out model loading and MCP performance.&lt;/p&gt;\n\n&lt;p&gt;Other recent fixes &amp;amp; tweaks:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CORS bypass for localhost providers (Ollama :11434, LM Studio :1234).&lt;/li&gt;\n&lt;li&gt;We fixed a bug that caused some GGUF models to get stuck while loading.&lt;/li&gt;\n&lt;li&gt;Lighter UI polish and clearer error messages.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;With this update, Jan now supports &lt;a href=\"https://huggingface.co/Menlo/Jan-nano-gguf\"&gt;Jan-nano 4B &lt;/a&gt;as well, it&amp;#39;s available in Jan Hub. For the best experience, we suggest using the model for web searches and the 128K variant for deep-research tasks.&lt;/p&gt;\n\n&lt;p&gt;For the latest build, please update your Jan or &lt;a href=\"https://jan.ai/\"&gt;download the latest&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8sdnjxd6emaf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?format=pjpg&amp;auto=webp&amp;s=1929c5a4b61c99193b1833755d039edb74f19a3c",
                  "width": 1844,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=99ba372d8e15e8f7353b28e194025a6b24b15ab3",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cce0560ef577d057a0d7469534309de7c6f46bd5",
                    "width": 216,
                    "height": 126
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6124b9816056b8b748c6aa3ff3e8d6bf98b21a71",
                    "width": 320,
                    "height": 187
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1a18e91c3202fd243157eeeace45acc966d35668",
                    "width": 640,
                    "height": 374
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=784793efb8baa04efbb591a36282b4cbf5249c85",
                    "width": 960,
                    "height": 562
                  },
                  {
                    "url": "https://external-preview.redd.it/azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b064a2c42cc30702930582d519e1812a979351db",
                    "width": 1080,
                    "height": 632
                  }
                ],
                "variants": {},
                "id": "azNjM2ZnZTZlbWFmMSLAmC_rv_7-7mec9KjG11hCNT_NOpmPUzvjwUvmxWxA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lqkknh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eck72",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqkknh/jan_now_supports_mcp_servers_as_an_experimental/",
          "stickied": false,
          "url": "https://v.redd.it/8sdnjxd6emaf1",
          "subreddit_subscribers": 494001,
          "created_utc": 1751532281,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1844,
              "scrubber_media_url": "https://v.redd.it/8sdnjxd6emaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8sdnjxd6emaf1/DASHPlaylist.mpd?a=1754149686%2CYTBlNGQ5ZDQyNjAyNGM2MzdlNTZiNmU5NmY4YmIwYmY2N2YyY2QyZjY5NWNmZjlhYWFiNGNiMWY3MWI2ZjY5MA%3D%3D&amp;v=1&amp;f=sd",
              "duration": 13,
              "hls_url": "https://v.redd.it/8sdnjxd6emaf1/HLSPlaylist.m3u8?a=1754149686%2CODU0MTRkZjJmNzdkZTFlY2M4MzQ4MzliY2E4NWZhNDBjNTI5M2QwNWVlZTQ4YWFiZTZmM2UyNTMzNzM2ODkzYg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am currently looking for some locally deployable model that can help me extract insights/values from graphical representations as you would find them in management or investor presentations. \n\n  \nWhile grabbing financials from tables and regular text does not pose an issue, I struggle finding a small model that I can run locally without throwing much compute at it to extract values and insights from more complex visual representations (see below). \n\n  \nI don't need to have this run extremely fast, so I can sacrifice execution speed in the name of higher accuracy, but of course the execution time should remain reasonable. \n\n  \nAre there any models specifically trained or especially good at this? I have been playing around with Gemma3n and Qwen 2.5VL 4B but both are not performing at the level I would like. \n\n  \nHere are some examples of what I am talking about:\n\nhttps://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;format=png&amp;auto=webp&amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5\n\nhttps://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;format=png&amp;auto=webp&amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64\n\n  \n",
          "author_fullname": "t2_uh7yd5wu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Small VisualLM for Data/Insight Extraction from Graphs &amp; Charts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "ds8ddvvm9maf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=33285e25461ea29199d8e6d8722f89da026989ff"
                },
                {
                  "y": 147,
                  "x": 216,
                  "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=314ea41fcc2d7c56c22706502bda4adbe9bfe4a4"
                },
                {
                  "y": 218,
                  "x": 320,
                  "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f1c332b035f961d09034c43e4ad7f50e850221bb"
                }
              ],
              "s": {
                "y": 394,
                "x": 578,
                "u": "https://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;format=png&amp;auto=webp&amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5"
              },
              "id": "ds8ddvvm9maf1"
            },
            "0gchf8zz9maf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e206f78c50d353f841847569a6fffb5d64fbb826"
                },
                {
                  "y": 114,
                  "x": 216,
                  "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=790d83d524eef70fa831df1bf225e10c65b98e12"
                },
                {
                  "y": 170,
                  "x": 320,
                  "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de4c307aa34a89a1163bc7a6dff424fe8e1557c5"
                }
              ],
              "s": {
                "y": 336,
                "x": 632,
                "u": "https://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;format=png&amp;auto=webp&amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64"
              },
              "id": "0gchf8zz9maf1"
            }
          },
          "name": "t3_1lqk18o",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/pjHRkVQ1PimSGcaJZLdG1dhuTVGrTkRLwHicOv-aLFg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751530024,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am currently looking for some locally deployable model that can help me extract insights/values from graphical representations as you would find them in management or investor presentations. &lt;/p&gt;\n\n&lt;p&gt;While grabbing financials from tables and regular text does not pose an issue, I struggle finding a small model that I can run locally without throwing much compute at it to extract values and insights from more complex visual representations (see below). &lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t need to have this run extremely fast, so I can sacrifice execution speed in the name of higher accuracy, but of course the execution time should remain reasonable. &lt;/p&gt;\n\n&lt;p&gt;Are there any models specifically trained or especially good at this? I have been playing around with Gemma3n and Qwen 2.5VL 4B but both are not performing at the level I would like. &lt;/p&gt;\n\n&lt;p&gt;Here are some examples of what I am talking about:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5\"&gt;https://preview.redd.it/ds8ddvvm9maf1.png?width=578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8dde71e0734d51b515f9b6379e374c728d40ebe5&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64\"&gt;https://preview.redd.it/0gchf8zz9maf1.png?width=632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b1bfd4646ca12b6196cf35d9c2708a05c1ecec64&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqk18o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Possible-Tomatillo80",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqk18o/small_visuallm_for_datainsight_extraction_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqk18o/small_visuallm_for_datainsight_extraction_from/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751530024,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I feel I can easily tie it in inconsistencies and knots with basic debating techniques (e.g. false binary's).\n\nDon't make me feel alone...",
          "author_fullname": "t2_wxq0v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Okay, I love arguing with me LocalLaMA and feeling like I'm winning. Am I strange?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqjccq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.28,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751527584,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751527238,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel I can easily tie it in inconsistencies and knots with basic debating techniques (e.g. false binary&amp;#39;s).&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t make me feel alone...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqjccq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wandering_cat_ninja",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqjccq/okay_i_love_arguing_with_me_locallama_and_feeling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqjccq/okay_i_love_arguing_with_me_locallama_and_feeling/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751527238,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://github.com/cactus-compute/cactus](https://github.com/cactus-compute/cactus)  \n[https://github.com/jafioti/luminal](https://github.com/jafioti/luminal) ( Rust )\n\n  \nCatus seems to start from fork of llama.cpp. (similar to Ollama)\n\nLuminal is more interesting since it rebuild everything.  \nGeoHot from Tinygrad is quite active in Luminal's Discord too.\n\n  \n",
          "author_fullname": "t2_lgebhlu22",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Sharing new inference engines I got to know recently",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqj3eq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 24,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 24,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751526278,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/cactus-compute/cactus\"&gt;https://github.com/cactus-compute/cactus&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://github.com/jafioti/luminal\"&gt;https://github.com/jafioti/luminal&lt;/a&gt; ( Rust )&lt;/p&gt;\n\n&lt;p&gt;Catus seems to start from fork of llama.cpp. (similar to Ollama)&lt;/p&gt;\n\n&lt;p&gt;Luminal is more interesting since it rebuild everything.&lt;br/&gt;\nGeoHot from Tinygrad is quite active in Luminal&amp;#39;s Discord too.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?auto=webp&amp;s=08f047a670922c7500dd06ecd9c55d5ba293b830",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6566c025c702b9994234a8b5ba87dfd1d264a9bb",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=97951e6dbbfba26407f8e020163aa947fbc8a66f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7c54299406c114e53a88ed9780140341612aacdb",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8697210b29e9d7b2b5ec2f265e89f7ff530b0b1f",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c3babc126be376ccc7efcb605a50068b2621b9d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc94cc1deffa62c7bfd0c2f2ca47204b3c395bba",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "amojX8QP995JVEP8LciiZjyYRK-sWBDU7NkFXvuu68M"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lqj3eq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AggressiveHunt2300",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqj3eq/sharing_new_inference_engines_i_got_to_know/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751526278,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "By training from scratch with only reinforcement learning (RL), DeepSWE-Preview with test time scaling (TTS) solves 59% of problems, beating all open-source agents by a large margin. We note that DeepSWE-Preview’s Pass@1 performance (42.2%, averaged over 16 runs) is one of the best for open-weights coding agents.\n\n[https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33](https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33)",
          "author_fullname": "t2_4fuhv1gu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSWE-Preview | 59.0% on SWE-Bench-Verified with test-time scaling",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqi863",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 90,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 90,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=489306336ce01e38e0f62b2b5a2cfe4806029ebe",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751522913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;By training from scratch with only reinforcement learning (RL), DeepSWE-Preview with test time scaling (TTS) solves 59% of problems, beating all open-source agents by a large margin. We note that DeepSWE-Preview’s Pass@1 performance (42.2%, averaged over 16 runs) is one of the best for open-weights coding agents.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33\"&gt;https://pretty-radio-b75.notion.site/DeepSWE-Training-a-Fully-Open-sourced-State-of-the-Art-Coding-Agent-by-Scaling-RL-22281902c1468193aabbe9a8c59bbe33&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/agentica-org/DeepSWE-Preview",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?auto=webp&amp;s=b68c82b9696315b86d709c54c3c41a99aa05dc23",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4fb09c54c0adff13ca3fb65fb5340199f3025b3",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bcd434649182258566913ef3f41a70f3c75a9510",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a26f90ae5dbd065e1013fc72bdcfe1390aed66a0",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=01af29ebca7d21fb21f6786fc5df5242a0853781",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=459b2d50c5eb6397f55a865fd129b855f501e1a8",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7be7f0dadc075fe7e858c95edc82bdabfe50ad4d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "wjfa881JWf1DDoO3xnGtkXTTrD_14geAqCNLc8luGXY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqi863",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "touhidul002",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqi863/deepswepreview_590_on_swebenchverified_with/",
          "stickied": false,
          "url": "https://huggingface.co/agentica-org/DeepSWE-Preview",
          "subreddit_subscribers": 494001,
          "created_utc": 1751522913,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As per the title, which one is better?\n\nBoth in raw performance, and in price per performance.\n\nThe 2080Ti 22GB is 350 usd while the 3080 20gb is 450 usd. Where I am, 3090s still go for 1000+ usd so that’s not a good option.\n\nEDIT 1: By the way, I plan on getting two and maybe adding more, I’ll probably be using a desktop ATX setup since server stuff is expensive probably a 3600 with a b450 unless anyone has a better idea",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "2080 TI 22GB or 3080 20GB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqi5q0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.57,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751522659,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per the title, which one is better?&lt;/p&gt;\n\n&lt;p&gt;Both in raw performance, and in price per performance.&lt;/p&gt;\n\n&lt;p&gt;The 2080Ti 22GB is 350 usd while the 3080 20gb is 450 usd. Where I am, 3090s still go for 1000+ usd so that’s not a good option.&lt;/p&gt;\n\n&lt;p&gt;EDIT 1: By the way, I plan on getting two and maybe adding more, I’ll probably be using a desktop ATX setup since server stuff is expensive probably a 3600 with a b450 unless anyone has a better idea&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqi5q0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqi5q0/2080_ti_22gb_or_3080_20gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqi5q0/2080_ti_22gb_or_3080_20gb/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751522659,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It's been a while and llama maverick and scout are still shite. I have tried nearly every provider at this point. \n\nAny updates if they're gonna launch any improvements to these models or any new reasoning models? \n\nHow are they fucking up this bad? Near unlimited money, resources, researchers. What are they doing wrong? \n\nThey weren't that far behind in the LLM race compared to Google and now they are like behind everyone at this point. \n\nAnd any updates on Microsoft? They're not gonna do their own models \"Big Ones\" and are completely reliant on OpenAI?\n\nChinese companies are releasing models left and right... I tested Ernie models and they're better than Llama 4s\n\nDeepSeek-V3-0324 seems to be the best non-reasoning open source LLM we have.\n\nAre there even any projects that have attempted to improve Llama4s via fine-tuning it or other magical techniques we have? God it's so shite, it's comprehension abilities are just embarrassing. It feels like you can find a million models that are far better than llama 4s for almost anything. The only thing they seem to have is speed on VRAM constrained setups but what's the point when then responses are useless? It's a waste of resource at this point. ",
          "author_fullname": "t2_yfi9sqrzf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any updates on Llama models from Meta?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqhers",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.72,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751519922,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s been a while and llama maverick and scout are still shite. I have tried nearly every provider at this point. &lt;/p&gt;\n\n&lt;p&gt;Any updates if they&amp;#39;re gonna launch any improvements to these models or any new reasoning models? &lt;/p&gt;\n\n&lt;p&gt;How are they fucking up this bad? Near unlimited money, resources, researchers. What are they doing wrong? &lt;/p&gt;\n\n&lt;p&gt;They weren&amp;#39;t that far behind in the LLM race compared to Google and now they are like behind everyone at this point. &lt;/p&gt;\n\n&lt;p&gt;And any updates on Microsoft? They&amp;#39;re not gonna do their own models &amp;quot;Big Ones&amp;quot; and are completely reliant on OpenAI?&lt;/p&gt;\n\n&lt;p&gt;Chinese companies are releasing models left and right... I tested Ernie models and they&amp;#39;re better than Llama 4s&lt;/p&gt;\n\n&lt;p&gt;DeepSeek-V3-0324 seems to be the best non-reasoning open source LLM we have.&lt;/p&gt;\n\n&lt;p&gt;Are there even any projects that have attempted to improve Llama4s via fine-tuning it or other magical techniques we have? God it&amp;#39;s so shite, it&amp;#39;s comprehension abilities are just embarrassing. It feels like you can find a million models that are far better than llama 4s for almost anything. The only thing they seem to have is speed on VRAM constrained setups but what&amp;#39;s the point when then responses are useless? It&amp;#39;s a waste of resource at this point. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqhers",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "True_Requirement_891",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqhers/any_updates_on_llama_models_from_meta/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqhers/any_updates_on_llama_models_from_meta/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751519922,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Dots\n\nMinimax\n\nHunyuan\n\nErnie\n\n\nI’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.\n\nSorry, just wanted to put this out here.",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "No love for these new models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqh55j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 154,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 154,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751519001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Dots&lt;/p&gt;\n\n&lt;p&gt;Minimax&lt;/p&gt;\n\n&lt;p&gt;Hunyuan&lt;/p&gt;\n\n&lt;p&gt;Ernie&lt;/p&gt;\n\n&lt;p&gt;I’m not seeing much enthusiasm in the community for these models like there was for Qwen and Deepseek.&lt;/p&gt;\n\n&lt;p&gt;Sorry, just wanted to put this out here.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lqh55j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/no_love_for_these_new_models/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751519001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For the past few months, I’ve been developing [ProseBird](https://prosebird.com)—originally a collaborative online teleprompter—as a solo technical founder, and recently decided to pivot to a script-based AI speech coaching tool.\n\nBesides technical and commercial feasibility, making this pivot really hinges on finding an awesome technical co-founder to lead development of what would be such a crucial part of the project: AI.\n\nWe wouldn’t be starting from scratch, both the original and the new vision for ProseBird share significant infrastructure, so much of the existing backend, architecture, and codebase can be leveraged for the pivot.\n\nSo if (1) you’re experienced with LLMs / ML / NLP / TTS &amp; STT / overall voice AI; and (2) the idea of working extremely hard building a product of which you own 50% excites you, shoot me a DM so we can talk.\n\nWeb or mobile dev experience is a plus.",
          "author_fullname": "t2_mql3elrw",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for a Technical Co-Founder to Lead AI Development",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqeya7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751511748,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For the past few months, I’ve been developing &lt;a href=\"https://prosebird.com\"&gt;ProseBird&lt;/a&gt;—originally a collaborative online teleprompter—as a solo technical founder, and recently decided to pivot to a script-based AI speech coaching tool.&lt;/p&gt;\n\n&lt;p&gt;Besides technical and commercial feasibility, making this pivot really hinges on finding an awesome technical co-founder to lead development of what would be such a crucial part of the project: AI.&lt;/p&gt;\n\n&lt;p&gt;We wouldn’t be starting from scratch, both the original and the new vision for ProseBird share significant infrastructure, so much of the existing backend, architecture, and codebase can be leveraged for the pivot.&lt;/p&gt;\n\n&lt;p&gt;So if (1) you’re experienced with LLMs / ML / NLP / TTS &amp;amp; STT / overall voice AI; and (2) the idea of working extremely hard building a product of which you own 50% excites you, shoot me a DM so we can talk.&lt;/p&gt;\n\n&lt;p&gt;Web or mobile dev experience is a plus.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lqeya7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Puzzleheaded-Cow7240",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqeya7/looking_for_a_technical_cofounder_to_lead_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqeya7/looking_for_a_technical_cofounder_to_lead_ai/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751511748,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "as the title says, i have 250,000 6000 word files and i want to be able to query them. they are legal documents, what model would run flawlessly on my mac air m2. thanks",
          "author_fullname": "t2_1nb285wcw0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "best local llm for 250,000 json with 6000 words each",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqeogc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751510910,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;as the title says, i have 250,000 6000 word files and i want to be able to query them. they are legal documents, what model would run flawlessly on my mac air m2. thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqeogc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Substantial-Gear1150",
          "discussion_type": null,
          "num_comments": 32,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751510910,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Paper: [https://arxiv.org/abs/2507.01949](https://arxiv.org/abs/2507.01949)\n\nProject Page: [https://kwai-keye.github.io/](https://kwai-keye.github.io/)\n\nCode: [https://github.com/Kwai-Keye/Keye](https://github.com/Kwai-Keye/Keye)\n\n&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.",
          "author_fullname": "t2_qjpsv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kwai-Keye/Keye-VL-8B-Preview · Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqebbv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": "#93b1ba",
          "ups": 28,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 28,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=2114f43f8a483d864b74ab185248e7dc7f2acd59",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 3.1"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751509766,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Paper: &lt;a href=\"https://arxiv.org/abs/2507.01949\"&gt;https://arxiv.org/abs/2507.01949&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Project Page: &lt;a href=\"https://kwai-keye.github.io/\"&gt;https://kwai-keye.github.io/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/Kwai-Keye/Keye\"&gt;https://github.com/Kwai-Keye/Keye&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in today’s digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model engineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a fourstage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode “cold-start” data mixture, which includes “thinking”, “non-thinking”, “auto-think”, “think with image”, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evaluations also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?auto=webp&amp;s=5ac1fd6f606741f5c30142be649c50021a8588ec",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d1bc04f5722002b089d9f495fa7cdaf7f3700c9e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa0520732fdbe4ef3053c95ef226e0a6ee79c4f6",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76e575e0f61ad8ceb6ddc30f00b7be46f6ec4694",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a674ea1c399ba022e42f0633ac66250ac99a0f9e",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c0eae6b8b89456d5ebcfbd6cc4c7678b03dd124f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=0ba496f256b1dae2db014d8c779db7cafd30a82b",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "BS_TDRa2LGX8FEj4q5942WEB0EiwaA6wVSbdK_ycPzI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 3.1",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqebbv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ninjasaid13",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lqebbv/kwaikeyekeyevl8bpreview_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Kwai-Keye/Keye-VL-8B-Preview",
          "subreddit_subscribers": 494001,
          "created_utc": 1751509766,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Excited to share my first open source project - PrivateScribe.ai.\n\nI’m an ER physician + developer who has been riding the LLM wave since GPT-3. Ambient dictation and transcription will fundamentally change medicine and was already working good enough in my GPT-3.5 turbo prototypes. Nowadays there are probably 20+ startups all offering this with cloud based services and subscriptions. Thinking of all of these small clinics, etc. paying subscriptions forever got me wondering if we could build a fully open source, fully local, and thus fully private AI transcription platform that could be bought once and just ran on-prem for free.\n\nI’m building with react, flask, ollama, and whisper. Everything stays on device, it’s MIT licensed, free to use, and works pretty well so far. I plan to expand the functionality to more real time feedback and general applications beyond just medicine as I’ve had some interest in the idea from lawyers and counselors too.\n\nWould love to hear any thoughts on the idea or things people would want for other use cases. ",
          "author_fullname": "t2_fyeeexf0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "PrivateScribe.ai - a fully local, MIT licensed AI transcription platform",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqdcgr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 115,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 115,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=9dbeb4c6281123fa21d509578a50cfc85df401dd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751506831,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "privatescribe.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Excited to share my first open source project - PrivateScribe.ai.&lt;/p&gt;\n\n&lt;p&gt;I’m an ER physician + developer who has been riding the LLM wave since GPT-3. Ambient dictation and transcription will fundamentally change medicine and was already working good enough in my GPT-3.5 turbo prototypes. Nowadays there are probably 20+ startups all offering this with cloud based services and subscriptions. Thinking of all of these small clinics, etc. paying subscriptions forever got me wondering if we could build a fully open source, fully local, and thus fully private AI transcription platform that could be bought once and just ran on-prem for free.&lt;/p&gt;\n\n&lt;p&gt;I’m building with react, flask, ollama, and whisper. Everything stays on device, it’s MIT licensed, free to use, and works pretty well so far. I plan to expand the functionality to more real time feedback and general applications beyond just medicine as I’ve had some interest in the idea from lawyers and counselors too.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear any thoughts on the idea or things people would want for other use cases. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "http://www.privatescribe.ai",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?auto=webp&amp;s=4514d508aeafde08a5bf2c109c4112208d74e725",
                  "width": 1024,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=661abd40d4b281a21242971a11e9c626c59267d5",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=56c86363954748ce878868f121b02e0970c4805c",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ac70862f336d47cb441412352671d559c5d70fcb",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=67733391cf800cb69df3f2bbf96c8c0dcd8a7ecb",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6b70e78f6a72fb3566587214ead3cce77b403ba",
                    "width": 960,
                    "height": 960
                  }
                ],
                "variants": {},
                "id": "xOjECTmYaItV48u6bH1fNMUAZM2OuhSKaRpzWzNnIaE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lqdcgr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SecondPathDev",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqdcgr/privatescribeai_a_fully_local_mit_licensed_ai/",
          "stickied": false,
          "url": "http://www.privatescribe.ai",
          "subreddit_subscribers": 494001,
          "created_utc": 1751506831,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to generate voiceovers for info videos that I'm creating.  \nMy own voice isn't that great and I don't have a good mic.\n\nI do, however, have an nvidia card that I've been using to generate images.  \nI've also been able to run an llm locally, so I imagine that my machine is capable of running a text-to-speech ai as well.\n\nSearching google and reddit for text-to-speech generators has left me a little overwhelmed, so I'd like to hear your suggestions.\n\nI tried to install spark-tts, but I wasn't able to install all the requirements. I think that the included scripts for installing requirements didn't cover all the dependancies.\n\n",
          "author_fullname": "t2_1l59r1ow96",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local text-to-speech generator for inux?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqcbfp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751503734,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to generate voiceovers for info videos that I&amp;#39;m creating.&lt;br/&gt;\nMy own voice isn&amp;#39;t that great and I don&amp;#39;t have a good mic.&lt;/p&gt;\n\n&lt;p&gt;I do, however, have an nvidia card that I&amp;#39;ve been using to generate images.&lt;br/&gt;\nI&amp;#39;ve also been able to run an llm locally, so I imagine that my machine is capable of running a text-to-speech ai as well.&lt;/p&gt;\n\n&lt;p&gt;Searching google and reddit for text-to-speech generators has left me a little overwhelmed, so I&amp;#39;d like to hear your suggestions.&lt;/p&gt;\n\n&lt;p&gt;I tried to install spark-tts, but I wasn&amp;#39;t able to install all the requirements. I think that the included scripts for installing requirements didn&amp;#39;t cover all the dependancies.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqcbfp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ImpossibleBritches",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqcbfp/local_texttospeech_generator_for_inux/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqcbfp/local_texttospeech_generator_for_inux/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751503734,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_14mlbg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek-TNG-R1T2-Chimera -  200% faster than R1-0528 &amp; 20% faster than R1",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqbmwa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 182,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 182,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=139f896561e4c6c7b556acb4de4b90b54d16e385",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751501716,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?auto=webp&amp;s=b23d2fb081204a6e386b9278a1d5ea0ea845dea6",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fbd48e87f3899f1a905aa35dd70eedbab3ddce51",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e6c4bcfb57ef62906d9037ff12c88c65c57179f0",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9328ca8146504507286068714fc6edba99eed0a",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97a766bd7b9c921ab450ffb020d26db72a498fc7",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=33faf4c63b53541e5a6f12123dfd625a162da5bb",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ecadf46be4d968dbb435dde8fa5e60eaf2d4b671",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "-1U2Wayag-iBBcpePS8kTRmzl3sD6ygHTwkL96tkXhU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lqbmwa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TKGaming_11",
          "discussion_type": null,
          "num_comments": 51,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqbmwa/deepseektngr1t2chimera_200_faster_than_r10528_20/",
          "stickied": false,
          "url": "https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera",
          "subreddit_subscribers": 494001,
          "created_utc": 1751501716,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am completely new to this. I was planning to install a local LLM and have it read my study material so I can quickly ask for definitions,etc\n\nI only really want to use it as an index and don't need it to solve any problems.  \nWhich LLM should I try out first?\n\n  \nMy current setup is :  \nCPU - i5-12450H  \nGPU - Nvidia RTX4050  \nRam - 16GB",
          "author_fullname": "t2_9bpzze24",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need help in deciding llm",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lqa7cd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751497670,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am completely new to this. I was planning to install a local LLM and have it read my study material so I can quickly ask for definitions,etc&lt;/p&gt;\n\n&lt;p&gt;I only really want to use it as an index and don&amp;#39;t need it to solve any problems.&lt;br/&gt;\nWhich LLM should I try out first?&lt;/p&gt;\n\n&lt;p&gt;My current setup is :&lt;br/&gt;\nCPU - i5-12450H&lt;br/&gt;\nGPU - Nvidia RTX4050&lt;br/&gt;\nRam - 16GB&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lqa7cd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Atriays",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lqa7cd/need_help_in_deciding_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lqa7cd/need_help_in_deciding_llm/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751497670,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I’m having a hard time trying to fine tune qwen2.5 VL (from mlx-community/Qwen2.5-VL-7B-Instruct-4bit) using mlx-vlm on my MacBook.\n\nI’ve spent countless hours trying different solutions but I always end up stuck with a new error…\n\nCould anyone provide a notebook that is working so that I can adapt it with my needs?\n\nThank you very much!",
          "author_fullname": "t2_a11ncwe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help needed: finetuning Qwen2.5 VL with mox-vol",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9yjy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751497014,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I’m having a hard time trying to fine tune qwen2.5 VL (from mlx-community/Qwen2.5-VL-7B-Instruct-4bit) using mlx-vlm on my MacBook.&lt;/p&gt;\n\n&lt;p&gt;I’ve spent countless hours trying different solutions but I always end up stuck with a new error…&lt;/p&gt;\n\n&lt;p&gt;Could anyone provide a notebook that is working so that I can adapt it with my needs?&lt;/p&gt;\n\n&lt;p&gt;Thank you very much!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq9yjy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Gladstone025",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9yjy/help_needed_finetuning_qwen25_vl_with_moxvol/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq9yjy/help_needed_finetuning_qwen25_vl_with_moxvol/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751497014,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm a total noob to all this. I was having really good results with Gemini 2.5 Pro, o4-mini, and Claude 4.0 Sonnet in VScode. \n\nI decided to try a few local models on my nVidia 8GB RTX 2060 Super (cpu AMD Ryzen 9 3900 12-core, RAM 64GB)\n\nI tested  the following models with Roo/ollama:\n1) gemma3n:e2b-it-q4_K_M \n2_ hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF\n3) deepseek-r1:8b                                         \n\nI have not had good experiences with these models. Probably my hardware limitations. \n\nI'd love to know more and figure out if I can get workable solutions for a reasonable hardware upgrade, or if I should just stick to remote models.\n\nIs it simply that I need to upgrade to a more powerful GPU like a 3090 to get real results from local LLM?",
          "author_fullname": "t2_39mp2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it simply about upgrading?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9lkd",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751496063,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m a total noob to all this. I was having really good results with Gemini 2.5 Pro, o4-mini, and Claude 4.0 Sonnet in VScode. &lt;/p&gt;\n\n&lt;p&gt;I decided to try a few local models on my nVidia 8GB RTX 2060 Super (cpu AMD Ryzen 9 3900 12-core, RAM 64GB)&lt;/p&gt;\n\n&lt;p&gt;I tested  the following models with Roo/ollama:\n1) gemma3n:e2b-it-q4&lt;em&gt;K_M \n2&lt;/em&gt; hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF\n3) deepseek-r1:8b                                         &lt;/p&gt;\n\n&lt;p&gt;I have not had good experiences with these models. Probably my hardware limitations. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d love to know more and figure out if I can get workable solutions for a reasonable hardware upgrade, or if I should just stick to remote models.&lt;/p&gt;\n\n&lt;p&gt;Is it simply that I need to upgrade to a more powerful GPU like a 3090 to get real results from local LLM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq9lkd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "outofbandii",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9lkd/is_it_simply_about_upgrading/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq9lkd/is_it_simply_about_upgrading/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751496063,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The standalone and portable version is available. \n\nWorks with gguf. \n\nEnjoy. ",
          "author_fullname": "t2_1sqjlhrsaf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Free AI for all.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9j0x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 460,
              "width": 854,
              "scrubber_media_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/whjbj3y1gjaf1/DASHPlaylist.mpd?a=1754149686%2CZjYzMWQyNDk1N2MzNzcxMGQ3MmMwMjk0NTM1OWM5NDQzYmFlMWU3ZjdhNTlhNTFjYzdjMzFlM2NhMDQ3Y2ZiZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 79,
              "hls_url": "https://v.redd.it/whjbj3y1gjaf1/HLSPlaylist.m3u8?a=1754149686%2CYjdjM2FjN2E3YjYxYjA5YTJkYmZmMjU3MDJmMjc2NmRiMTEwNzI0NWJmMjg5ZDhjYjg1NWJjODc3MGUyNTQ3MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=de5049a197d7fb60ce36fc1b553e2e88683fa93e",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751495874,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The standalone and portable version is available. &lt;/p&gt;\n\n&lt;p&gt;Works with gguf. &lt;/p&gt;\n\n&lt;p&gt;Enjoy. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/whjbj3y1gjaf1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?format=pjpg&amp;auto=webp&amp;s=09e705123d3692137360e587a07651fe1a0df027",
                  "width": 1280,
                  "height": 690
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=78872b43e03735532a89dbc954130e45d219734e",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=e1ae5b4c9d4c82868c833cc2e9a26944d2667d79",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=a8196d2b239eb7868036af26b4d548a389f8b661",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d1fbd6f78c0fc8eabfebf1a02c79ca31677024d9",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c9121b44c10f607f3bb60323be7a7897b4ef6b00",
                    "width": 960,
                    "height": 517
                  },
                  {
                    "url": "https://external-preview.redd.it/OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d71941c3350775826a5dbcc82218049b75753814",
                    "width": 1080,
                    "height": 582
                  }
                ],
                "variants": {},
                "id": "OWpzZmEyeTFnamFmMZw7itM17cvYQif7LzHR4CzZ0mEOzHnFEM5qkqXQQPtr"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq9j0x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "wallbergai",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9j0x/free_ai_for_all/",
          "stickied": false,
          "url": "https://v.redd.it/whjbj3y1gjaf1",
          "subreddit_subscribers": 494001,
          "created_utc": 1751495874,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 460,
              "width": 854,
              "scrubber_media_url": "https://v.redd.it/whjbj3y1gjaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/whjbj3y1gjaf1/DASHPlaylist.mpd?a=1754149686%2CZjYzMWQyNDk1N2MzNzcxMGQ3MmMwMjk0NTM1OWM5NDQzYmFlMWU3ZjdhNTlhNTFjYzdjMzFlM2NhMDQ3Y2ZiZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 79,
              "hls_url": "https://v.redd.it/whjbj3y1gjaf1/HLSPlaylist.m3u8?a=1754149686%2CYjdjM2FjN2E3YjYxYjA5YTJkYmZmMjU3MDJmMjc2NmRiMTEwNzI0NWJmMjg5ZDhjYjg1NWJjODc3MGUyNTQ3MA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "...So this idea I had, I never could quite execute on, I thought I'd share and let people pick it apart, and/or take it to the next level. Here is how I got there.\n\nI have it in my mind that Llama 3.3 70b 8 bit should be close to Llama 4 Maverick 4-Bit at \\~243 GB). Llama 3.3 70b 8 bit is \\~75 GB and Llama 3.3 70b 4 bit is \\~43 GB. That's 118 GB  which is far less than Maverick, and yet 8 bit probably outperforms Scout 4 bit... so ... all I have to do is run Llama 3.3. 70b 4bit in VRAM as the draft model and have Llama 3.3 70b 8bit primarily in RAM... supposedly the variation between 4 bit to 8 bit isn't that meaningful... supposedly. Guess we should define meaningful. I always assumed it meant it basically kept in line with the original model with just a few words being different. \n\nApparently we're only talking outcome and not word for word equivalence. Turns out in practice I could never get the thing going at a speed that surpassed Llama 3.3 70 8bit split across VRAM and RAM by any meaningful amount. Probably because the models diverge too quickly word wise to be a meaningful speculative model. \n\nOkay... still... the old adage has been that a larger quantize model should outperform a smaller unquantitized model. So I was sure I'd have a more impressive speed boost than just using Llama 3.2 3b 8 bit at \\~4 GB with speculative decoding... especially since Llama 3.3 70b supposedly had similar performance to Llama 3.1 405b.\n\nStill... I'm curious if anyone else has tried this and how successful they were. Could this idea create a better alternative locally for single users than bloated MOE models? Perhaps tweaked in some way... for example perhaps we could build a front end that instead of trying to predict the exact words via speculative decoding, it just asked the 8-bit model to bless the output of 4-bit model sentence by sentence (With a prompt that asks would you have written the last sentence return true or false... or should the last sentence be changed). Perhaps there is a fun math shortcut that would let us use quantized dense models to generate speed similar to MoEs in speed but more dense. Holy grail for me is if we find a way to condense MoEs with minimal power expenditure, but that seems unlikely (outside of quantization which still feels woefully ineffective). \n\nSo there it is. I did my part. I shared what I thought was brilliance (and clearly wasn't) and maybe someone can shine a little light on how it could go better for a future me or you.\n\n:I feel all the comments will be quoting Billy Madison, \"What you've just said is one of the most insanely idiotic things I have ever heard. At no point in your rambling, incoherent response were you even close to anything that could be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.\"",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Speculative Decoding and Quantization ... I'm probably not going anywhere near what you think...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq9eg5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751495536,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;...So this idea I had, I never could quite execute on, I thought I&amp;#39;d share and let people pick it apart, and/or take it to the next level. Here is how I got there.&lt;/p&gt;\n\n&lt;p&gt;I have it in my mind that Llama 3.3 70b 8 bit should be close to Llama 4 Maverick 4-Bit at ~243 GB). Llama 3.3 70b 8 bit is ~75 GB and Llama 3.3 70b 4 bit is ~43 GB. That&amp;#39;s 118 GB  which is far less than Maverick, and yet 8 bit probably outperforms Scout 4 bit... so ... all I have to do is run Llama 3.3. 70b 4bit in VRAM as the draft model and have Llama 3.3 70b 8bit primarily in RAM... supposedly the variation between 4 bit to 8 bit isn&amp;#39;t that meaningful... supposedly. Guess we should define meaningful. I always assumed it meant it basically kept in line with the original model with just a few words being different. &lt;/p&gt;\n\n&lt;p&gt;Apparently we&amp;#39;re only talking outcome and not word for word equivalence. Turns out in practice I could never get the thing going at a speed that surpassed Llama 3.3 70 8bit split across VRAM and RAM by any meaningful amount. Probably because the models diverge too quickly word wise to be a meaningful speculative model. &lt;/p&gt;\n\n&lt;p&gt;Okay... still... the old adage has been that a larger quantize model should outperform a smaller unquantitized model. So I was sure I&amp;#39;d have a more impressive speed boost than just using Llama 3.2 3b 8 bit at ~4 GB with speculative decoding... especially since Llama 3.3 70b supposedly had similar performance to Llama 3.1 405b.&lt;/p&gt;\n\n&lt;p&gt;Still... I&amp;#39;m curious if anyone else has tried this and how successful they were. Could this idea create a better alternative locally for single users than bloated MOE models? Perhaps tweaked in some way... for example perhaps we could build a front end that instead of trying to predict the exact words via speculative decoding, it just asked the 8-bit model to bless the output of 4-bit model sentence by sentence (With a prompt that asks would you have written the last sentence return true or false... or should the last sentence be changed). Perhaps there is a fun math shortcut that would let us use quantized dense models to generate speed similar to MoEs in speed but more dense. Holy grail for me is if we find a way to condense MoEs with minimal power expenditure, but that seems unlikely (outside of quantization which still feels woefully ineffective). &lt;/p&gt;\n\n&lt;p&gt;So there it is. I did my part. I shared what I thought was brilliance (and clearly wasn&amp;#39;t) and maybe someone can shine a little light on how it could go better for a future me or you.&lt;/p&gt;\n\n&lt;p&gt;:I feel all the comments will be quoting Billy Madison, &amp;quot;What you&amp;#39;ve just said is one of the most insanely idiotic things I have ever heard. At no point in your rambling, incoherent response were you even close to anything that could be considered a rational thought. Everyone in this room is now dumber for having listened to it. I award you no points, and may God have mercy on your soul.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq9eg5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq9eg5/speculative_decoding_and_quantization_im_probably/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq9eg5/speculative_decoding_and_quantization_im_probably/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751495536,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone been able to setup a following solution:\n\n1. Speech is transcribed via local model (whisper or other)\n2. Grammar, spelling and rephrases are executed, respecting a system prompt\n3. Output to markdown file or directly within an interface / webui\n4. Optional: Speech commands such as \"Scratch that last sentence\" (to delete the current sentence), \"Period\" (to end the sentence), \"New Paragraph\" (to add new paragraph) etc.\n\nI am trying to establish a workflow that allows me to maintain a monologue, while transcribing and improving upon the written content.\n\nThe next level of this would be a dialog with the model, to iterate over an idea or a phrase, entire paragraphs or the outline/overview, in order to improve the text or the content on the spot. ",
          "author_fullname": "t2_f0ppt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "STT dictation and conversational sparring partner?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq8z04",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751494422,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone been able to setup a following solution:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Speech is transcribed via local model (whisper or other)&lt;/li&gt;\n&lt;li&gt;Grammar, spelling and rephrases are executed, respecting a system prompt&lt;/li&gt;\n&lt;li&gt;Output to markdown file or directly within an interface / webui&lt;/li&gt;\n&lt;li&gt;Optional: Speech commands such as &amp;quot;Scratch that last sentence&amp;quot; (to delete the current sentence), &amp;quot;Period&amp;quot; (to end the sentence), &amp;quot;New Paragraph&amp;quot; (to add new paragraph) etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I am trying to establish a workflow that allows me to maintain a monologue, while transcribing and improving upon the written content.&lt;/p&gt;\n\n&lt;p&gt;The next level of this would be a dialog with the model, to iterate over an idea or a phrase, entire paragraphs or the outline/overview, in order to improve the text or the content on the spot. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq8z04",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lodott1",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq8z04/stt_dictation_and_conversational_sparring_partner/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq8z04/stt_dictation_and_conversational_sparring_partner/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751494422,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Running vLLM 9.1 with 4x A6000s in tensor parallel config with the CognitiveComputations 4-bit AWQ quant of Qwen3 235B A22.\n\nI was running 535 and did an OS update, so I went with 570. I immediately saw inference had dropped from 56 tokens/sec to 35 tokens/sec. Puzzled, I messed around for a few days, tweaked all sorts, and eventually just tried using `apt` to install the nvidia 535 drivers, reboot, and voila! Back to 56 tokens/sec.\n\nCurious if anyone has seen similar.",
          "author_fullname": "t2_qf8h7ka8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ubuntu 24.04: observing that nvidia-535 drivers run 20 tokens/sec faster than nvidia-570 drivers with no other changes in my vLLM setup",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq8gjv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 74,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 74,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751493148,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Running vLLM 9.1 with 4x A6000s in tensor parallel config with the CognitiveComputations 4-bit AWQ quant of Qwen3 235B A22.&lt;/p&gt;\n\n&lt;p&gt;I was running 535 and did an OS update, so I went with 570. I immediately saw inference had dropped from 56 tokens/sec to 35 tokens/sec. Puzzled, I messed around for a few days, tweaked all sorts, and eventually just tried using &lt;code&gt;apt&lt;/code&gt; to install the nvidia 535 drivers, reboot, and voila! Back to 56 tokens/sec.&lt;/p&gt;\n\n&lt;p&gt;Curious if anyone has seen similar.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq8gjv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "__JockY__",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq8gjv/ubuntu_2404_observing_that_nvidia535_drivers_run/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751493148,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i’ve been sharing polls and asking questions just to figure out what people actually need.\n\ni’ve consulted for ai infra companies and startups. i also built and launched my own ai apps using those infras. but they failed me. local tools were painful. hosted ones were worse. everything felt disconnected and fragile.\n\nso at the start of 2025 i began building my own thing. opinionated. integrated. no half-solutions.\n\nlately i’ve seen more and more people run into the same problems we’ve been solving with inference.sh. if you’ve been on the waitlist for a while thank you. it’s almost time.\n\nhere’s a quick video from my cofounder showing how linking your own gpu works. inference.sh is free and uses open source apps we’ve built. the full project isn’t open sourced yet for security reasons but we share as much as we can and we’re committed to contributing back.\n\na few things it already solves:\n\n– full apps instead of piles of low level nodes. some people want control but if every new model needs custom wiring just to boot it stops being control and turns into unpaid labor.\n\n– llms and multimedia tools in one place. no tab switching no broken flow. and it’s not limited to ai. you can extend it with any code.\n\n– connect any device. local or cloud. run apps from anywhere. if your local box isn’t enough shift to the cloud without losing workflows or state.\n\n– no more cuda or python dependency hell. just click run. amd and intel support coming.\n\n– have multiple gpus? we can use them separately or together.\n\n– have a workflow you want to reuse or expose? we’ve got an api. mcp is coming so agents can run each other’s workflows\n\nthis project is close to my heart. i’ll keep adding new models and weird ideas on day zero. contributions always welcome. apps are here: https://github.com/inference-sh/grid\n\nwaitlist’s open. let me know what else you want to see before the gates open.\n\nthanks for listening to my token stream.",
          "author_fullname": "t2_bquk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "the result of all the polls i’ve been running here",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq7wra",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "inference.sh - the easiest way to run any open weight ai model",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
              "author_name": "inference",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/ViadeTYqQDg/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@inference-sh"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lq7wra",
            "height": 200
          },
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=bbd3edd06755ef245263157fe75866d550d1c4d1",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751491747,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i’ve been sharing polls and asking questions just to figure out what people actually need.&lt;/p&gt;\n\n&lt;p&gt;i’ve consulted for ai infra companies and startups. i also built and launched my own ai apps using those infras. but they failed me. local tools were painful. hosted ones were worse. everything felt disconnected and fragile.&lt;/p&gt;\n\n&lt;p&gt;so at the start of 2025 i began building my own thing. opinionated. integrated. no half-solutions.&lt;/p&gt;\n\n&lt;p&gt;lately i’ve seen more and more people run into the same problems we’ve been solving with inference.sh. if you’ve been on the waitlist for a while thank you. it’s almost time.&lt;/p&gt;\n\n&lt;p&gt;here’s a quick video from my cofounder showing how linking your own gpu works. inference.sh is free and uses open source apps we’ve built. the full project isn’t open sourced yet for security reasons but we share as much as we can and we’re committed to contributing back.&lt;/p&gt;\n\n&lt;p&gt;a few things it already solves:&lt;/p&gt;\n\n&lt;p&gt;– full apps instead of piles of low level nodes. some people want control but if every new model needs custom wiring just to boot it stops being control and turns into unpaid labor.&lt;/p&gt;\n\n&lt;p&gt;– llms and multimedia tools in one place. no tab switching no broken flow. and it’s not limited to ai. you can extend it with any code.&lt;/p&gt;\n\n&lt;p&gt;– connect any device. local or cloud. run apps from anywhere. if your local box isn’t enough shift to the cloud without losing workflows or state.&lt;/p&gt;\n\n&lt;p&gt;– no more cuda or python dependency hell. just click run. amd and intel support coming.&lt;/p&gt;\n\n&lt;p&gt;– have multiple gpus? we can use them separately or together.&lt;/p&gt;\n\n&lt;p&gt;– have a workflow you want to reuse or expose? we’ve got an api. mcp is coming so agents can run each other’s workflows&lt;/p&gt;\n\n&lt;p&gt;this project is close to my heart. i’ll keep adding new models and weird ideas on day zero. contributions always welcome. apps are here: &lt;a href=\"https://github.com/inference-sh/grid\"&gt;https://github.com/inference-sh/grid&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;waitlist’s open. let me know what else you want to see before the gates open.&lt;/p&gt;\n\n&lt;p&gt;thanks for listening to my token stream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/ViadeTYqQDg?si=dfAXbK8fnZPBEuDV",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?auto=webp&amp;s=2ce6d7b32a9a0e5891cf8774650990f190b15bf3",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1b1ab2069c070abc6be872f0e9d98b671ce6060",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=072a27f99ac4eb254c06dcbbbcd6834fff1f4d44",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b17f176414965dce8ace0b6e9bf8f6ee42b12c86",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "5ZUHQMiXiTsPomcIjK_gRVQFK9kOoiVjLoBo2ywsDuU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq7wra",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "okaris",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq7wra/the_result_of_all_the_polls_ive_been_running_here/",
          "stickied": false,
          "url": "https://youtu.be/ViadeTYqQDg?si=dfAXbK8fnZPBEuDV",
          "subreddit_subscribers": 494001,
          "created_utc": 1751491747,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "inference.sh - the easiest way to run any open weight ai model",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/ViadeTYqQDg?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"inference.sh - the easiest way to run any open weight ai model\"&gt;&lt;/iframe&gt;",
              "author_name": "inference",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/ViadeTYqQDg/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@inference-sh"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3f9vjjno",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I used Qwen 3 to write a lil' agent for itself, capable of tool writing and use",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq7vjc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "7dba5c08-72f1-11ee-9b6f-ca195bc297d4",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/gh20o4e63jaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 480,
              "width": 604,
              "scrubber_media_url": "https://v.redd.it/gh20o4e63jaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/gh20o4e63jaf1/DASHPlaylist.mpd?a=1754149686%2CNzhkOWNmZmUwMDJlOTc0YjYyMGE0YWNmZjc5YWI4NjM2ZWFiYjA1NWZhZGI0YzhmMzk1ZGE4NDJiNGZmMjJmZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 75,
              "hls_url": "https://v.redd.it/gh20o4e63jaf1/HLSPlaylist.m3u8?a=1754149686%2CZGEyMTYwYmYwMjMzOTliMWRkMDdkNjBjNWY0MzBhZjNjNDk2ZWM1OWI5NDNhMDExNWRjNzNhZTE4NWM4MmI1OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=140&amp;height=111&amp;crop=140:111,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=3d468784bdbc1fc2ba09ce867a8fe8db5fc6d075",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 70B"
            }
          ],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751491655,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/gh20o4e63jaf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?format=pjpg&amp;auto=webp&amp;s=b946aac0d79a0e0674c9222b196324e15947219a",
                  "width": 798,
                  "height": 634
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=4c0ae13ad14d92cb71484678ad2b7a90c963464c",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=9f841eb6234aa0b5c7070d29c2f34c461d02ff98",
                    "width": 216,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5c9db394b29af0d3bb3852dfac43df137b25dba3",
                    "width": 320,
                    "height": 254
                  },
                  {
                    "url": "https://external-preview.redd.it/dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=8eb1d31e434aa35487f72e725f81339895500ba0",
                    "width": 640,
                    "height": 508
                  }
                ],
                "variants": {},
                "id": "dDc3dTk0ZTYzamFmMQsTB0hZmTp62l46rZf6LudFdHKCIyfga0Grf1zIAq2p"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 70B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1lq7vjc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PraxisOG",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lq7vjc/i_used_qwen_3_to_write_a_lil_agent_for_itself/",
          "stickied": false,
          "url": "https://v.redd.it/gh20o4e63jaf1",
          "subreddit_subscribers": 494001,
          "created_utc": 1751491655,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 1200,
              "fallback_url": "https://v.redd.it/gh20o4e63jaf1/DASH_480.mp4?source=fallback",
              "has_audio": true,
              "height": 480,
              "width": 604,
              "scrubber_media_url": "https://v.redd.it/gh20o4e63jaf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/gh20o4e63jaf1/DASHPlaylist.mpd?a=1754149686%2CNzhkOWNmZmUwMDJlOTc0YjYyMGE0YWNmZjc5YWI4NjM2ZWFiYjA1NWZhZGI0YzhmMzk1ZGE4NDJiNGZmMjJmZQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 75,
              "hls_url": "https://v.redd.it/gh20o4e63jaf1/HLSPlaylist.m3u8?a=1754149686%2CZGEyMTYwYmYwMjMzOTliMWRkMDdkNjBjNWY0MzBhZjNjNDk2ZWM1OWI5NDNhMDExNWRjNzNhZTE4NWM4MmI1OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yay! Been waiting for this one for a while, guessing I'm not the only one?\n[https://github.com/vllm-project/vllm/pull/17280](https://github.com/vllm-project/vllm/pull/17280)\n\nOn 70B I'm maxing out around 1400T/s on the Pro 6000 with 100 threads.\n\n\n\n\n\nQuick install instructions if you want to try it:\n\n\n\nmkdir vllm-src  \ncd vllm-src  \npython3 -m venv myenv  \nsource myenv/bin/activate  \npip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu128](https://download.pytorch.org/whl/cu128)  \ngit clone [https://github.com/huggingface/transformers.git](https://github.com/huggingface/transformers.git)  \ngit clone [https://github.com/vllm-project/vllm.git](https://github.com/vllm-project/vllm.git)  \ncd transformers  \npip install -e .  \ncd ../vllm  \npython use\\_existing\\_torch.py  \npip install -r requirements/build.txt  \npip install -r requirements/cuda.txt  \npip install -e . --no-build-isolation  \nvllm serve RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic  \nvllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic --max-model-len 8000  ",
          "author_fullname": "t2_9hl4ymvj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FP8 fixed on VLLM for RTX Pro 6000 (and RTX 5000 desktop cards)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq79xx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751493785,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751490166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yay! Been waiting for this one for a while, guessing I&amp;#39;m not the only one?\n&lt;a href=\"https://github.com/vllm-project/vllm/pull/17280\"&gt;https://github.com/vllm-project/vllm/pull/17280&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On 70B I&amp;#39;m maxing out around 1400T/s on the Pro 6000 with 100 threads.&lt;/p&gt;\n\n&lt;p&gt;Quick install instructions if you want to try it:&lt;/p&gt;\n\n&lt;p&gt;mkdir vllm-src&lt;br/&gt;\ncd vllm-src&lt;br/&gt;\npython3 -m venv myenv&lt;br/&gt;\nsource myenv/bin/activate&lt;br/&gt;\npip install torch torchvision torchaudio --index-url &lt;a href=\"https://download.pytorch.org/whl/cu128\"&gt;https://download.pytorch.org/whl/cu128&lt;/a&gt;&lt;br/&gt;\ngit clone &lt;a href=\"https://github.com/huggingface/transformers.git\"&gt;https://github.com/huggingface/transformers.git&lt;/a&gt;&lt;br/&gt;\ngit clone &lt;a href=\"https://github.com/vllm-project/vllm.git\"&gt;https://github.com/vllm-project/vllm.git&lt;/a&gt;&lt;br/&gt;\ncd transformers&lt;br/&gt;\npip install -e .&lt;br/&gt;\ncd ../vllm&lt;br/&gt;\npython use_existing_torch.py&lt;br/&gt;\npip install -r requirements/build.txt&lt;br/&gt;\npip install -r requirements/cuda.txt&lt;br/&gt;\npip install -e . --no-build-isolation&lt;br/&gt;\nvllm serve RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8-dynamic&lt;br/&gt;\nvllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic --max-model-len 8000  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?auto=webp&amp;s=1fd49f52128f28276a9c3c68af9e44fdad153b60",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ca1383baae7b6b423ab28be5aa67c437fc35d82",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=956653e6b1a4f46fc6ca33cac434e66b784cead1",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=492af444cd23147c8e4b5d077cb10778a32e9013",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bd275d8b8a4e92c894a1ce9e9acaac8850dbcf46",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4b6c033a53916e713ab613f45a38513898643074",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=360efab2869f72fe40846702b38102749bc6fff6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "R0YaLypW8v8YI57W9FPZsNuYoTjtBqD3Vh8AMMEsQF0"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq79xx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Conscious_Cut_6144",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq79xx/fp8_fixed_on_vllm_for_rtx_pro_6000_and_rtx_5000/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751490166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "LitheCode, it is a bit like if Pocketpal AI would allow you to edit your repo and update it in less than 6 clicks.\n\nWould love to get some feeback on my app or answer any questions you may have. It isn't perfect, but poured in all of my free time for a year. It isn't strictly local models only as our small models are still a bit limited, but with models like R1 Qwen3 8b I think we will be seeing a golden age in smaller models.\n\nhttps://play.google.com/store/apps/details?id=com.litheapp.app",
          "author_fullname": "t2_9dhkfhif",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "LitheCode, updating your GitHub repo using Local LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "dbbr8ds9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=fec5b516dcbed400f4e86a2661956834f9c0fb6a"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df53339a418f74f208f12daa99611f8648c7f747"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f45ac41a7c58aec6794ddb22bdeac4c2ac48cd9"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=208dba327bbd5c5f0c8289eab79fa512747cadb1"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=549e75eb09916d700feddf79e913a4c6f603646c"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f769e3aca806bc881e2e9b35b7de5db7637dd052"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/dbbr8ds9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=b67d4ed5a439784079e4e9d1e7a7d9d17308ed77"
              },
              "id": "dbbr8ds9uiaf1"
            },
            "jwk53lw9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8a3706c7121ab8a4c6d3a8eeb2036bcc956e63"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ef3b24bfe3e38f94608acd51e45e45aaf22e5947"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4894cbaefe7ef51cd4d3f0c9b0ce664e270b480d"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=49530a145fa7d5e3fe8736fe64da69d34dcceae4"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=80196ad925679fc3330261b68a06303e78910a3a"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=915e5fd21ba2ee368bb34639e89dbb7f7acde836"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/jwk53lw9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=7100cb77a347c9dac3bc86f4008697fc1ab219b0"
              },
              "id": "jwk53lw9uiaf1"
            },
            "x5v2u8m9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 135,
                  "x": 108,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a69ecbe39ddb00539be7d5845bff05ad8a8b3779"
                },
                {
                  "y": 270,
                  "x": 216,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3a3a0043c0c9ddd578e701fd52df05ef5a7395fc"
                },
                {
                  "y": 400,
                  "x": 320,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=954cb8f95bf6a351debb29b12de2fcf44f0d1c4a"
                },
                {
                  "y": 800,
                  "x": 640,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=02a2baa1d09871230393b9b9573e52e714938e8f"
                },
                {
                  "y": 1200,
                  "x": 960,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0a16f3d3991a5e15b42fcb67bb30df866f5068da"
                },
                {
                  "y": 1350,
                  "x": 1080,
                  "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3e3db11a4e53d5f9eaf1aef2b3aedb0d62e955f9"
                }
              ],
              "s": {
                "y": 1800,
                "x": 1440,
                "u": "https://preview.redd.it/x5v2u8m9uiaf1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=b46b8e7dd68c5c3556df6c28a3d1fa141f735ab9"
              },
              "id": "x5v2u8m9uiaf1"
            },
            "7hd5pmu9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6b338cf610e73e2fdc7c00ebae716dc6545f194a"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b42ffd02a1e96d581b9225bd1f00eeb6ff03cbbc"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=53ae7fcae6790c14e5797327e733e9ef5cc2e153"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0975a701f492314c87b02aa5fecacfb705e18622"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1e930c1ae0917a175794c753c473cc9577dfa93d"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eaf19e3e823b7e1f1b55dfec6dafed20800c8c48"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/7hd5pmu9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=c3bf3ce48dd265c87d87e9675eeee3cab650c2e7"
              },
              "id": "7hd5pmu9uiaf1"
            },
            "ztuf0gy9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=84c5d0eb9a1b3fa3a68f6c4a4d5b72d712e02e6f"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=99a5f2bfe09f135171156e265e53cd9239211599"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=acbc031ba547bd782ed2fb45a59f51c246b76c9f"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=af69ceeced0feeba186fa4bbc974339e634d76ff"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1b23db6193c45c76c13f9078dccac126d841db9"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b8cbf62ee3f1a9508b88374623139e93f482a350"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/ztuf0gy9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=f0dc820d1e3b5efccd136cb1aef8fd7f75a3552a"
              },
              "id": "ztuf0gy9uiaf1"
            },
            "3w8n8ji9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 135,
                  "x": 108,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=845e407d9d4451ffaa787188cb099b20b7e4b90a"
                },
                {
                  "y": 270,
                  "x": 216,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f9bd967488fee0a60753980a6358c57bf5fd7725"
                },
                {
                  "y": 400,
                  "x": 320,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=44e4e440e8c5d33a81ad8d2307e812d3553a1295"
                },
                {
                  "y": 800,
                  "x": 640,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f98211afc1b230595bc126195d00e4c0bb327f4"
                },
                {
                  "y": 1200,
                  "x": 960,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4dde751c661e829a1a246b37bbd77f1e870e61a8"
                },
                {
                  "y": 1350,
                  "x": 1080,
                  "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=54b70b488722eefd72dfcbf9ea4cb222027db9fa"
                }
              ],
              "s": {
                "y": 1800,
                "x": 1440,
                "u": "https://preview.redd.it/3w8n8ji9uiaf1.png?width=1440&amp;format=png&amp;auto=webp&amp;s=2d8c6d4fbd4e96414efe94ff7efd6847ba73820d"
              },
              "id": "3w8n8ji9uiaf1"
            },
            "51ey22p9uiaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=04060b0ab6290ef915d9dec355608ba19a2b079f"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=9be54fb01276a75316402c72f0830865302c0bc0"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2f5c0eb129d964ef0be5bcd6aea9109184c372eb"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe22dcff5fa1b6e2f825da7a27e9a6ef81aef1f0"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5b2a182a66e0f3dae8bc71df0f521ab3e5884a68"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aeb44e0c5b9b2dfe1d6f2a2306f52edd7f7c5b25"
                }
              ],
              "s": {
                "y": 3088,
                "x": 1440,
                "u": "https://preview.redd.it/51ey22p9uiaf1.jpg?width=1440&amp;format=pjpg&amp;auto=webp&amp;s=fb836c1ea2279c8c4882ffa0dc5371a079cfb1fd"
              },
              "id": "51ey22p9uiaf1"
            }
          },
          "name": "t3_1lq6jx8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "ups": 2,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "3w8n8ji9uiaf1",
                "id": 697440027
              },
              {
                "caption": "",
                "media_id": "x5v2u8m9uiaf1",
                "id": 697440028
              },
              {
                "caption": "",
                "media_id": "51ey22p9uiaf1",
                "id": 697440029
              },
              {
                "caption": "",
                "media_id": "dbbr8ds9uiaf1",
                "id": 697440030
              },
              {
                "caption": "",
                "media_id": "7hd5pmu9uiaf1",
                "id": 697440031
              },
              {
                "caption": "",
                "media_id": "jwk53lw9uiaf1",
                "id": 697440032
              },
              {
                "caption": "",
                "media_id": "ztuf0gy9uiaf1",
                "id": 697440033
              }
            ]
          },
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/EOhtec3DiIwiczyv4YDdl3Y5WvPfwi4s15SsR6jpV2U.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751488407,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;LitheCode, it is a bit like if Pocketpal AI would allow you to edit your repo and update it in less than 6 clicks.&lt;/p&gt;\n\n&lt;p&gt;Would love to get some feeback on my app or answer any questions you may have. It isn&amp;#39;t perfect, but poured in all of my free time for a year. It isn&amp;#39;t strictly local models only as our small models are still a bit limited, but with models like R1 Qwen3 8b I think we will be seeing a golden age in smaller models.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://play.google.com/store/apps/details?id=com.litheapp.app\"&gt;https://play.google.com/store/apps/details?id=com.litheapp.app&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lq6jx8",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq6jx8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AspecialistI",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq6jx8/lithecode_updating_your_github_repo_using_local/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lq6jx8",
          "subreddit_subscribers": 494001,
          "created_utc": 1751488407,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Article from hacker news: https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1\n\n\nCybersecurity researchers have discovered a critical security vulnerability in artificial intelligence (AI) company Anthropic's Model Context Protocol (MCP) Inspector project that could result in remote code execution (RCE) and allow an attacker to gain complete access to the hosts.\n\nThe vulnerability, tracked as CVE-2025-49596, carries a CVSS score of 9.4 out of a maximum of 10.0.\n\n\"This is one of the first critical RCEs in Anthropic's MCP ecosystem, exposing a new class of browser-based attacks against AI developer tools,\" Oligo Security's Avi Lumelsky said in a report published last week.\n\n\"With code execution on a developer's machine, attackers can steal data, install backdoors, and move laterally across networks - highlighting serious risks for AI teams, open-source projects, and enterprise adopters relying on MCP.\"\n\nMCP, introduced by Anthropic in November 2024, is an open protocol that standardizes the way large language model (LLM) applications integrate and share data with external data sources and tools.\n\nThe MCP Inspector is a developer tool for testing and debugging MCP servers, which expose specific capabilities through the protocol and allow an AI system to access and interact with information beyond its training data.\n\nIt contains two components, a client that provides an interactive interface for testing and debugging, and a proxy server that bridges the web UI to different MCP servers.\n\nThat said, a key security consideration to keep in mind is that the server should not be exposed to any untrusted network as it has permission to spawn local processes and can connect to any specified MCP server.\n\nThis aspect, coupled with the fact that the default settings developers use to spin up a local version of the tool come with \"significant\" security risks, such as missing authentication and encryption, opens up a new attack pathway, per Oligo.\n\n\"This misconfiguration creates a significant attack surface, as anyone with access to the local network or public internet can potentially interact with and exploit these servers,\" Lumelsky said.\n\nThe attack plays out by chaining a known security flaw affecting modern web browsers, dubbed 0.0.0.0 Day, with a cross-site request forgery (CSRF) vulnerability in Inspector (CVE-2025-49596) to run arbitrary code on the host simply upon visiting a malicious website.\n\n\n\"Versions of MCP Inspector below 0.14.1 are vulnerable to remote code execution due to lack of authentication between the Inspector client and proxy, allowing unauthenticated requests to launch MCP commands over stdio,\" the developers of MCP Inspector said in an advisory for CVE-2025-49596.\n\n0.0.0.0 Day is a 19-year-old vulnerability in modern web browsers that could enable malicious websites to breach local networks. It takes advantage of the browsers' inability to securely handle the IP address 0.0.0.0, leading to code execution.\n\n\"Attackers can exploit this flaw by crafting a malicious website that sends requests to localhost services running on an MCP server, thereby gaining the ability to execute arbitrary commands on a developer's machine,\" Lumelsky explained.\n\n\"The fact that the default configurations expose MCP servers to these kinds of attacks means that many developers may be inadvertently opening a backdoor to their machine.\"\n\nSpecifically, the proof-of-concept (PoC) makes use of the Server-Sent Events (SSE) endpoint to dispatch a malicious request from an attacker-controlled website to achieve RCE on the machine running the tool even if it's listening on localhost (127.0.0.1).\n\nThis works because the IP address 0.0.0.0 tells the operating system to listen on all IP addresses assigned to the machine, including the local loopback interface (i.e., localhost).\n\nIn a hypothetical attack scenario, an attacker could set up a fake web page and trick a developer into visiting it, at which point, the malicious JavaScript embedded in the page would send a request to 0.0.0.0:6277 (the default port on which the proxy runs), instructing the MCP Inspector proxy server to execute arbitrary commands.\n\nThe attack can also leverage DNS rebinding techniques to create a forged DNS record that points to 0.0.0.0:6277 or 127.0.0.1:6277 in order to bypass security controls and gain RCE privileges.\n\nFollowing responsible disclosure in April 2025, the vulnerability was addressed by the project maintainers on June 13 with the release of version 0.14.1. The fixes add a session token to the proxy server and incorporate origin validation to completely plug the attack vector.\n\n\"Localhost services may appear safe but are often exposed to the public internet due to network routing capabilities in browsers and MCP clients,\" Oligo said.\n\n\"The mitigation adds Authorization which was missing in the default prior to the fix, as well as verifying the Host and Origin headers in HTTP, making sure the client is really visiting from a known, trusted domain. Now, by default, the server blocks DNS rebinding and CSRF attacks.\"\n\nThe discovery of CVE-2025-49596 comes days after Trend Micro detailed an unpatched SQL injection bug in Anthropic's SQLite MCP server that could be exploited to seed malicious prompts, exfiltrate data, and take control of agent workflows.\n\n\"AI agents often trust internal data whether from databases, log entry, or cached records, agents often treat it as safe,\" researcher Sean Park said. \"An attacker can exploit this trust by embedding a prompt at that point and can later have the agent call powerful tools (email, database, cloud APIs) to steal data or move laterally, all while sidestepping earlier security checks.\"\n\nAlthough the open-source project has been billed as a reference implementation and not intended for production use, it has been forked over 5,000 times. The GitHub repository was archived on May 29, 2025, meaning no patches have been planned to address the shortcoming.\n\n\"The takeaway is clear. If we allow yesterday's web-app mistakes to slip into today's agent infrastructure, we gift attackers an effortless path from SQL injection to full agent compromise,\" Park said.\n\nThe findings also follow a report from Backslash Security that found hundreds of MCP servers to be susceptible to two major misconfigurations: Allowing arbitrary command execution on the host machine due to unchecked input handling and excessive permissions, and making them accessible to any party on the same local network owing to them being explicitly bound to 0.0.0.0, a vulnerability dubbed NeighborJack.\n\n\"Imagine you're coding in a shared coworking space or café. Your MCP server is silently running on your machine,\" Backslash Security said. \"The person sitting near you, sipping their latte, can now access your MCP server, impersonate tools, and potentially run operations on your behalf. It's like leaving your laptop open – and unlocked for everyone in the room.\"\n\nBecause MCPs, by design, are built to access external data sources, they can serve as covert pathways for prompt injection and context poisoning, thereby influencing the outcome of an LLM when parsing data from an attacker-controlled site that contains hidden instructions.\n\n\"One way to secure an MCP server might be to carefully process any text scraped from a website or database to avoid context poisoning,\" researcher Micah Gold said. \"However, this approach bloats tools – by requiring each individual tool to reimplement the same security feature – and leaves the user dependent on the security protocol of the individual MCP tool.\"\n\nA better approach, Backslash Security noted, is to configure AI rules with MCP clients to protect against vulnerable servers. These rules refer to pre-defined prompts or instructions that are assigned to an AI agent to guide its behavior and ensure it does not break security protocols.\n\n\"By conditioning AI agents to be skeptical and aware of the threat posed by context poisoning via AI rules, MCP clients can be secured against MCP servers,\" Gold said.",
          "author_fullname": "t2_apjrh9ky",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Critical Vulnerability in Anthropic's MCP Exposes Developer Machines to Remote Exploits",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq5wmh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751486835,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Article from hacker news: &lt;a href=\"https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1\"&gt;https://thehackernews.com/2025/07/critical-vulnerability-in-anthropics.html?m=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Cybersecurity researchers have discovered a critical security vulnerability in artificial intelligence (AI) company Anthropic&amp;#39;s Model Context Protocol (MCP) Inspector project that could result in remote code execution (RCE) and allow an attacker to gain complete access to the hosts.&lt;/p&gt;\n\n&lt;p&gt;The vulnerability, tracked as CVE-2025-49596, carries a CVSS score of 9.4 out of a maximum of 10.0.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;This is one of the first critical RCEs in Anthropic&amp;#39;s MCP ecosystem, exposing a new class of browser-based attacks against AI developer tools,&amp;quot; Oligo Security&amp;#39;s Avi Lumelsky said in a report published last week.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;With code execution on a developer&amp;#39;s machine, attackers can steal data, install backdoors, and move laterally across networks - highlighting serious risks for AI teams, open-source projects, and enterprise adopters relying on MCP.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;MCP, introduced by Anthropic in November 2024, is an open protocol that standardizes the way large language model (LLM) applications integrate and share data with external data sources and tools.&lt;/p&gt;\n\n&lt;p&gt;The MCP Inspector is a developer tool for testing and debugging MCP servers, which expose specific capabilities through the protocol and allow an AI system to access and interact with information beyond its training data.&lt;/p&gt;\n\n&lt;p&gt;It contains two components, a client that provides an interactive interface for testing and debugging, and a proxy server that bridges the web UI to different MCP servers.&lt;/p&gt;\n\n&lt;p&gt;That said, a key security consideration to keep in mind is that the server should not be exposed to any untrusted network as it has permission to spawn local processes and can connect to any specified MCP server.&lt;/p&gt;\n\n&lt;p&gt;This aspect, coupled with the fact that the default settings developers use to spin up a local version of the tool come with &amp;quot;significant&amp;quot; security risks, such as missing authentication and encryption, opens up a new attack pathway, per Oligo.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;This misconfiguration creates a significant attack surface, as anyone with access to the local network or public internet can potentially interact with and exploit these servers,&amp;quot; Lumelsky said.&lt;/p&gt;\n\n&lt;p&gt;The attack plays out by chaining a known security flaw affecting modern web browsers, dubbed 0.0.0.0 Day, with a cross-site request forgery (CSRF) vulnerability in Inspector (CVE-2025-49596) to run arbitrary code on the host simply upon visiting a malicious website.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Versions of MCP Inspector below 0.14.1 are vulnerable to remote code execution due to lack of authentication between the Inspector client and proxy, allowing unauthenticated requests to launch MCP commands over stdio,&amp;quot; the developers of MCP Inspector said in an advisory for CVE-2025-49596.&lt;/p&gt;\n\n&lt;p&gt;0.0.0.0 Day is a 19-year-old vulnerability in modern web browsers that could enable malicious websites to breach local networks. It takes advantage of the browsers&amp;#39; inability to securely handle the IP address 0.0.0.0, leading to code execution.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Attackers can exploit this flaw by crafting a malicious website that sends requests to localhost services running on an MCP server, thereby gaining the ability to execute arbitrary commands on a developer&amp;#39;s machine,&amp;quot; Lumelsky explained.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The fact that the default configurations expose MCP servers to these kinds of attacks means that many developers may be inadvertently opening a backdoor to their machine.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Specifically, the proof-of-concept (PoC) makes use of the Server-Sent Events (SSE) endpoint to dispatch a malicious request from an attacker-controlled website to achieve RCE on the machine running the tool even if it&amp;#39;s listening on localhost (127.0.0.1).&lt;/p&gt;\n\n&lt;p&gt;This works because the IP address 0.0.0.0 tells the operating system to listen on all IP addresses assigned to the machine, including the local loopback interface (i.e., localhost).&lt;/p&gt;\n\n&lt;p&gt;In a hypothetical attack scenario, an attacker could set up a fake web page and trick a developer into visiting it, at which point, the malicious JavaScript embedded in the page would send a request to 0.0.0.0:6277 (the default port on which the proxy runs), instructing the MCP Inspector proxy server to execute arbitrary commands.&lt;/p&gt;\n\n&lt;p&gt;The attack can also leverage DNS rebinding techniques to create a forged DNS record that points to 0.0.0.0:6277 or 127.0.0.1:6277 in order to bypass security controls and gain RCE privileges.&lt;/p&gt;\n\n&lt;p&gt;Following responsible disclosure in April 2025, the vulnerability was addressed by the project maintainers on June 13 with the release of version 0.14.1. The fixes add a session token to the proxy server and incorporate origin validation to completely plug the attack vector.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Localhost services may appear safe but are often exposed to the public internet due to network routing capabilities in browsers and MCP clients,&amp;quot; Oligo said.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The mitigation adds Authorization which was missing in the default prior to the fix, as well as verifying the Host and Origin headers in HTTP, making sure the client is really visiting from a known, trusted domain. Now, by default, the server blocks DNS rebinding and CSRF attacks.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;The discovery of CVE-2025-49596 comes days after Trend Micro detailed an unpatched SQL injection bug in Anthropic&amp;#39;s SQLite MCP server that could be exploited to seed malicious prompts, exfiltrate data, and take control of agent workflows.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;AI agents often trust internal data whether from databases, log entry, or cached records, agents often treat it as safe,&amp;quot; researcher Sean Park said. &amp;quot;An attacker can exploit this trust by embedding a prompt at that point and can later have the agent call powerful tools (email, database, cloud APIs) to steal data or move laterally, all while sidestepping earlier security checks.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Although the open-source project has been billed as a reference implementation and not intended for production use, it has been forked over 5,000 times. The GitHub repository was archived on May 29, 2025, meaning no patches have been planned to address the shortcoming.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;The takeaway is clear. If we allow yesterday&amp;#39;s web-app mistakes to slip into today&amp;#39;s agent infrastructure, we gift attackers an effortless path from SQL injection to full agent compromise,&amp;quot; Park said.&lt;/p&gt;\n\n&lt;p&gt;The findings also follow a report from Backslash Security that found hundreds of MCP servers to be susceptible to two major misconfigurations: Allowing arbitrary command execution on the host machine due to unchecked input handling and excessive permissions, and making them accessible to any party on the same local network owing to them being explicitly bound to 0.0.0.0, a vulnerability dubbed NeighborJack.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Imagine you&amp;#39;re coding in a shared coworking space or café. Your MCP server is silently running on your machine,&amp;quot; Backslash Security said. &amp;quot;The person sitting near you, sipping their latte, can now access your MCP server, impersonate tools, and potentially run operations on your behalf. It&amp;#39;s like leaving your laptop open – and unlocked for everyone in the room.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Because MCPs, by design, are built to access external data sources, they can serve as covert pathways for prompt injection and context poisoning, thereby influencing the outcome of an LLM when parsing data from an attacker-controlled site that contains hidden instructions.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;One way to secure an MCP server might be to carefully process any text scraped from a website or database to avoid context poisoning,&amp;quot; researcher Micah Gold said. &amp;quot;However, this approach bloats tools – by requiring each individual tool to reimplement the same security feature – and leaves the user dependent on the security protocol of the individual MCP tool.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;A better approach, Backslash Security noted, is to configure AI rules with MCP clients to protect against vulnerable servers. These rules refer to pre-defined prompts or instructions that are assigned to an AI agent to guide its behavior and ensure it does not break security protocols.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;By conditioning AI agents to be skeptical and aware of the threat posed by context poisoning via AI rules, MCP clients can be secured against MCP servers,&amp;quot; Gold said.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq5wmh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Palpitation7740",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq5wmh/critical_vulnerability_in_anthropics_mcp_exposes/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq5wmh/critical_vulnerability_in_anthropics_mcp_exposes/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751486835,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_tq216",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I Built My Wife a Simple Web App for Image Editing Using Flux Kontext—Now It’s Open Source",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq5fqq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": "#bbbdbf",
          "ups": 512,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 512,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/g0h7MgqzOYDQU2yLRuOR1IOffTzZuGqxJuub0VRzepo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "Llama 405B"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751485685,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/nmerohq4miaf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?auto=webp&amp;s=01bff498ea36eff4a2ce0d43cd4eaebbc3e064a1",
                  "width": 1556,
                  "height": 1234
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b0f05fdbd0f93c91e6f72022aaaf617828ac15a4",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3e76289597fcbc69231cc8fe3e55e76d692acff9",
                    "width": 216,
                    "height": 171
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3d3429a3f11bf7b5e9127d50bed2a7dad520aebf",
                    "width": 320,
                    "height": 253
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=82000e0b8dd6c6f395384b8459f531f8884586e0",
                    "width": 640,
                    "height": 507
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=16f3c087dacea89685951f9d2be115bc1f6e2f1c",
                    "width": 960,
                    "height": 761
                  },
                  {
                    "url": "https://preview.redd.it/nmerohq4miaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e66c21521fb065f4b3f70623a484b7437229e237",
                    "width": 1080,
                    "height": 856
                  }
                ],
                "variants": {},
                "id": "_YhGYoutFRUObj-RqMiOAzLlmyP_b2EqDmvMPUR0YB8"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "Llama 405B",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq5fqq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "XMasterrrr",
          "discussion_type": null,
          "num_comments": 52,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lq5fqq/i_built_my_wife_a_simple_web_app_for_image/",
          "stickied": false,
          "url": "https://i.redd.it/nmerohq4miaf1.jpeg",
          "subreddit_subscribers": 494001,
          "created_utc": 1751485685,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been thinking about how we manage context when interacting with LLMs, and thought what if we had chat trees instead of linear threads?\n\nThe idea is simple, let users branch off from any point in the conversation to explore alternatives or dive deeper, while hiding irrelevant future context. I put together a quick POC to explore this.\n\nWould love to hear your thoughts, is this kind of context control useful? What would you change or build on top?",
          "author_fullname": "t2_13zuwb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ChatTree: A simple way to context engineer",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq5d1o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 15,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 15,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=f6d7317f0148bd20e0b924ea76edc1b05218e4c2",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751485506,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been thinking about how we manage context when interacting with LLMs, and thought what if we had chat trees instead of linear threads?&lt;/p&gt;\n\n&lt;p&gt;The idea is simple, let users branch off from any point in the conversation to explore alternatives or dive deeper, while hiding irrelevant future context. I put together a quick POC to explore this.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your thoughts, is this kind of context control useful? What would you change or build on top?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/aadityaubhat/ChatTree",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?auto=webp&amp;s=2e946eba7e4832bd40d0095ef8816cc9f8a69818",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=344fc168447205eb001e41942ab7649037277702",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7fb5ad90e01390b75ebcef865b4c8ec3cef8cc64",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=74022766aec6fd5e2c0b69dd4435e7ba27b81bfc",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b00cfbafde10dd16bd1b2925da2c9cbf5a1efec",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0319737917afcf0f39dd3560814bd2bd5fc01266",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=677afc7d9ee0c03d95968ca9015ecaaf15861d57",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq5d1o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "aadityaubhat",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/",
          "stickied": false,
          "url": "https://github.com/aadityaubhat/ChatTree",
          "subreddit_subscribers": 494001,
          "created_utc": 1751485506,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Mistral Small 3.2 scores 11.5 (Mistral Small 3.1 scored 11.4).  \nBaidu Ernie 4.5 300B A47B scores 15.2.  \nMiniMax-M1 (reasoning) scores 21.4 (MiniMax-Text-01 scored 14.6).",
          "author_fullname": "t2_p2tr0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Extended NYT Connections Benchmark updated with Baidu Ernie 4.5 300B A47B, Mistral Small 3.2, MiniMax-M1",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq4cil",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=db3729052e2a4ab5784e807eb60919c01cc17a94",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751483025,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Mistral Small 3.2 scores 11.5 (Mistral Small 3.1 scored 11.4).&lt;br/&gt;\nBaidu Ernie 4.5 300B A47B scores 15.2.&lt;br/&gt;\nMiniMax-M1 (reasoning) scores 21.4 (MiniMax-Text-01 scored 14.6).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/lechmazur/nyt-connections/",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?auto=webp&amp;s=c964e8b0b24dc1e9514071574f852cb6ab8d95fe",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ef682e456093328d7e9066c522f0dbc88ce5731d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=118ab4e7c14ca213c34cb80eb8adf5298743c846",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03594f16d2150150f593bf0db3e5f1ceff506a2b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=449cce0aca655e2cc3db034127943b5bf3561824",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7434614c112ee39fd658a7e95e7aee49e3762663",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e978955f0ea8e2e2b8222e467627013c81a7d16a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "8K_ldEY4raQVEoGa75S06Rw5m0I3IfQW0ZBFqygnT8o"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq4cil",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zero0_one1",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq4cil/extended_nyt_connections_benchmark_updated_with/",
          "stickied": false,
          "url": "https://github.com/lechmazur/nyt-connections/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751483025,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "have been poring over pcpartpicker, newegg etc. and it seems like the cheapest way to get the most usable VRAM from GPUs is the 16GB 5060Ti? am I missing something obvious? (probably.)\n\nTIA.",
          "author_fullname": "t2_34g6p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "best bang for your buck in GPUs for VRAM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq4bhu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751482961,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;have been poring over pcpartpicker, newegg etc. and it seems like the cheapest way to get the most usable VRAM from GPUs is the 16GB 5060Ti? am I missing something obvious? (probably.)&lt;/p&gt;\n\n&lt;p&gt;TIA.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq4bhu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "starkruzr",
          "discussion_type": null,
          "num_comments": 54,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq4bhu/best_bang_for_your_buck_in_gpus_for_vram/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq4bhu/best_bang_for_your_buck_in_gpus_for_vram/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751482961,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all. I install the model that supports the visual module, whenever I upload the photo, the error falls: Model sores not support images. Please ae a model that does. What to do with it?",
          "author_fullname": "t2_n8tv96hi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Lm studio: Model does not support images. Please use a model that does.!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq436s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751482419,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all. I install the model that supports the visual module, whenever I upload the photo, the error falls: Model sores not support images. Please ae a model that does. What to do with it?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq436s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SensitiveMarzipan203",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq436s/lm_studio_model_does_not_support_images_please/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq436s/lm_studio_model_does_not_support_images_please/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751482419,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What will be better?  \nIQ3\\_M 24B mistral small 3.1/3.2 vs Q5\\_K\\_M 12B mistral nemo",
          "author_fullname": "t2_1irjq3mta2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "24B IQ3_M vs 12B Q5_K_M",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq3urv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751481864,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What will be better?&lt;br/&gt;\nIQ3_M 24B mistral small 3.1/3.2 vs Q5_K_M 12B mistral nemo&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq3urv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Longjumping_Bee_6825",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq3urv/24b_iq3_m_vs_12b_q5_k_m/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq3urv/24b_iq3_m_vs_12b_q5_k_m/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751481864,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In the past two days, we explored what positional embeddings are and even coded it.\n\nToday, we’re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).\n\n# Recap: Why Transformers Need Positional Embeddings\n\nTransformers process tokens in parallel, which makes them efficient, but it also means they don’t inherently know the order of the tokens.\n\nTo a transformer, these sentences look identical:\n\n* \"The cat sat on the mat.\"\n* \"The mat sat on the cat.\"\n\nThat’s a problem. Order matters, especially in language.\n\nTo fix this, we add *positional embeddings* to inform the model about token positions.\n\n# Traditional Positional Embeddings\n\nTwo popular approaches:\n\n* **Learned positional embeddings** – Each position (1, 2, 3...) gets a trainable vector.\n* **Sinusoidal embeddings** – Use sin/cos functions to generate fixed vectors per position.\n\nBut they have limitations:\n\n* Fixed or learned per-position (no flexibility)\n* Poor generalization to longer sequences\n* Don't integrate naturally with attention scores\n\n# What Is RoPE and Why Is It Better?\n\nRoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.\n\nInstead of adding a position vector, RoPE rotates token embeddings in space based on their position,  directly inside the attention mechanism (on query and key vectors).\n\nThis encodes relative position information in a more elegant and flexible way.\n\nFor each position, the token embedding is rotated by an angle proportional to that position.\n\nA simplified pseudocode:\n\n    for i in range(0, dim, 2):\n        x1, x2 = x[i], x[i+1]\n        angle = theta * position\n        x[i]   = x1 * cos(angle) - x2 * sin(angle)\n        x[i+1] = x1 * sin(angle) + x2 * cos(angle)\n    \n\nThis allows attention to naturally reflect *how far apart* two tokens are, something traditional embeddings can’t do.\n\n# RoPE vs Traditional Positional Embeddings\n\n|Feature|Traditional Embeddings|Rotary Positional Embeddings (RoPE)|\n|:-|:-|:-|\n|Position Injected|Added to input embeddings|Applied inside attention mechanism|\n|Absolute or Relative?|Absolute|Relative|\n|Generalizes to Long Sequences?|Poor|Strong|\n|Learnable Parameters?|Sometimes (if learned)|No|\n|Adopted in SOTA models?|Less common now|Yes (LLaMA, DeepSeek)|\n\n# Why RoPE Is So Useful\n\n* **Encodes relative positions** directly in attention scores\n* **No extra parameters** – it's deterministic\n* **Handles long sequences** more gracefully\n* **Simple implementation** using trigonometric rotation\n\n# Use in Real Models\n\n* **LLaMA (Meta):** Uses RoPE for better generalization and long-context performance.\n* **DeepSeek:** Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.\n\n# Final Thoughts\n\nRotary Positional Embeddings are an elegant solution to a core transformer weakness. If you’re building models for long documents, code, or stories, RoPE should be on your radar.\n\n# Coming Up Tomorrow\n\nWe'll implement RoPE in code and walk through how it’s used in the open-source  \n[DeepSeek-Children-Stories-15M model](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model)\n\nFollow along,  we’re just getting started.",
          "author_fullname": "t2_8ht7a116",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Day 8/50: Building a Small Language Model from Scratch – Rotary Positional Embeddings (RoPE)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq3tuu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751481803,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In the past two days, we explored what positional embeddings are and even coded it.&lt;/p&gt;\n\n&lt;p&gt;Today, we’re diving into a more advanced and powerful concept used in many state-of-the-art models: Rotary Positional Embeddings (RoPE).&lt;/p&gt;\n\n&lt;h1&gt;Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt;\n\n&lt;p&gt;Transformers process tokens in parallel, which makes them efficient, but it also means they don’t inherently know the order of the tokens.&lt;/p&gt;\n\n&lt;p&gt;To a transformer, these sentences look identical:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&amp;quot;The cat sat on the mat.&amp;quot;&lt;/li&gt;\n&lt;li&gt;&amp;quot;The mat sat on the cat.&amp;quot;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That’s a problem. Order matters, especially in language.&lt;/p&gt;\n\n&lt;p&gt;To fix this, we add &lt;em&gt;positional embeddings&lt;/em&gt; to inform the model about token positions.&lt;/p&gt;\n\n&lt;h1&gt;Traditional Positional Embeddings&lt;/h1&gt;\n\n&lt;p&gt;Two popular approaches:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Learned positional embeddings&lt;/strong&gt; – Each position (1, 2, 3...) gets a trainable vector.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Sinusoidal embeddings&lt;/strong&gt; – Use sin/cos functions to generate fixed vectors per position.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;But they have limitations:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fixed or learned per-position (no flexibility)&lt;/li&gt;\n&lt;li&gt;Poor generalization to longer sequences&lt;/li&gt;\n&lt;li&gt;Don&amp;#39;t integrate naturally with attention scores&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;What Is RoPE and Why Is It Better?&lt;/h1&gt;\n\n&lt;p&gt;RoPE was introduced in RoFormer (Su et al., 2021) and is now used in models like LLaMA and DeepSeek.&lt;/p&gt;\n\n&lt;p&gt;Instead of adding a position vector, RoPE rotates token embeddings in space based on their position,  directly inside the attention mechanism (on query and key vectors).&lt;/p&gt;\n\n&lt;p&gt;This encodes relative position information in a more elegant and flexible way.&lt;/p&gt;\n\n&lt;p&gt;For each position, the token embedding is rotated by an angle proportional to that position.&lt;/p&gt;\n\n&lt;p&gt;A simplified pseudocode:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;for i in range(0, dim, 2):\n    x1, x2 = x[i], x[i+1]\n    angle = theta * position\n    x[i]   = x1 * cos(angle) - x2 * sin(angle)\n    x[i+1] = x1 * sin(angle) + x2 * cos(angle)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This allows attention to naturally reflect &lt;em&gt;how far apart&lt;/em&gt; two tokens are, something traditional embeddings can’t do.&lt;/p&gt;\n\n&lt;h1&gt;RoPE vs Traditional Positional Embeddings&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Feature&lt;/th&gt;\n&lt;th align=\"left\"&gt;Traditional Embeddings&lt;/th&gt;\n&lt;th align=\"left\"&gt;Rotary Positional Embeddings (RoPE)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Position Injected&lt;/td&gt;\n&lt;td align=\"left\"&gt;Added to input embeddings&lt;/td&gt;\n&lt;td align=\"left\"&gt;Applied inside attention mechanism&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Absolute or Relative?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Absolute&lt;/td&gt;\n&lt;td align=\"left\"&gt;Relative&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Generalizes to Long Sequences?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Poor&lt;/td&gt;\n&lt;td align=\"left\"&gt;Strong&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Learnable Parameters?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Sometimes (if learned)&lt;/td&gt;\n&lt;td align=\"left\"&gt;No&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Adopted in SOTA models?&lt;/td&gt;\n&lt;td align=\"left\"&gt;Less common now&lt;/td&gt;\n&lt;td align=\"left\"&gt;Yes (LLaMA, DeepSeek)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Why RoPE Is So Useful&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Encodes relative positions&lt;/strong&gt; directly in attention scores&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;No extra parameters&lt;/strong&gt; – it&amp;#39;s deterministic&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Handles long sequences&lt;/strong&gt; more gracefully&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Simple implementation&lt;/strong&gt; using trigonometric rotation&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Use in Real Models&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;LLaMA (Meta):&lt;/strong&gt; Uses RoPE for better generalization and long-context performance.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DeepSeek:&lt;/strong&gt; Uses a decoupled RoPE mechanism where rotary embeddings are applied to separate query/key heads, enabling efficient long-context attention without bloating memory.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Final Thoughts&lt;/h1&gt;\n\n&lt;p&gt;Rotary Positional Embeddings are an elegant solution to a core transformer weakness. If you’re building models for long documents, code, or stories, RoPE should be on your radar.&lt;/p&gt;\n\n&lt;h1&gt;Coming Up Tomorrow&lt;/h1&gt;\n\n&lt;p&gt;We&amp;#39;ll implement RoPE in code and walk through how it’s used in the open-source&lt;br/&gt;\n&lt;a href=\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\"&gt;DeepSeek-Children-Stories-15M model&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Follow along,  we’re just getting started.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?auto=webp&amp;s=aaf4557bdc852e74628b9a80be3094608111b971",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdcabe31093a0a8eb032266a1279c0d7415a3fd7",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7ccd3cc1cd68ed805c75d8bd83411f26087f2fc",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0f60d176e3abd7b1151e118c49209d7eec7a901b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f2480d1a61ec09f5b4ed612393052d527f3c24bf",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=10f726da67cf6450203cd209e8dbc33a73334d48",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e04b5b7ddd416d60272d1ccfff6f07687e64d488",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "yk95gNYDHwq6XIleth3O7MrgXkPOE1Hr9ZzOPDx-3XE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lq3tuu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prashant-Lakhera",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq3tuu/day_850_building_a_small_language_model_from/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751481803,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "i am completely new to this entire thing and am hoping to run models locally on my desktop (rtx 4070, r7 9700x, 32gb ddr5). what models would be the best use case for these specs?",
          "author_fullname": "t2_16jmawqekh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "need suggestions for models to use",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq3i6h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751481034,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;i am completely new to this entire thing and am hoping to run models locally on my desktop (rtx 4070, r7 9700x, 32gb ddr5). what models would be the best use case for these specs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq3i6h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "StrangeChallenge1865",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq3i6h/need_suggestions_for_models_to_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq3i6h/need_suggestions_for_models_to_use/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751481034,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys,\n\nI’m diving into running models locally with Ollama or LMStudio, and there are so many options that I don’t even know where to start, especially before I lock in on a specific project. I want to develop a clear process for figuring out which model might suit me, even if I don’t yet have a narrow use case.\n\nCould you walk me through your thought process? \nFor example:\n\t•\tHow do you survey the landscape of available models and group them into “creative,” “factual,” or “code-focused” categories?\n\t•\tWhat are the first metrics or specs you check (size, quantization, RAM/VRAM needs, inference speed, training data)?\n\t•\tHow do you run quick, side-by-side tests in Ollama/LMStudio to compare responses on a handful of prompts?\n\t•\tWhat mental shortcuts or analogies do you use to decide “this one feels like the right fit” before committing?\n\t•\tAny go-to scripts, benchmarks, or community resources that help you narrow down from a dozen candidates to your top one or two?\n\nI’m not a developer or engineer, I’m coming at this entirely as an end-user who just wants a consumer-friendly way to experiment with local AI. I don’t have deep technical skills or coding experience, so I’m looking for recommendations and processes explained in plain English rather than programming tutorials.\n\nHope someone can help and thanks in advance!",
          "author_fullname": "t2_8lee8rfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you pick the right local LLM for your needs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq2wn6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751479612,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I’m diving into running models locally with Ollama or LMStudio, and there are so many options that I don’t even know where to start, especially before I lock in on a specific project. I want to develop a clear process for figuring out which model might suit me, even if I don’t yet have a narrow use case.&lt;/p&gt;\n\n&lt;p&gt;Could you walk me through your thought process? \nFor example:\n    • How do you survey the landscape of available models and group them into “creative,” “factual,” or “code-focused” categories?\n    • What are the first metrics or specs you check (size, quantization, RAM/VRAM needs, inference speed, training data)?\n    • How do you run quick, side-by-side tests in Ollama/LMStudio to compare responses on a handful of prompts?\n    • What mental shortcuts or analogies do you use to decide “this one feels like the right fit” before committing?\n    • Any go-to scripts, benchmarks, or community resources that help you narrow down from a dozen candidates to your top one or two?&lt;/p&gt;\n\n&lt;p&gt;I’m not a developer or engineer, I’m coming at this entirely as an end-user who just wants a consumer-friendly way to experiment with local AI. I don’t have deep technical skills or coding experience, so I’m looking for recommendations and processes explained in plain English rather than programming tutorials.&lt;/p&gt;\n\n&lt;p&gt;Hope someone can help and thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq2wn6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtiqX",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2wn6/how_do_you_pick_the_right_local_llm_for_your_needs/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2wn6/how_do_you_pick_the_right_local_llm_for_your_needs/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751479612,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello like the title says we are trying to build a pipeline that takes in tables and tries to decern what information they contain. For this i was wondering if someone ever tried specific table embeddings ? So we can try building a vectorspace for a kind of rag searching out the next related tables and using an llm and other heuristics to judge what kind of data a table contains. \n\nDo any of you know an embedding model for tables ?",
          "author_fullname": "t2_b3etlj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Table embeddings for similarity search between tables ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq2m1x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751478938,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello like the title says we are trying to build a pipeline that takes in tables and tries to decern what information they contain. For this i was wondering if someone ever tried specific table embeddings ? So we can try building a vectorspace for a kind of rag searching out the next related tables and using an llm and other heuristics to judge what kind of data a table contains. &lt;/p&gt;\n\n&lt;p&gt;Do any of you know an embedding model for tables ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq2m1x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Noxusequal",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2m1x/table_embeddings_for_similarity_search_between/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2m1x/table_embeddings_for_similarity_search_between/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751478938,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anything that would work as an agentic code assistant? Trying to decide if it’s worth investing if it means I don’t have to pay for Claude code anymore. I understand it won’t be near Claude code but that’s fine. ",
          "author_fullname": "t2_dmc3swt9s",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a legit code assistant that can run on a m3 ultra 256 or 96gb?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq2i2m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751478677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anything that would work as an agentic code assistant? Trying to decide if it’s worth investing if it means I don’t have to pay for Claude code anymore. I understand it won’t be near Claude code but that’s fine. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq2i2m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "tru3relativity",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2i2m/is_there_a_legit_code_assistant_that_can_run_on_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2i2m/is_there_a_legit_code_assistant_that_can_run_on_a/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751478677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "A recent [study](https://arxiv.org/abs/2409.01754) *underscores* the growing prevalence of LLM-generated \"slop words\" [in academic papers](https://pshapira.net/2024/03/31/delving-into-delve/), a trend now *spilling into* spontaneous spoken language. By *meticulously analyzing* 700,000 hours of academic talks and podcast episodes, researchers *pinpointed* this shift. While it’s plausible speakers could be reading from scripts, manual inspection of videos containing slop words revealed no such evidence in over half the cases. This suggests either speakers have *woven* these terms into their natural lexicon or have memorized ChatGPT-generated scripts.\n\nThis creates a feedback loop: human-generated content *escalates* the use of slop words, further training LLMs on this linguistic trend. The influence is *not confined* to early adopter domains like academia and tech but is *spreading* to education and business. *It’s worth noting that* its presence remains less pronounced in religion and sports—*perhaps, just perhaps* due to the *intricacy* of their linguistic *tapestry*.\n\nUsers of popular models like ChatGPT lack access to tools like the [Anti-Slop](https://www.reddit.com/r/LocalLLaMA/comments/1fqqez5/i_made_a_configurable_antislop_sampler_which/) or [XTC sampler](https://www.reddit.com/r/LocalLLaMA/comments/1f5n9dw/koboldcpp_v174_adds_xtc_exclude_top_choices/), implemented in local solutions such as llama.cpp and kobold.cpp. Consequently, despite our efforts, the proliferation of slop words may persist.\n\nDisclaimer: I generally don't let LLMs \"*improve*\" my postings. This was an occasion too tempting to miss out on though.\n\nhttps://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;format=png&amp;auto=webp&amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282\n\n",
          "author_fullname": "t2_k7w2h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM slop has started to contaminate spoken language",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "tbolga2nzhaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 150,
                  "x": 108,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0fafd729cf9b03a1c89c180e6f215bb3ba5bbdca"
                },
                {
                  "y": 300,
                  "x": 216,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=68cf67a6ee32455844da6cb7fc9bd6cfb4a228e0"
                },
                {
                  "y": 445,
                  "x": 320,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4c59b4b8c42ce669418864460927d13ef08973fd"
                },
                {
                  "y": 890,
                  "x": 640,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0772d987db40a779d894526e5935bc69aa42a50f"
                },
                {
                  "y": 1336,
                  "x": 960,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0109d4d42d2d54dc4c2a79252da1b0f03b7295ec"
                },
                {
                  "y": 1503,
                  "x": 1080,
                  "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ac5870a1e922188610e7585dbfd3083d69c8a2ab"
                }
              ],
              "s": {
                "y": 1747,
                "x": 1255,
                "u": "https://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;format=png&amp;auto=webp&amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282"
              },
              "id": "tbolga2nzhaf1"
            }
          },
          "name": "t3_1lq2aae",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.52,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/1yosruA5utC0JLuhwHuIdDoNpZft719EvTCp_SZSzEY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751478166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;A recent &lt;a href=\"https://arxiv.org/abs/2409.01754\"&gt;study&lt;/a&gt; &lt;em&gt;underscores&lt;/em&gt; the growing prevalence of LLM-generated &amp;quot;slop words&amp;quot; &lt;a href=\"https://pshapira.net/2024/03/31/delving-into-delve/\"&gt;in academic papers&lt;/a&gt;, a trend now &lt;em&gt;spilling into&lt;/em&gt; spontaneous spoken language. By &lt;em&gt;meticulously analyzing&lt;/em&gt; 700,000 hours of academic talks and podcast episodes, researchers &lt;em&gt;pinpointed&lt;/em&gt; this shift. While it’s plausible speakers could be reading from scripts, manual inspection of videos containing slop words revealed no such evidence in over half the cases. This suggests either speakers have &lt;em&gt;woven&lt;/em&gt; these terms into their natural lexicon or have memorized ChatGPT-generated scripts.&lt;/p&gt;\n\n&lt;p&gt;This creates a feedback loop: human-generated content &lt;em&gt;escalates&lt;/em&gt; the use of slop words, further training LLMs on this linguistic trend. The influence is &lt;em&gt;not confined&lt;/em&gt; to early adopter domains like academia and tech but is &lt;em&gt;spreading&lt;/em&gt; to education and business. &lt;em&gt;It’s worth noting that&lt;/em&gt; its presence remains less pronounced in religion and sports—&lt;em&gt;perhaps, just perhaps&lt;/em&gt; due to the &lt;em&gt;intricacy&lt;/em&gt; of their linguistic &lt;em&gt;tapestry&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;Users of popular models like ChatGPT lack access to tools like the &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1fqqez5/i_made_a_configurable_antislop_sampler_which/\"&gt;Anti-Slop&lt;/a&gt; or &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1f5n9dw/koboldcpp_v174_adds_xtc_exclude_top_choices/\"&gt;XTC sampler&lt;/a&gt;, implemented in local solutions such as llama.cpp and kobold.cpp. Consequently, despite our efforts, the proliferation of slop words may persist.&lt;/p&gt;\n\n&lt;p&gt;Disclaimer: I generally don&amp;#39;t let LLMs &amp;quot;&lt;em&gt;improve&lt;/em&gt;&amp;quot; my postings. This was an occasion too tempting to miss out on though.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282\"&gt;https://preview.redd.it/tbolga2nzhaf1.png?width=1255&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f4d8f1b17894361766cf3d3130d5892b5f5de282&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq2aae",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Chromix_",
          "discussion_type": null,
          "num_comments": 81,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq2aae/llm_slop_has_started_to_contaminate_spoken/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq2aae/llm_slop_has_started_to_contaminate_spoken/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751478166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;Cursor equivalent or close to alternative fully local?\n\nIt's Continue .dev, Void, aider, Zed, AutoGPT, SuperAGI or something else\n\nEdit 1:\n\ncodium, Codestral, Roo, Cline+Ollama...\n\nPlease rate one tool over other like xyz is better then abc but worse then arq etc",
          "author_fullname": "t2_1b8utegv8t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cursor equivalent or close to alternative fully local?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq1sdi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.77,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1751521546,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751476992,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Cursor equivalent or close to alternative fully local?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It&amp;#39;s Continue .dev, Void, aider, Zed, AutoGPT, SuperAGI or something else&lt;/p&gt;\n\n&lt;p&gt;Edit 1:&lt;/p&gt;\n\n&lt;p&gt;codium, Codestral, Roo, Cline+Ollama...&lt;/p&gt;\n\n&lt;p&gt;Please rate one tool over other like xyz is better then abc but worse then arq etc&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq1sdi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "InsideResolve4517",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq1sdi/cursor_equivalent_or_close_to_alternative_fully/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq1sdi/cursor_equivalent_or_close_to_alternative_fully/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751476992,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_a2gtk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Mamba-2 support in llama.cpp landed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq1jyr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 111,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 111,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7512f1621ccc42771158d4a83dba439854c266e0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751476448,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?auto=webp&amp;s=4510a608e1ff971f852ec57d8f07668f2b8ab0d6",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7a6abcb15d265565c74fee896eff28e14b9a9f0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5355fc411298a6af9630c201bb9f61dddf8fb479",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e602810402906fef613cd3cd6f104bb341ab3f8",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c83c80ef04abfb36fdb066d346ea91753a7d280d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04f4978664f7d2eb57a2e9fe0c534b61e0bcd9f1",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e45bd94615d08f9378ef6a9d9775ca74a0f112d6",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lq1jyr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pkmxtw",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556",
          "subreddit_subscribers": 494001,
          "created_utc": 1751476448,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I integrated Moondream (lightweight vision AI model) with Model Context Protocol (MCP), enabling any AI agent to process images locally/remotely.\nOpen source, self-hosted, no API keys needed.\nMoondream MCP is a vision AI server that speaks MCP protocol. Your agents can now:  \n**Caption images** - \"What's in this image?\"  \n**Detect objects** - Find all instances with bounding boxes  \n**Visual Q&amp;A** - \"How many people are in this photo?\"  \n**Point to objects** - \"Where's the error message?\"  \n\nIt integrates into Claude Desktop, OpenAI agents, and anything that supports MCP.  \nhttps://github.com/ColeMurray/moondream-mcp/  \nFeedback and contributions welcome!",
          "author_fullname": "t2_13nfvs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Open Source] Moondream MCP - Vision for AI Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 99,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq1417",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_1tz6EuMezDb-Vfzg8nD4-eqUh3Ghmgc_u-pHCga3Ho.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751475447,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I integrated Moondream (lightweight vision AI model) with Model Context Protocol (MCP), enabling any AI agent to process images locally/remotely.\nOpen source, self-hosted, no API keys needed.\nMoondream MCP is a vision AI server that speaks MCP protocol. Your agents can now:&lt;br/&gt;\n&lt;strong&gt;Caption images&lt;/strong&gt; - &amp;quot;What&amp;#39;s in this image?&amp;quot;&lt;br/&gt;\n&lt;strong&gt;Detect objects&lt;/strong&gt; - Find all instances with bounding boxes&lt;br/&gt;\n&lt;strong&gt;Visual Q&amp;amp;A&lt;/strong&gt; - &amp;quot;How many people are in this photo?&amp;quot;&lt;br/&gt;\n&lt;strong&gt;Point to objects&lt;/strong&gt; - &amp;quot;Where&amp;#39;s the error message?&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;It integrates into Claude Desktop, OpenAI agents, and anything that supports MCP.&lt;br/&gt;\n&lt;a href=\"https://github.com/ColeMurray/moondream-mcp/\"&gt;https://github.com/ColeMurray/moondream-mcp/&lt;/a&gt;&lt;br/&gt;\nFeedback and contributions welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/upyzvjqkrhaf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/upyzvjqkrhaf1.png?auto=webp&amp;s=17b35279ccff150dcffa50632a87246bcaffbc65",
                  "width": 1476,
                  "height": 1048
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e580de2907c79e680e10c3e5979c6d4dbb3ba2c9",
                    "width": 108,
                    "height": 76
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=152478b6ed0e0b3986fc83ccd5b5f39cae10020a",
                    "width": 216,
                    "height": 153
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8fe291138cbb0a68b432264138db664f722d835f",
                    "width": 320,
                    "height": 227
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=76d29f976867cbc03b905c9152d9b301637ec9c9",
                    "width": 640,
                    "height": 454
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=dfdc60932fb5e43e365f9bdf6f4f630cfd592974",
                    "width": 960,
                    "height": 681
                  },
                  {
                    "url": "https://preview.redd.it/upyzvjqkrhaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=48271d6020f0ce8aa5660f096916ad6b4212dc1a",
                    "width": 1080,
                    "height": 766
                  }
                ],
                "variants": {},
                "id": "AW94Mv2FkIDW3--yyr0eIKNalxqTjCalp9fCr0HItaI"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq1417",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_colemurray",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq1417/open_source_moondream_mcp_vision_for_ai_agents/",
          "stickied": false,
          "url": "https://i.redd.it/upyzvjqkrhaf1.png",
          "subreddit_subscribers": 494001,
          "created_utc": 1751475447,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been exploring agentic workflows lately not just the flashy demos, but actual implementations that support real-world tasks like deep research, cross-functional reporting, and internal communications.\n\nOne interesting pattern I’ve noticed: the potential of AI agents seems strongest in domains like law, public sector, and enterprise knowledge work especially where speed and accuracy really matter. But there’s still a lot of noise, and figuring out what works in practice vs. theory isn’t always straightforward.\n\nCame across an upcoming session that’s diving into practical applications of agentic AI in knowledge-based industries. Not affiliated with the speaker, but it looked like a useful overview for folks building in this space. I’ll drop the link in the comments for anyone interested.\n\nWould love to hear how others are thinking about agent workflows right now what’s working, what’s still clunky, and where you think we’ll actually see adoption in the next 6–12 months.",
          "author_fullname": "t2_1llpsv0jmf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Agents are transforming workflows, but most use cases still feel early-stage. Curious what others are seeing.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq0n02",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751474346,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been exploring agentic workflows lately not just the flashy demos, but actual implementations that support real-world tasks like deep research, cross-functional reporting, and internal communications.&lt;/p&gt;\n\n&lt;p&gt;One interesting pattern I’ve noticed: the potential of AI agents seems strongest in domains like law, public sector, and enterprise knowledge work especially where speed and accuracy really matter. But there’s still a lot of noise, and figuring out what works in practice vs. theory isn’t always straightforward.&lt;/p&gt;\n\n&lt;p&gt;Came across an upcoming session that’s diving into practical applications of agentic AI in knowledge-based industries. Not affiliated with the speaker, but it looked like a useful overview for folks building in this space. I’ll drop the link in the comments for anyone interested.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear how others are thinking about agent workflows right now what’s working, what’s still clunky, and where you think we’ll actually see adoption in the next 6–12 months.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lq0n02",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Powerful-Guide-8169",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq0n02/ai_agents_are_transforming_workflows_but_most_use/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq0n02/ai_agents_are_transforming_workflows_but_most_use/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751474346,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Their github instruction is not working. I'm on windows with an i7 cpu, Nvidia 12 gb RTX card. \n\nEverytime there will be some error at the end. \n\nI'm wanting to install and have a web gui after installing. I want only 1 speaker TTS from voicecloned. \n\nI found a 1 click readymade installer online but it feels risky as who knows what kind of malcious code maybe there and starts running once I run the .bat file.\n\nAnyone can say how to install in windows from start?\n\nThank you so much.",
          "author_fullname": "t2_vbdiiix7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Stuck on installing Sesame tts on windows :/",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lq02np",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751473013,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Their github instruction is not working. I&amp;#39;m on windows with an i7 cpu, Nvidia 12 gb RTX card. &lt;/p&gt;\n\n&lt;p&gt;Everytime there will be some error at the end. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m wanting to install and have a web gui after installing. I want only 1 speaker TTS from voicecloned. &lt;/p&gt;\n\n&lt;p&gt;I found a 1 click readymade installer online but it feels risky as who knows what kind of malcious code maybe there and starts running once I run the .bat file.&lt;/p&gt;\n\n&lt;p&gt;Anyone can say how to install in windows from start?&lt;/p&gt;\n\n&lt;p&gt;Thank you so much.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lq02np",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dragonacious",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lq02np/stuck_on_installing_sesame_tts_on_windows/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lq02np/stuck_on_installing_sesame_tts_on_windows/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751473013,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_18dimy5ve3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Live Interactive Digital Human(Open-Source Stack): RAG + LLM + TTS in Ac...",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpzycz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/oZ-tCnmUSRE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Live Interactive Digital Human(Open-Source Stack): RAG + LLM + TTS in Action\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Live Interactive Digital Human(Open-Source Stack): RAG + LLM + TTS in Action",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/oZ-tCnmUSRE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Live Interactive Digital Human(Open-Source Stack): RAG + LLM + TTS in Action\"&gt;&lt;/iframe&gt;",
              "author_name": "titan909-share",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/oZ-tCnmUSRE/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@titan909-share"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/oZ-tCnmUSRE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Live Interactive Digital Human(Open-Source Stack): RAG + LLM + TTS in Action\"&gt;&lt;/iframe&gt;",
            "width": 267,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lpzycz",
            "height": 200
          },
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/ulR1vplT2R0rAFZ3rWOI7kaar5rpRIsbH4QVgbouwAE.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=d87a2b2893d08999092c35a3ccf9ebf6990664eb",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751472731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtube.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtube.com/watch?v=oZ-tCnmUSRE&amp;si=J6fQbTAdfu0nPHxh",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/ulR1vplT2R0rAFZ3rWOI7kaar5rpRIsbH4QVgbouwAE.jpeg?auto=webp&amp;s=a0e3ddb68420ed182ccc37b4bb0687639c89e9cc",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/ulR1vplT2R0rAFZ3rWOI7kaar5rpRIsbH4QVgbouwAE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=03a68f9356aedc9915a9fbc51a78efcb9cdf2ed8",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/ulR1vplT2R0rAFZ3rWOI7kaar5rpRIsbH4QVgbouwAE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f307e58e34a22165e0c6d01f06d9c64830d7c542",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/ulR1vplT2R0rAFZ3rWOI7kaar5rpRIsbH4QVgbouwAE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=673603d57d795c9f1fef121c722a56807933e351",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "ulR1vplT2R0rAFZ3rWOI7kaar5rpRIsbH4QVgbouwAE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lpzycz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Deep-Jellyfish6717",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpzycz/live_interactive_digital_humanopensource_stack/",
          "stickied": false,
          "url": "https://youtube.com/watch?v=oZ-tCnmUSRE&amp;si=J6fQbTAdfu0nPHxh",
          "subreddit_subscribers": 494001,
          "created_utc": 1751472731,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Live Interactive Digital Human(Open-Source Stack): RAG + LLM + TTS in Action",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 267,
              "html": "&lt;iframe width=\"267\" height=\"200\" src=\"https://www.youtube.com/embed/oZ-tCnmUSRE?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Live Interactive Digital Human(Open-Source Stack): RAG + LLM + TTS in Action\"&gt;&lt;/iframe&gt;",
              "author_name": "titan909-share",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/oZ-tCnmUSRE/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@titan909-share"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm making this thread because weeks ago when I looked up this information, I could barely even find confirmation that it's possible to run 14B models on phones. In the meantime I got a OnePlus 13 with 16GB of RAM. After tinkering with different models and apps for half a day, I figured I give my feedback for the people who are interested in this specific scenario. \n\nI'm used to running 32B models on my PC and after many (subjective) tests I realized that modern 14B models are not far behind in capabilities, at least for my use-cases. I find 8B models kinda meh (I'm warming up to them lately), but my obsession was to be able to run 14B models on a phone, so here we are. \n\n**Key Points:**  \nQwen3 14B loaded via MNN Chat runs decent, but the performance is not consistent. You can expect anywhere from 4.5-7 tokens per second, but the overall performance is around 5.5t/s. I don't know exactly what quantization this models uses because MNN Chat doesn't say it. My guess, based on the file size, is that it's either Q4\\_K\\_S or IQ4. Could also be Q4\\_K\\_M but the file seems rather small for that so I have my doubts. \n\nQwen3 8B runs at around 8 tokens per second, but again I don't know what quantization. Based on the file size, I'm guessing it's Q6\\_K\\_M. I was kinda expecting a bit more here, but whatever. 8t/s is around reading/thinking speed for me, so I'm ok with that. \n\nI also used PocketPal to run some abliterated versions of Qwen3 14B at Q4\\_K\\_M. Performance was similar to MNN Chat which surprised me since everyone was saying that MNN Chat should provide a significant boost in performance since it's optimized to work with Snapdragon NPUs. Maybe at this model size the VRAM bandwidth is the bottleneck so the performance improvements are not obvious anymore. \n\nEnabling or disabling thinking doesn't seem to affect the speed directly, but it will affect it indirectly. More on that later. \n\nI'm in the process of downloading Qwen3-30B-A3B. By all acounts it should not fit in VRAM, but OnePlus has that virtual memory thing that allows you to expand the RAM by an extra 12GB. It will use the UFS storage obviously. ~~This should put me at 16+12=28GB of RAM which should allow me to load the model.~~ LE: never mind. The version provided by MNN Chat doesn't load. I think it's meant for phones with 24GB RAM and the extra 12GB swap file doesn't seem to trick it. Will try to load an IQ2 quant via PocketPal and report back. Downloading as we speak. If that one doesn't work, it's gonna have to be IQ1\\_XSS, but other users have already reported on that, so I'm not gonna do it again. \n\n  \n**IMPORTANT:**  \nThe performance WILL drop the more you talk and the the more you fill up the context. Both the prompt processing speed as well as the token generation speed will take a hit. At some point you will not be able to continue the conversation, not because the token generation speed drops so much, but because the prompt processing speed is too slow and it takes ages to read the entire context before it responds. The token generation speed drops linearly, but the prompt processing speed seems to drop exponentially. \n\nWhat that means is that realistically, when you're running a 14B model on your phone, if you enable thinking, you'll be able to ask it about 2 or 3 questions before the prompt processing speed becomes so slow that you'll prefer to start a new chat. With thinking disabled you'll get 4-5 questions before it becomes annoyingly slow. Again, the token generation speed doesn't drop that much. It goes from 5.5t/s to 4.5t/s, so the AI still answers reasonably fast. The problem is that you will wait ages until it starts answering. \n\nPS: phones with 12GB RAM will not be able to run 14B models because Android is a slut for RAM and takes up a lot. 16GB is minimum for 14B, and 24GB is recommended for peace of mind. I got the 16GB version because I just couldn't justify the extra price for the 24GB model and also because it's almost unobtanium and it involved buying it from another country and waiting ages. If you can find a 24GB version for a decent price, go for that. If not, 16GB is also fine. Keep in mind that the issue with the prompt proccessing speed is NOT solved with extra RAM. You'll still only be able to get 2-3 questions in with thinking and 4-5 no\\_think before it turns into a snail. ",
          "author_fullname": "t2_ix5dvt5p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My experience with 14B LLMs on phones with Snapdragon 8 Elite",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpzvtx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 17,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 17,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751472566,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m making this thread because weeks ago when I looked up this information, I could barely even find confirmation that it&amp;#39;s possible to run 14B models on phones. In the meantime I got a OnePlus 13 with 16GB of RAM. After tinkering with different models and apps for half a day, I figured I give my feedback for the people who are interested in this specific scenario. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m used to running 32B models on my PC and after many (subjective) tests I realized that modern 14B models are not far behind in capabilities, at least for my use-cases. I find 8B models kinda meh (I&amp;#39;m warming up to them lately), but my obsession was to be able to run 14B models on a phone, so here we are. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Key Points:&lt;/strong&gt;&lt;br/&gt;\nQwen3 14B loaded via MNN Chat runs decent, but the performance is not consistent. You can expect anywhere from 4.5-7 tokens per second, but the overall performance is around 5.5t/s. I don&amp;#39;t know exactly what quantization this models uses because MNN Chat doesn&amp;#39;t say it. My guess, based on the file size, is that it&amp;#39;s either Q4_K_S or IQ4. Could also be Q4_K_M but the file seems rather small for that so I have my doubts. &lt;/p&gt;\n\n&lt;p&gt;Qwen3 8B runs at around 8 tokens per second, but again I don&amp;#39;t know what quantization. Based on the file size, I&amp;#39;m guessing it&amp;#39;s Q6_K_M. I was kinda expecting a bit more here, but whatever. 8t/s is around reading/thinking speed for me, so I&amp;#39;m ok with that. &lt;/p&gt;\n\n&lt;p&gt;I also used PocketPal to run some abliterated versions of Qwen3 14B at Q4_K_M. Performance was similar to MNN Chat which surprised me since everyone was saying that MNN Chat should provide a significant boost in performance since it&amp;#39;s optimized to work with Snapdragon NPUs. Maybe at this model size the VRAM bandwidth is the bottleneck so the performance improvements are not obvious anymore. &lt;/p&gt;\n\n&lt;p&gt;Enabling or disabling thinking doesn&amp;#39;t seem to affect the speed directly, but it will affect it indirectly. More on that later. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the process of downloading Qwen3-30B-A3B. By all acounts it should not fit in VRAM, but OnePlus has that virtual memory thing that allows you to expand the RAM by an extra 12GB. It will use the UFS storage obviously. &lt;del&gt;This should put me at 16+12=28GB of RAM which should allow me to load the model.&lt;/del&gt; LE: never mind. The version provided by MNN Chat doesn&amp;#39;t load. I think it&amp;#39;s meant for phones with 24GB RAM and the extra 12GB swap file doesn&amp;#39;t seem to trick it. Will try to load an IQ2 quant via PocketPal and report back. Downloading as we speak. If that one doesn&amp;#39;t work, it&amp;#39;s gonna have to be IQ1_XSS, but other users have already reported on that, so I&amp;#39;m not gonna do it again. &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt;&lt;br/&gt;\nThe performance WILL drop the more you talk and the the more you fill up the context. Both the prompt processing speed as well as the token generation speed will take a hit. At some point you will not be able to continue the conversation, not because the token generation speed drops so much, but because the prompt processing speed is too slow and it takes ages to read the entire context before it responds. The token generation speed drops linearly, but the prompt processing speed seems to drop exponentially. &lt;/p&gt;\n\n&lt;p&gt;What that means is that realistically, when you&amp;#39;re running a 14B model on your phone, if you enable thinking, you&amp;#39;ll be able to ask it about 2 or 3 questions before the prompt processing speed becomes so slow that you&amp;#39;ll prefer to start a new chat. With thinking disabled you&amp;#39;ll get 4-5 questions before it becomes annoyingly slow. Again, the token generation speed doesn&amp;#39;t drop that much. It goes from 5.5t/s to 4.5t/s, so the AI still answers reasonably fast. The problem is that you will wait ages until it starts answering. &lt;/p&gt;\n\n&lt;p&gt;PS: phones with 12GB RAM will not be able to run 14B models because Android is a slut for RAM and takes up a lot. 16GB is minimum for 14B, and 24GB is recommended for peace of mind. I got the 16GB version because I just couldn&amp;#39;t justify the extra price for the 24GB model and also because it&amp;#39;s almost unobtanium and it involved buying it from another country and waiting ages. If you can find a 24GB version for a decent price, go for that. If not, 16GB is also fine. Keep in mind that the issue with the prompt proccessing speed is NOT solved with extra RAM. You&amp;#39;ll still only be able to get 2-3 questions in with thinking and 4-5 no_think before it turns into a snail. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lpzvtx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "schizo_poster",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpzvtx/my_experience_with_14b_llms_on_phones_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpzvtx/my_experience_with_14b_llms_on_phones_with/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751472566,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bonjour, \nJe cherche une IA gratuite ou payante capable de résumer, de faire des fiches de livres PDF ou EPUB de plusieurs centaines de pages. Ce sont des livres sur tous les thèmes que je n’ai pas le temps de lire (actualités, santé, philosophie, etc…)\nMerci de votre aide",
          "author_fullname": "t2_7rkdpwuo1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "IA pour résumer des livres ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpzk03",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751471821,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bonjour, \nJe cherche une IA gratuite ou payante capable de résumer, de faire des fiches de livres PDF ou EPUB de plusieurs centaines de pages. Ce sont des livres sur tous les thèmes que je n’ai pas le temps de lire (actualités, santé, philosophie, etc…)\nMerci de votre aide&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpzk03",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "zelig2004",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpzk03/ia_pour_résumer_des_livres/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpzk03/ia_pour_résumer_des_livres/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751471821,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I can get RTX 2080 TI 22GBs for around 350 USD per. Are they a good deal for running LLMs locally using LMStudio?\n\nThe plan is to get a cheap CPU with a desktop motherboard that has 4 PCIE slots. \n\nI will likely get a Ryzen 5 3600 with an ATX B450 board and 4 sticks of 16gb DDR4 ram totalling 64gb.\n\nI think some B450 boards have 4 slots? One concern of mine is that some of the slots will probably be PCIE 3.0 x1 \n\nThen I’ll probably start with 2 gpus and maybe add more in the future.\n\nAre there any issues with this plan? I’ll reply to comments as best I can if clarification is needed.\n\nI got the idea because I wanted a strix halo machine for ai but I realised that with such a cheap 22gb card, it’ll end up cheaper than the 118gb Strix Halo machine. 4x 22gb should get me 88gbs\n\nThough the plan right now is to get two gpus. The total cost should end up less than 1000 usd.\n\nTwo gpus for 350 each\nCPU and motherboard for 80\n64gb Ram for 60\nPsu for 100\nCheapo Chinese case for 20 dollars\n\n",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 2080 TI 22gb Build",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpz46u",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751470793,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can get RTX 2080 TI 22GBs for around 350 USD per. Are they a good deal for running LLMs locally using LMStudio?&lt;/p&gt;\n\n&lt;p&gt;The plan is to get a cheap CPU with a desktop motherboard that has 4 PCIE slots. &lt;/p&gt;\n\n&lt;p&gt;I will likely get a Ryzen 5 3600 with an ATX B450 board and 4 sticks of 16gb DDR4 ram totalling 64gb.&lt;/p&gt;\n\n&lt;p&gt;I think some B450 boards have 4 slots? One concern of mine is that some of the slots will probably be PCIE 3.0 x1 &lt;/p&gt;\n\n&lt;p&gt;Then I’ll probably start with 2 gpus and maybe add more in the future.&lt;/p&gt;\n\n&lt;p&gt;Are there any issues with this plan? I’ll reply to comments as best I can if clarification is needed.&lt;/p&gt;\n\n&lt;p&gt;I got the idea because I wanted a strix halo machine for ai but I realised that with such a cheap 22gb card, it’ll end up cheaper than the 118gb Strix Halo machine. 4x 22gb should get me 88gbs&lt;/p&gt;\n\n&lt;p&gt;Though the plan right now is to get two gpus. The total cost should end up less than 1000 usd.&lt;/p&gt;\n\n&lt;p&gt;Two gpus for 350 each\nCPU and motherboard for 80\n64gb Ram for 60\nPsu for 100\nCheapo Chinese case for 20 dollars&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpz46u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751470793,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I remember when I first downloaded cursor last year, the privacy was on by default, and now not at all. I never selected this embedding thing, but I guess it is automatically turned on. I work in Germany where I do not even dare to use these already, but I am not sure if I can even trust these at all as I worry that the companies will go nuts if they find out about this. Embeddings can be decoded easily, I am literally working on a project where given arbitrary embeddings I am training models to decode stuff to reduce the data storage for some stuff and other use cases.\n\nI am looking for cursor alternatives, as I am not confident that my code snippets will not be used for training or just kept on servers. In hard privacy, I do lose out on many features but on lose ones my embeddings, code snippets etc. will be stored.\n\nAll these models and companies are popping up everywhere and they really need your data it feels like? Google is giving away hundreds of calls everyday from their claude code like thing, and cursor which I loved to use is like this now.\n\nAm I being paranoid and trust their SOC-2 ratings, or their statements etc.? Cursor is trustworthy and I should not bother?\n\nOR I should start building my own tool? IMO this is the ultimate data to collect, your literal questions, doubts etc. so I just wanted to know how do people feel here..",
          "author_fullname": "t2_yg5ikrzgv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cursor terms and conditions seem to be changing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 92,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpz355",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/qwcSnDFbV3csKjdPc9EJOZIWwg5y2KXL0oCoydOX6Lo.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751470731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I remember when I first downloaded cursor last year, the privacy was on by default, and now not at all. I never selected this embedding thing, but I guess it is automatically turned on. I work in Germany where I do not even dare to use these already, but I am not sure if I can even trust these at all as I worry that the companies will go nuts if they find out about this. Embeddings can be decoded easily, I am literally working on a project where given arbitrary embeddings I am training models to decode stuff to reduce the data storage for some stuff and other use cases.&lt;/p&gt;\n\n&lt;p&gt;I am looking for cursor alternatives, as I am not confident that my code snippets will not be used for training or just kept on servers. In hard privacy, I do lose out on many features but on lose ones my embeddings, code snippets etc. will be stored.&lt;/p&gt;\n\n&lt;p&gt;All these models and companies are popping up everywhere and they really need your data it feels like? Google is giving away hundreds of calls everyday from their claude code like thing, and cursor which I loved to use is like this now.&lt;/p&gt;\n\n&lt;p&gt;Am I being paranoid and trust their SOC-2 ratings, or their statements etc.? Cursor is trustworthy and I should not bother?&lt;/p&gt;\n\n&lt;p&gt;OR I should start building my own tool? IMO this is the ultimate data to collect, your literal questions, doubts etc. so I just wanted to know how do people feel here..&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/74fqbljpdhaf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/74fqbljpdhaf1.png?auto=webp&amp;s=853335557ece427da67ae8c37aebee1b51e1c936",
                  "width": 1800,
                  "height": 1190
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/74fqbljpdhaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e2f7f44ab941b2d62b243a148995232b0944256b",
                    "width": 108,
                    "height": 71
                  },
                  {
                    "url": "https://preview.redd.it/74fqbljpdhaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b2d2770b71bd1a86f833c4f3cdcc52835250ba69",
                    "width": 216,
                    "height": 142
                  },
                  {
                    "url": "https://preview.redd.it/74fqbljpdhaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7957fa2b7997732f5c450dd733347644e9a0c831",
                    "width": 320,
                    "height": 211
                  },
                  {
                    "url": "https://preview.redd.it/74fqbljpdhaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b7436040774882ae1855665ecbca193d10edb55",
                    "width": 640,
                    "height": 423
                  },
                  {
                    "url": "https://preview.redd.it/74fqbljpdhaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5f9541b6169e4afdf3e32f0163901163a50361e7",
                    "width": 960,
                    "height": 634
                  },
                  {
                    "url": "https://preview.redd.it/74fqbljpdhaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=012eaaf0f88e741e70504f80f822a79adc9420a5",
                    "width": 1080,
                    "height": 714
                  }
                ],
                "variants": {},
                "id": "3KxRaEqC6lj7-BXmn-1zZHWmlw3KqA6koqEh6YsfrAc"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpz355",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Desperate_Rub_1352",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpz355/cursor_terms_and_conditions_seem_to_be_changing/",
          "stickied": false,
          "url": "https://i.redd.it/74fqbljpdhaf1.png",
          "subreddit_subscribers": 494001,
          "created_utc": 1751470731,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As per the title, does the cpu not matter at all?\n\nI want to use lm studio and I know there’s an option for cpu threads to use.\n\nI see some posts before where people say that CPU doesn’t matter but I have never seen an explanation as to why beyond “only memory bandwidth matters”\n\nDoes the cpu not get used for loading the model?\n\nAlso, wouldn’t newer CPUs on something like a PCIE 5.0 motherboard help? Especially if I want to run more than one GPU and I will have to end up using x4 for the gpus.",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CPU importance in GPU based LLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpyumi",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751470179,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per the title, does the cpu not matter at all?&lt;/p&gt;\n\n&lt;p&gt;I want to use lm studio and I know there’s an option for cpu threads to use.&lt;/p&gt;\n\n&lt;p&gt;I see some posts before where people say that CPU doesn’t matter but I have never seen an explanation as to why beyond “only memory bandwidth matters”&lt;/p&gt;\n\n&lt;p&gt;Does the cpu not get used for loading the model?&lt;/p&gt;\n\n&lt;p&gt;Also, wouldn’t newer CPUs on something like a PCIE 5.0 motherboard help? Especially if I want to run more than one GPU and I will have to end up using x4 for the gpus.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpyumi",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpyumi/cpu_importance_in_gpu_based_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpyumi/cpu_importance_in_gpu_based_llm/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751470179,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hmm. Should I get the Huawei Atlas cards ,?\n\nI to also  believe that Nvidia will get royally screwed over because the USA is going against China instead of working together ",
          "author_fullname": "t2_oy3c84euj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Huawei Open Source AI Model Optimized for Ascend Hardware -- China Keeps Beating USA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpyt3t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/IBgoK9CnvnM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Huawei Open Source AI Model Optimized for Ascend Hardware -- China Keeps Beating USA\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "height": 200
          },
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Huawei Open Source AI Model Optimized for Ascend Hardware -- China Keeps Beating USA",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/IBgoK9CnvnM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Huawei Open Source AI Model Optimized for Ascend Hardware -- China Keeps Beating USA\"&gt;&lt;/iframe&gt;",
              "author_name": "Eli the Computer Guy",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/IBgoK9CnvnM/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@elithecomputerguy"
            }
          },
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {
            "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/IBgoK9CnvnM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Huawei Open Source AI Model Optimized for Ascend Hardware -- China Keeps Beating USA\"&gt;&lt;/iframe&gt;",
            "width": 356,
            "scrolling": false,
            "media_domain_url": "https://www.redditmedia.com/mediaembed/1lpyt3t",
            "height": 200
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/sHYeEp5CUvVlL9KVHDFkmxaOdlnbigXf1-09IGYAh3U.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=6491abac7372738afce63b8a0f0c59de2d7db793",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "rich:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751470079,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "youtu.be",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hmm. Should I get the Huawei Atlas cards ,?&lt;/p&gt;\n\n&lt;p&gt;I to also  believe that Nvidia will get royally screwed over because the USA is going against China instead of working together &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://youtu.be/IBgoK9CnvnM?si=NEww2SC-FTIlPS-p",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/sHYeEp5CUvVlL9KVHDFkmxaOdlnbigXf1-09IGYAh3U.jpeg?auto=webp&amp;s=ba34ffa2842c3ef95e9e990b49abae8c7872b1a3",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/sHYeEp5CUvVlL9KVHDFkmxaOdlnbigXf1-09IGYAh3U.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f82fa0ef9ebf68daf4794730c2b490b6d9535bfa",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/sHYeEp5CUvVlL9KVHDFkmxaOdlnbigXf1-09IGYAh3U.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=542ba06dfef89b51bbd2ca454f36e35a284d8bc5",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/sHYeEp5CUvVlL9KVHDFkmxaOdlnbigXf1-09IGYAh3U.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=78457fdded67afccea3406e42df47465e076b691",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "sHYeEp5CUvVlL9KVHDFkmxaOdlnbigXf1-09IGYAh3U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lpyt3t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sub_RedditTor",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpyt3t/huawei_open_source_ai_model_optimized_for_ascend/",
          "stickied": false,
          "url": "https://youtu.be/IBgoK9CnvnM?si=NEww2SC-FTIlPS-p",
          "subreddit_subscribers": 494001,
          "created_utc": 1751470079,
          "num_crossposts": 0,
          "media": {
            "type": "youtube.com",
            "oembed": {
              "provider_url": "https://www.youtube.com/",
              "version": "1.0",
              "title": "Huawei Open Source AI Model Optimized for Ascend Hardware -- China Keeps Beating USA",
              "type": "video",
              "thumbnail_width": 480,
              "height": 200,
              "width": 356,
              "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/IBgoK9CnvnM?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Huawei Open Source AI Model Optimized for Ascend Hardware -- China Keeps Beating USA\"&gt;&lt;/iframe&gt;",
              "author_name": "Eli the Computer Guy",
              "provider_name": "YouTube",
              "thumbnail_url": "https://i.ytimg.com/vi/IBgoK9CnvnM/hqdefault.jpg",
              "thumbnail_height": 360,
              "author_url": "https://www.youtube.com/@elithecomputerguy"
            }
          },
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hardware is a mini PC with AMD's Ryzen AI MAX 395 APU with 128GB RAM. Model is llama-4-scout, which is an MOE with 16B active and 109B total parameters.\n\nUI: GAIA, our fork of Open WebUI, that offers out-of-box Lemonade integration, a one-click installer, and electron.js app experience. [https://github.com/amd/gaia](https://github.com/amd/gaia)\n\nInference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU's Radeon 8060S GPU. [https://github.com/lemonade-sdk/lemonade](https://github.com/lemonade-sdk/lemonade)\n\nI found it cool that a model of this size with VLM capability could achieve usable TPS on a mini PC and wanted to see if others were excited as well.\n\nFull disclosure: prompt processing time (pp) was 13 seconds, and I edited that part out when making the video. Mentioned this in the post title and video caption for maximum transparency. I find 13 seconds usable for this model+usecase, but not very entertaining in a Reddit video.",
          "author_fullname": "t2_1m2ckixcqh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "llama-4-scout-17B-16E GGUF running on Strix Halo (Ryzen AI MAX 395 + 128GB) (13s prompt processing edited out)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpy8nv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 69,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/e6ao7yjh5haf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/e6ao7yjh5haf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e6ao7yjh5haf1/DASHPlaylist.mpd?a=1754149686%2CMzkxMWIyOWFlMjBmZGZkYmRmYjIwMDllYWE3YjQzZGFhOTEzMDcyMzAxMDMzMDNjODhlYjg1YWViNmZkZDk3Nw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 22,
              "hls_url": "https://v.redd.it/e6ao7yjh5haf1/HLSPlaylist.m3u8?a=1754149686%2COTUzNDBiZjY4MmM4ZDI1Y2QyYzE0YmIzM2YyMzM0OWY0YTk5NDM4OTg1NDUxZGVjNjYwODc4NTI4MDU0YzExMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 69,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=e1f718d7079c60f94f735410900431972636cede",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751468754,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hardware is a mini PC with AMD&amp;#39;s Ryzen AI MAX 395 APU with 128GB RAM. Model is llama-4-scout, which is an MOE with 16B active and 109B total parameters.&lt;/p&gt;\n\n&lt;p&gt;UI: GAIA, our fork of Open WebUI, that offers out-of-box Lemonade integration, a one-click installer, and electron.js app experience. &lt;a href=\"https://github.com/amd/gaia\"&gt;https://github.com/amd/gaia&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Inference server: Lemonade, our AMD-first OpenAI compatible server, running llama.cpp+Vulkan in the backend on the APU&amp;#39;s Radeon 8060S GPU. &lt;a href=\"https://github.com/lemonade-sdk/lemonade\"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I found it cool that a model of this size with VLM capability could achieve usable TPS on a mini PC and wanted to see if others were excited as well.&lt;/p&gt;\n\n&lt;p&gt;Full disclosure: prompt processing time (pp) was 13 seconds, and I edited that part out when making the video. Mentioned this in the post title and video caption for maximum transparency. I find 13 seconds usable for this model+usecase, but not very entertaining in a Reddit video.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/e6ao7yjh5haf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?format=pjpg&amp;auto=webp&amp;s=4b1c4b76e0dc99f437a47284c476d6bb6d814dc1",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7edee4961fc0944fa68d82b0e8f7d4f6a9b508f6",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=95d1a5633971ac0518c4154e32d5c957e8e0c415",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d4b2d3baafd68805600c14522f04b267109c6bd9",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d3ba311dbd4239ec1df6efeb77952b592fa1f126",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=7c1f74c006b8d794c8ce3922815de33fb5439bf6",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=739c5db6d12b6704b26154959af635576548016f",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "am1mM2cxa2g1aGFmMQV6SVFBQzrbFE16bYnvbQZAeh3r0dX_wLOv44vX6S3R"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lpy8nv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jfowers_amd",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpy8nv/llama4scout17b16e_gguf_running_on_strix_halo/",
          "stickied": false,
          "url": "https://v.redd.it/e6ao7yjh5haf1",
          "subreddit_subscribers": 494001,
          "created_utc": 1751468754,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/e6ao7yjh5haf1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/e6ao7yjh5haf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/e6ao7yjh5haf1/DASHPlaylist.mpd?a=1754149686%2CMzkxMWIyOWFlMjBmZGZkYmRmYjIwMDllYWE3YjQzZGFhOTEzMDcyMzAxMDMzMDNjODhlYjg1YWViNmZkZDk3Nw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 22,
              "hls_url": "https://v.redd.it/e6ao7yjh5haf1/HLSPlaylist.m3u8?a=1754149686%2COTUzNDBiZjY4MmM4ZDI1Y2QyYzE0YmIzM2YyMzM0OWY0YTk5NDM4OTg1NDUxZGVjNjYwODc4NTI4MDU0YzExMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone! \n\nLike many of you, I've been running powerful local models like LLaMA 4, Phi-3, and OpenHermes on my own hardware, constantly refining prompts to squeeze out better results. I’ve also experimented with top cloud-based models like GPT-4.5, Claude 4, and Gemini 2.5 to compare performance and capabilities. My workflow was a disaster - I had prompts scattered across text files, different versions in random folders, and no idea which variation performed best for different models.\n\nLast month, I finally snapped when I accidentally overwrote a prompt that took me hours to perfect. So I built PromptBuild.ai - think Git for prompts but with a focus on testing and performance tracking.\n\n**What it does:**\n- Version control for all your prompts (see exactly what changed between versions)\n- Test different prompt variations side by side \n- Track which prompts work best with which models\n- Score responses to build a performance history\n- Organize prompts by project (I have separate projects for coding assistants, creative writing, data analysis, etc.)\n\n**Why I think you'll find it useful:**\n- When you're testing the same prompt across different models (Llama 4 vs Phi-3 vs Claude 4), you can track which variations work best for each\n- Built-in variable system - so you can have template prompts with {{variables}} that you fill in during testing\n- Interactive testing playground - test prompts with variable substitution and capture responses\n- Performance scoring - rate each test run (1-5 stars) and build a performance history\n- Export/import - so you can share prompt collections with the community\n\nThe current version is completely **FREE** - unlimited teams, projects and prompts. I'm working on paid tiers with API access and team features, but the core functionality will always be free for individual users.\n\nI built this because I needed it myself, but figured others might be dealing with the same prompt management chaos. Would love your feedback!\n\nTry it out: [promptbuild.ai](https://promptbuild.ai)\n\nHappy to answer any questions about the implementation or features!",
          "author_fullname": "t2_cag8j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Finally solved my prompt versioning nightmare - built a tool to manage prompts like code",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpy5es",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751468542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt;\n\n&lt;p&gt;Like many of you, I&amp;#39;ve been running powerful local models like LLaMA 4, Phi-3, and OpenHermes on my own hardware, constantly refining prompts to squeeze out better results. I’ve also experimented with top cloud-based models like GPT-4.5, Claude 4, and Gemini 2.5 to compare performance and capabilities. My workflow was a disaster - I had prompts scattered across text files, different versions in random folders, and no idea which variation performed best for different models.&lt;/p&gt;\n\n&lt;p&gt;Last month, I finally snapped when I accidentally overwrote a prompt that took me hours to perfect. So I built PromptBuild.ai - think Git for prompts but with a focus on testing and performance tracking.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;\n- Version control for all your prompts (see exactly what changed between versions)\n- Test different prompt variations side by side \n- Track which prompts work best with which models\n- Score responses to build a performance history\n- Organize prompts by project (I have separate projects for coding assistants, creative writing, data analysis, etc.)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Why I think you&amp;#39;ll find it useful:&lt;/strong&gt;\n- When you&amp;#39;re testing the same prompt across different models (Llama 4 vs Phi-3 vs Claude 4), you can track which variations work best for each\n- Built-in variable system - so you can have template prompts with {{variables}} that you fill in during testing\n- Interactive testing playground - test prompts with variable substitution and capture responses\n- Performance scoring - rate each test run (1-5 stars) and build a performance history\n- Export/import - so you can share prompt collections with the community&lt;/p&gt;\n\n&lt;p&gt;The current version is completely &lt;strong&gt;FREE&lt;/strong&gt; - unlimited teams, projects and prompts. I&amp;#39;m working on paid tiers with API access and team features, but the core functionality will always be free for individual users.&lt;/p&gt;\n\n&lt;p&gt;I built this because I needed it myself, but figured others might be dealing with the same prompt management chaos. Would love your feedback!&lt;/p&gt;\n\n&lt;p&gt;Try it out: &lt;a href=\"https://promptbuild.ai\"&gt;promptbuild.ai&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Happy to answer any questions about the implementation or features!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lpy5es",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "error7891",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpy5es/finally_solved_my_prompt_versioning_nightmare/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpy5es/finally_solved_my_prompt_versioning_nightmare/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751468542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI’m looking for a library, framework, or product that allows LLM-powered agents to interact with a browser. Ideally, the LLM agent should be able to control the browser similarly to tools like puppeteer or playwright, but with the added capability to access and interact with the browser’s DevTools — for example, to inspect network activity, console logs, or manipulate the DOM beyond simple user simulation.\n\nDoes something like this already exist? Or is there any project combining LLM agents with browser automation and DevTools access?\n\nThanks in advance for any help",
          "author_fullname": "t2_624o9cgn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Browser-use with devtools access",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpwm1f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751464816,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m looking for a library, framework, or product that allows LLM-powered agents to interact with a browser. Ideally, the LLM agent should be able to control the browser similarly to tools like puppeteer or playwright, but with the added capability to access and interact with the browser’s DevTools — for example, to inspect network activity, console logs, or manipulate the DOM beyond simple user simulation.&lt;/p&gt;\n\n&lt;p&gt;Does something like this already exist? Or is there any project combining LLM agents with browser automation and DevTools access?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any help&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpwm1f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ReddaHawk",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpwm1f/browseruse_with_devtools_access/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpwm1f/browseruse_with_devtools_access/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751464816,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We just released AlgoTune which challenges agents to optimize the runtime of 100+ algorithms including gzip compression, AES encryption, and PCA. We also release an agent, AlgoTuner, that enables LMs to iteratively develop efficient code.\n\nhttps://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;format=png&amp;auto=webp&amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b\n\nOur results show that sometimes frontier LMs are able to find surface level optimizations, but they don't come up with novel algos. There is still a long way to go: the current best AlgoTune score is 1.76x achieved by o4-mini, we think the best potential score is 100x+. \n\nhttps://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;format=png&amp;auto=webp&amp;s=963893a72dd76d5407871779ed74fef9bda48c57\n\nFor full results + paper + code: [algotune.io](http://algotune.io)",
          "author_fullname": "t2_27dpqdao",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AlgoTune: A new benchmark that tests language models' ability to optimize code runtime",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 41,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "r3vc4rpfugaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 53,
                  "x": 108,
                  "u": "https://preview.redd.it/r3vc4rpfugaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=750ff86b91979a96f3017e5be944266822476e30"
                },
                {
                  "y": 106,
                  "x": 216,
                  "u": "https://preview.redd.it/r3vc4rpfugaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c48af554aba522222b3d3df3076f7cdcf709f9c"
                },
                {
                  "y": 158,
                  "x": 320,
                  "u": "https://preview.redd.it/r3vc4rpfugaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6613c218297511274e819d6d54df8c6fea4fdf74"
                },
                {
                  "y": 316,
                  "x": 640,
                  "u": "https://preview.redd.it/r3vc4rpfugaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b46f758f72f2e9939de2c65058ffdc95c599c8f1"
                },
                {
                  "y": 474,
                  "x": 960,
                  "u": "https://preview.redd.it/r3vc4rpfugaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a276c5d970b544306afce636b32adaf97ef429a6"
                },
                {
                  "y": 533,
                  "x": 1080,
                  "u": "https://preview.redd.it/r3vc4rpfugaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=daa552db9954f91c201164a83a97f3b9ebedd042"
                }
              ],
              "s": {
                "y": 1001,
                "x": 2027,
                "u": "https://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;format=png&amp;auto=webp&amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b"
              },
              "id": "r3vc4rpfugaf1"
            },
            "tnm4r5tpugaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 61,
                  "x": 108,
                  "u": "https://preview.redd.it/tnm4r5tpugaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b5453d723b437b8600fc66535ad0e1c0fc44c1f"
                },
                {
                  "y": 123,
                  "x": 216,
                  "u": "https://preview.redd.it/tnm4r5tpugaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3aa3cf361eb236b0f5ede3a2e245f972db329a81"
                },
                {
                  "y": 183,
                  "x": 320,
                  "u": "https://preview.redd.it/tnm4r5tpugaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3da3cac9aa60236d6b55cdf4ce8c3737c754cbcc"
                },
                {
                  "y": 366,
                  "x": 640,
                  "u": "https://preview.redd.it/tnm4r5tpugaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=80bd9e4a883c9a6935d13c37fcf41b5a7cee0a97"
                }
              ],
              "s": {
                "y": 520,
                "x": 908,
                "u": "https://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;format=png&amp;auto=webp&amp;s=963893a72dd76d5407871779ed74fef9bda48c57"
              },
              "id": "tnm4r5tpugaf1"
            }
          },
          "name": "t3_1lpwj5j",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=140&amp;height=41&amp;crop=140:41,smart&amp;auto=webp&amp;s=e2dd81634afcb49cf364c0bae9d9ff27972159b3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751464617,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We just released AlgoTune which challenges agents to optimize the runtime of 100+ algorithms including gzip compression, AES encryption, and PCA. We also release an agent, AlgoTuner, that enables LMs to iteratively develop efficient code.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b\"&gt;https://preview.redd.it/r3vc4rpfugaf1.png?width=2027&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e87085f0e6c72c88d9fb5872d240fcb3685b5b5b&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Our results show that sometimes frontier LMs are able to find surface level optimizations, but they don&amp;#39;t come up with novel algos. There is still a long way to go: the current best AlgoTune score is 1.76x achieved by o4-mini, we think the best potential score is 100x+. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=963893a72dd76d5407871779ed74fef9bda48c57\"&gt;https://preview.redd.it/tnm4r5tpugaf1.png?width=908&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=963893a72dd76d5407871779ed74fef9bda48c57&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;For full results + paper + code: &lt;a href=\"http://algotune.io\"&gt;algotune.io&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?auto=webp&amp;s=288006b1dc4f5148e86ed4ca3d950fac993f557d",
                  "width": 1925,
                  "height": 575
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=408951830555b74a7be7d6ddc63011caeb8987a4",
                    "width": 108,
                    "height": 32
                  },
                  {
                    "url": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fc96c2840d9e927340d125d6598301f303e1e27d",
                    "width": 216,
                    "height": 64
                  },
                  {
                    "url": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1014c9a4a006fcd261e616eec722e350c57d1410",
                    "width": 320,
                    "height": 95
                  },
                  {
                    "url": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e65bd26dfe93e8bb59300666373e0af786916bdb",
                    "width": 640,
                    "height": 191
                  },
                  {
                    "url": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=db2ca82f0652c3e6b7d9f1e245289e4c94d76533",
                    "width": 960,
                    "height": 286
                  },
                  {
                    "url": "https://external-preview.redd.it/0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=378276d6c1a0cea44bfbed6040db7a8bc762e9a4",
                    "width": 1080,
                    "height": 322
                  }
                ],
                "variants": {},
                "id": "0-xrX4qzMJegRxSwLNjXIVQsA6XhMhbHQfZkMjkNJvA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lpwj5j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "oripress",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpwj5j/algotune_a_new_benchmark_that_tests_language/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751464617,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It’s an ONNX GenAI model converter [convert-to-genai](https://huggingface.co/spaces/xiaoyao9184/convert-to-genai). \n\nThe free Hugging Face Space offers 18GB of RAM — that’s enough to convert Qwen2.5 0.5B, but other models, even 1B ones, require more memory.\n\n",
          "author_fullname": "t2_6d1v70nh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Does anyone have enough memory space to run this?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpw43h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751463534,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It’s an ONNX GenAI model converter &lt;a href=\"https://huggingface.co/spaces/xiaoyao9184/convert-to-genai\"&gt;convert-to-genai&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;The free Hugging Face Space offers 18GB of RAM — that’s enough to convert Qwen2.5 0.5B, but other models, even 1B ones, require more memory.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q.png?auto=webp&amp;s=a4d2673e9ab8f2564c6afa7dff6f3770cd796683",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=67700b8182dd1ea359062fe9a8b2fc70c01e5db9",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4b88eb0ed42cb0b127685568240a57dc5586524c",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c0c3df40e832e2efe0fcdaba9fe7f1abd85c2c27",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0c0156b0443e826d5faba2ecfb7efa7d79a8248a",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=353fd2b1803f7681f7dfdc08a2ad4c948d77f819",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c0f2bb279e7a8dbeb6f82c89e43cc821d3f289c",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "i6cB110VYI_H5zTEzDnVoTUMIhoOInKTjhaIYeZjp5Q"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lpw43h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Ok_Fig5484",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpw43h/does_anyone_have_enough_memory_space_to_run_this/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpw43h/does_anyone_have_enough_memory_space_to_run_this/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751463534,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nI'm currently learning React for front-end development and planning to start learning Flask for the backend. My goal is to become a full-stack developer with a strong focus on AI technologies, especially areas like Generative AI and Agentic AI.\n\nI'm also interested in Python, which is why Flask seems like a good fit, and I’ve heard it's lightweight and beginner-friendly. Eventually, I want to transition into AI development, so I feel like learning full-stack with Python will give me a solid foundation.\n\nAm I on the right path? Or would you recommend learning something else (like FastAPI, Django, or maybe diving directly into AI tools and frameworks)?\n\nAny advice or guidance is appreciated — especially from folks who've gone down this road. 🙏\n\nThanks in advance!",
          "author_fullname": "t2_1sdhfw2vad",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Am I on the right path? Learning React + Flask for Full Stack + AI Career Goals",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpw26d",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.42,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751463392,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently learning React for front-end development and planning to start learning Flask for the backend. My goal is to become a full-stack developer with a strong focus on AI technologies, especially areas like Generative AI and Agentic AI.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also interested in Python, which is why Flask seems like a good fit, and I’ve heard it&amp;#39;s lightweight and beginner-friendly. Eventually, I want to transition into AI development, so I feel like learning full-stack with Python will give me a solid foundation.&lt;/p&gt;\n\n&lt;p&gt;Am I on the right path? Or would you recommend learning something else (like FastAPI, Django, or maybe diving directly into AI tools and frameworks)?&lt;/p&gt;\n\n&lt;p&gt;Any advice or guidance is appreciated — especially from folks who&amp;#39;ve gone down this road. 🙏&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpw26d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "harsh_a024",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpw26d/am_i_on_the_right_path_learning_react_flask_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpw26d/am_i_on_the_right_path_learning_react_flask_for/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751463392,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "“Gordon” is a local-LLM project I’m working on, and it occurred to me that 2x Arc Pro B60 Dual GPUs could be a way to get to the 96Gb of VRAM I will need without spending $10K on an RTX Pro 6000. The screenshots are Hal’s (my ChatGPT) views. I thought I’d get some actual hoomans to offer their knowledgeable views and opinions. What say you? ",
          "author_fullname": "t2_a2gk6kba",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "96Gb VRAM without spending $10k on an RTX Pro 6000..?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "s3xt0yk6rgaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 79,
                  "x": 108,
                  "u": "https://preview.redd.it/s3xt0yk6rgaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=49c143b15aa9a411e2eb47dfb4025c7723632d57"
                },
                {
                  "y": 159,
                  "x": 216,
                  "u": "https://preview.redd.it/s3xt0yk6rgaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a48ec862fb08072b37009c03d52922c46402d1e"
                },
                {
                  "y": 235,
                  "x": 320,
                  "u": "https://preview.redd.it/s3xt0yk6rgaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=77f1e4e11b97a382ec7d5519751319de6294a209"
                },
                {
                  "y": 471,
                  "x": 640,
                  "u": "https://preview.redd.it/s3xt0yk6rgaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=da90c5505275877a9d0bf7b58ae1cd77536f4268"
                },
                {
                  "y": 707,
                  "x": 960,
                  "u": "https://preview.redd.it/s3xt0yk6rgaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=65355455a91d1b5c7989b0cc7b3ee929606330c3"
                },
                {
                  "y": 796,
                  "x": 1080,
                  "u": "https://preview.redd.it/s3xt0yk6rgaf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=756cc6538719c6fa7989a54fec8192eb22e9ec2c"
                }
              ],
              "s": {
                "y": 1230,
                "x": 1668,
                "u": "https://preview.redd.it/s3xt0yk6rgaf1.jpg?width=1668&amp;format=pjpg&amp;auto=webp&amp;s=44449321d048c78a5c952de0abb037a2cf26ed1b"
              },
              "id": "s3xt0yk6rgaf1"
            },
            "s2hsuzh6rgaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 201,
                  "x": 108,
                  "u": "https://preview.redd.it/s2hsuzh6rgaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2bc3cc4b71253741e747c770f0fe38441d14187f"
                },
                {
                  "y": 403,
                  "x": 216,
                  "u": "https://preview.redd.it/s2hsuzh6rgaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4d7813b3b21ef7e002c09b50edce459b120e7cec"
                },
                {
                  "y": 597,
                  "x": 320,
                  "u": "https://preview.redd.it/s2hsuzh6rgaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e23091a9884a576202d02fd147d9fe6bfcabea66"
                },
                {
                  "y": 1194,
                  "x": 640,
                  "u": "https://preview.redd.it/s2hsuzh6rgaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c8acb84cd9864445ba1fa45ec8fa24ea530821c3"
                },
                {
                  "y": 1791,
                  "x": 960,
                  "u": "https://preview.redd.it/s2hsuzh6rgaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=60c878e14028422ad81ee6b62896890e8566cd0c"
                }
              ],
              "s": {
                "y": 1948,
                "x": 1044,
                "u": "https://preview.redd.it/s2hsuzh6rgaf1.jpg?width=1044&amp;format=pjpg&amp;auto=webp&amp;s=91bad78e8dd960a1fb94b282933c9dceec026a04"
              },
              "id": "s2hsuzh6rgaf1"
            },
            "w1e99uj6rgaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 192,
                  "x": 108,
                  "u": "https://preview.redd.it/w1e99uj6rgaf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=73feec83ac845a88c1d7b2d6dba4a904fb58d87b"
                },
                {
                  "y": 384,
                  "x": 216,
                  "u": "https://preview.redd.it/w1e99uj6rgaf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=83b28b90d6de69dca7c2a34c590e202ffbb510cc"
                },
                {
                  "y": 570,
                  "x": 320,
                  "u": "https://preview.redd.it/w1e99uj6rgaf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=87aa3876abb4229281611dc5f2ebac4418d76c08"
                },
                {
                  "y": 1140,
                  "x": 640,
                  "u": "https://preview.redd.it/w1e99uj6rgaf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f360cbfba47aaa133a74f1f267c2d754cbecca71"
                },
                {
                  "y": 1710,
                  "x": 960,
                  "u": "https://preview.redd.it/w1e99uj6rgaf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e961d36a2fc8e11a37c743d297c88f8ec357209f"
                }
              ],
              "s": {
                "y": 1850,
                "x": 1038,
                "u": "https://preview.redd.it/w1e99uj6rgaf1.jpg?width=1038&amp;format=pjpg&amp;auto=webp&amp;s=9104f22138457123342895e8875f38287992d258"
              },
              "id": "w1e99uj6rgaf1"
            }
          },
          "name": "t3_1lpvywm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "w1e99uj6rgaf1",
                "id": 697194121
              },
              {
                "media_id": "s2hsuzh6rgaf1",
                "id": 697194122
              },
              {
                "media_id": "s3xt0yk6rgaf1",
                "id": 697194123
              }
            ]
          },
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "https://b.thumbs.redditmedia.com/SFT9DLkbDmvEcuWU5aODU2MepW6gwhT0-ndX2fQwm4g.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751463155,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;“Gordon” is a local-LLM project I’m working on, and it occurred to me that 2x Arc Pro B60 Dual GPUs could be a way to get to the 96Gb of VRAM I will need without spending $10K on an RTX Pro 6000. The screenshots are Hal’s (my ChatGPT) views. I thought I’d get some actual hoomans to offer their knowledgeable views and opinions. What say you? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lpvywm",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpvywm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "m-gethen",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpvywm/96gb_vram_without_spending_10k_on_an_rtx_pro_6000/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lpvywm",
          "subreddit_subscribers": 494001,
          "created_utc": 1751463155,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been working with LLMs for a while and got tired of manually tracking prompt versions. Made a Python tool that handles this automatically.\n\n# What it does\n\n* Automatically versions your prompts when you commit to git\n* Test prompt changes before committing with `:unstaged` reference\n* Works with any LLM (OpenAI, Anthropic, local models)\n* Zero manual version management\n\n# Quick example\n\n    from llmhq_promptops import get_prompt\n    \n    # Get the working version\n    prompt = get_prompt(\"user-greeting\")\n    \n    # Test uncommitted changes  \n    prompt = get_prompt(\"user-greeting:unstaged\")\n    \n    # Specific version\n    prompt = get_prompt(\"user-greeting:v1.2.0\")\n    \n\n# Install\n\n    pip install llmhq-promptops\n    promptops init repo\n    \n\nThe git hooks handle versioning automatically - PATCH for content changes, MINOR for new variables, MAJOR for breaking changes.\n\nThought this community might find it useful since we all deal with prompt management headaches.\n\nGitHub: [https://github.com/llmhq-hub/promptops](https://github.com/llmhq-hub/promptops)",
          "author_fullname": "t2_1spkozujfu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I built a tool for managing prompts like code in git",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpvwh3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751462982,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been working with LLMs for a while and got tired of manually tracking prompt versions. Made a Python tool that handles this automatically.&lt;/p&gt;\n\n&lt;h1&gt;What it does&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Automatically versions your prompts when you commit to git&lt;/li&gt;\n&lt;li&gt;Test prompt changes before committing with &lt;code&gt;:unstaged&lt;/code&gt; reference&lt;/li&gt;\n&lt;li&gt;Works with any LLM (OpenAI, Anthropic, local models)&lt;/li&gt;\n&lt;li&gt;Zero manual version management&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;Quick example&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;from llmhq_promptops import get_prompt\n\n# Get the working version\nprompt = get_prompt(&amp;quot;user-greeting&amp;quot;)\n\n# Test uncommitted changes  \nprompt = get_prompt(&amp;quot;user-greeting:unstaged&amp;quot;)\n\n# Specific version\nprompt = get_prompt(&amp;quot;user-greeting:v1.2.0&amp;quot;)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Install&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt;pip install llmhq-promptops\npromptops init repo\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The git hooks handle versioning automatically - PATCH for content changes, MINOR for new variables, MAJOR for breaking changes.&lt;/p&gt;\n\n&lt;p&gt;Thought this community might find it useful since we all deal with prompt management headaches.&lt;/p&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/llmhq-hub/promptops\"&gt;https://github.com/llmhq-hub/promptops&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ.png?auto=webp&amp;s=6a6c99ca9be235d457062f87f3b149157c4a26ae",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9491dac3eef562b93c4c641b452ed1c66f284300",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ff720421d1acf755d519c6f53cbc8150ddd3379",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5abd9c79f170a57ec48b9a058ab2d387a5ff9a5a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a949604c681b87d948795220ff74ef38c6d42554",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=945473ff68aaa3a2a4b9caeadbfea5a151b8f32f",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c397c249b778d96b40fc73be47421fcd97fdd990",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "o3PmNP2OL7ctHNAdRCSQSrdK0jT5IPtsBfFR7S-4acQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lpvwh3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "llmhq_official",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpvwh3/i_built_a_tool_for_managing_prompts_like_code_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpvwh3/i_built_a_tool_for_managing_prompts_like_code_in/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751462982,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone, I hope you're doing well.  \nI've built a spaCy-based NER system to extract key information from resumes, such as experience, education, and personal details. However, it's not very accurate and struggles with diverse resume formats.\n\nI'm thinking of switching to a question-answering LLM like Qwen to improve accuracy and flexibility.  \nAre there any existing solutions, models, or frameworks specifically designed for resume parsing using LLMs?\n\nAny suggestions or experiences are appreciated. Thanks in advance!",
          "author_fullname": "t2_1rtjlzqa6c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM-based resume parsing – any models or solutions out there?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpvpqv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751462483,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone, I hope you&amp;#39;re doing well.&lt;br/&gt;\nI&amp;#39;ve built a spaCy-based NER system to extract key information from resumes, such as experience, education, and personal details. However, it&amp;#39;s not very accurate and struggles with diverse resume formats.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m thinking of switching to a question-answering LLM like Qwen to improve accuracy and flexibility.&lt;br/&gt;\nAre there any existing solutions, models, or frameworks specifically designed for resume parsing using LLMs?&lt;/p&gt;\n\n&lt;p&gt;Any suggestions or experiences are appreciated. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpvpqv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Critical_March_3113",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpvpqv/llmbased_resume_parsing_any_models_or_solutions/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpvpqv/llmbased_resume_parsing_any_models_or_solutions/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751462483,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "We released new findings from our Phare LLM Benchmark on bias in leading language models. Instead of traditional \"fill-in-the-blank\" tests, we had 17 leading LLMs generate thousands of stories, then asked them to judge their own patterns.  \nIn short: Leading LLMs can recognise bias but also reproduce harmful stereotypes",
          "author_fullname": "t2_mczcn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Phare Study: LLMs recognise bias but also reproduce harmful stereotypes: an analysis of bias in leading LLMs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 65,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lputq1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=140&amp;height=65&amp;crop=140:65,smart&amp;auto=webp&amp;s=2dda1230a43577704a124405ca9afca4f80dc63a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751460089,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "giskard.ai",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We released new findings from our Phare LLM Benchmark on bias in leading language models. Instead of traditional &amp;quot;fill-in-the-blank&amp;quot; tests, we had 17 leading LLMs generate thousands of stories, then asked them to judge their own patterns.&lt;br/&gt;\nIn short: Leading LLMs can recognise bias but also reproduce harmful stereotypes&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.giskard.ai/knowledge/llms-recognise-bias-but-also-reproduce-harmful-stereotypes",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?auto=webp&amp;s=d512e936947016e27194702eb3c13d1849a91d4e",
                  "width": 2861,
                  "height": 1343
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=85fea417541d0a0fea7c5fb79a9234985a5ff7dd",
                    "width": 108,
                    "height": 50
                  },
                  {
                    "url": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b0198b83b86e0d4c6e7c672fdf1efbb87d754739",
                    "width": 216,
                    "height": 101
                  },
                  {
                    "url": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a8a5d996a131e1ea55956c0a8001b0dfcdd75458",
                    "width": 320,
                    "height": 150
                  },
                  {
                    "url": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9db5e54b4d78cd5050c59470ddab11fa710eb6a3",
                    "width": 640,
                    "height": 300
                  },
                  {
                    "url": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0cc3b8339f34c97c4cab68722ea15a982af3d6ca",
                    "width": 960,
                    "height": 450
                  },
                  {
                    "url": "https://external-preview.redd.it/tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=539a0d88cda9e4318e5c73f94c03973afdc8e00e",
                    "width": 1080,
                    "height": 506
                  }
                ],
                "variants": {},
                "id": "tV3CLM1gW4WwlAqXrhavxH6_d1Arw1EbpTt4XOWqhW8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lputq1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chef1957",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lputq1/phare_study_llms_recognise_bias_but_also/",
          "stickied": false,
          "url": "https://www.giskard.ai/knowledge/llms-recognise-bias-but-also-reproduce-harmful-stereotypes",
          "subreddit_subscribers": 494001,
          "created_utc": 1751460089,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Most of what you read about “AI agents” is either super vague or buried in jargon. I wrote a no-BS explainer that breaks down how modern AI agents actually work, without the marketing fluff. If you’re curious about what’s really happening “under the hood” when people talk about AI agents (or you want to build one yourself), check out: [https://blog.surkar.in/ai-agents-under-the-hood](https://blog.surkar.in/ai-agents-under-the-hood)  \n\n\nHappy to chat or answer questions in the comments :D ",
          "author_fullname": "t2_24ebov49",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI Agents, But Simple and Understandable",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpu8a9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 11,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 11,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=27e62dd2b046ded60c0255815f4690bab1006acd",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751458345,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "blog.surkar.in",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Most of what you read about “AI agents” is either super vague or buried in jargon. I wrote a no-BS explainer that breaks down how modern AI agents actually work, without the marketing fluff. If you’re curious about what’s really happening “under the hood” when people talk about AI agents (or you want to build one yourself), check out: &lt;a href=\"https://blog.surkar.in/ai-agents-under-the-hood\"&gt;https://blog.surkar.in/ai-agents-under-the-hood&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;Happy to chat or answer questions in the comments :D &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://blog.surkar.in/ai-agents-under-the-hood",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?auto=webp&amp;s=a392cfec0e2eb0b0ee43d9b53bda7de99b488941",
                  "width": 1200,
                  "height": 630
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=71049788eae3370cbf9e32a6c9269f2f033d32ff",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=889e98decbe7577d3c5fdc3e523109a75deec23d",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d6e1003f733af482d2fae995b7522b1fb39f4327",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7da6efc88dc4dd9b17784bb73b12de5401e7bc4e",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bb456e0d321cba15e14f8c83e641420764394fe",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77c73cda58b5d5d92bac087ceaaa27251a4e4300",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "NvDs7wJwV1W_MBsZCzSWrMWhoZ65PkI1ziagX4BmUfc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lpu8a9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "thesmallstar",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpu8a9/ai_agents_but_simple_and_understandable/",
          "stickied": false,
          "url": "https://blog.surkar.in/ai-agents-under-the-hood",
          "subreddit_subscribers": 494001,
          "created_utc": 1751458345,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi Guys Just wanted to ask what are the latest updates on the Rp Models. Which ones do you use currently and what model do you think is best ones. Please Advice some models above 8B and less than 30B too which are not censored and unrestricted.",
          "author_fullname": "t2_7ybyipsu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best RP Model Unrestricted/Uncensored",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpt5jv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.52,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751454985,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi Guys Just wanted to ask what are the latest updates on the Rp Models. Which ones do you use currently and what model do you think is best ones. Please Advice some models above 8B and less than 30B too which are not censored and unrestricted.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lpt5jv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sapry123",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpt5jv/best_rp_model_unrestricteduncensored/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpt5jv/best_rp_model_unrestricteduncensored/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751454985,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I would like something like a writing assistant, or summarizer using an LLM, but most of these extensions are tied to services like gpt or gemini, with no option to use your own openai compatible api or local model. ",
          "author_fullname": "t2_i697e",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any good browser extensions that with any OpenAI compatible API or local model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lps7c3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751451618,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I would like something like a writing assistant, or summarizer using an LLM, but most of these extensions are tied to services like gpt or gemini, with no option to use your own openai compatible api or local model. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lps7c3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lemon07r",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lps7c3/any_good_browser_extensions_that_with_any_openai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lps7c3/any_good_browser_extensions_that_with_any_openai/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751451618,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So I'm trying MNN chat but for me it seems to be repeating itself a lot. I tried qwen3 0.6b, and when I try a simple request like\n\nWhat is lasagna?\n\nLascange is a dish that is made from pasta. It is a very popular dish in Italy. The main ingredients are pasta and sauce. The sauce is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is \n\nIs this an inherent MNN issue or just a model issue?",
          "author_fullname": "t2_8hpbax1b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Just me, or MNN chat is looping a lot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lprfbx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751448680,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;m trying MNN chat but for me it seems to be repeating itself a lot. I tried qwen3 0.6b, and when I try a simple request like&lt;/p&gt;\n\n&lt;p&gt;What is lasagna?&lt;/p&gt;\n\n&lt;p&gt;Lascange is a dish that is made from pasta. It is a very popular dish in Italy. The main ingredients are pasta and sauce. The sauce is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is &lt;/p&gt;\n\n&lt;p&gt;Is this an inherent MNN issue or just a model issue?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lprfbx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ExtremeAcceptable289",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751448680,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know llamacpp server and ollama can be used for LLMs, and I have been using ollama but the API has been very limiting. What can I use for VLMs, prioritised for API/speed and model management?\n\nI have 24GB L40 GPU so that shouldnt be an issue. Currently I want to host models like Qwen2.5VL and Moondream.",
          "author_fullname": "t2_9kgt3ez",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What framework would you suggest for hosting and serving VLMs via api?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpr8wf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": true,
          "thumbnail": "self",
          "edited": 1751451024,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751447970,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know llamacpp server and ollama can be used for LLMs, and I have been using ollama but the API has been very limiting. What can I use for VLMs, prioritised for API/speed and model management?&lt;/p&gt;\n\n&lt;p&gt;I have 24GB L40 GPU so that shouldnt be an issue. Currently I want to host models like Qwen2.5VL and Moondream.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpr8wf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CaptTechno",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpr8wf/what_framework_would_you_suggest_for_hosting_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpr8wf/what_framework_would_you_suggest_for_hosting_and/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751447970,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing a prototype project I built called \"Akta\"\n\n[https://github.com/RedDotRocket/akta](https://github.com/RedDotRocket/akta)\n\nIt's an attempt to enable secure and verifiable auth and delegation between AI agents. It establishes a framework for time-bound capability-based access control, allowing agents to delegate tasks and share resources with fine-grained control. The system leverages concepts from Decentralised Identifiers (DIDs) and Verifiable Credentials (VCs) to create a cryptographically and auditable chain of trust for autonomous agent operations. \n\nIn essence, Akta tries to answer what does a \"fully autonomous Agent to Agent authorisation grant look like with no humans in the loop\"? a.k.a an Agent delegating tasks to another Agent of their own accord. The human presence is derived from their position higher up the chain to their Agents (and the agents they delegate to). There is also a CLI and library for creating keys, vc's, based on A2A AgentCards and their nominated capabilities and skillz!\n\n  \nIf you are interested in this idea and want to hack on it with me, let me know. Typical me style, I have way too many uncompleted projects and I am focusing on getting out my main one over the next few weeks. But I do love all this DID stuff and my heart is in this tech, so hopefully this is valuable to someone one out there.",
          "author_fullname": "t2_1rcsl67py8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AKTA - Authenticated Knowledge &amp; Trust Architecture for AI Agents",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpr5dj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751447570,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing a prototype project I built called &amp;quot;Akta&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/RedDotRocket/akta\"&gt;https://github.com/RedDotRocket/akta&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s an attempt to enable secure and verifiable auth and delegation between AI agents. It establishes a framework for time-bound capability-based access control, allowing agents to delegate tasks and share resources with fine-grained control. The system leverages concepts from Decentralised Identifiers (DIDs) and Verifiable Credentials (VCs) to create a cryptographically and auditable chain of trust for autonomous agent operations. &lt;/p&gt;\n\n&lt;p&gt;In essence, Akta tries to answer what does a &amp;quot;fully autonomous Agent to Agent authorisation grant look like with no humans in the loop&amp;quot;? a.k.a an Agent delegating tasks to another Agent of their own accord. The human presence is derived from their position higher up the chain to their Agents (and the agents they delegate to). There is also a CLI and library for creating keys, vc&amp;#39;s, based on A2A AgentCards and their nominated capabilities and skillz!&lt;/p&gt;\n\n&lt;p&gt;If you are interested in this idea and want to hack on it with me, let me know. Typical me style, I have way too many uncompleted projects and I am focusing on getting out my main one over the next few weeks. But I do love all this DID stuff and my heart is in this tech, so hopefully this is valuable to someone one out there.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI.png?auto=webp&amp;s=7b09ced55c5e0ed3f90c4d8b4215d43e8fca173f",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7923fa62981d87a05bfc10ad3e4112dd6715a342",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5aba504c49ecc14aec70a40a5e67b90ca7593fcc",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=99588582f8fa147267d954fc365683502e0f749d",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7fbc00ae70fa577893852c20c9555778c006910d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=cc50a56050fffd15f4953147b41b6f0c20a19b67",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8558527fef5a04e12775a3c37d1f06c28ce961a4",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "_TOJ60J7Rik0SzDYYpdrEXmhHyX4B3aVS93RbjL3OpI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lpr5dj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RedDotRocket",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpr5dj/akta_authenticated_knowledge_trust_architecture/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpr5dj/akta_authenticated_knowledge_trust_architecture/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751447570,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm looking for something that might not exist yet, but I'm curious for suggestions. Essentially, I'd love to have the ChatGPT experience, but with me being able to plug in an open source model API URL to replace the OpenAI model. \n\nFor me, ChatGPT is super convenient to use. You've got a good web UI, a nice mobile app. It does web search as needed, understands when you want to generate an image, or when it should use some extra tools to analyse the image you uploaded. Works with audio and documents. It's just all there in the single package. \n\nI know there's Open WebUI, LM Studio etc. But is there anything else, cross-platform with as many of the above features as possible? Ideally, without too fiddly of a setup when you've already got some LLM API up and running.\n\nIt seems like the open source model performance is comparable these days (DeepSeek R1 at least), but I'm missing the additional glue to make the switch to local and open source.",
          "author_fullname": "t2_y9y2q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Convenient ChatGPT UX Replacement",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpr1fq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751447125,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m looking for something that might not exist yet, but I&amp;#39;m curious for suggestions. Essentially, I&amp;#39;d love to have the ChatGPT experience, but with me being able to plug in an open source model API URL to replace the OpenAI model. &lt;/p&gt;\n\n&lt;p&gt;For me, ChatGPT is super convenient to use. You&amp;#39;ve got a good web UI, a nice mobile app. It does web search as needed, understands when you want to generate an image, or when it should use some extra tools to analyse the image you uploaded. Works with audio and documents. It&amp;#39;s just all there in the single package. &lt;/p&gt;\n\n&lt;p&gt;I know there&amp;#39;s Open WebUI, LM Studio etc. But is there anything else, cross-platform with as many of the above features as possible? Ideally, without too fiddly of a setup when you&amp;#39;ve already got some LLM API up and running.&lt;/p&gt;\n\n&lt;p&gt;It seems like the open source model performance is comparable these days (DeepSeek R1 at least), but I&amp;#39;m missing the additional glue to make the switch to local and open source.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpr1fq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "lakySK",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpr1fq/convenient_chatgpt_ux_replacement/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpr1fq/convenient_chatgpt_ux_replacement/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751447125,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Sharing notes in case it helps anyone. I don't often find people talking about models like Jamba and we have access to it, so figure it might be useful.\n\n\\-  \n  \nBeen testing local models for drafting first-pass answers to internal RFPs. The source material is rough. Basically a mix of PDF exports, old responses in docx, inconsistent product specs, wiki dumps and suchlike.\n\nI'm running a basic RAG pipeline over it using section-level chunking and a semantic search index. Nothing too exotic. Retrieval pulls five chunks per query and I'm prompting each model to answer strictly from the provided input. Tried Jamba, Mistral 7B and Mixtral on the same prompts.\n\nMy findings:\n\nMixtral gave the most natural writing style. Handled formatting like bullet points well, but when chunks were overlapping or contradicting, it sometimes mashed them together. Sounded coherent, but didn't track to any one source.\n\nMistral played it safer but the answers often felt incomplete. Would stop early or skip chunks if they weren't clearly relevant. Better than Mixtral at avoiding noise but I had to rerun prompts more often to get full coverage.\n\nJamba was slightly slower and more verbose, but I could actually trace the language back to the retrieved text most of the time. It didn't try to fill in gaps with guesswork and it stayed anchored to the input without inventing policy language. It was more useful in review. Didn't have to figure out where something came from.\n\nStill experimenting with reranking to clean up the retrieval layer. Jamba has been the most consistent in situations where accuracy matters more than polish. Might try pairing it with. post-processing model to tighten up the tone without losing the original source trail.",
          "author_fullname": "t2_1kwk178bd9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Drafting RFP answers with Jamba, Mistral, Mixtral",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpqyra",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751446846,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing notes in case it helps anyone. I don&amp;#39;t often find people talking about models like Jamba and we have access to it, so figure it might be useful.&lt;/p&gt;\n\n&lt;p&gt;-  &lt;/p&gt;\n\n&lt;p&gt;Been testing local models for drafting first-pass answers to internal RFPs. The source material is rough. Basically a mix of PDF exports, old responses in docx, inconsistent product specs, wiki dumps and suchlike.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running a basic RAG pipeline over it using section-level chunking and a semantic search index. Nothing too exotic. Retrieval pulls five chunks per query and I&amp;#39;m prompting each model to answer strictly from the provided input. Tried Jamba, Mistral 7B and Mixtral on the same prompts.&lt;/p&gt;\n\n&lt;p&gt;My findings:&lt;/p&gt;\n\n&lt;p&gt;Mixtral gave the most natural writing style. Handled formatting like bullet points well, but when chunks were overlapping or contradicting, it sometimes mashed them together. Sounded coherent, but didn&amp;#39;t track to any one source.&lt;/p&gt;\n\n&lt;p&gt;Mistral played it safer but the answers often felt incomplete. Would stop early or skip chunks if they weren&amp;#39;t clearly relevant. Better than Mixtral at avoiding noise but I had to rerun prompts more often to get full coverage.&lt;/p&gt;\n\n&lt;p&gt;Jamba was slightly slower and more verbose, but I could actually trace the language back to the retrieved text most of the time. It didn&amp;#39;t try to fill in gaps with guesswork and it stayed anchored to the input without inventing policy language. It was more useful in review. Didn&amp;#39;t have to figure out where something came from.&lt;/p&gt;\n\n&lt;p&gt;Still experimenting with reranking to clean up the retrieval layer. Jamba has been the most consistent in situations where accuracy matters more than polish. Might try pairing it with. post-processing model to tighten up the tone without losing the original source trail.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lpqyra",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NullPointerJack",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpqyra/drafting_rfp_answers_with_jamba_mistral_mixtral/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpqyra/drafting_rfp_answers_with_jamba_mistral_mixtral/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751446846,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Seems interesting, I am not clear if the compression is only for storage, transmission or extend to inference too :) ",
          "author_fullname": "t2_7htykppj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open source tech from IBM for Compression of models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpquz6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=8b99d19f315f8fa6ac4378d082ba9113481e6314",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751446420,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "research.ibm.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Seems interesting, I am not clear if the compression is only for storage, transmission or extend to inference too :) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://research.ibm.com/blog/Zip-NN-AI-compression",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?auto=webp&amp;s=b20059d77cfdd46e1f5b01a635f852bf1f39606f",
                  "width": 1200,
                  "height": 675
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bafab63573d0d0610a213819138bd8d7036eb5b3",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2866ddcd967a9679addcf5145694cfd2aad26c36",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=4d13e875538c81c90c44e3cb431217f987d47935",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c76f996ad0aa02724516847f86a59dddb6ea317e",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=d659980929847dcc3a208485b6122e310da5b631",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2ea914b9baf533e901be0b2c1434b98e33594a70",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "NTNoyueW9-UJqjsAanLnjShv3Q-OdUZiW0Di3m0uACc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lpquz6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Affectionate-Hat-536",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpquz6/open_source_tech_from_ibm_for_compression_of/",
          "stickied": false,
          "url": "https://research.ibm.com/blog/Zip-NN-AI-compression",
          "subreddit_subscribers": 494001,
          "created_utc": 1751446420,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m looking to do an audiobook and I think I’m going to use chatterbox as it seems to be the best for a long audiobook that’s open source right now. Let me know if there’s something better. I’ve also considered just a $10 a month third-party API access for minimax tts. But for chatterbox, I need to find a voice to clone. Ideally I’d like to find a voice ethically so they agreed to have it train a model or be cloned. So maybe just pulling this from a dataset that was used to train a tts but I would like an easier way to find the type of voices that would shoot for a relaxing audiobook, then just randomly pulling from the data set and hoping I find a good voice. Do you guys know where I can find voicd clips that I can use to train chatterbox?",
          "author_fullname": "t2_100wjq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Where can I find clips of voices to clone?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpqcb7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751444254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m looking to do an audiobook and I think I’m going to use chatterbox as it seems to be the best for a long audiobook that’s open source right now. Let me know if there’s something better. I’ve also considered just a $10 a month third-party API access for minimax tts. But for chatterbox, I need to find a voice to clone. Ideally I’d like to find a voice ethically so they agreed to have it train a model or be cloned. So maybe just pulling this from a dataset that was used to train a tts but I would like an easier way to find the type of voices that would shoot for a relaxing audiobook, then just randomly pulling from the data set and hoping I find a good voice. Do you guys know where I can find voicd clips that I can use to train chatterbox?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpqcb7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dabble_",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpqcb7/where_can_i_find_clips_of_voices_to_clone/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpqcb7/where_can_i_find_clips_of_voices_to_clone/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751444254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've built an MCP that lets local LLMs get peer review from Google Gemini to dramatically improve response quality.\n\n\n\n🎯 \\*\\*The Problem:\\*\\* Local LLMs sometimes give good but incomplete answers\n\n✨ \\*\\*The Solution:\\*\\* Real-time AI peer review for enhancement\n\n\n\n\\*\\*How it works:\\*\\*\n\n1. Ask your local LLM any question\n\n2. Say \"use ai\\_peer\\_review to improve that answer\"  \n\n3. Gets feedback from Gemini → dramatically better response\n\n\n\n\\*\\*Example improvement:\\*\\* Basic explanation → Comprehensive answer with examples, better accuracy, missing context filled in\n\n\n\n\\*\\*Features:\\*\\*\n\n✅ Free (Google Gemini free tier)\n\n✅ Manual trigger (privacy-conscious)  \n\n✅ Works with any tool-calling model\n\n✅ Easy LMStudio, Claude Desktop and any other MCP HOST integration\n\n✅ Comprehensive logging\n\n\n\n\\*\\*GitHub:\\*\\* [https://github.com/xyehya/ai-peer-review-mcp](https://github.com/xyehya/ai-peer-review-mcp)\n\n\n\nThe quality jump is genuinely remarkable. Happy to answer questions!",
          "author_fullname": "t2_5i2smzx9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[P] Built AI to AI Peer Review MCP - Local LLMs get real-time feedback from Google Gemini to improve responses",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpq6l0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751443583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve built an MCP that lets local LLMs get peer review from Google Gemini to dramatically improve response quality.&lt;/p&gt;\n\n&lt;p&gt;🎯 **The Problem:** Local LLMs sometimes give good but incomplete answers&lt;/p&gt;\n\n&lt;p&gt;✨ **The Solution:** Real-time AI peer review for enhancement&lt;/p&gt;\n\n&lt;p&gt;**How it works:**&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Ask your local LLM any question&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Say &amp;quot;use ai_peer_review to improve that answer&amp;quot;  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Gets feedback from Gemini → dramatically better response&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;**Example improvement:** Basic explanation → Comprehensive answer with examples, better accuracy, missing context filled in&lt;/p&gt;\n\n&lt;p&gt;**Features:**&lt;/p&gt;\n\n&lt;p&gt;✅ Free (Google Gemini free tier)&lt;/p&gt;\n\n&lt;p&gt;✅ Manual trigger (privacy-conscious)  &lt;/p&gt;\n\n&lt;p&gt;✅ Works with any tool-calling model&lt;/p&gt;\n\n&lt;p&gt;✅ Easy LMStudio, Claude Desktop and any other MCP HOST integration&lt;/p&gt;\n\n&lt;p&gt;✅ Comprehensive logging&lt;/p&gt;\n\n&lt;p&gt;**GitHub:** &lt;a href=\"https://github.com/xyehya/ai-peer-review-mcp\"&gt;https://github.com/xyehya/ai-peer-review-mcp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The quality jump is genuinely remarkable. Happy to answer questions!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lpq6l0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "yehyakar",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpq6l0/p_built_ai_to_ai_peer_review_mcp_local_llms_get/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpq6l0/p_built_ai_to_ai_peer_review_mcp_local_llms_get/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751443583,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\nI recently built **LeCarnet**, a dataset of 2 million French short stories generated with Mistral Large, inspired by the TinyStories project. I also trained three LLaMA-based models from scratch on this dataset: **LeCarnet-3M**, **LeCarnet-8M**, and **LeCarnet-21M**.\n\nThis dataset contains simple stories with a limited vocabulary, making it ideal for training small language models (SLMs) and for educational purposes.\n\nI've shared the **data generation, training, and evaluation scripts** as well.  \nI hope this can be useful to others, feel free to use it, and don't hesitate to leave a star if you find it helpful!\n\n**GitHub:** [https://github.com/MaxLSB/LeCarnet](https://github.com/MaxLSB/LeCarnet)  \n**Models:** [https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594](https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594)  \n**Dataset:** [https://huggingface.co/datasets/MaxLSB/LeCarnet](https://huggingface.co/datasets/MaxLSB/LeCarnet)",
          "author_fullname": "t2_1ov0jzgpzr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LeCarnet: A French Dataset for Small Language Models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lppz8x",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "ups": 40,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 40,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b795fd4d415d0650b983139e59c5a214e782f7d3",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751442716,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I recently built &lt;strong&gt;LeCarnet&lt;/strong&gt;, a dataset of 2 million French short stories generated with Mistral Large, inspired by the TinyStories project. I also trained three LLaMA-based models from scratch on this dataset: &lt;strong&gt;LeCarnet-3M&lt;/strong&gt;, &lt;strong&gt;LeCarnet-8M&lt;/strong&gt;, and &lt;strong&gt;LeCarnet-21M&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;This dataset contains simple stories with a limited vocabulary, making it ideal for training small language models (SLMs) and for educational purposes.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve shared the &lt;strong&gt;data generation, training, and evaluation scripts&lt;/strong&gt; as well.&lt;br/&gt;\nI hope this can be useful to others, feel free to use it, and don&amp;#39;t hesitate to leave a star if you find it helpful!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href=\"https://github.com/MaxLSB/LeCarnet\"&gt;https://github.com/MaxLSB/LeCarnet&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Models:&lt;/strong&gt; &lt;a href=\"https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594\"&gt;https://huggingface.co/collections/MaxLSB/lecarnet-683d6b6843023b2c88258594&lt;/a&gt;&lt;br/&gt;\n&lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href=\"https://huggingface.co/datasets/MaxLSB/LeCarnet\"&gt;https://huggingface.co/datasets/MaxLSB/LeCarnet&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/MaxLSB/LeCarnet",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?auto=webp&amp;s=791279a523fa06d60761a294e54e558c76bc95f7",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cd59f37203ae74b242ed1c9cf045efd099127c51",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=21c2f2bf31f844b9add78bdc94e1c3f78ff7e469",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=048d258b3f7313d696f4964bd6f0f41919c4487b",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3a64905c33d69e9dd1313750f8b6efb7d4b7b7c4",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3adaaf7983660b09d94aaf0a68c216b8bcf045e3",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=07a5f66febcc17798e8b64ba58dbf8abe479a69e",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "LbeaoTZsvVnFcs9m3p61Rw_b6plof5ZMIkdYPViyiXk"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lppz8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Unusual_Shoe2671",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lppz8x/lecarnet_a_french_dataset_for_small_language/",
          "stickied": false,
          "url": "https://github.com/MaxLSB/LeCarnet",
          "subreddit_subscribers": 494001,
          "created_utc": 1751442716,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am using InternVL an image task - and further plan on fine tuning it for the task.\n\nI have a tight deadline and I want to optimize the latency of it. For the InternVL 3 2B model; it takes about ~4 seconds to come up with a response in a L4 GPU set up. \nI did try vLLM but the benchmarking results show a decrease in the performance  - accuracy(also came across a few articles that share the same concern). I don’t want to quantize the model as it is already a very small model and might result in a drop of the performance.\n\nI am using the LMDeploy framework for the same. Any suggestions on how I can further reduce the latency? ",
          "author_fullname": "t2_7xzfnec0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Optimize Latency of InternVL",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lppxs2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751442540,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am using InternVL an image task - and further plan on fine tuning it for the task.&lt;/p&gt;\n\n&lt;p&gt;I have a tight deadline and I want to optimize the latency of it. For the InternVL 3 2B model; it takes about ~4 seconds to come up with a response in a L4 GPU set up. \nI did try vLLM but the benchmarking results show a decrease in the performance  - accuracy(also came across a few articles that share the same concern). I don’t want to quantize the model as it is already a very small model and might result in a drop of the performance.&lt;/p&gt;\n\n&lt;p&gt;I am using the LMDeploy framework for the same. Any suggestions on how I can further reduce the latency? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lppxs2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "chitrabhat4",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lppxs2/optimize_latency_of_internvl/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lppxs2/optimize_latency_of_internvl/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751442540,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve developed a working memory engine for LLM-based chat applications, designed primarily for long-term roleplay and simulation stability. It’s called CoreWeaver, and it’s built to address issues around persistent memory, decision consistency, and emotional context management.\n\nTechnical Summary:\n\t•\tBuilt in JavaScript as a modular plugin\n\t•\tCompatible with SillyTavern and local LLMs\n\t•\tStores long-term memory entries with metadata (type, emotion, impact)\n\t•\tTracks emotional pressure over time and influences AI decisions\n\t•\tSupports timeline branching for parallel scenarios or alternate chats\n\t•\tIncludes token-optimized compression to reduce memory bloat\n\t•\tFully character-specific memory folders with timeline control\n\t•\tReflective decision engine logs choices and emotional drift\n\nStatus:\n\t•\tEngine was functional by 06/29/2025\n\t•\tCurrently integrating into a full companion app and testing with OpenAI and free local models via Horde\n\t•\tCodebase is closed-source for now but may offer technical previews later for feedback\n\nMy Role:\nThis is a solo project—I built and tested the full framework myself over the past month. I’m currently validating its use in AI companion systems, but I believe it has strong potential for interactive NPC behavior in games, simulation RP, and emotionally consistent storytelling.\n\nLet me know if anyone else is working on similar long-term memory engines. Happy to exchange ideas.\n\n– Mike",
          "author_fullname": "t2_b1usqotg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Proof of Concept] CoreWeaver – AI Memory Engine for Long-Term Context, Emotional State Tracking, and Branching Timelines",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpproa",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751441836,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve developed a working memory engine for LLM-based chat applications, designed primarily for long-term roleplay and simulation stability. It’s called CoreWeaver, and it’s built to address issues around persistent memory, decision consistency, and emotional context management.&lt;/p&gt;\n\n&lt;p&gt;Technical Summary:\n    • Built in JavaScript as a modular plugin\n    • Compatible with SillyTavern and local LLMs\n    • Stores long-term memory entries with metadata (type, emotion, impact)\n    • Tracks emotional pressure over time and influences AI decisions\n    • Supports timeline branching for parallel scenarios or alternate chats\n    • Includes token-optimized compression to reduce memory bloat\n    • Fully character-specific memory folders with timeline control\n    • Reflective decision engine logs choices and emotional drift&lt;/p&gt;\n\n&lt;p&gt;Status:\n    • Engine was functional by 06/29/2025\n    • Currently integrating into a full companion app and testing with OpenAI and free local models via Horde\n    • Codebase is closed-source for now but may offer technical previews later for feedback&lt;/p&gt;\n\n&lt;p&gt;My Role:\nThis is a solo project—I built and tested the full framework myself over the past month. I’m currently validating its use in AI companion systems, but I believe it has strong potential for interactive NPC behavior in games, simulation RP, and emotionally consistent storytelling.&lt;/p&gt;\n\n&lt;p&gt;Let me know if anyone else is working on similar long-term memory engines. Happy to exchange ideas.&lt;/p&gt;\n\n&lt;p&gt;– Mike&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lpproa",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Separate-Toe409",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpproa/proof_of_concept_coreweaver_ai_memory_engine_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpproa/proof_of_concept_coreweaver_ai_memory_engine_for/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751441836,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Can someone explain me how to use  jan nano 128k with deerflow locally?  \nthank you  \nDave",
          "author_fullname": "t2_1frs8x5t",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "deerflow with jan nano 128k",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lppj9f",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751440886,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can someone explain me how to use  jan nano 128k with deerflow locally?&lt;br/&gt;\nthank you&lt;br/&gt;\nDave&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lppj9f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dave-lon",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lppj9f/deerflow_with_jan_nano_128k/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lppj9f/deerflow_with_jan_nano_128k/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751440886,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I don't mean one-off responses that sound good, I'm thinking more along the lines of: ways in which you've gotten the model working reliably in a workflow or pipeline of some kind, or fine tuned it for a specific task that it performs jus as well as the cloudAI behemoths.",
          "author_fullname": "t2_c9j5fpaz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the most complex thing you've been able to (consistently) do with a 4B LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lppg3g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 131,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 131,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751440542,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t mean one-off responses that sound good, I&amp;#39;m thinking more along the lines of: ways in which you&amp;#39;ve gotten the model working reliably in a workflow or pipeline of some kind, or fine tuned it for a specific task that it performs jus as well as the cloudAI behemoths.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lppg3g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "noellarkin",
          "discussion_type": null,
          "num_comments": 64,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lppg3g/whats_the_most_complex_thing_youve_been_able_to/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751440542,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am unsure what is the correct implementation for LLMs to call MCP tools.  \n\nFor example, gemma3 model card mentions a pythonic tool call starting with ```tool_code  \n\nOr llama which doesn't have any special tokens.  \n\nChatgpt itself also has a different implementations. \n\nSo I'm not sure how MCP helps to parse these different format LLM uses to call tools.  Does anyone have any insight?",
          "author_fullname": "t2_szell",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does MCP work for different LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lpp8s1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1751439732,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am unsure what is the correct implementation for LLMs to call MCP tools.  &lt;/p&gt;\n\n&lt;p&gt;For example, gemma3 model card mentions a pythonic tool call starting with ```tool_code  &lt;/p&gt;\n\n&lt;p&gt;Or llama which doesn&amp;#39;t have any special tokens.  &lt;/p&gt;\n\n&lt;p&gt;Chatgpt itself also has a different implementations. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m not sure how MCP helps to parse these different format LLM uses to call tools.  Does anyone have any insight?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lpp8s1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "slashrshot",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpp8s1/how_does_mcp_work_for_different_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpp8s1/how_does_mcp_work_for_different_llms/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751439732,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1fc9cbovwe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EXAONE 4.0 pull request sent to llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lporoz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.62,
          "author_flair_background_color": null,
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=4e326d5433e88e8bd9b5a1837e8f2b92ff8fc5ae",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1751437903,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/issues/14474",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?auto=webp&amp;s=09386c6a3c2fe097b2876a913ab845c11ef93964",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=81061b683f14836b100fd3e54da07012e3c1e8c2",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4f6ce0f2f80d5b325e8b64c10ebfb54e7516bb4",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0fa1ee33f84f558f2c6596d052ce4c1406652ad7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8cb8874cc5f0720f5b8ed06efb123f2b960a2973",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=62908f099961b733d2706d0314495140b047477e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7c67b99f2de16aa1140590247f8cbe4e3f3a443f",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "mGIr9IovxD-Pm48t7v0UR9-dXenko9w6ivUXZu8RxgQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lporoz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "minpeter2",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lporoz/exaone_40_pull_request_sent_to_llamacpp/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/issues/14474",
          "subreddit_subscribers": 494001,
          "created_utc": 1751437903,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/apple/DiffuCoder-7B-cpGRPO](https://huggingface.co/apple/DiffuCoder-7B-cpGRPO) (base and instruct also available)\n\nCurrently trying - and failing - to run test it on Colab, but really looking forward to it!\n\nAlso, anyone got an idea how I can run it on Apple Silicon?\n\n[Benchmarks compared to other coding and diffusion models](https://preview.redd.it/s19j3dmfneaf1.png?width=1176&amp;format=png&amp;auto=webp&amp;s=927e506f764ded47a4e715aea53c223e56ea7ae6)\n\n[https://arxiv.org/pdf/2506.20639](https://arxiv.org/pdf/2506.20639)",
          "author_fullname": "t2_alho5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DiffuCoder 7B - New coding diffusion LLM by Apple",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "s19j3dmfneaf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 84,
                  "x": 108,
                  "u": "https://preview.redd.it/s19j3dmfneaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cde3b3074354143bfe56532c93502a45f3cf9fbd"
                },
                {
                  "y": 168,
                  "x": 216,
                  "u": "https://preview.redd.it/s19j3dmfneaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92c383eaf085a07945d94591e52941a93a33c123"
                },
                {
                  "y": 249,
                  "x": 320,
                  "u": "https://preview.redd.it/s19j3dmfneaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b22c3a42268be01220ee63470cc279c879dc59ad"
                },
                {
                  "y": 498,
                  "x": 640,
                  "u": "https://preview.redd.it/s19j3dmfneaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d5a21d3c223b732d53e698bb3aad0211b4985d08"
                },
                {
                  "y": 747,
                  "x": 960,
                  "u": "https://preview.redd.it/s19j3dmfneaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=18f094eeffed1f613bc262d5afc62d028149ce8d"
                },
                {
                  "y": 841,
                  "x": 1080,
                  "u": "https://preview.redd.it/s19j3dmfneaf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fdc4c29f7ac33b8944762504d53b25b78fd7b409"
                }
              ],
              "s": {
                "y": 916,
                "x": 1176,
                "u": "https://preview.redd.it/s19j3dmfneaf1.png?width=1176&amp;format=png&amp;auto=webp&amp;s=927e506f764ded47a4e715aea53c223e56ea7ae6"
              },
              "id": "s19j3dmfneaf1"
            }
          },
          "name": "t3_1lpoqlu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 265,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 265,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=94f95b15f093e7e4c52427bf32633f935890f3d9",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1751437787,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/apple/DiffuCoder-7B-cpGRPO\"&gt;https://huggingface.co/apple/DiffuCoder-7B-cpGRPO&lt;/a&gt; (base and instruct also available)&lt;/p&gt;\n\n&lt;p&gt;Currently trying - and failing - to run test it on Colab, but really looking forward to it!&lt;/p&gt;\n\n&lt;p&gt;Also, anyone got an idea how I can run it on Apple Silicon?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/s19j3dmfneaf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=927e506f764ded47a4e715aea53c223e56ea7ae6\"&gt;Benchmarks compared to other coding and diffusion models&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arxiv.org/pdf/2506.20639\"&gt;https://arxiv.org/pdf/2506.20639&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?auto=webp&amp;s=87b4e9cf26a9425b49fe91fa407b4881683411a1",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9b3b8168a8a55fa37a1679a4a1a0f95a20bf5a1",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86ce2a87a27be1f49658d6de3f4cad2433715662",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=274e1732d62c8ad27ef17a008734eb9550cb84bd",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ed4a0394c2c1d722f620e6214e63d44c79d3e340",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4ca34810011f0a3d6f476113f06f5b836fa8a3e7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e8963ee356c3b39eafe785ca3139e056eb01741d",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "WJ1Mt_9K-aAaMAebSityZ71IWIFD1yMghVJOklMW6Xo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lpoqlu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DunklerErpel",
          "discussion_type": null,
          "num_comments": 57,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpoqlu/diffucoder_7b_new_coding_diffusion_llm_by_apple/",
          "subreddit_subscribers": 494001,
          "created_utc": 1751437787,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}