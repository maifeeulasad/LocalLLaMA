{
  "kind": "Listing",
  "data": {
    "after": "t3_1lxseu8",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. \n\nI had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. \n\nI prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.",
          "author_fullname": "t2_dissgzyl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Let’s talk about models you believed are more Hyped than Hot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyvkhr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752420473,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;My suggestion for how to make this profitable is list the hyped model and explain what it is very bad at for you… then list one or two models and the environment you use them in daily that do a better job. &lt;/p&gt;\n\n&lt;p&gt;I had multiple people gushing over how effective Reka was for creative writing, and so I tried it in a RP conversation in Silly Tavern and also in regular story generation in Oobabooga’s text generation UI. I wasn’t happy with either. &lt;/p&gt;\n\n&lt;p&gt;I prefer llama 3.3 70b and Gemma 27b over it in those environments … though I love Reka’s license.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyvkhr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "silenceimpaired",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvkhr/lets_talk_about_models_you_believed_are_more/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752420473,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like \"here's a scenario, what do you think is the most realistic thing to happen?\" or \"what do you think would be a good solution to this issue?\". I found it quite bad in this regard.\n\n* It frequently made things up, even when specifically instructed not to do so. **It then clarified it was trying to come up with a helpful looking answer using fragmented data**, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.\n\n* If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion by using sources that do not exist. **At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.** It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.\n\n* Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it's own idea.\n\n* If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like \"why are you not revealing the character's thoughts here?\" or \"why are you not taking X into account?\". Free ChatGPT is actually much better in this regard.\n\n* It has possibly the most restrictive content filters i have seen out of all the AI chat bots i have tried. It's very prudish.",
          "author_fullname": "t2_11hnt7w9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Tried Kimi K2 for writing and reasoning, and was not impressed.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyvah4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419788,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried using Kimi k2 to flesh out setting/plot ideas. E.G. I would say things like &amp;quot;here&amp;#39;s a scenario, what do you think is the most realistic thing to happen?&amp;quot; or &amp;quot;what do you think would be a good solution to this issue?&amp;quot;. I found it quite bad in this regard.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;It frequently made things up, even when specifically instructed not to do so. &lt;strong&gt;It then clarified it was trying to come up with a helpful looking answer using fragmented data&lt;/strong&gt;, instead of using verifiable sources only. It also said i would need to tell it to use verifiable sources only if i wanted it to not use fragments.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If Kimi k2 believes it is correct, it will become very stubborn and refuse to consider the possibility it may be wrong. Which is particularly problematic when it arrives at the wrong conclusion by using sources that do not exist. &lt;strong&gt;At one point, it suddenly claimed that NASA had done a study to test if men could tell whether their genitals were being stimulated by a man or woman while they were blindfolded.&lt;/strong&gt; It kept insisting this study was real and refused to consider the possibility it might be wrong till i asked it for the direct page number in the study, at which point it said it could not find that experiment in the pdf and admitted it was wrong.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Kimi k2 frequently makes a lot of assumptions on its own, which it then uses to argue that it is correct. E.G. I tried to discuss a setting with magic in it. It then made several assumptions about how the magic worked, and then kept arguing with me based on the assumption that the magic worked that way, even though it was it&amp;#39;s own idea.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;If asked to actually write a scene, it produces very superficial writing and i have to keep prompting it things like &amp;quot;why are you not revealing the character&amp;#39;s thoughts here?&amp;quot; or &amp;quot;why are you not taking X into account?&amp;quot;. Free ChatGPT is actually much better in this regard.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;It has possibly the most restrictive content filters i have seen out of all the AI chat bots i have tried. It&amp;#39;s very prudish.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyvah4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GlompSpark",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyvah4/tried_kimi_k2_for_writing_and_reasoning_and_was/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419788,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone has tried something like that? I just put: create a google chrome extension that blocks websites. it's just something that takes a list of websites and blocks them. The extension does not work in both codes provided by the LLMs.",
          "author_fullname": "t2_cihv7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Testing ChatGPT and Claude capabilities to \"simple projects\": Block Site extension for Google Chrome",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyv7s7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419608,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone has tried something like that? I just put: create a google chrome extension that blocks websites. it&amp;#39;s just something that takes a list of websites and blocks them. The extension does not work in both codes provided by the LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyv7s7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "helioscarbex",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv7s7/testing_chatgpt_and_claude_capabilities_to_simple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyv7s7/testing_chatgpt_and_claude_capabilities_to_simple/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419608,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Ever since there're code completions, I wish I could have something similar when texting people. Now there's finally a decent method for that.\n\nThe app works on any endpoint that's OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.\n\nI tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!\n\nHere's a brief guide to make this work with ollama:\n\n* Download the app from GitHub: [https://github.com/coreply/coreply](https://github.com/coreply/coreply)\n* Download `gemma3:4b-it-qat` in ollama\n* Set environment variable `OLLAMA_HOST` to [`0.0.0.0`](http://0.0.0.0) on the computer running ollama and restart ollama\n* In the Coreply app, set the API URL to `http://192.168.xxx.xxx:11434/v1/`(replace [`192.168.xxx.xxx`](http://192.168.xxx.xxx) with the IP address of the ollama machine), Model name `gemma3:4b-it-qat`\n* Grant permissions and turn on the app. Enjoy your texting suggestions!\n\nMy laptop isn't powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.\n\nLet me know how's your experience with it!",
          "author_fullname": "t2_l6eo8ggy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How I use Gemma 3 to help me reply my texts",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyv750",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755013022%2CMjk5N2YxOGE3MzExNGQxMmZlNWJlODJkMzIxOTJmOWU1N2I0MjAwNjVlNzYwNjQyMDRkYjlhNThhMTdhNGQ5Mw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755013022%2CMGZhZWI5NDAzMjFjODExNGMzNDVkODMwOWU2NWU0NjIzMzQyZmE4YWI0NjhhOWRlOGRlM2YzZTY1MjkxZWY1OA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=140&amp;height=34&amp;crop=140:34,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=4ea93f7d06620c65936cfa4562d1f7121ab5794f",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752419560,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Ever since there&amp;#39;re code completions, I wish I could have something similar when texting people. Now there&amp;#39;s finally a decent method for that.&lt;/p&gt;\n\n&lt;p&gt;The app works on any endpoint that&amp;#39;s OpenAI compatible. Once you set it up, it gives you texting completions right inside WhatsApp, Signal, and some other texting apps.&lt;/p&gt;\n\n&lt;p&gt;I tested it with Gemma 3 4B running on my AMD Ryzen 4700u laptop. The results come out slow, but the quality is totally acceptable (the video is trimmed, but the suggestions come from Gemma 3 4B). I can imagine if you have a powerful setup, you can get these texting suggestions with a fully local setup!&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a brief guide to make this work with ollama:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Download the app from GitHub: &lt;a href=\"https://github.com/coreply/coreply\"&gt;https://github.com/coreply/coreply&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Download &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt; in ollama&lt;/li&gt;\n&lt;li&gt;Set environment variable &lt;code&gt;OLLAMA_HOST&lt;/code&gt; to &lt;a href=\"http://0.0.0.0\"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; on the computer running ollama and restart ollama&lt;/li&gt;\n&lt;li&gt;In the Coreply app, set the API URL to &lt;code&gt;http://192.168.xxx.xxx:11434/v1/&lt;/code&gt;(replace &lt;a href=\"http://192.168.xxx.xxx\"&gt;&lt;code&gt;192.168.xxx.xxx&lt;/code&gt;&lt;/a&gt; with the IP address of the ollama machine), Model name &lt;code&gt;gemma3:4b-it-qat&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;Grant permissions and turn on the app. Enjoy your texting suggestions!&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My laptop isn&amp;#39;t powerful enough, so for daily use, I use Gemini 2.0 Flash, just change the URL, API Key, and model name.&lt;/p&gt;\n\n&lt;p&gt;Let me know how&amp;#39;s your experience with it!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/48w6qb1mincf1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?format=pjpg&amp;auto=webp&amp;s=4315959609b1452ddac1019bc82dfcd0d2ecd932",
                  "width": 1080,
                  "height": 264
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=ca55479fe3e543c483ca9fd1a6e1c7663b1e1469",
                    "width": 108,
                    "height": 26
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=bb9e1e7f65de33e3f9e6aac96c0c769244d7247e",
                    "width": 216,
                    "height": 52
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=cb1d98d3e21b6fb43e522b5e2c12bfd0b0606cc7",
                    "width": 320,
                    "height": 78
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=150f56510dfbd5b6d84de28d6521c41bb3feede9",
                    "width": 640,
                    "height": 156
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0f618077403fd8aaf60fa19cfeb0ddd4cf30fc8d",
                    "width": 960,
                    "height": 234
                  },
                  {
                    "url": "https://external-preview.redd.it/NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b18333794ad268d43549283f55cc34c53f54a5c2",
                    "width": 1080,
                    "height": 264
                  }
                ],
                "variants": {},
                "id": "NnNqeWViMW1pbmNmMRSzfaNqfuwOOC92Xq4viycMqSPOW2XTomk7HN62InbQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyv750",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sean01-eth",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv750/how_i_use_gemma_3_to_help_me_reply_my_texts/",
          "stickied": false,
          "url": "https://v.redd.it/48w6qb1mincf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419560,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 450,
              "fallback_url": "https://v.redd.it/48w6qb1mincf1/DASH_270.mp4?source=fallback",
              "has_audio": false,
              "height": 118,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/48w6qb1mincf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/48w6qb1mincf1/DASHPlaylist.mpd?a=1755013022%2CMjk5N2YxOGE3MzExNGQxMmZlNWJlODJkMzIxOTJmOWU1N2I0MjAwNjVlNzYwNjQyMDRkYjlhNThhMTdhNGQ5Mw%3D%3D&amp;v=1&amp;f=sd",
              "duration": 7,
              "hls_url": "https://v.redd.it/48w6qb1mincf1/HLSPlaylist.m3u8?a=1755013022%2CMGZhZWI5NDAzMjFjODExNGMzNDVkODMwOWU2NWU0NjIzMzQyZmE4YWI0NjhhOWRlOGRlM2YzZTY1MjkxZWY1OA%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys, noobie here.\n\nI am using OBS and there is a plugin called 'localvocal'.  \nI can choose there several LLMs etc.  \nWhich one should be the best for my use case? How can I add other LLMs from huggingface?\n\nAny help is appreciated, thank you!",
          "author_fullname": "t2_7mkjq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM model for live translation into subtitles [RU-EN]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyv5uc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752419469,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, noobie here.&lt;/p&gt;\n\n&lt;p&gt;I am using OBS and there is a plugin called &amp;#39;localvocal&amp;#39;.&lt;br/&gt;\nI can choose there several LLMs etc.&lt;br/&gt;\nWhich one should be the best for my use case? How can I add other LLMs from huggingface?&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated, thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyv5uc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TuGuX",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyv5uc/llm_model_for_live_translation_into_subtitles_ruen/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyv5uc/llm_model_for_live_translation_into_subtitles_ruen/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752419469,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’ve been building an AI assistant (Caelum) that can control a system using natural language, but I didn’t want it running raw shell commands or hallucinating `subprocess` calls. That’s unreliable and messy, so I built a structured `do()` system with plugin routing, safety flags, and argument parsing. Each command is a plugin, and you can write one in like 10–15 lines of code. Plugins auto-register and are isolated, so there’s no hardcoded logic or brittle wrappers.\n\nRight now it supports 39 commands, all modular, and you can interact with it using structured phrases or natural language if you add a mapping layer. It’s async-friendly, works with local agents, and is designed to grow without becoming a spaghetti monster.\n\nI originally posted this in another thread and realized quickly that it was the wrong crowd. This isn’t a CLI enhancement. It’s a system automation backbone that gives LLMs a safe, predictable way to control the OS through plugins, not shell access.\n\nIf you’re working on local agents or LLM-powered tools and want something that bridges into actual system control without chaos, I’d be happy to talk more about how it works. \n\n[https://github.com/BlackBeardJW/caelum-sys](https://github.com/BlackBeardJW/caelum-sys)  \n[https://pypi.org/project/caelum-sys/](https://pypi.org/project/caelum-sys/)",
          "author_fullname": "t2_dhf7dmo0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Built a plugin-based system automation layer for LLMs, safe, modular, and dead simple to extend",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyuxj5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752418915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been building an AI assistant (Caelum) that can control a system using natural language, but I didn’t want it running raw shell commands or hallucinating &lt;code&gt;subprocess&lt;/code&gt; calls. That’s unreliable and messy, so I built a structured &lt;code&gt;do()&lt;/code&gt; system with plugin routing, safety flags, and argument parsing. Each command is a plugin, and you can write one in like 10–15 lines of code. Plugins auto-register and are isolated, so there’s no hardcoded logic or brittle wrappers.&lt;/p&gt;\n\n&lt;p&gt;Right now it supports 39 commands, all modular, and you can interact with it using structured phrases or natural language if you add a mapping layer. It’s async-friendly, works with local agents, and is designed to grow without becoming a spaghetti monster.&lt;/p&gt;\n\n&lt;p&gt;I originally posted this in another thread and realized quickly that it was the wrong crowd. This isn’t a CLI enhancement. It’s a system automation backbone that gives LLMs a safe, predictable way to control the OS through plugins, not shell access.&lt;/p&gt;\n\n&lt;p&gt;If you’re working on local agents or LLM-powered tools and want something that bridges into actual system control without chaos, I’d be happy to talk more about how it works. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/BlackBeardJW/caelum-sys\"&gt;https://github.com/BlackBeardJW/caelum-sys&lt;/a&gt;&lt;br/&gt;\n&lt;a href=\"https://pypi.org/project/caelum-sys/\"&gt;https://pypi.org/project/caelum-sys/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?auto=webp&amp;s=2a2a988a09da5bc5ad4f13e5a79bd3559c0d9808",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=603529e7fcabe20144806792dd5e7c2476b13cfe",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64e3526af339fa9467fd5d5e7d75032005389d24",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fb3942c412a45a44ec1098ffb7f97bab33ea85e4",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=417f48a9f26f6fdf936c7f469c3797e861d17912",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=de6ca03a4ece691622a59b07094afb40147cfb4a",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d4bd124f47d49ba62d35cdffc6ded4e2d57584d7",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "wGF2WkFacBqaY1u9I-h9qjml9wj3Hxc5p-hofX39V7U"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyuxj5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BlackBeardJW",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyuxj5/built_a_pluginbased_system_automation_layer_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyuxj5/built_a_pluginbased_system_automation_layer_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752418915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey folks, I’m getting serious AI fever.\n\nI know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.\n\nIs it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?\n\nI know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.\n\nIs it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?\n\nShould I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?\n\n",
          "author_fullname": "t2_qq6spcu23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI fever D:",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyu7bf",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752417048,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey folks, I’m getting serious AI fever.&lt;/p&gt;\n\n&lt;p&gt;I know there are a lot of enthusiasts here, so I’m looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.&lt;/p&gt;\n\n&lt;p&gt;Is it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?&lt;/p&gt;\n\n&lt;p&gt;I know there’s that mini PC from NVIDIA (DGX Spark), but it’s pretty weak. The memory bandwidth is a terrible joke.&lt;/p&gt;\n\n&lt;p&gt;Is it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?&lt;/p&gt;\n\n&lt;p&gt;Should I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyu7bf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Czydera",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752417048,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello all,\n\nSoon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I've done... helps.\n\nSending code to an internet service is usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.\n\nI am looking at the p16s Gen 4 AMD  with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.\n\nThe first one will be more affordable than the second one...\n\nWould current big models run normally on a laptop like that P16s?\n\nThank you all in advance.",
          "author_fullname": "t2_albb0nfc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for my next laptop soon",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lytioc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752415228,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello all,&lt;/p&gt;\n\n&lt;p&gt;Soon I will be looking for my next laptop, I am an industrial programmer, sometimes asking AI for a specific algorithm implementation, check some code I&amp;#39;ve done... helps.&lt;/p&gt;\n\n&lt;p&gt;Sending code to an internet service is usually breaks the NDA so I thought on using something like JAN to execute the models in my own computer and get an extra source of help to do my work... currently with my Thinkpad P14s Gen 2 AMD with 32GB RAM and a 5850u CPU the speed is... terrible.&lt;/p&gt;\n\n&lt;p&gt;I am looking at the p16s Gen 4 AMD  with 64 or 96 GB of RAM and the AMD Ryzen AI 9 HX PRO 370 CPU with Integrated AMD Radeon 890M Graphics and Integrated AMD Ryzen AI, up to 50 TOPS or, when they decide to make it available a Thinkpad P1 Gen 8 with the latest 7 or 9 intel CPU and a dedicated GPU.&lt;/p&gt;\n\n&lt;p&gt;The first one will be more affordable than the second one...&lt;/p&gt;\n\n&lt;p&gt;Would current big models run normally on a laptop like that P16s?&lt;/p&gt;\n\n&lt;p&gt;Thank you all in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lytioc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "robotecnik",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lytioc/looking_for_my_next_laptop_soon/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752415228,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD's GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?\n",
          "author_fullname": "t2_9wlqkcl5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How can I figure out the speed in tokens per second that my model will run on the CPU?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyt372",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752414028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD&amp;#39;s GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyt372",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Holiday-Picture6796",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752414028,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "With every religious text or practice of import in all languages each, etc? Anyone know of any \"godly ai\"' .. or is that unnecessary because the current models already have all the texts?",
          "author_fullname": "t2_2oqnjla8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is anyone training a religion model?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1lyt0zp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.35,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752413859,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;With every religious text or practice of import in all languages each, etc? Anyone know of any &amp;quot;godly ai&amp;quot;&amp;#39; .. or is that unnecessary because the current models already have all the texts?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyt0zp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SeasonNo3107",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyt0zp/is_anyone_training_a_religion_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyt0zp/is_anyone_training_a_religion_model/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752413859,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So 6 months ago I discussed some information about the at the time not launched [910C accelerator here](https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/).\n\nThe details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/\n\nAnd semianalysis (regarding the 800 tflop bf16 performance) https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/\n\nSince then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.\n\nOn the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.\n\nRecently attended a conference where senior Huawei executives verbally discussed their future plans:\n\n1. They are aiming for a launch of the 920 in H2 2026 or H1 2027 \n\n2. The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?\n\n3. The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.\n\n4. A quote near verbatim \"the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.\" It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.",
          "author_fullname": "t2_nm52x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Rumor] Huawei 920 accelerator coming H2 2026",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lysqk7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.84,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 13,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 13,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752413617,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752413033,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So 6 months ago I discussed some information about the at the time not launched &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1iadomi/rumor_huawei_910c_will_double_910b_performance/\"&gt;910C accelerator here&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;The details I mentioned were later also discussed by Reuters months later (regarding 910C being a doubling of 910B) &lt;a href=\"https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/\"&gt;https://www.reuters.com/world/china/huawei-readies-new-ai-chip-mass-shipment-china-seeks-nvidia-alternatives-sources-2025-04-21/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And semianalysis (regarding the 800 tflop bf16 performance) &lt;a href=\"https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/\"&gt;https://semianalysis.com/2025/04/16/huawei-ai-cloudmatrix-384-chinas-answer-to-nvidia-gb200-nvl72/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Since then Huawei has been aggressively seeding the 910B accelerator (yes the prior gen 910B with 8 accelerators per server) for free to anyone who may have a credible use case. Apparently many universities have been gifted 910B servers in H1 2025. My understanding is that they have gifted 10s of thousands of 910B accelerators to different universities over the last few months.&lt;/p&gt;\n\n&lt;p&gt;On the other hand, the 910C seems to be available only at their approved cloud vendors, and not available for public purchase.&lt;/p&gt;\n\n&lt;p&gt;Recently attended a conference where senior Huawei executives verbally discussed their future plans:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;They are aiming for a launch of the 920 in H2 2026 or H1 2027 &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The 920 will again adopt a chiplet architecture, and have scaled configurations. so I guess the 920 is the name of the compute chiplet?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The biggest challenge for 910C yield is apparently packaging. I was surprised to hear this, since I used to believe that chiplets improved yield. They mentioned that lithography yield was good, with significant losses during packaging.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;A quote near verbatim &amp;quot;the darkest period for Huawei accelerators will be the remainder of 2025 and the first half of 2026, after that the situation will significantly improve.&amp;quot; It was not clear if they were referring to lithography or packaging or in general. But given the context they discussed this in, I was under the impression that they believed significant production breakthroughs were close at hand for their own 7nm chip manufacturing fabs.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lysqk7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "44seconds",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lysqk7/rumor_huawei_920_accelerator_coming_h2_2026/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752413033,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.\n\nAs per the title, I have attempted to run Qwen3-235B-A22B using \\`llama-server\\` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?\n\nMachine specification are:\n\n    CPU: i3-12100F (12th Gen Intel)\n    RAM: 128GB (4\\*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\n    Motherboard: MSI PRO B660M-A WIFI DDR4\n    GPU: GeForce RTX 3090 24GB VRAM\n\n(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)\n\n`llama-server` launch options:\n\n    llama-server \\\n      --host 0.0.0.0 \\\n      --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n      --ctx-size 16384 \\\n      --n-gpu-layers 99 \\\n      --flash-attn \\\n      --threads 3 \\\n      -ot \"exps=CPU\" \\\n      --seed 3407 \\\n      --prio 3 \\\n      --temp 0.6 \\\n      --min-p 0.0 \\\n      --top-p 0.95 \\\n      --top-k 20 \\\n      --no-mmap \\\n      --no-warmup \\\n      --mlock\n\nAny advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)",
          "author_fullname": "t2_tfa1mcp1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B @ 0.7t/s. Hardware or configuration bottleneck?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lysmo9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752412727,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Preface: Just a disclaimer that the machine this is running on was never intended to be an inference machine. I am using it (to the dismay of its actual at-the-keyboard user!) due to it being the only machine I could fit the GPU into.&lt;/p&gt;\n\n&lt;p&gt;As per the title, I have attempted to run Qwen3-235B-A22B using `llama-server` on the machine that I felt is most capable of doing so, but I get very poor performance at 0.7t/s at most. Is anyone able to advise if I can get it up to the 5t/s I see others mentioning achieving on this machine?&lt;/p&gt;\n\n&lt;p&gt;Machine specification are:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;CPU: i3-12100F (12th Gen Intel)\nRAM: 128GB (4\\*32GB) @ 2133 MT/s (Corsair CMK128GX4M4A2666C16)\nMotherboard: MSI PRO B660M-A WIFI DDR4\nGPU: GeForce RTX 3090 24GB VRAM\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;(Note: There is another GPU in this machine which is being used for the display. The 3090 is only used for inference.)&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; launch options:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server \\\n  --host 0.0.0.0 \\\n  --model unsloth/Qwen3-235B-A22B-GGUF/UD-Q2_K_XL/Qwen3-235B-A22B-UD-Q2_K_XL-00001-of-00002.gguf \\\n  --ctx-size 16384 \\\n  --n-gpu-layers 99 \\\n  --flash-attn \\\n  --threads 3 \\\n  -ot &amp;quot;exps=CPU&amp;quot; \\\n  --seed 3407 \\\n  --prio 3 \\\n  --temp 0.6 \\\n  --min-p 0.0 \\\n  --top-p 0.95 \\\n  --top-k 20 \\\n  --no-mmap \\\n  --no-warmup \\\n  --mlock\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Any advice is much appreciated (again, by me, maybe not so much by the user! They are very understanding though..)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lysmo9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ConnectionOutside485",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lysmo9/qwen3235ba22b_07ts_hardware_or_configuration/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752412727,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If there is any reference or if anyone has clear idea please do reply.\n\n\nI have a 64gb ram 8core machine.\n3billion parameters models response running via ollama is slower than 600gb models api response.\nHow insane is that.?\n\nQuestion: how do you decide on infra\nIf a model is 600B params, each param is one byte so it goes to nearly 600gb.\nNow what kinda of system requirements does this model need to be running? \nShould a cpu be able to do 600 billion calculations per second or something?\n\nWhat kinda ram requirements does this need?\nSay if this is not a moe model, does it need 600Gb of ram to get started with this?\n\nNow how does the system requirements ram and cpu differ for moe and non moe models.\n\n",
          "author_fullname": "t2_vewp49tm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What does it take to run llms?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqwil",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.36,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752407430,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If there is any reference or if anyone has clear idea please do reply.&lt;/p&gt;\n\n&lt;p&gt;I have a 64gb ram 8core machine.\n3billion parameters models response running via ollama is slower than 600gb models api response.\nHow insane is that.?&lt;/p&gt;\n\n&lt;p&gt;Question: how do you decide on infra\nIf a model is 600B params, each param is one byte so it goes to nearly 600gb.\nNow what kinda of system requirements does this model need to be running? \nShould a cpu be able to do 600 billion calculations per second or something?&lt;/p&gt;\n\n&lt;p&gt;What kinda ram requirements does this need?\nSay if this is not a moe model, does it need 600Gb of ram to get started with this?&lt;/p&gt;\n\n&lt;p&gt;Now how does the system requirements ram and cpu differ for moe and non moe models.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyqwil",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Impossible_Nose_2956",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqwil/what_does_it_take_to_run_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqwil/what_does_it_take_to_run_llms/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752407430,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Could it be because V-JEPA2 gave them strong confidence? [https://arxiv.org/abs/2506.09985](https://arxiv.org/abs/2506.09985)",
          "author_fullname": "t2_xvwcc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why has Meta started throwing billions at AI now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqhqq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752406004,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Could it be because V-JEPA2 gave them strong confidence? &lt;a href=\"https://arxiv.org/abs/2506.09985\"&gt;https://arxiv.org/abs/2506.09985&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyqhqq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "VR-Person",
          "discussion_type": null,
          "num_comments": 31,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqhqq/why_has_meta_started_throwing_billions_at_ai_now/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqhqq/why_has_meta_started_throwing_billions_at_ai_now/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752406004,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Working on a hackathon project and used 'exa' for AI web search. It's so dogwater, it literally kept making up sources and didn't even TRY to parse the output. If I have to put EXTRA work into LEARNING to use your damn service, what am i paying you for??? Like come on man... at least make it easier, if I knew it was like that i'd just make my own service.",
          "author_fullname": "t2_7l6ugv3d",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are these random AI services?? Why are they so bad?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyqefd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.29,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752405674,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Working on a hackathon project and used &amp;#39;exa&amp;#39; for AI web search. It&amp;#39;s so dogwater, it literally kept making up sources and didn&amp;#39;t even TRY to parse the output. If I have to put EXTRA work into LEARNING to use your damn service, what am i paying you for??? Like come on man... at least make it easier, if I knew it was like that i&amp;#39;d just make my own service.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lyqefd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Affectionate-Divide8",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyqefd/what_are_these_random_ai_services_why_are_they_so/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyqefd/what_are_these_random_ai_services_why_are_they_so/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752405674,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tried vertex.. 35 tps\n\nHuggingFace\nwith q6 from unsloth 48 tps\noriginal from Google 35 tps\n\nI need 100tps.. please help\n\nI know not much about inference infrastructure.\n",
          "author_fullname": "t2_86t4cp3p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help Needed for MedGemma 27B",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq7mc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404980,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tried vertex.. 35 tps&lt;/p&gt;\n\n&lt;p&gt;HuggingFace\nwith q6 from unsloth 48 tps\noriginal from Google 35 tps&lt;/p&gt;\n\n&lt;p&gt;I need 100tps.. please help&lt;/p&gt;\n\n&lt;p&gt;I know not much about inference infrastructure.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyq7mc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "FewOwl9332",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq7mc/help_needed_for_medgemma_27b/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq7mc/help_needed_for_medgemma_27b/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752404980,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all,\n\nI'm building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.\n\nI've shortlisted Meta's LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic's model requirements . I'm planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.\n\nI did look at [https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix](https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix) but it is somewhat out of date now.\n\nI have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)\n\nLooking for help with:\n\n* Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?\n* Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?\n* Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?\n\nI have some constraints:\n\n* Must be air-gapped\n* I can't use Chinese, Israeli or similar products. CISO doesn't allow it. I know some of the Chinese models would be a good fit, but its a no-go.\n* Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure\n\nWould love to hear from anyone who’s done this in production or lab.\n\nThanks in advance!\n\n",
          "author_fullname": "t2_s8xklsb6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM to back Elastic AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq22j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404403,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m building a fully air-gapped deployment that integrates with Elastic Security and Observability, including Elastic AI Assistant via OpenInference API. My use case involves log summarisation, alert triage, threat intel enrichment (using MISP), and knowledge base retrieval. About 5000 users, about 2000 servers. All on-prem.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve shortlisted Meta&amp;#39;s LLaMA 4 Maverick 17B 128E Instruct model as a candidate for this setup. Reason is it is instruction-tuned, long-context, and MoE-optimised. It fits Elastic&amp;#39;s model requirements . I&amp;#39;m planning to run it at full precision (BF16 or FP16) using vLLM or Ollama, but happy to adapt if others have better suggestions.&lt;/p&gt;\n\n&lt;p&gt;I did look at &lt;a href=\"https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix\"&gt;https://www.elastic.co/docs/solutions/security/ai/large-language-model-performance-matrix&lt;/a&gt; but it is somewhat out of date now.&lt;/p&gt;\n\n&lt;p&gt;I have a pretty solid budget (though 3 A100s is probably the limit once the rest of the hardware is taken into account)&lt;/p&gt;\n\n&lt;p&gt;Looking for help with:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Model feedback: Anyone using LLaMA 4 Maverick or other Elastic-supported models (like Mistral Instruct or LLaMA 3.1 Instruct)?&lt;/li&gt;\n&lt;li&gt;Hardware: What server setup did you use? Any success with Dell XE7745, HPE GPU nodes, or DIY rigs with A100s/H100s?&lt;/li&gt;\n&lt;li&gt;Fine-tuning: Anyone LoRA-fine-tuned Maverick or similar for log alerting, ECS fields, or threat context?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have some constraints:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Must be air-gapped&lt;/li&gt;\n&lt;li&gt;I can&amp;#39;t use Chinese, Israeli or similar products. CISO doesn&amp;#39;t allow it. I know some of the Chinese models would be a good fit, but its a no-go.&lt;/li&gt;\n&lt;li&gt;Need to support long-context summarisation, RAG-style enrichment, and Elastic Assistant prompt structure&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to hear from anyone who’s done this in production or lab.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?auto=webp&amp;s=9a1b4684102bb8c94296cbfa71ad3a31d0c0f257",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8e7dcc983e13bd8aed1654a54d718d49f54cdaae",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6db7b95b72b6ba957c849b5433b50158cc281a2e",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=54ac1a61a829459657d4fbc629848f4a7b86377b",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=19ae5d46a63ca2b66411c4548a6cff1870e42360",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b5dbba47c48a53ff43f7f01db90cc69cd595f0e8",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=994bf868b62aa1188f41e6e7f154a8365d35c7fe",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "G2yA00beNF7t7h7F-Vm0zYQ1_GPWt2mKaYLvl77xrgc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyq22j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OldManCyberNinja",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq22j/local_llm_to_back_elastic_ai/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752404403,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone!\n\nWanted to ask a question that's been on my mind recently.\n\nI've done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.\n\nBut how is LLM evaluation done in real life (i.e. in industry)? If I'm a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it's doing a good job?\n\nIs it only product related metrics like customer satisfaction and existing benchmarks like in the industry? ",
          "author_fullname": "t2_owqzu42m8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "LLM evaluation in real life?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyq1yh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752404393,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;Wanted to ask a question that&amp;#39;s been on my mind recently.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve done LLM research in academia in various forms, each time I thought of a way to improve a certain aspect of LLMs for different tasks, and when asked to prove that my alteration actually improved upon something I almost always had a benchmark to test myself.&lt;/p&gt;\n\n&lt;p&gt;But how is LLM evaluation done in real life (i.e. in industry)? If I&amp;#39;m a company that wants to offer a strong coding-assistant, research-assistant or any other type of LLM product - How do I make sure that it&amp;#39;s doing a good job?&lt;/p&gt;\n\n&lt;p&gt;Is it only product related metrics like customer satisfaction and existing benchmarks like in the industry? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyq1yh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Plastic-Bus-7003",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyq1yh/llm_evaluation_in_real_life/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752404393,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI’m developing a tool that allows you to create full applications by simply describing what you want in plain English—no complicated setup, no boilerplate code.\n\nHere’s what it currently offers:\n\t•\tSupports over 10 programming languages\n\t•\tLets you connect your GitHub repository\n\t•\tCan fix bugs or make improvements in your existing projects\n\t•\tWorks like Bolt.new or similar AI dev platforms, but with:\n\t•\tFaster response times\n\t•\tNo repetitive errors\n\t•\tNo excessive token usage\n\nIt’s currently in the development phase, but I plan to launch it for free to everyone at the start.\n\nI’m looking for honest feedback. What features would you find useful? What problems should I prioritize solving?\n\nYour input will directly influence how I shape this tool. Looking forward to hearing your thoughts in the comments.",
          "author_fullname": "t2_u5t9kq5vv",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building an App That Builds Apps – Feedback Appreciated",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Generation"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 93,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyptl7",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Generation",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/OYZ5M27a9upsQcZRuC8QqWr8XyHjw-uCoGp77BH_QKk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752403499,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m developing a tool that allows you to create full applications by simply describing what you want in plain English—no complicated setup, no boilerplate code.&lt;/p&gt;\n\n&lt;p&gt;Here’s what it currently offers:\n    • Supports over 10 programming languages\n    • Lets you connect your GitHub repository\n    • Can fix bugs or make improvements in your existing projects\n    • Works like Bolt.new or similar AI dev platforms, but with:\n    • Faster response times\n    • No repetitive errors\n    • No excessive token usage&lt;/p&gt;\n\n&lt;p&gt;It’s currently in the development phase, but I plan to launch it for free to everyone at the start.&lt;/p&gt;\n\n&lt;p&gt;I’m looking for honest feedback. What features would you find useful? What problems should I prioritize solving?&lt;/p&gt;\n\n&lt;p&gt;Your input will directly influence how I shape this tool. Looking forward to hearing your thoughts in the comments.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/0t2fav6bfmcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?auto=webp&amp;s=7851a3fd0e9a8f4050435c357dd9c8e1b79a5b08",
                  "width": 1536,
                  "height": 1024
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=36351b492606d376ec84f6c3266514145d42d6c5",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e8b5638135d991191e0382cc144b86ce44d89ece",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9ee54bc62ef610c65ae8dccc836b6ecc130dc999",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=5e5c7aff51a855990bc95d8e61e471afcef695f3",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=5728de62d148c87ec6be7ba70dec9a2f1481ae2b",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/0t2fav6bfmcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ca6d3ef9e20be3411788c7a738bb7ef1aa547740",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "osOMZVrkvVKxIqXVX5EQh7Ig9gY66_BSUdZF017hArA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "23bddba8-ff56-11ed-9688-1a11994b71f7",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#b5a3d0",
          "id": "1lyptl7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Prestigious_Skin6507",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyptl7/building_an_app_that_builds_apps_feedback/",
          "stickied": false,
          "url": "https://i.redd.it/0t2fav6bfmcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752403499,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_2q7ua5a8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Wrote a deep dive on LLM tool calling with step-by-step REST and Spring AI examples",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyozcn",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=140&amp;height=140&amp;crop=140:140,smart&amp;auto=webp&amp;s=65fdf939c9fee2a51d4ce7ef5c5a36373a796f6a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752400163,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "muthuishere.medium.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?auto=webp&amp;s=ef577e27fadc95bfe3b1744d7ac8872e6e554c70",
                  "width": 610,
                  "height": 936
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bc6872452b4829cdf316ac0294b8c4c189b660af",
                    "width": 108,
                    "height": 165
                  },
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=389d4bc4bbd718d16dd11308be232fde6ab93a01",
                    "width": 216,
                    "height": 331
                  },
                  {
                    "url": "https://external-preview.redd.it/1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6b426f7ec6446f2ee47476b0c04d150cea9b8a5",
                    "width": 320,
                    "height": 491
                  }
                ],
                "variants": {},
                "id": "1FwVKbMgCNO8FIf_E8SSF1AT1y6r8EhRo4JtU_kQIJY"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lyozcn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "muthuishere2101",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyozcn/wrote_a_deep_dive_on_llm_tool_calling_with/",
          "stickied": false,
          "url": "https://muthuishere.medium.com/understanding-tool-function-calling-in-llms-step-by-step-examples-in-rest-and-spring-ai-2149ecd6b18b",
          "subreddit_subscribers": 498343,
          "created_utc": 1752400163,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While I am extremely grateful that people do post the leaked system prompt online for inspiration, but also curious how its actually possible?\n\nThere are three things that come to my mind:\n\n1. ***Using some prompt injection (re-iteratively)***: Some kind of jailbreak prompt and see if same things are being repeated, assuming that is what the actual system prompt is\n2. ***Inspecting the client side code if possible***: For applications intercepting the api requests / client side bundle to find system prompts if any? This sounds hard\n3. ***Changing the request server***: Maybe having a custom model running on my server and changing the base url for the request to hit my resource instead of the default one? Somehow getting the information from there?\n\nIf anyone has any idea how it works, would love to understand. If any resources to read would also be super helpful! Thanks!",
          "author_fullname": "t2_9oi573ps0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How are people actually able to get the system prompt of these AI companies?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyonb4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.7,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752398786,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While I am extremely grateful that people do post the leaked system prompt online for inspiration, but also curious how its actually possible?&lt;/p&gt;\n\n&lt;p&gt;There are three things that come to my mind:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Using some prompt injection (re-iteratively)&lt;/em&gt;&lt;/strong&gt;: Some kind of jailbreak prompt and see if same things are being repeated, assuming that is what the actual system prompt is&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Inspecting the client side code if possible&lt;/em&gt;&lt;/strong&gt;: For applications intercepting the api requests / client side bundle to find system prompts if any? This sounds hard&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Changing the request server&lt;/em&gt;&lt;/strong&gt;: Maybe having a custom model running on my server and changing the base url for the request to hit my resource instead of the default one? Somehow getting the information from there?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If anyone has any idea how it works, would love to understand. If any resources to read would also be super helpful! Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyonb4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "divyamchandel",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyonb4/how_are_people_actually_able_to_get_the_system/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752398786,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So i finally have my LM studio hosting my Models and have AnythingLLM doing my RAG ,  soi thought i would extend to agents ,,, look at Youtube , but nothing is working , its constantly saying that \"I currently **don’t have direct web browsing capabilitie\", what am i doing wrong ?** \n\nhttps://preview.redd.it/1soone6wrlcf1.png?width=931&amp;format=png&amp;auto=webp&amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667\n\nhttps://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;format=png&amp;auto=webp&amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99\n\n",
          "author_fullname": "t2_14166b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Need Help with Agents and AnythingLLM",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "1soone6wrlcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=87daa798ce437071bb40eaaf62b399dc934508d9"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fd84be40eb0b5b3ab77ece5cd4d4b7d8fab1aaee"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=95e37d071beb8b835a3ec3919b6c90d72ccc2e38"
                },
                {
                  "y": 656,
                  "x": 640,
                  "u": "https://preview.redd.it/1soone6wrlcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e84b7ab1f4e5bea326dc353d22f6d75c490e637"
                }
              ],
              "s": {
                "y": 955,
                "x": 931,
                "u": "https://preview.redd.it/1soone6wrlcf1.png?width=931&amp;format=png&amp;auto=webp&amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667"
              },
              "id": "1soone6wrlcf1"
            },
            "a7kj8x8zrlcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0787a02d775b147571850bba9707437fb31fb99c"
                },
                {
                  "y": 220,
                  "x": 216,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6b6d6c31c7fcd66248bc4f4ea5bf49a438764d60"
                },
                {
                  "y": 326,
                  "x": 320,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=717f5ba61fe04c1e4e83e28883ef60482fd5cbe9"
                },
                {
                  "y": 653,
                  "x": 640,
                  "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e5585085769a63ad7ab9923808f9ec1a9706193"
                }
              ],
              "s": {
                "y": 836,
                "x": 819,
                "u": "https://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;format=png&amp;auto=webp&amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99"
              },
              "id": "a7kj8x8zrlcf1"
            }
          },
          "name": "t3_1lynwk4",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/loJ7ObJSRfe1SFeOI0xzI0YfjSwu4U7qV205hbtTOkE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752395690,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So i finally have my LM studio hosting my Models and have AnythingLLM doing my RAG ,  soi thought i would extend to agents ,,, look at Youtube , but nothing is working , its constantly saying that &amp;quot;I currently &lt;strong&gt;don’t have direct web browsing capabilitie&amp;quot;, what am i doing wrong ?&lt;/strong&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/1soone6wrlcf1.png?width=931&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667\"&gt;https://preview.redd.it/1soone6wrlcf1.png?width=931&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b5ce8a44f994ec89013184d4fe86bba8c4a0667&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99\"&gt;https://preview.redd.it/a7kj8x8zrlcf1.png?width=819&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3cfd1ed30008ef4d5fae0c66d229ae0581480d99&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lynwk4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "uber-linny",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lynwk4/need_help_with_agents_and_anythingllm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lynwk4/need_help_with_agents_and_anythingllm/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752395690,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.\\]\n\nLet's say you have a local TTS model that takes a speaker embedding `spk_emb`, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.\n\nHere is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.\n\n    import torch\n    # 1. Initialize the embedding, either randomly or nearest neighbor\n    spk_emb = torch.randn(1, 512) # if batch size 1, dim 512\n    spk_emb.requires_grad = True\n    # 2. Initialize the model and freeze its parameters\n    model = YourModelClass.from_pretrained('TODO')\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device).eval()\n    for p in model.parameters():\n        p.requires_grad = False\n    # 3. Optimizer and dataset, LR is up to you\n    optimizer = torch.optim.Adam([spk_emb], lr=0.001)\n    TODO_your_dataset_of_text_audio_pairs = [\n    ('This is some text.', 'corresponding_audio.wav'),\n    # ...\n    ]\n    # 4. Barebones training loop. You can add a learning rate scheduler, etc.\n    for epoch in range(10): # how many epochs is up to you\n        for text, audio in TODO_your_dataset_of_text_audio_pairs:\n            loss = model.forward_with_loss(text, audio, spk_emb)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\nThe big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.\n\nAnd that's it. If you have any questions you can post them below.",
          "author_fullname": "t2_1e2jjp1mqg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Dark Arts: Speaker embedding gradient descent for local TTS models",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lymlgp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.73,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752392740,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752390519,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;[As with all my posts, the code and text are organic with no LLM involved. Note that I myself have not confirmed that this works in all cases--I personally have no interest in voice cloning--but in my head the theory is strong and I am confident it should work. Plus, there is historical precedent in soft prompting and control vectors.]&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s say you have a local TTS model that takes a speaker embedding &lt;code&gt;spk_emb&lt;/code&gt;, but the model to produce the speaker embedding is unavailable. You can simply apply gradient descent on the speaker embedding and freeze everything else.&lt;/p&gt;\n\n&lt;p&gt;Here is the pseudocode. You will need to change the code depending on the model you are using, and there are plenty of knobs to tune.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;import torch\n# 1. Initialize the embedding, either randomly or nearest neighbor\nspk_emb = torch.randn(1, 512) # if batch size 1, dim 512\nspk_emb.requires_grad = True\n# 2. Initialize the model and freeze its parameters\nmodel = YourModelClass.from_pretrained(&amp;#39;TODO&amp;#39;)\ndevice = &amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;\nmodel.to(device).eval()\nfor p in model.parameters():\n    p.requires_grad = False\n# 3. Optimizer and dataset, LR is up to you\noptimizer = torch.optim.Adam([spk_emb], lr=0.001)\nTODO_your_dataset_of_text_audio_pairs = [\n(&amp;#39;This is some text.&amp;#39;, &amp;#39;corresponding_audio.wav&amp;#39;),\n# ...\n]\n# 4. Barebones training loop. You can add a learning rate scheduler, etc.\nfor epoch in range(10): # how many epochs is up to you\n    for text, audio in TODO_your_dataset_of_text_audio_pairs:\n        loss = model.forward_with_loss(text, audio, spk_emb)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The big caveat here is that you cannot get blood out of a stone; if a speaker is firmly out-of-distribution for the model, no amount of gradient descent will get you to where you want to go.&lt;/p&gt;\n\n&lt;p&gt;And that&amp;#39;s it. If you have any questions you can post them below.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1lymlgp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rzvzn",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lymlgp/dark_arts_speaker_embedding_gradient_descent_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752390519,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I currently have an ASUS TUF Gaming F15, and before people start telling me to give up on local models, let me just say that I have currently been able to successfully run various LLMs and even Images Diffusion models locally with very little issues (mainly just speed and sometimes lag due to OOM). I can easily run 7B Q4_K_Ms and Stable Diffusion/Flux. However, my RAM and GPU max out during such tasks and even sometimes when opening chrome with multiple tabs.\n\nSo I was thinking of upgrading my RAM (since upgrading my GPU is not an option). I currently have 16 GB built-in with an upgrade slot in which I plan on adding 32 GB. Is this a wise decision? Would it be better to have matching RAMs? (16&amp;16/32&amp;32)",
          "author_fullname": "t2_2l48bxrf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I have a Laptop with 3050 Ti 4GB VRAM, will upgrading my RAM from 16 to 48 help?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lymewq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.66,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752389844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I currently have an ASUS TUF Gaming F15, and before people start telling me to give up on local models, let me just say that I have currently been able to successfully run various LLMs and even Images Diffusion models locally with very little issues (mainly just speed and sometimes lag due to OOM). I can easily run 7B Q4_K_Ms and Stable Diffusion/Flux. However, my RAM and GPU max out during such tasks and even sometimes when opening chrome with multiple tabs.&lt;/p&gt;\n\n&lt;p&gt;So I was thinking of upgrading my RAM (since upgrading my GPU is not an option). I currently have 16 GB built-in with an upgrade slot in which I plan on adding 32 GB. Is this a wise decision? Would it be better to have matching RAMs? (16&amp;amp;16/32&amp;amp;32)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lymewq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GamerWael",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lymewq/i_have_a_laptop_with_3050_ti_4gb_vram_will/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752389844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "First behemoth was \"delayed\" by meta and it looks like it is never coming out. Now R2 is delayed by deepseek. Does that mean the end for deepseek too?",
          "author_fullname": "t2_sm168dt0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "When a model is delayed because the boss isn't happy, is it doomed forever?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyltyb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.24,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752387578,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;First behemoth was &amp;quot;delayed&amp;quot; by meta and it looks like it is never coming out. Now R2 is delayed by deepseek. Does that mean the end for deepseek too?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyltyb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMrsPotts",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyltyb/when_a_model_is_delayed_because_the_boss_isnt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyltyb/when_a_model_is_delayed_because_the_boss_isnt/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752387578,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://eqbench.com/](https://eqbench.com/)\n\nWriting samples:\n\n[https://eqbench.com/results/creative-writing-v3/moonshotai\\_\\_Kimi-K2-Instruct.html](https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html)\n\nEQ-Bench responses:\n\n[https://eqbench.com/results/eqbench3\\_reports/moonshotai\\_\\_kimi-k2-instruct.html](https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html)",
          "author_fullname": "t2_pp9qh5t8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Kimi-K2 takes top spot on EQ-Bench3 and Creative Writing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bltx3wip1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c91069e6404ebfdd239fca53b74ad27ebf361cb9"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=46558502029394a5450bb7a8b1fd7772f775b63e"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f8b74bea5bfacbcf65bd04fad681f91f30e63cb"
                },
                {
                  "y": 657,
                  "x": 640,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ea6d2eccaab12c39f50549d82e4485fbfe48cf88"
                },
                {
                  "y": 985,
                  "x": 960,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f04df6fdbd708b96f6a89ab335a6247ac5c6fab"
                },
                {
                  "y": 1109,
                  "x": 1080,
                  "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f38d0ec7cc27f92b56854e865fea4d742abb0b6d"
                }
              ],
              "s": {
                "y": 1641,
                "x": 1598,
                "u": "https://preview.redd.it/bltx3wip1lcf1.png?width=1598&amp;format=png&amp;auto=webp&amp;s=17f01c666d495066ceec9a5e3e74c86504144791"
              },
              "id": "bltx3wip1lcf1"
            },
            "slfo69pq1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 104,
                  "x": 108,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80dd9f9ea9f4c233481f726f596a93be59337d18"
                },
                {
                  "y": 208,
                  "x": 216,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7dc16b4166123dad30767234d268c2ba9cfe207b"
                },
                {
                  "y": 309,
                  "x": 320,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2aa653aee8a77a1c60f613c8eee527eaad88bd09"
                },
                {
                  "y": 618,
                  "x": 640,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=920c5b3b7ca71ba826946d79ece48662e7e61663"
                },
                {
                  "y": 928,
                  "x": 960,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7644b88db8a83e82b2c5386f3aafca392976d38"
                },
                {
                  "y": 1044,
                  "x": 1080,
                  "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc43edacc6efbed8788f53ba27cc9baf10ea878"
                }
              ],
              "s": {
                "y": 1552,
                "x": 1605,
                "u": "https://preview.redd.it/slfo69pq1lcf1.png?width=1605&amp;format=png&amp;auto=webp&amp;s=bc768dc8812c1370ff4a7130928bfb3b76a0bc2d"
              },
              "id": "slfo69pq1lcf1"
            },
            "7dy0n72s1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 69,
                  "x": 108,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f769e577a79198993e84c8eb762589e7a68bfb8"
                },
                {
                  "y": 139,
                  "x": 216,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=86550c6ee8e16b8738d8dc52faa646a74726b22f"
                },
                {
                  "y": 206,
                  "x": 320,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4098fae6e966e2e07a7fe5fa68dc8f1e0842b77a"
                },
                {
                  "y": 413,
                  "x": 640,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4a695523e542b1689dbec502a3e44b4523f96dc"
                },
                {
                  "y": 620,
                  "x": 960,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=69f2c06f0306a5e4a0df7529218415f269ff0ecb"
                },
                {
                  "y": 697,
                  "x": 1080,
                  "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9d1a2d4446112aab3c4639bd2b2bafcd744e7628"
                }
              ],
              "s": {
                "y": 1031,
                "x": 1596,
                "u": "https://preview.redd.it/7dy0n72s1lcf1.png?width=1596&amp;format=png&amp;auto=webp&amp;s=141d64df955b06e799e4f4c0b45e24ddb1211acd"
              },
              "id": "7dy0n72s1lcf1"
            },
            "46yhtq9r1lcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 110,
                  "x": 108,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=47a8d5e353281c77928ebe675d58f1a425091614"
                },
                {
                  "y": 221,
                  "x": 216,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dfbb994f47e60e6719c2c54e5a812ca2c8cdc38e"
                },
                {
                  "y": 328,
                  "x": 320,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a5cb664395530a7771d865e53841a1040c702271"
                },
                {
                  "y": 657,
                  "x": 640,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c1174fd8b0b51963ad2c4fce60bc48d732f7da7"
                },
                {
                  "y": 986,
                  "x": 960,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=16afcea1d3023190bbe776ef801cbf5ac2a7cc39"
                },
                {
                  "y": 1109,
                  "x": 1080,
                  "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b2ef583e66dfa8ed312dfce180f7388aaf630c8b"
                }
              ],
              "s": {
                "y": 1634,
                "x": 1590,
                "u": "https://preview.redd.it/46yhtq9r1lcf1.png?width=1590&amp;format=png&amp;auto=webp&amp;s=9dade8cde09a83f0bab6a44c0f0352e0c91ac03c"
              },
              "id": "46yhtq9r1lcf1"
            }
          },
          "name": "t3_1lylo75",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": "transparent",
          "ups": 507,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "bltx3wip1lcf1",
                "id": 704803294
              },
              {
                "media_id": "slfo69pq1lcf1",
                "id": 704803295
              },
              {
                "media_id": "46yhtq9r1lcf1",
                "id": 704803296
              },
              {
                "media_id": "7dy0n72s1lcf1",
                "id": 704803297
              }
            ]
          },
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 507,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_mu9EQ2-CS-NLztYt8TCn8nhmS5cqsN6BOfAQW9BupA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":Llama:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/23w2nhjj1e9f1_t5_81eyvm/Llama"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752386963,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://eqbench.com/\"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Writing samples:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html\"&gt;https://eqbench.com/results/creative-writing-v3/moonshotai__Kimi-K2-Instruct.html&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;EQ-Bench responses:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html\"&gt;https://eqbench.com/results/eqbench3_reports/moonshotai__kimi-k2-instruct.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1lylo75",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":Llama:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1lylo75",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_sqrkl",
          "discussion_type": null,
          "num_comments": 108,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lylo75/kimik2_takes_top_spot_on_eqbench3_and_creative/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1lylo75",
          "subreddit_subscribers": 498343,
          "created_utc": 1752386963,
          "num_crossposts": 4,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am wondering if anyone did this before, at least I couldn't find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? ",
          "author_fullname": "t2_iaby02kl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you make Loras for Qwen coder /  devstral?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyl697",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.76,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752385140,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am wondering if anyone did this before, at least I couldn&amp;#39;t find information on it. I want to fine tune a coding model without changing the whole model (for hardware restriction reasons). Loras, in theory, would do that. But how? For image and video generation this is pretty much solved and common, but llms? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lyl697",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ComprehensiveBird317",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyl697/how_do_you_make_loras_for_qwen_coder_devstral/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752385140,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was testing the [SmolLM3-3B-WebGPU](https://huggingface.co/spaces/HuggingFaceTB/SmolLM3-3B-WebGPU) Hugging Face Space to check its token speed on my machine (a solid 46 t/s!) before downloading and running it locally. When I prompted it with: \"Are you peter griffin?\", it just generated a 4000-token list of \"Key Takeaways\" about its existence:\n\nhttps://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0\n\nI was only able to trigger this behavior on that specific HF Space (Although, it doesn't seem to be a one time thing. I was able to get *very* similar responses by asking it the same question again in a new tab, after refreshing). I've since downloaded the model and wasn't able to replicate this locally. The model via the Hugging Face Inference also behaves as expected. Could this be caused by the ONNX conversion for WebGPU, or maybe some specific sampling parameters on the space? Has anyone seen anything like this?",
          "author_fullname": "t2_11ymelm88b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM-3B when asked if it was Peter Griffin",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "qn27gqdtqkcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5c31923b903748465c2259725f23e079c2ea73a8"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b441435486467eb144ac3a85148118bb880565bd"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=37bcaa29f3570585da99d0d25a9438e8459920fb"
                },
                {
                  "y": 1280,
                  "x": 640,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ed1acc873492d4236adb25dd0e709e3df9377d"
                },
                {
                  "y": 1920,
                  "x": 960,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3d8ad902650fb83dd6448352c83bae33a5753233"
                },
                {
                  "y": 2160,
                  "x": 1080,
                  "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1565304f9d2b0ba64264e8ed511d4be2b1b9dfe4"
                }
              ],
              "s": {
                "y": 4000,
                "x": 1080,
                "u": "https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0"
              },
              "id": "qn27gqdtqkcf1"
            }
          },
          "name": "t3_1lykqbu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=a7d62e9ff53b573cd757755fe5e699f00ac015e3",
          "edited": 1752384451,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752383565,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was testing the &lt;a href=\"https://huggingface.co/spaces/HuggingFaceTB/SmolLM3-3B-WebGPU\"&gt;SmolLM3-3B-WebGPU&lt;/a&gt; Hugging Face Space to check its token speed on my machine (a solid 46 t/s!) before downloading and running it locally. When I prompted it with: &amp;quot;Are you peter griffin?&amp;quot;, it just generated a 4000-token list of &amp;quot;Key Takeaways&amp;quot; about its existence:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0\"&gt;https://preview.redd.it/qn27gqdtqkcf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a20b4efa6d3891cb83f425a5380358b335640e0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was only able to trigger this behavior on that specific HF Space (Although, it doesn&amp;#39;t seem to be a one time thing. I was able to get &lt;em&gt;very&lt;/em&gt; similar responses by asking it the same question again in a new tab, after refreshing). I&amp;#39;ve since downloaded the model and wasn&amp;#39;t able to replicate this locally. The model via the Hugging Face Inference also behaves as expected. Could this be caused by the ONNX conversion for WebGPU, or maybe some specific sampling parameters on the space? Has anyone seen anything like this?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?auto=webp&amp;s=d140bfdc43765e123922c8ba668ec751fc4dc9d7",
                  "width": 1895,
                  "height": 1066
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=879705209dd5c5f6fab885d338990dee069409f5",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=112e2bc99d3df6e13bff48eee3d8bf43a406d375",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=36f952335f74de0ad9ad33f45afb0c1187581bb7",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=74c41be70bfcf32292dc40a30f75326535854875",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=99b886056edb2c2a7133fe2596573b623ffe50da",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fa87a49c5e5dfcadbe097b266f345195db9eec42",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "tuLFQadP8tHP69V_jd2l5xXDX_8ASMZX4NQj9QhtyOg"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lykqbu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Humble_Hovercraft199",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykqbu/smollm3b_when_asked_if_it_was_peter_griffin/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752383565,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to translate english text to various languages, these include European as well as Asian languages. But since models have problems with asian languages, I trying to make my project work best for European Languages like Spanish, French, German, etc.\n\nCould you guys suggest some open source models to me that can help me perform this task well.",
          "author_fullname": "t2_fqfie7mlc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which model is best for translation?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lykpo6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.55,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752383503,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to translate english text to various languages, these include European as well as Asian languages. But since models have problems with asian languages, I trying to make my project work best for European Languages like Spanish, French, German, etc.&lt;/p&gt;\n\n&lt;p&gt;Could you guys suggest some open source models to me that can help me perform this task well.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lykpo6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "slipped-and-fell",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykpo6/which_model_is_best_for_translation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykpo6/which_model_is_best_for_translation/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752383503,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually [better to do RAG](https://www.databricks.com/blog/long-context-rag-performance-llms).\n\nWhy is that? Does the limit come from architecture or training data?\n\nI could see one problem being too much noise/distraction in the attention scores (like in [this paper](https://arxiv.org/pdf/2410.05258)).\n\nHowever, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.\n\nWhat is the consensus, and how long might it be until the problem is solved?",
          "author_fullname": "t2_1bvkixdlpc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What Causes Poor Long-Context Performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lykf92",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752382484,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;While some models (Gemini, MiniMax, Llama4) claim context lengths in the 1M+ token range, performance beyond ~100K tokens is usually quite poor. Beyond those lengths is it is usually &lt;a href=\"https://www.databricks.com/blog/long-context-rag-performance-llms\"&gt;better to do RAG&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Why is that? Does the limit come from architecture or training data?&lt;/p&gt;\n\n&lt;p&gt;I could see one problem being too much noise/distraction in the attention scores (like in &lt;a href=\"https://arxiv.org/pdf/2410.05258\"&gt;this paper&lt;/a&gt;).&lt;/p&gt;\n\n&lt;p&gt;However, I could also see it being from a lack of long-context training data. A novel is around 100K tokens, so it lines up that performance beyond that degrades due to lack of examples. I believe the creators of Fiction.liveBench have also mentioned the difficulty of creating extremely long context benchmarks.&lt;/p&gt;\n\n&lt;p&gt;What is the consensus, and how long might it be until the problem is solved?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?auto=webp&amp;s=446351fd930a6a3c2288234f2b8553cdffe12e5b",
                  "width": 3047,
                  "height": 1600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=afbb01428ea71e1677bafb9d67ec3dca25236c74",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=66d49f2ee6b1a122d5ca0ad7a49746262dbd0110",
                    "width": 216,
                    "height": 113
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=78d808914c4c69536a9936bd8a7d16f7632e83e8",
                    "width": 320,
                    "height": 168
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=06d504ba6e45772c5e08586046db3251ad0af53b",
                    "width": 640,
                    "height": 336
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d3882037f7df9fd68eaf79f4448fa60bccbb01b",
                    "width": 960,
                    "height": 504
                  },
                  {
                    "url": "https://external-preview.redd.it/88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5854508c74dcb490f587749a0bee1621d5c15b2e",
                    "width": 1080,
                    "height": 567
                  }
                ],
                "variants": {},
                "id": "88QOjYTKpMZIM86KNi6b42N407jrxbaPAjbmnwm1Z0A"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lykf92",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "simulated-souls",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykf92/what_causes_poor_longcontext_performance/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752382484,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1. I want to stick to using vscode\n2. Currently using chatgpt plus for coding but dont like going back and forth between windows\n3. Is there anything like copilot (keep being told it sucks) but powered by an LLM of my choice eg. something by OpenAI or Anthropic?\n4. I dont understand why Claude Code is the king now when the chatting is via a terminal....isnt that bad UX if you ask a question and you get a snippet of code and you cant even press a copy button for the snippet?",
          "author_fullname": "t2_txuj9o55",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What LLMs work with VScode like copilot?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lykf38",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752382470,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;I want to stick to using vscode&lt;/li&gt;\n&lt;li&gt;Currently using chatgpt plus for coding but dont like going back and forth between windows&lt;/li&gt;\n&lt;li&gt;Is there anything like copilot (keep being told it sucks) but powered by an LLM of my choice eg. something by OpenAI or Anthropic?&lt;/li&gt;\n&lt;li&gt;I dont understand why Claude Code is the king now when the chatting is via a terminal....isnt that bad UX if you ask a question and you get a snippet of code and you cant even press a copy button for the snippet?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lykf38",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sprmgtrb",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lykf38/what_llms_work_with_vscode_like_copilot/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lykf38/what_llms_work_with_vscode_like_copilot/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752382470,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any suggestions for providers to use for GLM-4. Tried open router but it's very slow even with max tokens set to 8K. Need generation time to be &lt;4 minutes ideally. ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What providers are people using for GLM-4?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyjm7t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752379729,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any suggestions for providers to use for GLM-4. Tried open router but it&amp;#39;s very slow even with max tokens set to 8K. Need generation time to be &amp;lt;4 minutes ideally. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyjm7t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyjm7t/what_providers_are_people_using_for_glm4/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyjm7t/what_providers_are_people_using_for_glm4/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752379729,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m working on a browser automation system that follows a planned sequence of UI actions, but needs an LLM to resolve which DOM element to click when there are multiple similar options. I’ve been using **Browser-Use**, which is solid for tracking state/actions, but execution is too slow — especially when an LLM is in the loop at each step.\n\n**Example flow (on Google settings):**\n\n1. Go to [myaccount.google.com](https://myaccount.google.com)\n2. Click “Data &amp; privacy”\n3. Scroll down\n4. Click “Delete a service or your account”\n5. Click “Delete your Google Account”\n\n**Looking for suggestions:**\n\n* Fastest models for small structured decision tasks \n* Ways to be under 1s per step (ideally &lt;500ms)\n\nI don’t need full chat reasoning — just high-confidence decisions from small JSON lists.\n\nWould love to hear what setups/models have worked for you in similar low-latency UI agent tasks 🙏",
          "author_fullname": "t2_8fuwhsmu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Help] Fastest model for real-time UI automation? (Browser-Use too slow)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyjgwv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 12,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 12,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752379254,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m working on a browser automation system that follows a planned sequence of UI actions, but needs an LLM to resolve which DOM element to click when there are multiple similar options. I’ve been using &lt;strong&gt;Browser-Use&lt;/strong&gt;, which is solid for tracking state/actions, but execution is too slow — especially when an LLM is in the loop at each step.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Example flow (on Google settings):&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Go to &lt;a href=\"https://myaccount.google.com\"&gt;myaccount.google.com&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Click “Data &amp;amp; privacy”&lt;/li&gt;\n&lt;li&gt;Scroll down&lt;/li&gt;\n&lt;li&gt;Click “Delete a service or your account”&lt;/li&gt;\n&lt;li&gt;Click “Delete your Google Account”&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Looking for suggestions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fastest models for small structured decision tasks &lt;/li&gt;\n&lt;li&gt;Ways to be under 1s per step (ideally &amp;lt;500ms)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I don’t need full chat reasoning — just high-confidence decisions from small JSON lists.&lt;/p&gt;\n\n&lt;p&gt;Would love to hear what setups/models have worked for you in similar low-latency UI agent tasks 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyjgwv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "BulkyAd7044",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyjgwv/help_fastest_model_for_realtime_ui_automation/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752379254,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The International Math Olympiad will take place on 15th and 16th July in Australia.\nGoogle Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024.\nAny open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.\n\nhttps://youtu.be/vJjgtOcXq8A",
          "author_fullname": "t2_2y1uo4e2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do you think an AI will achieve gold medal in 2025 International Math Olympad (tomorrow)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyj81f",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 67,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 67,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752378438,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The International Math Olympiad will take place on 15th and 16th July in Australia.\nGoogle Deepmind will attempt to win a gold medal with their models AlphaProof and AlphaGeometry, after announcing a silver medal performance in 2024.\nAny open-source model that wins a gold medal will receive a $5 million AIMO prize from XTX markets.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/vJjgtOcXq8A\"&gt;https://youtu.be/vJjgtOcXq8A&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?auto=webp&amp;s=e75662789bd989c9174581c02e50a281f52c2bbd",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f46c2aab57d92285291510c11c6203406a312438",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=bb4bd5e2ac6ba3f9443d7d7290197bbc4f7f2f51",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=df1b5c91733b08002716383c6d970bec36dfbf46",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "AcBUFtk7lDqLasMm0XdsCf0q3gy8tW9LCvLdK4SfhIc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyj81f",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mathsTeacher82",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyj81f/do_you_think_an_ai_will_achieve_gold_medal_in/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752378438,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I come across many v100 32g gpus, ecc all intact for $360 on chinese second hand market (I live in China) and can easily get stuff like bifurcated 300G nvlink sxm2 to pcie adapters etc. for no more than $40. \n\nAlso, if I get the 16gb version of the v100, it only costs $80 per card. \n\nWouldn't this be a better deal than something like a 4060ti or even 3090s (if I get 3 32gb v100s) for LLMs?",
          "author_fullname": "t2_1cqq3fards",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "32g SXM2 V100s for $360, Good Deal for LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyiyvq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752377595,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I come across many v100 32g gpus, ecc all intact for $360 on chinese second hand market (I live in China) and can easily get stuff like bifurcated 300G nvlink sxm2 to pcie adapters etc. for no more than $40. &lt;/p&gt;\n\n&lt;p&gt;Also, if I get the 16gb version of the v100, it only costs $80 per card. &lt;/p&gt;\n\n&lt;p&gt;Wouldn&amp;#39;t this be a better deal than something like a 4060ti or even 3090s (if I get 3 32gb v100s) for LLMs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyiyvq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "starikari",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyiyvq/32g_sxm2_v100s_for_360_good_deal_for_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyiyvq/32g_sxm2_v100s_for_360_good_deal_for_llms/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752377595,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\nHi LocalLLaMA community,\n\nI am a researcher, and recently I have noticed that LLMs such as OpenAI's and Google's are not good at generating academic-style and/or beautiful plots. Open sourced model also doesn’t work well. Beyond the simple plots which they can do just fine, anything more advanced that includes LaTex tikz library etc, will simply just fail.\n\nHas anyone encounter similar issues? If so, any suggestions or recommendations on this? Thank you so much!\n\nTL;DR: Trying to use LLMs to generate academic-style plots but they are not good at all.",
          "author_fullname": "t2_kb2yczc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Any suggestions for generating academic-style/advanced plots?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyitq9",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752377125,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LocalLLaMA community,&lt;/p&gt;\n\n&lt;p&gt;I am a researcher, and recently I have noticed that LLMs such as OpenAI&amp;#39;s and Google&amp;#39;s are not good at generating academic-style and/or beautiful plots. Open sourced model also doesn’t work well. Beyond the simple plots which they can do just fine, anything more advanced that includes LaTex tikz library etc, will simply just fail.&lt;/p&gt;\n\n&lt;p&gt;Has anyone encounter similar issues? If so, any suggestions or recommendations on this? Thank you so much!&lt;/p&gt;\n\n&lt;p&gt;TL;DR: Trying to use LLMs to generate academic-style plots but they are not good at all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyitq9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "plsendfast",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyitq9/any_suggestions_for_generating/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752377125,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I finally got a RTX 3060 12GB to start using AI. Now I wanted to know what's the heaviest it can run and if there are new methods of increasing performance by now. Ideally, I can't read at speed of light so models that might run at 4-6 words per second is enough.\n\nI can't upgrade from 12GB to 32GB ram yet, so what is this GPU capable of running asides from Wizard Viccuna 13b?",
          "author_fullname": "t2_eljq22kg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Heaviest model that can be ran with RTX 3060 12Gb?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyhuuq",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752373998,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got a RTX 3060 12GB to start using AI. Now I wanted to know what&amp;#39;s the heaviest it can run and if there are new methods of increasing performance by now. Ideally, I can&amp;#39;t read at speed of light so models that might run at 4-6 words per second is enough.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t upgrade from 12GB to 32GB ram yet, so what is this GPU capable of running asides from Wizard Viccuna 13b?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyhuuq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "WEREWOLF_BX13",
          "discussion_type": null,
          "num_comments": 18,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyhuuq/heaviest_model_that_can_be_ran_with_rtx_3060_12gb/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752373998,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Maybe it'd be more of an LLM tool designed for book writing than the other way around but I'm looking for software that can utilize a locally running LLM to help me write a book.\n\nHoping for something where I can include descriptions of characters, set the scenes, basic outline and such. Then let the LLM do the bulk of the work.\n\nDoes this sort of thing exist?",
          "author_fullname": "t2_6fu5vgz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there any book writing software that can utilize an local LLM?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyhnhw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752373360,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe it&amp;#39;d be more of an LLM tool designed for book writing than the other way around but I&amp;#39;m looking for software that can utilize a locally running LLM to help me write a book.&lt;/p&gt;\n\n&lt;p&gt;Hoping for something where I can include descriptions of characters, set the scenes, basic outline and such. Then let the LLM do the bulk of the work.&lt;/p&gt;\n\n&lt;p&gt;Does this sort of thing exist?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyhnhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "123android",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyhnhw/is_there_any_book_writing_software_that_can/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyhnhw/is_there_any_book_writing_software_that_can/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752373360,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I feel like everyday I come here someone mentions a a new tool or a newly released model or software that I never heard off. Where in earth are you going to get your most up to dated trusted news/info? ",
          "author_fullname": "t2_21pp8tew",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How do you keep up with all these things?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyfngg",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752367147,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I feel like everyday I come here someone mentions a a new tool or a newly released model or software that I never heard off. Where in earth are you going to get your most up to dated trusted news/info? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyfngg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ontologicalmemes",
          "discussion_type": null,
          "num_comments": 36,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyfngg/how_do_you_keep_up_with_all_these_things/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752367147,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/n6thrdocbjcf1.png?width=381&amp;format=png&amp;auto=webp&amp;s=ea962f86d426d755e1a178186a31bdced2351b52\n\nI was asking it about it's role.",
          "author_fullname": "t2_71knjqyi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anybody else broken Meta \"Ai\" yet?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "n6thrdocbjcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 108,
                  "x": 108,
                  "u": "https://preview.redd.it/n6thrdocbjcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d8fef11c203bed792989222428082f27cb1414a9"
                },
                {
                  "y": 216,
                  "x": 216,
                  "u": "https://preview.redd.it/n6thrdocbjcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c6ebb61b965e289849f88a745b60b299cb81d9c3"
                },
                {
                  "y": 320,
                  "x": 320,
                  "u": "https://preview.redd.it/n6thrdocbjcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8689ab3f9ae7967985f69cbad10aaf4930a452e1"
                }
              ],
              "s": {
                "y": 382,
                "x": 381,
                "u": "https://preview.redd.it/n6thrdocbjcf1.png?width=381&amp;format=png&amp;auto=webp&amp;s=ea962f86d426d755e1a178186a31bdced2351b52"
              },
              "id": "n6thrdocbjcf1"
            }
          },
          "name": "t3_1lyf8g5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/frp0MfrS_YARZUwzgAGbcJ_xriEBxL8zW27dYrCU6_w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752365889,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/n6thrdocbjcf1.png?width=381&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea962f86d426d755e1a178186a31bdced2351b52\"&gt;https://preview.redd.it/n6thrdocbjcf1.png?width=381&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea962f86d426d755e1a178186a31bdced2351b52&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I was asking it about it&amp;#39;s role.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyf8g5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChrisZavadil",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyf8g5/anybody_else_broken_meta_ai_yet/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyf8g5/anybody_else_broken_meta_ai_yet/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752365889,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone who actually codes with local LLM on their laptops, what's your setup and are you happy with the quality and speed?  Should I even bother trying to code with an LLM that fits on a laptop GPU, or just tether back to my beefier home server or openrouter?",
          "author_fullname": "t2_4nw3v",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Laptop GPU for Agentic Coding -- Worth it?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyen05",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 9,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 9,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752364130,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone who actually codes with local LLM on their laptops, what&amp;#39;s your setup and are you happy with the quality and speed?  Should I even bother trying to code with an LLM that fits on a laptop GPU, or just tether back to my beefier home server or openrouter?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyen05",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "randomqhacker",
          "discussion_type": null,
          "num_comments": 29,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752364130,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "&gt;Over the past several months, DeepSeek's engineers have been working to refine R2 until Liang gives the green light for release, according to The Information. However, a fast adoption of R2 could be difficult due to a shortage of Nvidia server chips in China as a result of U.S. export regulations, the report said, citing employees of top Chinese cloud firms that offer DeepSeek's models to enterprise customers.\n\n&gt;A potential surge in demand for R2 would overwhelm Chinese cloud providers, who need advanced Nvidia chips to run AI models, the report said.\n\n&gt;DeepSeek did not immediately respond to a Reuters request for comment.\n\n&gt;DeepSeek has been in touch with some Chinese cloud companies, providing them with technical specifications to guide their plans for hosting and distributing the model from their servers, the report said.\n\n&gt;Among its cloud customers currently using R1, the majority are running the model with Nvidia's H20 chips, The Information said.\n\n&gt;Fresh export curbs imposed by the Trump administration in April have prevented Nvidia from selling in the Chinese market its H20 chips - the only AI processors it could legally export to the country at the time.",
          "author_fullname": "t2_1a48h7vf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DeepSeek R2 delayed",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lydp3k",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.41,
          "author_flair_background_color": "transparent",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/zL9tRhZpWOl3AEcjftBkZuyM6bXfKasoUrHMqHxHlJk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "a": ":X:",
              "e": "emoji",
              "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X"
            }
          ],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752361453,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Over the past several months, DeepSeek&amp;#39;s engineers have been working to refine R2 until Liang gives the green light for release, according to The Information. However, a fast adoption of R2 could be difficult due to a shortage of Nvidia server chips in China as a result of U.S. export regulations, the report said, citing employees of top Chinese cloud firms that offer DeepSeek&amp;#39;s models to enterprise customers.&lt;/p&gt;\n\n&lt;p&gt;A potential surge in demand for R2 would overwhelm Chinese cloud providers, who need advanced Nvidia chips to run AI models, the report said.&lt;/p&gt;\n\n&lt;p&gt;DeepSeek did not immediately respond to a Reuters request for comment.&lt;/p&gt;\n\n&lt;p&gt;DeepSeek has been in touch with some Chinese cloud companies, providing them with technical specifications to guide their plans for hosting and distributing the model from their servers, the report said.&lt;/p&gt;\n\n&lt;p&gt;Among its cloud customers currently using R1, the majority are running the model with Nvidia&amp;#39;s H20 chips, The Information said.&lt;/p&gt;\n\n&lt;p&gt;Fresh export curbs imposed by the Trump administration in April have prevented Nvidia from selling in the Chinese market its H20 chips - the only AI processors it could legally export to the country at the time.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/i6y02pp6yicf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/i6y02pp6yicf1.jpeg?auto=webp&amp;s=34ee951d11f9fef006389618b0dde4841b80ee4c",
                  "width": 1080,
                  "height": 654
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/i6y02pp6yicf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=5b9ade7b934554601c33a4907a46f290f5035d6b",
                    "width": 108,
                    "height": 65
                  },
                  {
                    "url": "https://preview.redd.it/i6y02pp6yicf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=82f382f7bdd7a6db11c7f97bed6582ca8c49fc5d",
                    "width": 216,
                    "height": 130
                  },
                  {
                    "url": "https://preview.redd.it/i6y02pp6yicf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f31b86c0a9acfeacf6b70cd44ec1c95add546793",
                    "width": 320,
                    "height": 193
                  },
                  {
                    "url": "https://preview.redd.it/i6y02pp6yicf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6671c7db624902945fe25d969148234fef57adba",
                    "width": 640,
                    "height": 387
                  },
                  {
                    "url": "https://preview.redd.it/i6y02pp6yicf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=fbb868e7b42f80c84a3526a90d6273b53ece9a66",
                    "width": 960,
                    "height": 581
                  },
                  {
                    "url": "https://preview.redd.it/i6y02pp6yicf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1c20c9068e888fdff3da07d29f7f0b39ed9434c3",
                    "width": 1080,
                    "height": 654
                  }
                ],
                "variants": {},
                "id": "VrR5vzoQg2UWwQ080ktXR0IHDvNNjH6QyfKDIDtx77M"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": ":X:",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lydp3k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "entsnack",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "dark",
          "permalink": "/r/LocalLLaMA/comments/1lydp3k/deepseek_r2_delayed/",
          "stickied": false,
          "url": "https://i.redd.it/i6y02pp6yicf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752361453,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hey,\n\nI’m working on a system that uses reranking to select the best LLM for each specific task. To do this, I want to use a trusted website as a knowledge base—ideally one that provides leaderboards across multiple benchmarks and tasks so I can retrieve reliable performance info for different models.\n\nQuestion 1:\nWhat websites or platforms do you recommend that have comprehensive, trusted leaderboards for LLMs across diverse benchmarks?\n\nQuestion 2:\nAlso, when deploying an LLM in production without ground truth labels, how do you measure its performance? I want to compare my solution against baselines like GPT, but:\n\nI don’t have ground truth data\n\nUsing an LLM as judge seems biased, especially if it’s similar to the baseline GPT model\n\nI have many use cases, so evaluation should be general and fair\n\nWhat metrics or strategies would you suggest to reliably know if my LLM solution is better or worse than GPT in real production scenarios?\n\nThanks in advance for your tips!",
          "author_fullname": "t2_1sxrasjt9q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for trusted websites with benchmark leaderboards to build LLM reranking — plus how to evaluate LLMs in production without ground truth?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyckyk",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752358347,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey,&lt;/p&gt;\n\n&lt;p&gt;I’m working on a system that uses reranking to select the best LLM for each specific task. To do this, I want to use a trusted website as a knowledge base—ideally one that provides leaderboards across multiple benchmarks and tasks so I can retrieve reliable performance info for different models.&lt;/p&gt;\n\n&lt;p&gt;Question 1:\nWhat websites or platforms do you recommend that have comprehensive, trusted leaderboards for LLMs across diverse benchmarks?&lt;/p&gt;\n\n&lt;p&gt;Question 2:\nAlso, when deploying an LLM in production without ground truth labels, how do you measure its performance? I want to compare my solution against baselines like GPT, but:&lt;/p&gt;\n\n&lt;p&gt;I don’t have ground truth data&lt;/p&gt;\n\n&lt;p&gt;Using an LLM as judge seems biased, especially if it’s similar to the baseline GPT model&lt;/p&gt;\n\n&lt;p&gt;I have many use cases, so evaluation should be general and fair&lt;/p&gt;\n\n&lt;p&gt;What metrics or strategies would you suggest to reliably know if my LLM solution is better or worse than GPT in real production scenarios?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for your tips!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lyckyk",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Realistic_Force688",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyckyk/looking_for_trusted_websites_with_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lyckyk/looking_for_trusted_websites_with_benchmark/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752358347,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;format=png&amp;auto=webp&amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a\n\nInternal networking components for Nvidia’s System: Is there are market for these components? ",
          "author_fullname": "t2_1hg4ipja0o",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Internal networking components for Nvidia’s System",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "rkv3n874jicf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 130,
                  "x": 108,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3efb7bdf44090f43dd5ea01d7c4ec4a5f29632d0"
                },
                {
                  "y": 260,
                  "x": 216,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=337859b9dfa64dd2dc2f0c879587740d59137a39"
                },
                {
                  "y": 385,
                  "x": 320,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=501d698b1db0b78ea72320ea1033f8450b8873fe"
                },
                {
                  "y": 771,
                  "x": 640,
                  "u": "https://preview.redd.it/rkv3n874jicf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2d45906216cd3cf646276c3a6a82e764bc94f5ee"
                }
              ],
              "s": {
                "y": 1042,
                "x": 864,
                "u": "https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;format=png&amp;auto=webp&amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a"
              },
              "id": "rkv3n874jicf1"
            }
          },
          "name": "t3_1lybx9x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/HiFDITuOLm4brlaiG8SdYrK-Wl1SnbxLPzr66D7CQBY.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752356511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a\"&gt;https://preview.redd.it/rkv3n874jicf1.png?width=864&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ee1e654f60ad70a48b9a82c5606dd8f1fa7c006a&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Internal networking components for Nvidia’s System: Is there are market for these components? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybx9x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "250sunnyisles",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybx9x/internal_networking_components_for_nvidias_system/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybx9x/internal_networking_components_for_nvidias_system/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752356511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've heard they are somewhat okay for llms and for like a little less than half the price of a 3060 they seem pretty enticing but I just need some advice on wether I should buy one of these two or pass on them.",
          "author_fullname": "t2_1p880h208f",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Should I buy Tesla K80 for 70€ or Tesla M10 for 110€?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybqtw",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752356014,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve heard they are somewhat okay for llms and for like a little less than half the price of a 3060 they seem pretty enticing but I just need some advice on wether I should buy one of these two or pass on them.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybqtw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Similar-Republic149",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybqtw/should_i_buy_tesla_k80_for_70_or_tesla_m10_for_110/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybqtw/should_i_buy_tesla_k80_for_70_or_tesla_m10_for_110/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752356014,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_es8s3sg80",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Unlocking AMD MI300X for High-Throughput, Low-Cost LLM Inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybm7b",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752355669,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "herdora.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.herdora.com/blog/the-overlooked-gpu",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lybm7b",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Upstairs-Fun8458",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybm7b/unlocking_amd_mi300x_for_highthroughput_lowcost/",
          "stickied": false,
          "url": "https://www.herdora.com/blog/the-overlooked-gpu",
          "subreddit_subscribers": 498343,
          "created_utc": 1752355669,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What's the simplest way to get an uncensored LLM with image generation set up in the cloud? If one doesn't need much customization and to play with many options, but just wants speed and ease-of-use, what's the best way?",
          "author_fullname": "t2_7xl0sekq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Runpod, Hugging Face, or what for super-simple uncensored LLM-in-the-cloud setup?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybh8e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.63,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752355300,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s the simplest way to get an uncensored LLM with image generation set up in the cloud? If one doesn&amp;#39;t need much customization and to play with many options, but just wants speed and ease-of-use, what&amp;#39;s the best way?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybh8e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "goldenapple212",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybh8e/runpod_hugging_face_or_what_for_supersimple/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybh8e/runpod_hugging_face_or_what_for_supersimple/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752355300,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why no aider polyglot benchmark test for qwen3-30b-a3b ?   \nWhat would the numbers be if someone passed the benchmark ?",
          "author_fullname": "t2_7zjipz97",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-30B-A3B aider polyglot score?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lybdr2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752355036,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why no aider polyglot benchmark test for qwen3-30b-a3b ?&lt;br/&gt;\nWhat would the numbers be if someone passed the benchmark ?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lybdr2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LogicalSink1366",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lybdr2/qwen330ba3b_aider_polyglot_score/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lybdr2/qwen330ba3b_aider_polyglot_score/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752355036,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In time-honored tradition we present the relative physical dimensions of the Workstation Pro 6000.",
          "author_fullname": "t2_1t7r9dkpud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Banana for scale",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 91,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyb8tz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.74,
          "author_flair_background_color": null,
          "ups": 26,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 26,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jCng4TuAvwrX1AV7UemR4VzBuvP3-H0aGZ6F9oKJ7rk.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752354670,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In time-honored tradition we present the relative physical dimensions of the Workstation Pro 6000.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/3gsbxg74eicf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?auto=webp&amp;s=5f8a046e9b08931daa9153d62b9cae47c75a77a9",
                  "width": 3962,
                  "height": 2580
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c4cff1cf527148cb67e28ceb515a52bec33ece4d",
                    "width": 108,
                    "height": 70
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=3802e417956d3284405dd8858361c8090a9d021e",
                    "width": 216,
                    "height": 140
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=659728b96cbf1ffe05578dd8f8f8dd3dedd76ecf",
                    "width": 320,
                    "height": 208
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4f149ec7379ca441b40ea5b4c75ffb6609f7405",
                    "width": 640,
                    "height": 416
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=31f73ee59fb986766a53e5cdc594d55c3f686c35",
                    "width": 960,
                    "height": 625
                  },
                  {
                    "url": "https://preview.redd.it/3gsbxg74eicf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8679591edcc69b07083d11b0c446de23eccc8a09",
                    "width": 1080,
                    "height": 703
                  }
                ],
                "variants": {},
                "id": "mDdP7oD7tzwjvVGCKQ4KVJzRxTtVIJCTqyz3vcP2noQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lyb8tz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackwell_tart",
          "discussion_type": null,
          "num_comments": 28,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyb8tz/banana_for_scale/",
          "stickied": false,
          "url": "https://i.redd.it/3gsbxg74eicf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752354670,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "- Screenshot: https://openrouter.ai/moonshotai\n- Announcement: https://moonshotai.github.io/Kimi-K2/\n- Model: https://huggingface.co/moonshotai/Kimi-K2-Instruct",
          "author_fullname": "t2_14okit",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Moonshot AI just made their moonshot",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 128,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lyaozv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 726,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 726,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/xVPlJS3y_4S_JgFzimM_t38EhPQ9pdHyidqsBpHzW4M.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752353207,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;Screenshot: &lt;a href=\"https://openrouter.ai/moonshotai\"&gt;https://openrouter.ai/moonshotai&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Announcement: &lt;a href=\"https://moonshotai.github.io/Kimi-K2/\"&gt;https://moonshotai.github.io/Kimi-K2/&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;Model: &lt;a href=\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/95q67pnr9icf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?auto=webp&amp;s=c3212fdbf5d068289e4e365e2b57a3a2158a32cf",
                  "width": 1941,
                  "height": 1782
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=f5225d3e72ca9cbedd6a6f3b7456742c3f7a2d71",
                    "width": 108,
                    "height": 99
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a90fd4c772f2ce00a3aa69f9a7d6023ff092d4b7",
                    "width": 216,
                    "height": 198
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5a8c70504908df2e6c29a0bef03f9c709496057",
                    "width": 320,
                    "height": 293
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2af006a61647e3c965c2e033c957c97e3e1f42cd",
                    "width": 640,
                    "height": 587
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f360d1f154858d51ea49b78d8794de1120ff330",
                    "width": 960,
                    "height": 881
                  },
                  {
                    "url": "https://preview.redd.it/95q67pnr9icf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72830cc0e94d363d613f18dbe56b2210ac66ee99",
                    "width": 1080,
                    "height": 991
                  }
                ],
                "variants": {},
                "id": "iIm4EKYFT0antT-Zwv7DxEcCvUorkX5MOtF-h16njUw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1lyaozv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Balance-",
          "discussion_type": null,
          "num_comments": 125,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lyaozv/moonshot_ai_just_made_their_moonshot/",
          "stickied": false,
          "url": "https://i.redd.it/95q67pnr9icf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752353207,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1lrrxvbn60",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Browser Use vs Model Context Protocol (MCP): Two Philosophies for AI Interaction with the Digital World",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lya4ks",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752351757,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "linkedin.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.linkedin.com/pulse/browser-use-vs-model-context-protocol-mcp-two-ai-interaction-wang-irqye/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lya4ks",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Crafty_Read_6928",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lya4ks/browser_use_vs_model_context_protocol_mcp_two/",
          "stickied": false,
          "url": "https://www.linkedin.com/pulse/browser-use-vs-model-context-protocol-mcp-two-ai-interaction-wang-irqye/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752351757,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone! First time poster - thought I'd share a project I've been working on - it's local LLama integration with HA and custom functions outside of HA; my main goal was to have a system that could understand descriptions of items instead of hard-names (like \"turn on the light above the desk\" instead of \"turn on the desk light\" and which could do so in multiple languages, without having to use English words in Spanish (for example).\n\nProject is still in the early stages but I do have ideas for it an intend to develop it further - feedback and thoughts are appreciated!\n\n[https://github.com/Nemesis533/Local\\_LLHAMA/](https://github.com/Nemesis533/Local_LLHAMA/)\n\nP.S - had to re-do the post as the other one was done with the wrong account.",
          "author_fullname": "t2_w0pml",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local Llama with Home Assistant Integration and Multilingual-Fuzzy naming",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly983h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752349402,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone! First time poster - thought I&amp;#39;d share a project I&amp;#39;ve been working on - it&amp;#39;s local LLama integration with HA and custom functions outside of HA; my main goal was to have a system that could understand descriptions of items instead of hard-names (like &amp;quot;turn on the light above the desk&amp;quot; instead of &amp;quot;turn on the desk light&amp;quot; and which could do so in multiple languages, without having to use English words in Spanish (for example).&lt;/p&gt;\n\n&lt;p&gt;Project is still in the early stages but I do have ideas for it an intend to develop it further - feedback and thoughts are appreciated!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Nemesis533/Local_LLHAMA/\"&gt;https://github.com/Nemesis533/Local_LLHAMA/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;P.S - had to re-do the post as the other one was done with the wrong account.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?auto=webp&amp;s=782265ef4cb49db7702d7bd8b0ea3f328c56fe75",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d129aaa0a1c8de108becec1331e907baf548e00e",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=213e35cc4efc2c49eedf98df38f78ff37abfd596",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=df6d87723b0399b7ab3f6c4827602a95748134e7",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1469b9cf298ce76c20f9bcbbc8e12ab413af3e5d",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1b4e43dfad83048395f032dad898b4ee6d153b7e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60856ce3ea82009d3504dfc51999f7dc05041eb8",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "rfWFqkf9v8783kPkMuv2GffIRIrnMh42wtNX-UcWXDU"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly983h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "NicolaZanarini533",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly983h/local_llama_with_home_assistant_integration_and/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly983h/local_llama_with_home_assistant_integration_and/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752349402,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "This whole thing is giving me WizardLM2 vibes.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly8fyj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 186,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 186,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/0MvAUHXRbVLvw9fpxJeHJvJiKmKUs3UmwSpvxALQEM4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752347384,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/kn56m7cgshcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?auto=webp&amp;s=59a0a5113d7efda17e67eb417bc9bdfdc972845f",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d47fc551fde0f69d3b2c5b28f04df0cfdf9c410",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=8510a3928b99b1f0d4c5fdba76a7b4a4ed90d447",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c547c00d3c5ded80b139847756e6f24778c292a4",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a11d5998e82e27b041a8e6dd74d76c55a2f8a104",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2df93661a1461d0901e12ea5b5b41cb79d2f94e3",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/kn56m7cgshcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49489e01812662b0bc81aa38fd05c5b31a316b45",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "GepSeYli2R4WjSQ1YHbDEYhIkFkRkNdQpj4AK3DFoS4"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ly8fyj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly8fyj/this_whole_thing_is_giving_me_wizardlm2_vibes/",
          "stickied": false,
          "url": "https://i.redd.it/kn56m7cgshcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752347384,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_2kndo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mlx-community/Kimi-Dev-72B-4bit-DWQ",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly894z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 48,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 48,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=72d2b357e724e0f945e73129c5d84c2c90c3cea5",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752346900,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?auto=webp&amp;s=c64aab1aed9e2d49380cc716994670f2d827840d",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3e7e74ad5490178118f9e4b34721c8061ae3ecfe",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df80ea7eb2a7c2b5c2ce50ed1283cab52fe4dd03",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=64920f558efe0645329b540325846e2773ccb0ea",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d6276c1fe0578c79ffa2210b8dfa820b87e4242",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ae2c71d5c9d82b6adcd1de3f6d1f56ecae619a7",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3ec1f50878546ff7d199e6e418dcaae525ef63a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "mpAxC0SvYZFldRQAxpLzzBwXNoDqpak4MT8PL4S0bMw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ly894z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Recoil42",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly894z/mlxcommunitykimidev72b4bitdwq/",
          "stickied": false,
          "url": "https://huggingface.co/mlx-community/Kimi-Dev-72B-4bit-DWQ",
          "subreddit_subscribers": 498343,
          "created_utc": 1752346900,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI’ve developed a tool that calculates the *optimal quantisation mix* tailored to your VRAM and RAM specifications specifically for the DeepSeek-R1-0528 model. If you’d like to try it out, you can find it here:  \n🔗 [GGUF Tool Suite on GitHub](https://github.com/Thireus/GGUF-Tool-Suite/)\n\nYou can also create custom quantisation recipes using this Colab notebook:  \n🔗 [Quant Recipe Pipeline](https://colab.research.google.com/github/Thireus/GGUF-Tool-Suite/blob/main/quant_recipe_pipeline.ipynb)\n\nOnce you have a recipe, use the [quant_downloader.sh](https://github.com/Thireus/GGUF-Tool-Suite/blob/main/quant_downloader.sh) script to download the model shards using the `.recipe` file. Please note that the scripts have mainly been tested in a Linux environment; support for macOS is planned. For best results, run the downloader on Linux. After downloading, load the model with `ik_llama` using [this patch](https://github.com/Thireus/ik_llama.cpp/commit/a66490410a366a9605234b94d67f3d9b7b389140) (also don’t forget to run `ulimit -n 99999` first).\n\nYou can find examples of recipes (including perplexity scores and other metrics) available here:  \n🔗 [Recipe Examples](https://github.com/Thireus/GGUF-Tool-Suite/tree/main/recipe_examples)\n\nI've tried to produce examples to benchmark against GGUF quants from other reputable creators such as unsloth, ubergarm, bartowski.\n\nFor full details and setup instructions, please refer to the repo’s README:  \n🔗 [GGUF Tool Suite README](https://github.com/Thireus/GGUF-Tool-Suite/)\n\nI’m also planning to publish an article soon that will explore the capabilities of the GGUF Tool Suite and demonstrate how it can be used to produce an optimised mixture of quants for other LLM models.\n\nI’d love to hear your feedback or answer any questions you may have!\n",
          "author_fullname": "t2_8u7n5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Introducing GGUF Tool Suite - Create and Optimise Quantisation Mix for DeepSeek-R1-0528 for Your Own Specs",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly84xd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752395931,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752346612,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I’ve developed a tool that calculates the &lt;em&gt;optimal quantisation mix&lt;/em&gt; tailored to your VRAM and RAM specifications specifically for the DeepSeek-R1-0528 model. If you’d like to try it out, you can find it here:&lt;br/&gt;\n🔗 &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/\"&gt;GGUF Tool Suite on GitHub&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;You can also create custom quantisation recipes using this Colab notebook:&lt;br/&gt;\n🔗 &lt;a href=\"https://colab.research.google.com/github/Thireus/GGUF-Tool-Suite/blob/main/quant_recipe_pipeline.ipynb\"&gt;Quant Recipe Pipeline&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Once you have a recipe, use the &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/blob/main/quant_downloader.sh\"&gt;quant_downloader.sh&lt;/a&gt; script to download the model shards using the &lt;code&gt;.recipe&lt;/code&gt; file. Please note that the scripts have mainly been tested in a Linux environment; support for macOS is planned. For best results, run the downloader on Linux. After downloading, load the model with &lt;code&gt;ik_llama&lt;/code&gt; using &lt;a href=\"https://github.com/Thireus/ik_llama.cpp/commit/a66490410a366a9605234b94d67f3d9b7b389140\"&gt;this patch&lt;/a&gt; (also don’t forget to run &lt;code&gt;ulimit -n 99999&lt;/code&gt; first).&lt;/p&gt;\n\n&lt;p&gt;You can find examples of recipes (including perplexity scores and other metrics) available here:&lt;br/&gt;\n🔗 &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/tree/main/recipe_examples\"&gt;Recipe Examples&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried to produce examples to benchmark against GGUF quants from other reputable creators such as unsloth, ubergarm, bartowski.&lt;/p&gt;\n\n&lt;p&gt;For full details and setup instructions, please refer to the repo’s README:&lt;br/&gt;\n🔗 &lt;a href=\"https://github.com/Thireus/GGUF-Tool-Suite/\"&gt;GGUF Tool Suite README&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I’m also planning to publish an article soon that will explore the capabilities of the GGUF Tool Suite and demonstrate how it can be used to produce an optimised mixture of quants for other LLM models.&lt;/p&gt;\n\n&lt;p&gt;I’d love to hear your feedback or answer any questions you may have!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?auto=webp&amp;s=7243f31103c2d94003681d0b82858d4108e3b776",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=791d882324dd951016663a2ce43b55901557ce50",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1e062fe80c217f3a281559e624465b7c16b3f417",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=33b7b94f116c5275ef617d0c15bc5d6d7c2bc55a",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=86b150bbdb03fe31c0459d1545d66ed999bc7931",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e887dcf77db029bbc01d10f6307b6ac00aed69b4",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b15c0446137fce63ca36558e3c438df37406c0c1",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "mj_iC-YyHvPh4mHsOjFii7sajPyqVswMWvlO_8Xqgks"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1ly84xd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Thireus",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly84xd/introducing_gguf_tool_suite_create_and_optimise/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752346612,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all!  \nI've just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.\n\n* **Educational:** Core algorithms are reimplemented from scratch for learning and transparency.\n* **CLI tools:** Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)\n* **Modular:** Clean separation between export, inference, and CLI.\n* **Safety:** Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)\n* **Future plans:** I would be curious to see how to extend it to support:\n   * fine-tuning of a small models\n   * optimize inference performance (e.g. matmul operations)\n   * WASM build to run inference in a browser\n\nBasically, I used [qwen3.c](https://github.com/adriancable/qwen3.c) as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.\n\nGitHub: [https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)",
          "author_fullname": "t2_fkwrm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Rust] qwen3-rs: Educational Qwen3 Architecture Inference (No Python, Minimal Deps)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly7sb0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 31,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 31,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752345715,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all!&lt;br/&gt;\nI&amp;#39;ve just released my [qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html), a Rust project for running and exporting Qwen3 models (Qwen3-0.6B, 4B, 8B, DeepSeek-R1-0528-Qwen3-8B, etc) with minimal dependencies and no Python required.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Educational:&lt;/strong&gt; Core algorithms are reimplemented from scratch for learning and transparency.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;CLI tools:&lt;/strong&gt; Export HuggingFace Qwen3 models to a custom binary format, then run inference (on CPU)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Modular:&lt;/strong&gt; Clean separation between export, inference, and CLI.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Safety:&lt;/strong&gt; Some unsafe code is used, mostly to work with memory mapping files (helpful to lower memory requirements on export/inference)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Future plans:&lt;/strong&gt; I would be curious to see how to extend it to support:\n\n&lt;ul&gt;\n&lt;li&gt;fine-tuning of a small models&lt;/li&gt;\n&lt;li&gt;optimize inference performance (e.g. matmul operations)&lt;/li&gt;\n&lt;li&gt;WASM build to run inference in a browser&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Basically, I used &lt;a href=\"https://github.com/adriancable/qwen3.c\"&gt;qwen3.c&lt;/a&gt; as a reference implementation translated from C/Python to Rust with a help of commercial LLMs (mostly Claude Sonnet 4). Please note that my primary goal is self learning in this field, so some inaccuracies can be definitely there.&lt;/p&gt;\n\n&lt;p&gt;GitHub: [&lt;a href=\"https://github.com/reinterpretcat/qwen3-rs%5D(vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)\"&gt;https://github.com/reinterpretcat/qwen3-rs](vscode-file://vscode-app/snap/code/198/usr/share/code/resources/app/out/vs/code/electron-sandbox/workbench/workbench.html)&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?auto=webp&amp;s=f64a6eef9fb25bb8dece4a00b49169cb6de85df2",
                  "width": 640,
                  "height": 640
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6a4a1ab699ce9984d57b0696bdd1f873de9e614",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=80ccbeb83c907fb5b897374c139c51e76825ec00",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5ad415a9157f412849b8def8bc5c576f5d41217",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3ac79d8600937790d6301fdd4917b87eabf6336a",
                    "width": 640,
                    "height": 640
                  }
                ],
                "variants": {},
                "id": "LxoqZs4q3Osj78IVaTXSrgUKqNHcOujrOF1Tg6_GYA4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ly7sb0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eis_kalt",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly7sb0/rust_qwen3rs_educational_qwen3_architecture/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752345715,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kyutai is one of the best text to speech models, with very low latency, real-time \"text streaming to audio\" generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it's able to generate very long audio files.\n\nIt's [one of the chart leaders in benchmarks](https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/).\n\nBut it's completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.\n\n---\n\nNow they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:\n\n# [https://github.com/kyutai-labs/delayed-streams-modeling/issues/64](https://github.com/kyutai-labs/delayed-streams-modeling/issues/64)",
          "author_fullname": "t2_4a13s1mr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Kyutai Text-to-Speech is considering opening up custom voice model training, but they are asking for community support!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly6cg6",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 87,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 87,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752342641,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752342109,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kyutai is one of the best text to speech models, with very low latency, real-time &amp;quot;text streaming to audio&amp;quot; generation (great for turning LLM output into audio in real-time), and great accuracy at following the text prompt. And unlike most other models, it&amp;#39;s able to generate very long audio files.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lqycp0/kyutai_tts_is_here_realtime_voicecloning/\"&gt;one of the chart leaders in benchmarks&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s completely locked down and can only output some terrible stock voices. They gave a weird justification about morality despite the fact that lots of other voice models already support voice training.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Now they are asking the community to voice their support for adding a training feature. If you have GitHub, go here and vote/let them know your thoughts:&lt;/p&gt;\n\n&lt;h1&gt;&lt;a href=\"https://github.com/kyutai-labs/delayed-streams-modeling/issues/64\"&gt;https://github.com/kyutai-labs/delayed-streams-modeling/issues/64&lt;/a&gt;&lt;/h1&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?auto=webp&amp;s=bb0d78e71acc290feb3be820e0ac3817a8b57580",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=367ade6726e17c3965008974bc2f09c6e6bc8ba5",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=96ca6419efae923e4873042319bfe37633d90d02",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bc4a96547220f829be62cf5452397d01f6b35fa2",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=52dd6c876880a544fb8f6932f59210dca6e626a0",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=381b0108d07c03c8894ff27591994d720b9a28a0",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d5977dd10040b49d6bd075f3f3ecfe7d8d4a969d",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "jYQMX0f3Fb0bWnMi64F2HOZ_4nfDILMheNU-lebBluE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1ly6cg6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pilkyton",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly6cg6/kyutai_texttospeech_is_considering_opening_up/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752342109,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey guys,\n\nI'm working on a project for multiple speakers, and was wondering what is the most natural sounding TTS model right now?\n\nI saw XTTS and ChatTTS, but those have been around for a while. Is there anything new that's local that sounds pretty good?\n\nThanks!",
          "author_fullname": "t2_86i4l",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What's the most natural sounding TTS model for local right now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly5g2t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752339858,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m working on a project for multiple speakers, and was wondering what is the most natural sounding TTS model right now?&lt;/p&gt;\n\n&lt;p&gt;I saw XTTS and ChatTTS, but those have been around for a while. Is there anything new that&amp;#39;s local that sounds pretty good?&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly5g2t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Siigari",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly5g2t/whats_the_most_natural_sounding_tts_model_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752339858,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello. I am wondering if there's a solution that checks a url using a local llm before deciding whether to allow or disallow a connection?\n\nUse case:\n\n\\- user types in a url\n\n\\- url is scraped and sent to the llm\n\n\\- llm decides to allow/disallow the visit as per instructions\n\nI am wondering if there's an open-source project that does this or  similar before I try to vibe-code it. Thank you for your help!\n\np.s. I am home-schooling my kids and want to make sure they remain focused on learning topics that are part of their program :-)",
          "author_fullname": "t2_ajuxt3cr4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Open-Source LLM-Based Solution for Online Content Filtering - Is There One?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly59tz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752339429,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I am wondering if there&amp;#39;s a solution that checks a url using a local llm before deciding whether to allow or disallow a connection?&lt;/p&gt;\n\n&lt;p&gt;Use case:&lt;/p&gt;\n\n&lt;p&gt;- user types in a url&lt;/p&gt;\n\n&lt;p&gt;- url is scraped and sent to the llm&lt;/p&gt;\n\n&lt;p&gt;- llm decides to allow/disallow the visit as per instructions&lt;/p&gt;\n\n&lt;p&gt;I am wondering if there&amp;#39;s an open-source project that does this or  similar before I try to vibe-code it. Thank you for your help!&lt;/p&gt;\n\n&lt;p&gt;p.s. I am home-schooling my kids and want to make sure they remain focused on learning topics that are part of their program :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly59tz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Southern_Sun_2106",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly59tz/opensource_llmbased_solution_for_online_content/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly59tz/opensource_llmbased_solution_for_online_content/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752339429,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Check it. 500mb ram, 500hetz cpu. Dial up. 200 watts. And it's internet ready. Sound blaster too ;]\n\nGonna run me that new \"llama\" model I've been hearing so much about. ",
          "author_fullname": "t2_12ed7j",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "New LLM DOS rig",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "d8elh9ux2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=76c6c15a461b45390db1485f621af1e7eeca8d27"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ed4c8cf5109d196c3526dfffbe22b8900498b7b6"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f672f6359b2f6350d4af8a4daca191e1788a3293"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=26225e20370826191a7a02417f1645efad6c8f2b"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=89c4fc1694dfd58f8e6994212034de1ffcb461cf"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9731e965c81285c7f76007f1f2816b256a91aa28"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/d8elh9ux2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=e6ae46b2cb332a931c7cdac44fcc398b25afc230"
              },
              "id": "d8elh9ux2hcf1"
            },
            "3y5lqeox2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bb8696c41c54a2a7231c9a634b4c8377c33958b6"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d1570e72cfe57809f313099e381fa602ae187f59"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e16cbd01bacb74f8ecb85e8f671270932fd3ceda"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b170a62277b42fef758d1e2d9988aa21dcad9819"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3c641396d51e36eef3d3240dc4239805f7c165e4"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=da4296d1c86b5867a5975eecce0853c7fb7f8967"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/3y5lqeox2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=3efd478d05476b1b0d1764eccae754f482f36a8d"
              },
              "id": "3y5lqeox2hcf1"
            },
            "3gs7irqx2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=318b67abf72e140a11688444869919471f3147bd"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec62f2bc9d73b58803f2b9271cd02c79ce380fa0"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f898d823da25aa87412ac96021df35fd7ef9a521"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ebe838d0a3f100fc22259ea8ea4c101951885282"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b26d6db9b6a0b61ef3f280259bca027a18fbafb0"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=909e44345e65a065dd95e9071bf36461620516b2"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/3gs7irqx2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=1d7d2a25d4a4758b5a55deffb5858b9725735a99"
              },
              "id": "3gs7irqx2hcf1"
            },
            "460q1llx2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=59e5dbc46987b97b11bb26cd6e609ebe6cf3e3dc"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8041d8e0c69a8177efea9b45329a977aedda3e1"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a6fd9b93013572743b6a05f657e5ce09359ca372"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=00669206a45782483c3f47d908d0b40f91e23cd2"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9b12277a297a99fb65ac5f78761c86ee307d2049"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d44830bcebbe39dce070961cc40c148058788d5c"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/460q1llx2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=cfb6830f63a2550b9f2926c43506ac428132b984"
              },
              "id": "460q1llx2hcf1"
            },
            "buaxlksx2hcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1870c1e474031d8ba2b0e373448c24b4d15a3b44"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=23ae941e10ef4dcb8c1acd16a5116e349744518b"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=885b12e0a3fe0fbdd9eefeca61a9748835223ab0"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0dc372f253cc84c2186352c446ce9d06cfc14b52"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=93556171b98795f033ebbefc51f949b719f5b76e"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c3676b5153d8f12f162251466cee5769b37edd72"
                }
              ],
              "s": {
                "y": 3000,
                "x": 4000,
                "u": "https://preview.redd.it/buaxlksx2hcf1.jpg?width=4000&amp;format=pjpg&amp;auto=webp&amp;s=92c07a54c438e4409329b6fe4f149a711afea278"
              },
              "id": "buaxlksx2hcf1"
            }
          },
          "name": "t3_1ly513g",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.65,
          "author_flair_background_color": null,
          "ups": 14,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "caption": "",
                "media_id": "460q1llx2hcf1",
                "id": 704385453
              },
              {
                "caption": "",
                "media_id": "3y5lqeox2hcf1",
                "id": 704385454
              },
              {
                "caption": "",
                "media_id": "3gs7irqx2hcf1",
                "id": 704385455
              },
              {
                "caption": "",
                "media_id": "buaxlksx2hcf1",
                "id": 704385456
              },
              {
                "caption": "",
                "media_id": "d8elh9ux2hcf1",
                "id": 704385457
              }
            ]
          },
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/mhIPD4DGyFibjeOOOVuTpLQDUf-Xt8GRBvGz7cgcnJE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752338802,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Check it. 500mb ram, 500hetz cpu. Dial up. 200 watts. And it&amp;#39;s internet ready. Sound blaster too ;]&lt;/p&gt;\n\n&lt;p&gt;Gonna run me that new &amp;quot;llama&amp;quot; model I&amp;#39;ve been hearing so much about. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1ly513g",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1ly513g",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alienanthony",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly513g/new_llm_dos_rig/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1ly513g",
          "subreddit_subscribers": 498343,
          "created_utc": 1752338802,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_7tlxcyy6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Okay kimi-k2 is an INSANE model WTF those one-shot animations",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 84,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly4zh8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 218,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/74d8efoh2hcf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1796,
              "scrubber_media_url": "https://v.redd.it/74d8efoh2hcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/74d8efoh2hcf1/DASHPlaylist.mpd?a=1755013022%2CYTkyOWMxZGRiMzE2M2NkY2ExZTYzZmQ0MmU2ODRkZjYxZGZhY2M0OTNlMDFiMmM4ODk5NDBkMGJmZTEwM2M3Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/74d8efoh2hcf1/HLSPlaylist.m3u8?a=1755013022%2CMTE0MzJjYTJiY2RmZTM5MmQ5OWI1ZDcwZjk4YjBhMjIxZjQ1ZGRkZmUyNWQyOTdkNTM2MGFhNzkxOTcyYjA3Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 218,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=140&amp;height=84&amp;crop=140:84,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=22934730e256af34e21d0c4f768ea8f376a8ffc8",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752338690,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/74d8efoh2hcf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?format=pjpg&amp;auto=webp&amp;s=787b0533766b72327a39325971e8912f4bb1d0dd",
                  "width": 3600,
                  "height": 2166
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1b3a502b5cf8dd662aa70b308038b5980910d273",
                    "width": 108,
                    "height": 64
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=30f85b0338577881956c844b4be93f247975442b",
                    "width": 216,
                    "height": 129
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0bb1a95f277373f56d7d9edc672b02e5ad1d58cc",
                    "width": 320,
                    "height": 192
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=173e65bd6cfe73a4442df7bc6c13aca66dcd0395",
                    "width": 640,
                    "height": 385
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=d49c640223ead711b9bdfa961ec2ce1b91602d1f",
                    "width": 960,
                    "height": 577
                  },
                  {
                    "url": "https://external-preview.redd.it/amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2642ff27b27bfc061a053f6d322fc7bb44dbf90f",
                    "width": 1080,
                    "height": 649
                  }
                ],
                "variants": {},
                "id": "amJmNzJnb2gyaGNmMR47PnkZil-qwhK39njev3B-56bQsfXI6t0qLjuoAfo4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly4zh8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "sirjoaco",
          "discussion_type": null,
          "num_comments": 25,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly4zh8/okay_kimik2_is_an_insane_model_wtf_those_oneshot/",
          "stickied": false,
          "url": "https://v.redd.it/74d8efoh2hcf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752338690,
          "num_crossposts": 2,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/74d8efoh2hcf1/DASH_1080.mp4?source=fallback",
              "has_audio": false,
              "height": 1080,
              "width": 1796,
              "scrubber_media_url": "https://v.redd.it/74d8efoh2hcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/74d8efoh2hcf1/DASHPlaylist.mpd?a=1755013022%2CYTkyOWMxZGRiMzE2M2NkY2ExZTYzZmQ0MmU2ODRkZjYxZGZhY2M0OTNlMDFiMmM4ODk5NDBkMGJmZTEwM2M3Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 15,
              "hls_url": "https://v.redd.it/74d8efoh2hcf1/HLSPlaylist.m3u8?a=1755013022%2CMTE0MzJjYTJiY2RmZTM5MmQ5OWI1ZDcwZjk4YjBhMjIxZjQ1ZGRkZmUyNWQyOTdkNTM2MGFhNzkxOTcyYjA3Mw%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": " Currently i have workstation. Which is powered by AMD EPYC 7452 32 core cpu with 256GB  RAM . The worksration has 5 x 4Gen pcie slots  and has A100 40Gb currently running with it. So i planned to upgrade it .I wanna load all the other 4 slots with either RTX 6000 ADA for with L40S . which can i go for????, i know there is gonna be a release of RTX blackwell series i cant use it since it needs  5TH gen pcie slots.PSU for the workstation is 2400w.\n\nMy questions are;  \n1. Which gpu should i choose and why?  \n2. Does the nvlink works on them. cuz some internet resources say it can be used or some say it doesn't.\n\nMY use cases are for   \nFine-tuning,Model distillation, Local inference ,Unity and omniverse .",
          "author_fullname": "t2_adm2b7kq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GPU UPGRADE!!!!NEED Suggestion!!!!.Upgrading current workstation either with 4x RTX 6000 ada or 4x L40s. Can i use NVlink bridge the pair them up.??",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly4xvb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.2,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752338570,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently i have workstation. Which is powered by AMD EPYC 7452 32 core cpu with 256GB  RAM . The worksration has 5 x 4Gen pcie slots  and has A100 40Gb currently running with it. So i planned to upgrade it .I wanna load all the other 4 slots with either RTX 6000 ADA for with L40S . which can i go for????, i know there is gonna be a release of RTX blackwell series i cant use it since it needs  5TH gen pcie slots.PSU for the workstation is 2400w.&lt;/p&gt;\n\n&lt;p&gt;My questions are;&lt;br/&gt;\n1. Which gpu should i choose and why?&lt;br/&gt;\n2. Does the nvlink works on them. cuz some internet resources say it can be used or some say it doesn&amp;#39;t.&lt;/p&gt;\n\n&lt;p&gt;MY use cases are for&lt;br/&gt;\nFine-tuning,Model distillation, Local inference ,Unity and omniverse .&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly4xvb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "logii33",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752338570,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For folks coding daily, what models are you getting the best results with? I know there are a lot of variables, and I’d like to avoid getting bogged down in the details like performance, prompt size, parameter counts, or quantization. What models is turning in the best results for coding for you personally.\n\nFor reference, I’m using an M4max MBP with 128gm ram.",
          "author_fullname": "t2_ot5sg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RL local llm for coding",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly4tus",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752338280,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For folks coding daily, what models are you getting the best results with? I know there are a lot of variables, and I’d like to avoid getting bogged down in the details like performance, prompt size, parameter counts, or quantization. What models is turning in the best results for coding for you personally.&lt;/p&gt;\n\n&lt;p&gt;For reference, I’m using an M4max MBP with 128gm ram.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly4tus",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "rts324",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly4tus/rl_local_llm_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly4tus/rl_local_llm_for_coding/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752338280,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "In a quest to make a tamagotchi which requires piano practice to feed (and maybe organise live piano recordings) I am trying out various research projects. So far I have implemented the excellent [piano transcription](https://github.com/bytedance/piano_transcription)  repo and I am getting really good MIDI back.  \n\n[screenshot of little webapp for piano transcription](https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=2346&amp;format=pjpg&amp;auto=webp&amp;s=1ba2cc34ef02418d3c6a7cd80c075a51192e3fff)\n\nHowever my initial idea to analyse this as ABC notation with [ChatMusician](https://huggingface.co/MaziyarPanahi/ChatMusician-GGUF) was wrong, piano of course, has more than a single \"mono\" track that could be represented in ABC. \n\nToday I found  [Clamp3](https://sanderwood.github.io/clamp3/) . Fixed their requirements.txt with the correct versions of numpy and scipy. But   \"2.31M music-text pairs, Zero-shot classification, Identify genre, mood, style &amp; more\" and then in their classification readme it's suddenly \"You need to train your own classifier and provide your own categories\".  Did I misunderstand something here? Where's the \"2.31M music-text pairs\"? Can that part of the project really be that much BS? \n\nNext up for me:  [MusicBert](https://github.com/malcolmsailor/musicbert_hf) and maybe try again with a standalone HuBert (really cool stuff seems to happen with this model like [voice based](https://github.com/mrw0nd3rfu1/Speech-Based-Emotion-Detection-Using-Fine-Tuned-HuBERT) emotion detection)  \n  \nAnybody done music classification and feel like sharing pointers? Otherwise enjoy my little rant about trying academic code (I know it is free, I have no reason to complain, what a time to be alive etc.)  \n  \n ",
          "author_fullname": "t2_4m6vm3ghs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Music Analysis - another attempt",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "eh4wn7xgwgcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 55,
                  "x": 108,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6e7c92cdf7bb578c9d03ed28c92fcaca7bca3298"
                },
                {
                  "y": 110,
                  "x": 216,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3901eb330108b6742c975fccb81c9798dd00ad7"
                },
                {
                  "y": 163,
                  "x": 320,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c53f2f1841cf4c9af86181e2c4663d3a35252f92"
                },
                {
                  "y": 326,
                  "x": 640,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=758d3c72ad793aa33ce6212b9079c4b0958837e8"
                },
                {
                  "y": 490,
                  "x": 960,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=892bb36236565148ae350a81ea341cc092c31481"
                },
                {
                  "y": 551,
                  "x": 1080,
                  "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c7a72d2a12073edd55d1f53a2bc5f73be16b8761"
                }
              ],
              "s": {
                "y": 1198,
                "x": 2346,
                "u": "https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=2346&amp;format=pjpg&amp;auto=webp&amp;s=1ba2cc34ef02418d3c6a7cd80c075a51192e3fff"
              },
              "id": "eh4wn7xgwgcf1"
            }
          },
          "name": "t3_1ly476r",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=b924b1e8c93b9c4d3e9b8a9a3aa4154cddb6e515",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1752336671,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In a quest to make a tamagotchi which requires piano practice to feed (and maybe organise live piano recordings) I am trying out various research projects. So far I have implemented the excellent &lt;a href=\"https://github.com/bytedance/piano_transcription\"&gt;piano transcription&lt;/a&gt;  repo and I am getting really good MIDI back.  &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/eh4wn7xgwgcf1.jpg?width=2346&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1ba2cc34ef02418d3c6a7cd80c075a51192e3fff\"&gt;screenshot of little webapp for piano transcription&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;However my initial idea to analyse this as ABC notation with &lt;a href=\"https://huggingface.co/MaziyarPanahi/ChatMusician-GGUF\"&gt;ChatMusician&lt;/a&gt; was wrong, piano of course, has more than a single &amp;quot;mono&amp;quot; track that could be represented in ABC. &lt;/p&gt;\n\n&lt;p&gt;Today I found  &lt;a href=\"https://sanderwood.github.io/clamp3/\"&gt;Clamp3&lt;/a&gt; . Fixed their requirements.txt with the correct versions of numpy and scipy. But   &amp;quot;2.31M music-text pairs, Zero-shot classification, Identify genre, mood, style &amp;amp; more&amp;quot; and then in their classification readme it&amp;#39;s suddenly &amp;quot;You need to train your own classifier and provide your own categories&amp;quot;.  Did I misunderstand something here? Where&amp;#39;s the &amp;quot;2.31M music-text pairs&amp;quot;? Can that part of the project really be that much BS? &lt;/p&gt;\n\n&lt;p&gt;Next up for me:  &lt;a href=\"https://github.com/malcolmsailor/musicbert_hf\"&gt;MusicBert&lt;/a&gt; and maybe try again with a standalone HuBert (really cool stuff seems to happen with this model like &lt;a href=\"https://github.com/mrw0nd3rfu1/Speech-Based-Emotion-Detection-Using-Fine-Tuned-HuBERT\"&gt;voice based&lt;/a&gt; emotion detection)  &lt;/p&gt;\n\n&lt;p&gt;Anybody done music classification and feel like sharing pointers? Otherwise enjoy my little rant about trying academic code (I know it is free, I have no reason to complain, what a time to be alive etc.)  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?auto=webp&amp;s=182248ac9ed7f1aec78d26c27509b4c99d40902e",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fcfd3fcf8d3e8cbb8d41b131df87efcfc87a2d81",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=17b6cc84bf2c4e8983e028ab23eb8e76f9f6a598",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c9de33a3adda6385d6acb3beb57a0beeca7edba3",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3717d00c3a50ca4d79f03c7947503c9e8d306545",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ba7b7a77391254da8f4b80a1e95ec85903390468",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2636373b98c9f447ab40b3c12df9db22b70beb04",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "z_1ffi2EBnCAbMFT2MnihKQQZrdw8qlio3KudIpri8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly476r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Not_your_guy_buddy42",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly476r/music_analysis_another_attempt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly476r/music_analysis_another_attempt/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752336671,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.\n\nSource: @rasbt on X",
          "author_fullname": "t2_jqxb4pte",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Interesting info about Kimi K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 73,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly42e5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 423,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 423,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3oX4rR_wq13aNVBkwN9gy-7Ly3awWuKmao4xX-wZHPw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752336334,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 is basically DeepSeek V3 but with fewer heads and more experts.&lt;/p&gt;\n\n&lt;p&gt;Source: @rasbt on X&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/klm2b78lvgcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?auto=webp&amp;s=d65bbcd2f22adb9cfb21adc9eac026b92732d6e6",
                  "width": 4096,
                  "height": 2142
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=de7eb96ece8068540bfea48d2417469c7f222dea",
                    "width": 108,
                    "height": 56
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=e5b3e2c8e32db02490d668ba0df54614ac60fe47",
                    "width": 216,
                    "height": 112
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1e27745898fe8ede1bbd082b7fec7ccae87d2bf",
                    "width": 320,
                    "height": 167
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=32a0ebb795c06ba955385d6c0102e57e0fd85423",
                    "width": 640,
                    "height": 334
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=9c7c244ab2396650f24e6eb34e6826ebd524c6e5",
                    "width": 960,
                    "height": 502
                  },
                  {
                    "url": "https://preview.redd.it/klm2b78lvgcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc382959f7974e6a25c0797c6abdfd2b739eef68",
                    "width": 1080,
                    "height": 564
                  }
                ],
                "variants": {},
                "id": "Mc3JulkX7jC-xZrG5vXyMeTKbsu1euUGH0q8C22y1zs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly42e5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Conversation9561",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly42e5/interesting_info_about_kimi_k2/",
          "stickied": false,
          "url": "https://i.redd.it/klm2b78lvgcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752336334,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\n\n",
          "author_fullname": "t2_obpis",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Cactus - Edge AI Inference Framework",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly3mrl",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1752335237,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "cactuscompute.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://cactuscompute.com/",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1ly3mrl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "dayanruben",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly3mrl/cactus_edge_ai_inference_framework/",
          "stickied": false,
          "url": "https://cactuscompute.com/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752335237,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently read an anonymous PDF entitled \"Pangu's Sorry\". It is a late-night confession written by an employee of Huawei Noah's Ark Laboratory, and the content is shocking. This article details the inside story of the whole process of Huawei's Pangu large model from research and development to \"suspected shell\", involving a large amount of undisclosed information. The relevant link is attached here: https://github.com/HW-whistleblower/True-Story-of-Pangu",
          "author_fullname": "t2_1bs4u37ku9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do you think of Huawei's Pangu model counterfeiting behaviour?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly3exz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.53,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752334694,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently read an anonymous PDF entitled &amp;quot;Pangu&amp;#39;s Sorry&amp;quot;. It is a late-night confession written by an employee of Huawei Noah&amp;#39;s Ark Laboratory, and the content is shocking. This article details the inside story of the whole process of Huawei&amp;#39;s Pangu large model from research and development to &amp;quot;suspected shell&amp;quot;, involving a large amount of undisclosed information. The relevant link is attached here: &lt;a href=\"https://github.com/HW-whistleblower/True-Story-of-Pangu\"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4.png?auto=webp&amp;s=03f01772e8e0f753ede0017ad43d29464c9e81ea",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d39f80b3ef397dbaa7f7208dbca68057d7f75121",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=171b6db557d19e05d40f91585d0e76aec92819d9",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c520e9fa4c7fd0eac243f7f8e12c135897352624",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=92178de8e92b6b12754c0c2d80a75f9babeab7b3",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b59deef0ddf07dd8e35df82caa79e6116cc597d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bd5b0c0adca48ba32f058c201dff22e562686300",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "aDTM33NC_RwQjSbCsYQdPHQSZnwO27TXeh0S3vx-2s4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1ly3exz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Disastrous-Prize-946",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly3exz/what_do_you_think_of_huaweis_pangu_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly3exz/what_do_you_think_of_huaweis_pangu_model/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752334694,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to build my own agent system similar to Claude Projects or ChatGPT Projects, where users can:\n\n* Upload documents that persist across conversations\n* Set custom instructions for the agent\n* Have the AI seamlessly reference uploaded materials\n\n**What I'm trying to replicate:**\n\n* Upload PDFs, docs, code files as \"context\" for an agent\n* Agent maintains this context across multiple chat sessions\n* Smooth integration (not obvious \"searching\" behavior like traditional RAG)\n* Custom system instructions that persist\n\n**Technical questions for implementation:**\n\n1. **Context Management:** Do you think they use traditional RAG with vector search, or just concatenate documents into the prompt? The behavior feels more like extended context than retrieval.\n2. **Token Limits:** How would you handle large documents exceeding context windows? Smart chunking? Summarization? Hierarchical retrieval?\n3. **Implementation patterns:** Has anyone built something similar? \n\n**Looking for:**\n\n* Architecture advice from anyone who's built similar systems\n* Open source implementations I could learn from\n* Insights into how the commercial systems might work\n\nAny suggestions on approach, tools?\n\n",
          "author_fullname": "t2_a0w73wk0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Building a Claude/ChatGPT Projects-like system: How to implement persistent context with uploaded documents?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly3dk9",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752334599,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to build my own agent system similar to Claude Projects or ChatGPT Projects, where users can:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upload documents that persist across conversations&lt;/li&gt;\n&lt;li&gt;Set custom instructions for the agent&lt;/li&gt;\n&lt;li&gt;Have the AI seamlessly reference uploaded materials&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;m trying to replicate:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Upload PDFs, docs, code files as &amp;quot;context&amp;quot; for an agent&lt;/li&gt;\n&lt;li&gt;Agent maintains this context across multiple chat sessions&lt;/li&gt;\n&lt;li&gt;Smooth integration (not obvious &amp;quot;searching&amp;quot; behavior like traditional RAG)&lt;/li&gt;\n&lt;li&gt;Custom system instructions that persist&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical questions for implementation:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Context Management:&lt;/strong&gt; Do you think they use traditional RAG with vector search, or just concatenate documents into the prompt? The behavior feels more like extended context than retrieval.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Token Limits:&lt;/strong&gt; How would you handle large documents exceeding context windows? Smart chunking? Summarization? Hierarchical retrieval?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Implementation patterns:&lt;/strong&gt; Has anyone built something similar? &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Looking for:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Architecture advice from anyone who&amp;#39;s built similar systems&lt;/li&gt;\n&lt;li&gt;Open source implementations I could learn from&lt;/li&gt;\n&lt;li&gt;Insights into how the commercial systems might work&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Any suggestions on approach, tools?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly3dk9",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Funny-Enthusiasm-610",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly3dk9/building_a_claudechatgpt_projectslike_system_how/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly3dk9/building_a_claudechatgpt_projectslike_system_how/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752334599,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I was testing LobeChat on unraid docker and noticed that settings and chats don’t persist — once the browser is closed, everything’s lost. I wanted to try the `lobehub/lobe-chat-database` version to enable persistence with Postgres + MinIO, but I keep getting a 500 error.\n\nI believe the database and env variables are set up correctly, but still no luck.\n\nHas anyone managed to get it running?",
          "author_fullname": "t2_144dwn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Anyone got lobe-chat-database working?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly36ht",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752334114,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was testing LobeChat on unraid docker and noticed that settings and chats don’t persist — once the browser is closed, everything’s lost. I wanted to try the &lt;code&gt;lobehub/lobe-chat-database&lt;/code&gt; version to enable persistence with Postgres + MinIO, but I keep getting a 500 error.&lt;/p&gt;\n\n&lt;p&gt;I believe the database and env variables are set up correctly, but still no luck.&lt;/p&gt;\n\n&lt;p&gt;Has anyone managed to get it running?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly36ht",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "reallionkiller",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly36ht/anyone_got_lobechatdatabase_working/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly36ht/anyone_got_lobechatdatabase_working/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752334114,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "LFM2 is a new generation of hybrid models developed by [Liquid AI](https://www.liquid.ai/), specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.\n\nWe're releasing the weights of three post-trained checkpoints with 350M, 700M, and 1.2B parameters. They provide the following key features to create AI-powered edge applications:\n\n* **Fast training &amp; inference** – LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.\n* **Best performance** – LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.\n* **New architecture** – LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.\n* **Flexible deployment** – LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.\n\nFind more information about LFM2 in our [blog post](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models).\n\nDue to their small size, **we recommend fine-tuning LFM2 models on narrow use cases** to maximize performance. They are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations. However, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.\n\n**Supported languages**: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.\n\n[https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF](https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF)\n\n[https://huggingface.co/LiquidAI/LFM2-350M-GGUF](https://huggingface.co/LiquidAI/LFM2-350M-GGUF)\n\n[https://huggingface.co/LiquidAI/LFM2-700M-GGUF](https://huggingface.co/LiquidAI/LFM2-700M-GGUF)\n\n[https://huggingface.co/mlabonne/LFM2-1.2B-Pirate](https://huggingface.co/mlabonne/LFM2-1.2B-Pirate)",
          "author_fullname": "t2_vqgbql9w",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Support for the LiquidAI LFM2 hybrid model family is now available in llama.cpp",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly35wd",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": "#bbbdbf",
          "ups": 23,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=ae2b10f187371926eccbe1411e53c6e9c8cf916b",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752334069,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;LFM2 is a new generation of hybrid models developed by &lt;a href=\"https://www.liquid.ai/\"&gt;Liquid AI&lt;/a&gt;, specifically designed for edge AI and on-device deployment. It sets a new standard in terms of quality, speed, and memory efficiency.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re releasing the weights of three post-trained checkpoints with 350M, 700M, and 1.2B parameters. They provide the following key features to create AI-powered edge applications:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Fast training &amp;amp; inference&lt;/strong&gt; – LFM2 achieves 3x faster training compared to its previous generation. It also benefits from 2x faster decode and prefill speed on CPU compared to Qwen3.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Best performance&lt;/strong&gt; – LFM2 outperforms similarly-sized models across multiple benchmark categories, including knowledge, mathematics, instruction following, and multilingual capabilities.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;New architecture&lt;/strong&gt; – LFM2 is a new hybrid Liquid model with multiplicative gates and short convolutions.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Flexible deployment&lt;/strong&gt; – LFM2 runs efficiently on CPU, GPU, and NPU hardware for flexible deployment on smartphones, laptops, or vehicles.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Find more information about LFM2 in our &lt;a href=\"https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models\"&gt;blog post&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Due to their small size, &lt;strong&gt;we recommend fine-tuning LFM2 models on narrow use cases&lt;/strong&gt; to maximize performance. They are particularly suited for agentic tasks, data extraction, RAG, creative writing, and multi-turn conversations. However, we do not recommend using them for tasks that are knowledge-intensive or require programming skills.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Supported languages&lt;/strong&gt;: English, Arabic, Chinese, French, German, Japanese, Korean, and Spanish.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF\"&gt;https://huggingface.co/LiquidAI/LFM2-1.2B-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LiquidAI/LFM2-350M-GGUF\"&gt;https://huggingface.co/LiquidAI/LFM2-350M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LiquidAI/LFM2-700M-GGUF\"&gt;https://huggingface.co/LiquidAI/LFM2-700M-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/mlabonne/LFM2-1.2B-Pirate\"&gt;https://huggingface.co/mlabonne/LFM2-1.2B-Pirate&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/14620",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?auto=webp&amp;s=b2adb0dd81c341c2711fa56c37aa39504e161352",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=922d22ba99cb62791a87c7c03e0c75e9f2c263bf",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b49e580cb8dbecbc01c22d2c96392fcf8f520945",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f6ea55a297103b80879b6be0763a4238799da398",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=04f1c27e8b636c45644b4c0883f4ab82ad671e94",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e5138bc294eebd5b4aafa2fa76b94a120c7c2af",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e5d4227b1b7c954df67c3b51afe5ea53a1a888fd",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "RAKCbtyHmD5AHPyuXInV3EboSSux75gI9ii4H-HnhzM"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1ly35wd",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jacek2023",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1ly35wd/support_for_the_liquidai_lfm2_hybrid_model_family/",
          "stickied": false,
          "url": "https://github.com/ggml-org/llama.cpp/pull/14620",
          "subreddit_subscribers": 498343,
          "created_utc": 1752334069,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello /r/LocalLLaMA\n\n\nAnyone had any success with using llama3.2-vision:11b to identity UI element from a screenshot?\n\nsomething like the following:\n\n- input screenshot\n- query: where is the back button?\n- output: (x,y, width, height)",
          "author_fullname": "t2_1oy83d8r",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using llama3.2-vision:11b for UI element identification",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly358h",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752334025,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello &lt;a href=\"/r/LocalLLaMA\"&gt;/r/LocalLLaMA&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Anyone had any success with using llama3.2-vision:11b to identity UI element from a screenshot?&lt;/p&gt;\n\n&lt;p&gt;something like the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;input screenshot&lt;/li&gt;\n&lt;li&gt;query: where is the back button?&lt;/li&gt;\n&lt;li&gt;output: (x,y, width, height)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly358h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mjTheThird",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly358h/using_llama32vision11b_for_ui_element/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly358h/using_llama32vision11b_for_ui_element/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752334025,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'm usually around here enjoying the discussions, and I've put together a short, 5-7 minute survey to better understand how all of you are using Large Language Models locally. I'm really curious about your setups, the tools and agents you're using, and what your day-to-day experience is like on the ground.\n\n\n\nBefore I jump in, I want to give a huge shout-out and thank you to the awesome people who helped me put this survey together! Their contributions were invaluable, and while they prefer to stay anonymous, know that their insights were super helpful in making this survey what it is.\n\n\n\nIf you're running LLMs on your own hardware, please consider taking a few minutes to share your insights.\n\n[https://qazwsx.aidaform.com/the-local-llm-landscape](https://qazwsx.aidaform.com/the-local-llm-landscape)\n\nAnd if you know other folks or communities who might fit the bill, it would be awesome if you could share it with them too! The more perspectives, the clearer the picture we get!\n\n\n\nThanks a ton for helping out!\n\nLink: [https://qazwsx.aidaform.com/the-local-llm-landscape](https://qazwsx.aidaform.com/the-local-llm-landscape)",
          "author_fullname": "t2_dtzmsoy3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How Are YOU Using LLMs? (A Quick Survey)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly256a",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752331453,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m usually around here enjoying the discussions, and I&amp;#39;ve put together a short, 5-7 minute survey to better understand how all of you are using Large Language Models locally. I&amp;#39;m really curious about your setups, the tools and agents you&amp;#39;re using, and what your day-to-day experience is like on the ground.&lt;/p&gt;\n\n&lt;p&gt;Before I jump in, I want to give a huge shout-out and thank you to the awesome people who helped me put this survey together! Their contributions were invaluable, and while they prefer to stay anonymous, know that their insights were super helpful in making this survey what it is.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re running LLMs on your own hardware, please consider taking a few minutes to share your insights.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://qazwsx.aidaform.com/the-local-llm-landscape\"&gt;https://qazwsx.aidaform.com/the-local-llm-landscape&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And if you know other folks or communities who might fit the bill, it would be awesome if you could share it with them too! The more perspectives, the clearer the picture we get!&lt;/p&gt;\n\n&lt;p&gt;Thanks a ton for helping out!&lt;/p&gt;\n\n&lt;p&gt;Link: &lt;a href=\"https://qazwsx.aidaform.com/the-local-llm-landscape\"&gt;https://qazwsx.aidaform.com/the-local-llm-landscape&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1ly256a",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kidupstart",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly256a/how_are_you_using_llms_a_quick_survey/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly256a/how_are_you_using_llms_a_quick_survey/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752331453,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Yesterday, I downloaded this model:  \n[https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf](https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf)  \nafter someone recommended it for erp in a comment. \"A mix between mistral and qwq? Sounds intriguing, I want to give it a try.\"\n\nIt loaded fine, but when I tried to chat with it in LM Studio, I got an error message:  \n\"Error rendering prompt with jinja template\"\n\nI asked perplexity how to fix this, and it gave me an answer that worked.\n\nPerplexity: \"A community-tested template for QwQ/Qwen-style models is:\n\n    {%- if messages[0]['role'] == 'system' %}\n    {{- messages[0]['content'] }}\n    {%- endif %}\n    {%- for message in messages %}\n    {{- '\\n' + message['role'] + ': ' + message['content'] }}\n    {%- endfor %}\n    {%- if add_generation_prompt %}\n    {{- '\\nassistant: ' }}\n    {%- endif %}\n\n\"  \n  \nInside LM Studio:  \n\\- In the bar on the left, click on the Folder icons, for \"My Models\".  \n\\- Locate the model you are using, and click on the gear icon.  \n\\- In the dialogue that pops up, choose the second tab, \"Prompt\".  \n\\- Copy the template above, and paste it into \"Template (Jinja)\". (After deleting the pervious template.)",
          "author_fullname": "t2_4nfz972n",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Beginner's tip: How to fix the Jinja template error in LM Studio (in my case: for Mistral-qwq-12b-merge)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly1d7v",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752332730,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752329386,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday, I downloaded this model:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf\"&gt;https://huggingface.co/Disya/Mistral-qwq-12b-merge-gguf&lt;/a&gt;&lt;br/&gt;\nafter someone recommended it for erp in a comment. &amp;quot;A mix between mistral and qwq? Sounds intriguing, I want to give it a try.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;It loaded fine, but when I tried to chat with it in LM Studio, I got an error message:&lt;br/&gt;\n&amp;quot;Error rendering prompt with jinja template&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I asked perplexity how to fix this, and it gave me an answer that worked.&lt;/p&gt;\n\n&lt;p&gt;Perplexity: &amp;quot;A community-tested template for QwQ/Qwen-style models is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;{%- if messages[0][&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}\n{{- messages[0][&amp;#39;content&amp;#39;] }}\n{%- endif %}\n{%- for message in messages %}\n{{- &amp;#39;\\n&amp;#39; + message[&amp;#39;role&amp;#39;] + &amp;#39;: &amp;#39; + message[&amp;#39;content&amp;#39;] }}\n{%- endfor %}\n{%- if add_generation_prompt %}\n{{- &amp;#39;\\nassistant: &amp;#39; }}\n{%- endif %}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&amp;quot;  &lt;/p&gt;\n\n&lt;p&gt;Inside LM Studio:&lt;br/&gt;\n- In the bar on the left, click on the Folder icons, for &amp;quot;My Models&amp;quot;.&lt;br/&gt;\n- Locate the model you are using, and click on the gear icon.&lt;br/&gt;\n- In the dialogue that pops up, choose the second tab, &amp;quot;Prompt&amp;quot;.&lt;br/&gt;\n- Copy the template above, and paste it into &amp;quot;Template (Jinja)&amp;quot;. (After deleting the pervious template.)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?auto=webp&amp;s=81d2784e68f37c4940200fd02cc522c54912e469",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d5ce391ab8ff43b14cef3df7d11941afbacd910",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=35d9ee5fefca1ddc5a5e5f8f1d9d3ba5a859421f",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b4b47af8e820d1e0356e4d7db98e5f08ff93e6cc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8003c769d8627f9a15a7601718bb8d62135811e4",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=80a431ce4f90f9523f4342fc02a39e9567bb804e",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=eb79402c0b3c6f1789c37488f88defecfe12fd63",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "Aqe7MuYuCs98rlQ_uLEiIYTmQfrGi3PwUr_uAeCI1c4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly1d7v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hugo-the-second",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly1d7v/beginners_tip_how_to_fix_the_jinja_template_error/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly1d7v/beginners_tip_how_to_fix_the_jinja_template_error/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752329386,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am doing research on determinism of LLM responses and want to run as the only job on the server but don't quite have the LLM ops skills to be confident in the backend setup. \n\nI currently use the standard hosted solutions (OpenAI and together.ai) and I assume that I am sharing input buffers/caches with other jobs which is likely the cause of non-determinism I see, substack post: [The Long Road to AGI Begins with Control](https://open.substack.com/pub/breckbaldwin/p/the-long-road-to-agi-begins-with).\n\nI have seen that locally run LLMs are deterministic so I wanted to validate earlier experiments but no longer have access to the hardware. I'd like to not be standing up an AWS server for each model and managing it.  \n\nI like the look of [https://www.inferless.com/](https://www.inferless.com/) which is a serverless GPU hosting service but don't quite have confidence of the execution environment. \n\nI am running locally with llama.cpp but have very limited memory, 8G, so figure I'd better go hit the cloud. \n\nSo I understand my options as:\n\n1. Stand up my own AWS box and run vLLM or llama.cpp with the tasks/models I want. I have not had good luck with this in the past and it was expensive to run a big box.   \n2. [https://www.inferless.com/](https://www.inferless.com/)  or some similar service--this looks more manageable but the instructions are a bit convoluted but I can probably get it going. The key here is no sharing of resources since that is the primary likely culprit for the non-determinism I am seeing.   \n3. Run locally, but can't run big models and am barely getting llama.cpp to work on 8Gb on M2 Air--current model is Llama-3.2-3B-Instruct-Q3\\_K\\_XL\n\nI'd like option 2. the most with a simpler \"setup\", \"run\" with automatic time out after 20 min. of inactivity. \n\nAny suggestions much appreciated.",
          "author_fullname": "t2_199sgjfhdc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Simplest way to run single batch jobs for experiments on determinism",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly19br",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752329093,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am doing research on determinism of LLM responses and want to run as the only job on the server but don&amp;#39;t quite have the LLM ops skills to be confident in the backend setup. &lt;/p&gt;\n\n&lt;p&gt;I currently use the standard hosted solutions (OpenAI and together.ai) and I assume that I am sharing input buffers/caches with other jobs which is likely the cause of non-determinism I see, substack post: &lt;a href=\"https://open.substack.com/pub/breckbaldwin/p/the-long-road-to-agi-begins-with\"&gt;The Long Road to AGI Begins with Control&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;I have seen that locally run LLMs are deterministic so I wanted to validate earlier experiments but no longer have access to the hardware. I&amp;#39;d like to not be standing up an AWS server for each model and managing it.  &lt;/p&gt;\n\n&lt;p&gt;I like the look of &lt;a href=\"https://www.inferless.com/\"&gt;https://www.inferless.com/&lt;/a&gt; which is a serverless GPU hosting service but don&amp;#39;t quite have confidence of the execution environment. &lt;/p&gt;\n\n&lt;p&gt;I am running locally with llama.cpp but have very limited memory, 8G, so figure I&amp;#39;d better go hit the cloud. &lt;/p&gt;\n\n&lt;p&gt;So I understand my options as:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Stand up my own AWS box and run vLLM or llama.cpp with the tasks/models I want. I have not had good luck with this in the past and it was expensive to run a big box.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://www.inferless.com/\"&gt;https://www.inferless.com/&lt;/a&gt;  or some similar service--this looks more manageable but the instructions are a bit convoluted but I can probably get it going. The key here is no sharing of resources since that is the primary likely culprit for the non-determinism I am seeing.&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;Run locally, but can&amp;#39;t run big models and am barely getting llama.cpp to work on 8Gb on M2 Air--current model is Llama-3.2-3B-Instruct-Q3_K_XL&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;I&amp;#39;d like option 2. the most with a simpler &amp;quot;setup&amp;quot;, &amp;quot;run&amp;quot; with automatic time out after 20 min. of inactivity. &lt;/p&gt;\n\n&lt;p&gt;Any suggestions much appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?auto=webp&amp;s=152dccb67db845eb779c39a35a30a1d5027bfe14",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e83ed5374520087c4f2e15d5222be950ed938a6d",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a1fe08560711cda49420ad4bb9dd7aa5dd596f0b",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3cc3ab3c8f3b634c2b3650e3dcf0f3bed6bb8d32",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac92a3bc5c497f22229820aac08c0c371cb3e618",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7f5744360718d7591425191b7f72221ca38bc80d",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=231f26134c09ef247941498e6e2fd4faae616b4a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "jqi0odKJy6fJscofKk0VuwXRmAfkJMHhX47K7LhbRFo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly19br",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Skiata",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly19br/simplest_way_to_run_single_batch_jobs_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly19br/simplest_way_to_run_single_batch_jobs_for/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752329093,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwgauoo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Grok4 consults with daddy on answers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly182t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=730df43e6d4092e672b90332d13a367436980081",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752329000,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "apnews.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://apnews.com/article/grok-4-elon-musk-xai-colossus-14d575fb490c2b679ed3111a1c83f857",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?auto=webp&amp;s=ec3e8df970936fc13c713b5741a7622ad9eb0296",
                  "width": 1440,
                  "height": 810
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=e484e8eb03faedc41073355bee78085c8f7b60a2",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=804e77168cba5ee7bf52b6f5eb869380ee806ec0",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6aa31dbb87e860a75a96155a9f7c0ca33c2ed07",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0df9d505d158966a668d005df773b60839dfdbee",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f7e9bd32f4696efe169171d003760dbe24282378",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=60a96436e48e5f12735479d0171d04153a115b55",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "XIKzXUflHPuqjOItssqrAXFOcag8oXYxdjPMUzrlgkI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1ly182t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "throwawayacc201711",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly182t/grok4_consults_with_daddy_on_answers/",
          "stickied": false,
          "url": "https://apnews.com/article/grok-4-elon-musk-xai-colossus-14d575fb490c2b679ed3111a1c83f857",
          "subreddit_subscribers": 498343,
          "created_utc": 1752329000,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Everytime I peek at a builders GPU options I feel I never see it go that high. Anyone ever hear of a reputable builder with that power?",
          "author_fullname": "t2_opo23",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Are there any builder companies that sell pre-assembled Blackwell 6000 machines?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly0oln",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752327498,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everytime I peek at a builders GPU options I feel I never see it go that high. Anyone ever hear of a reputable builder with that power?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly0oln",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "richardanaya",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly0oln/are_there_any_builder_companies_that_sell/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly0oln/are_there_any_builder_companies_that_sell/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752327498,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have 32 GB of ram and a 4060 TI 16 GB. What's the best model I can run right now?\n\nIs there a website where you can just enter your specs and it spits out compatible models?\n\nWhat's the best local UI right now? LM Studio?",
          "author_fullname": "t2_67hxg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "It's been a while, I'm out of date, suggest me a model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1ly0jnx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752327096,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have 32 GB of ram and a 4060 TI 16 GB. What&amp;#39;s the best model I can run right now?&lt;/p&gt;\n\n&lt;p&gt;Is there a website where you can just enter your specs and it spits out compatible models?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s the best local UI right now? LM Studio?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1ly0jnx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mmmm_frietjes",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ly0jnx/its_been_a_while_im_out_of_date_suggest_me_a_model/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752327096,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just started experimenting with LLM workflow using n8n, and I built a workflow to improve the translation quality of my local LLM, sure it works but I found it lacking some basic functions, like I need to write JavaScript for some very basic things \n\nI'm not an professional AI workflow developer, I just want to improve my local LLM's performance with minimal coding.\n\nWhat are your recommendations for a more user-friendly LLM workflow UIs that are good alternatives to n8n? Which UI are you using right now?\n\nThanks in advance!",
          "author_fullname": "t2_4gc7hf3m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What LLM Workflow UI Are You Using?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxz268",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752322677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just started experimenting with LLM workflow using n8n, and I built a workflow to improve the translation quality of my local LLM, sure it works but I found it lacking some basic functions, like I need to write JavaScript for some very basic things &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not an professional AI workflow developer, I just want to improve my local LLM&amp;#39;s performance with minimal coding.&lt;/p&gt;\n\n&lt;p&gt;What are your recommendations for a more user-friendly LLM workflow UIs that are good alternatives to n8n? Which UI are you using right now?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxz268",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AaronFeng47",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lxz268/what_llm_workflow_ui_are_you_using/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxz268/what_llm_workflow_ui_are_you_using/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752322677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1nisx8ggay",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "we have to delay it",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxyvto",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 2560,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 2560,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/2YTkmABEvZ-xDvkpMbhGYL3GX_nvQgreScEcpz6UlEE.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752322106,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/oma34zdapfcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?auto=webp&amp;s=954d7d66f90b748d8e5d4feb24c6aab764476f51",
                  "width": 1200,
                  "height": 880
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=cc062ecd361f54588924802e9a8d113aeaaaa827",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4c82b3926fc1a6c6306e5b970dfd4784fdc38d7",
                    "width": 216,
                    "height": 158
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=5f9c1cd39db814db4c5915419c4d114e086fd641",
                    "width": 320,
                    "height": 234
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=108d706d99e4ad19833dd94c54c220bc8d7544f5",
                    "width": 640,
                    "height": 469
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7072dec54580b443d3fc3fa563ae25d5f249bd31",
                    "width": 960,
                    "height": 704
                  },
                  {
                    "url": "https://preview.redd.it/oma34zdapfcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bdb9eaf6243bf6a7a2c8c28bc6460844654f7236",
                    "width": 1080,
                    "height": 792
                  }
                ],
                "variants": {},
                "id": "kv1bPWQK0QfjBY4lkNaJeVxIc-aQd3DEWwNTmfHkEhw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lxyvto",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ILoveMy2Balls",
          "discussion_type": null,
          "num_comments": 176,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxyvto/we_have_to_delay_it/",
          "stickied": false,
          "url": "https://i.redd.it/oma34zdapfcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752322106,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1aluyxp2yo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "\"We will release o3 wieghts next week\"",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxyj92",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 1337,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 800,
              "fallback_url": "https://v.redd.it/8iqku5brlfcf1/DASH_360.mp4?source=fallback",
              "has_audio": true,
              "height": 360,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/8iqku5brlfcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8iqku5brlfcf1/DASHPlaylist.mpd?a=1755013022%2CNWJhODVkN2IwOTFjODhjNzA4ZDE1ZmRkMDJmYWRmYzAzZjYyNjdjMmQyMTk3MjVhYjVkMWFkNGM3ODNjODNhNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/8iqku5brlfcf1/HLSPlaylist.m3u8?a=1755013022%2CODg2MDAzYjcyZWM1ZGEzNWQxNzBiNjY5MTUyNjExOTAyNDM1ZjE5NWI0ZDA4YTNmOTg4OGUyZDNiYjgyYjcyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 1337,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=140&amp;height=105&amp;crop=140:105,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=95673b6951668a67f35ba057d964af1a0c494b31",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752320929,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/8iqku5brlfcf1",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?format=pjpg&amp;auto=webp&amp;s=e5e2c20582d3be0b2fae4e0de79232d5060ef676",
                  "width": 480,
                  "height": 360
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=6338a91401d613f89c37a6107a739c43ba939ca6",
                    "width": 108,
                    "height": 81
                  },
                  {
                    "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c7d30b78ccd98502de6db6c25e26ba6c69ef48da",
                    "width": 216,
                    "height": 162
                  },
                  {
                    "url": "https://external-preview.redd.it/MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5b18a28ecd27599e16af8fe9723eaa668cb9067c",
                    "width": 320,
                    "height": 240
                  }
                ],
                "variants": {},
                "id": "MHB1MWw1YnJsZmNmMdL5zV43Lc9tDShB7DOvm21L1LTW10tUK0LGB85rL2PQ"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1lxyj92",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Qparadisee",
          "discussion_type": null,
          "num_comments": 79,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxyj92/we_will_release_o3_wieghts_next_week/",
          "stickied": false,
          "url": "https://v.redd.it/8iqku5brlfcf1",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320929,
          "num_crossposts": 1,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 800,
              "fallback_url": "https://v.redd.it/8iqku5brlfcf1/DASH_360.mp4?source=fallback",
              "has_audio": true,
              "height": 360,
              "width": 480,
              "scrubber_media_url": "https://v.redd.it/8iqku5brlfcf1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/8iqku5brlfcf1/DASHPlaylist.mpd?a=1755013022%2CNWJhODVkN2IwOTFjODhjNzA4ZDE1ZmRkMDJmYWRmYzAzZjYyNjdjMmQyMTk3MjVhYjVkMWFkNGM3ODNjODNhNQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 37,
              "hls_url": "https://v.redd.it/8iqku5brlfcf1/HLSPlaylist.m3u8?a=1755013022%2CODg2MDAzYjcyZWM1ZGEzNWQxNzBiNjY5MTUyNjExOTAyNDM1ZjE5NWI0ZDA4YTNmOTg4OGUyZDNiYjgyYjcyMQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been here since llama1 area.. what a crazy ride!  \nNow we have that little devstral 2507.  \nTo me it feels as good as deepseek R1 the first but runs on dual 3090 ! (Ofc q8 with 45k ctx).  \nDo you feel the same thing? Ho my.. open weights models won't be as fun without Mistral 🇨🇵\n\n(To me it feels like 8x7b again but better 😆 )",
          "author_fullname": "t2_cj9kap4bx",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Have you tried that new devstral?! Myyy! The next 8x7b?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxyg6z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.85,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 49,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 49,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752320636,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been here since llama1 area.. what a crazy ride!&lt;br/&gt;\nNow we have that little devstral 2507.&lt;br/&gt;\nTo me it feels as good as deepseek R1 the first but runs on dual 3090 ! (Ofc q8 with 45k ctx).&lt;br/&gt;\nDo you feel the same thing? Ho my.. open weights models won&amp;#39;t be as fun without Mistral 🇨🇵&lt;/p&gt;\n\n&lt;p&gt;(To me it feels like 8x7b again but better 😆 )&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxyg6z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Afternoon_4260",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxyg6z/have_you_tried_that_new_devstral_myyy_the_next/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320636,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_y35oj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Safety first, or whatever🙄",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxycdh",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": null,
          "ups": 162,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 162,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/Zzym4XXD6JbiVex-1mXTA-GS_8h0f2iI0X4SmG7Ksy8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752320256,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/idk5uvesjfcf1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?auto=webp&amp;s=6d69525514eee5345b46dc8b914fc4c1b26be574",
                  "width": 1125,
                  "height": 1125
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2cc3224e9e3248f9795321cf57fc214ae879cd51",
                    "width": 108,
                    "height": 108
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=12b0b288f01c6a23a110d4522f182ac091e2ed48",
                    "width": 216,
                    "height": 216
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7b0701a1ed34c1b6bf8d8a74218fff22b70f96a",
                    "width": 320,
                    "height": 320
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=326cdedce8274c918a8336924d8741c3576c2f5a",
                    "width": 640,
                    "height": 640
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=17aff077f82095d1ab7a525405702f4326c4bb85",
                    "width": 960,
                    "height": 960
                  },
                  {
                    "url": "https://preview.redd.it/idk5uvesjfcf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7b65e41b0a6339e460b04ff0532103f867ef5a74",
                    "width": 1080,
                    "height": 1080
                  }
                ],
                "variants": {},
                "id": "Id2wk8n7gqUJk-T46y2nf1kFJ57mzqGvG8Wuh0Ws0XU"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1lxycdh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Porespellar",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxycdh/safety_first_or_whatever/",
          "stickied": false,
          "url": "https://i.redd.it/idk5uvesjfcf1.jpeg",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320256,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi all, I wanted to rewrite my question and put it as a discussion, in December I will be building/buying a computer to be a Home companion/nas/plex/gaming system, it will be running 24/7 and be part of a disabled person's (me) safe space and will be both a companion and entertainment.\n\nIt will run PC games, Silly tavern, ooga, llmstudio, it will be used for vlogging, plex and fit into my 10gbe network it will also be a full steam game system which will stream via parsec or in-built steam to wherever I am in the house, I'll also use virtual desktop to run my VR games and fun.\n\nAwesome use cases like with Mantella having a playthrough of SkyrimVR where every npc is AI enabled and I spend all my time breaking 4th wall and explaining to them the concept of npc's\n\nIt is used for therapy and every part of my life.\n\nI prefer windows, both all the normal OS and I love Windows server 2022,\n\nSo IF want to run a good quality model beyond the basics (I've used 4090's, 3090, 4060ti) with large context and long term use.\n\nI would prefer it to be quiet (not silent but in the reasonable range of a gaming PC using a 5060ti using VR) Not a deal breaker but I can hope.\n\nPower I'd like it to idle under 150w ideally 100w (full load power use I don't mind)\n\nSo tell me how you would build a 10k system or below and your thoughts behind it.  remember it has to run a good size model at a speed that TTS and STT are fluid and feel like a conversation not a stutter stack. Deal with gaming.\n\nFor an example I have a Poweredge 730XD 128gb DDR4 48tb SAS. with two e5-2697AV4 cpu's.\n\nI was able by putting an rtx 4000 16gb in the above system use it for everything above except big models, it even streamed AAA games (it had a 36TB steam library :D ) to my mac air/steam deck/ tablet and low powered pc fine and did Virtual desktop for my quest 3. I was surprised how well the old Xeon could handle gaming (I game mostly in 1080p anyway)\n\nBut because of the old PCIE 3 architecture anything above an rtx 4000 was issuey, and it was sooo loud I had to keep it in the kitchen, and it idled at 320w.\n\nLooking for any ideas and like I said I will have the funds for this end of December , what would you put together and importantly why?\n\n  \n\\-------------------\n\nUpdate 1\n\nLooks like the choice is \n\nMac studio m3 ultra 512gb   \n\nor\n\nRTX 6000 pro.\n\n  \nI have an AM5 platform with an 8700g which isn't a slouch paired witrh 64gb ddr5, the 6000 would kind of fit in there.\n\nI have time to look into it all.",
          "author_fullname": "t2_nufca",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is your \"perfect\" £10,000 for Local LLM, Gaming, plex with the following conditional and context.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxybu4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752342589,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752320202,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I wanted to rewrite my question and put it as a discussion, in December I will be building/buying a computer to be a Home companion/nas/plex/gaming system, it will be running 24/7 and be part of a disabled person&amp;#39;s (me) safe space and will be both a companion and entertainment.&lt;/p&gt;\n\n&lt;p&gt;It will run PC games, Silly tavern, ooga, llmstudio, it will be used for vlogging, plex and fit into my 10gbe network it will also be a full steam game system which will stream via parsec or in-built steam to wherever I am in the house, I&amp;#39;ll also use virtual desktop to run my VR games and fun.&lt;/p&gt;\n\n&lt;p&gt;Awesome use cases like with Mantella having a playthrough of SkyrimVR where every npc is AI enabled and I spend all my time breaking 4th wall and explaining to them the concept of npc&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;It is used for therapy and every part of my life.&lt;/p&gt;\n\n&lt;p&gt;I prefer windows, both all the normal OS and I love Windows server 2022,&lt;/p&gt;\n\n&lt;p&gt;So IF want to run a good quality model beyond the basics (I&amp;#39;ve used 4090&amp;#39;s, 3090, 4060ti) with large context and long term use.&lt;/p&gt;\n\n&lt;p&gt;I would prefer it to be quiet (not silent but in the reasonable range of a gaming PC using a 5060ti using VR) Not a deal breaker but I can hope.&lt;/p&gt;\n\n&lt;p&gt;Power I&amp;#39;d like it to idle under 150w ideally 100w (full load power use I don&amp;#39;t mind)&lt;/p&gt;\n\n&lt;p&gt;So tell me how you would build a 10k system or below and your thoughts behind it.  remember it has to run a good size model at a speed that TTS and STT are fluid and feel like a conversation not a stutter stack. Deal with gaming.&lt;/p&gt;\n\n&lt;p&gt;For an example I have a Poweredge 730XD 128gb DDR4 48tb SAS. with two e5-2697AV4 cpu&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;I was able by putting an rtx 4000 16gb in the above system use it for everything above except big models, it even streamed AAA games (it had a 36TB steam library :D ) to my mac air/steam deck/ tablet and low powered pc fine and did Virtual desktop for my quest 3. I was surprised how well the old Xeon could handle gaming (I game mostly in 1080p anyway)&lt;/p&gt;\n\n&lt;p&gt;But because of the old PCIE 3 architecture anything above an rtx 4000 was issuey, and it was sooo loud I had to keep it in the kitchen, and it idled at 320w.&lt;/p&gt;\n\n&lt;p&gt;Looking for any ideas and like I said I will have the funds for this end of December , what would you put together and importantly why?&lt;/p&gt;\n\n&lt;p&gt;-------------------&lt;/p&gt;\n\n&lt;p&gt;Update 1&lt;/p&gt;\n\n&lt;p&gt;Looks like the choice is &lt;/p&gt;\n\n&lt;p&gt;Mac studio m3 ultra 512gb   &lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;RTX 6000 pro.&lt;/p&gt;\n\n&lt;p&gt;I have an AM5 platform with an 8700g which isn&amp;#39;t a slouch paired witrh 64gb ddr5, the 6000 would kind of fit in there.&lt;/p&gt;\n\n&lt;p&gt;I have time to look into it all.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxybu4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Quebber",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxybu4/what_is_your_perfect_10000_for_local_llm_gaming/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxybu4/what_is_your_perfect_10000_for_local_llm_gaming/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752320202,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://preview.redd.it/owdr102lifcf1.png?width=1041&amp;format=png&amp;auto=webp&amp;s=ce4b7f362b8d1934553e3cfa69f8252078aa99f0\n\nhehe",
          "author_fullname": "t2_jibio",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "newbie here. Is this normal? Am I doing everything wrong? Am I asking too much? Gemma3 4b was transcribing ok with some mistakes",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 103,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "owdr102lifcf1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 80,
                  "x": 108,
                  "u": "https://preview.redd.it/owdr102lifcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=083abb592bd1e2a19859969bee1c6b422fffd1fa"
                },
                {
                  "y": 160,
                  "x": 216,
                  "u": "https://preview.redd.it/owdr102lifcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ad3fadc24ba466a7e4475e581ef425b795357da"
                },
                {
                  "y": 237,
                  "x": 320,
                  "u": "https://preview.redd.it/owdr102lifcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f49de55abc9d73d1d7efae2509a9d3d362951a2d"
                },
                {
                  "y": 474,
                  "x": 640,
                  "u": "https://preview.redd.it/owdr102lifcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=af21b3a87826a7db0b44f2768c78ec66f71750ba"
                },
                {
                  "y": 711,
                  "x": 960,
                  "u": "https://preview.redd.it/owdr102lifcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8a0f8d1a0051a10debd6bae869b1ab3e43b9cd29"
                }
              ],
              "s": {
                "y": 772,
                "x": 1041,
                "u": "https://preview.redd.it/owdr102lifcf1.png?width=1041&amp;format=png&amp;auto=webp&amp;s=ce4b7f362b8d1934553e3cfa69f8252078aa99f0"
              },
              "id": "owdr102lifcf1"
            }
          },
          "name": "t3_1lxy8xz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.44,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/cml4NadfwMakHTFJBHnjC5OY0pcdZW8DCwbSPbko6_w.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752319915,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/owdr102lifcf1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce4b7f362b8d1934553e3cfa69f8252078aa99f0\"&gt;https://preview.redd.it/owdr102lifcf1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce4b7f362b8d1934553e3cfa69f8252078aa99f0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;hehe&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxy8xz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Super_Snowbro",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxy8xz/newbie_here_is_this_normal_am_i_doing_everything/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxy8xz/newbie_here_is_this_normal_am_i_doing_everything/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752319915,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "It is a difficult bit of UX to figure out and I didn’t want to go with what felt right to me. \n\n[View Poll](https://www.reddit.com/poll/1lxxgm2)",
          "author_fullname": "t2_bquk1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "i’m building a platform where you can use your local gpus, rent remote gpus, or use co-op shared gpus. what is more important to you?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxxgm2",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.23,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752317019,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It is a difficult bit of UX to figure out and I didn’t want to go with what felt right to me. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/poll/1lxxgm2\"&gt;View Poll&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxxgm2",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "okaris",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "poll_data": {
            "prediction_status": null,
            "total_stake_amount": null,
            "voting_end_timestamp": 1752576219278,
            "options": [
              {
                "text": "choosing which model variant (quant) to run. i can select the gpu layer",
                "id": "31161511"
              },
              {
                "text": "choosing which gpu to run on. i can choose the variant based on what that gpu can run.",
                "id": "31161512"
              },
              {
                "text": "other (something i may be missing?)",
                "id": "31161513"
              }
            ],
            "vote_updates_remained": null,
            "is_prediction": false,
            "resolved_option_id": null,
            "user_won_amount": null,
            "user_selection": null,
            "total_vote_count": 28,
            "tournament_id": null
          },
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxxgm2/im_building_a_platform_where_you_can_use_your/",
          "stickied": false,
          "mod_reports": [],
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxxgm2/im_building_a_platform_where_you_can_use_your/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752317019,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "How bad are laptops for running LLM’s? I am going to get a laptop this August and would love to run a 5b-7B local LLM. How feasible is this? \n\nAny serious hardware suggestions here would be much appreciated. Also how much I amount to spend here? Haha ",
          "author_fullname": "t2_21pp8tew",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Local LLM on laptop?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxx4sb",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752316171,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752315729,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How bad are laptops for running LLM’s? I am going to get a laptop this August and would love to run a 5b-7B local LLM. How feasible is this? &lt;/p&gt;\n\n&lt;p&gt;Any serious hardware suggestions here would be much appreciated. Also how much I amount to spend here? Haha &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxx4sb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ontologicalmemes",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752315729,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey, I am an llm privacy researcher, I need a SFF build as my personal machine, that I plan to travel with and use to show live demonstrations to potential enterprise clients, will host an 8B llm plus some basic overheads like BERT \n\nThe 5060ti is new, reliable ( i can buy for 450$ in my country) cheap and comes with warranty. New architecture so I assume some pytorch improvements, 4 bit llms?\n\nCons: super low bandwidth, VRAM not enough to host say 13B models, token per second is going to be abysmal, large contexts? I work with documents\n\nThe rtx 3090 ( 750$ gaming use 3 years out of warranty) is of course a beast, with 24 gigs of VRAM and almost 3x the bandwidth \n\nCons: risky, will it handle our loads well? Thermal failure? Higher TDP for sff? What if i get handed a bad card ( mining etc )\n\nPlease help me i am so confused 😕 \nThis community is awesome 🙏\n\n\n",
          "author_fullname": "t2_1t1panb3d8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rtx 5060ti 16gb vs Rtx 3090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxwpqp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752314043,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, I am an llm privacy researcher, I need a SFF build as my personal machine, that I plan to travel with and use to show live demonstrations to potential enterprise clients, will host an 8B llm plus some basic overheads like BERT &lt;/p&gt;\n\n&lt;p&gt;The 5060ti is new, reliable ( i can buy for 450$ in my country) cheap and comes with warranty. New architecture so I assume some pytorch improvements, 4 bit llms?&lt;/p&gt;\n\n&lt;p&gt;Cons: super low bandwidth, VRAM not enough to host say 13B models, token per second is going to be abysmal, large contexts? I work with documents&lt;/p&gt;\n\n&lt;p&gt;The rtx 3090 ( 750$ gaming use 3 years out of warranty) is of course a beast, with 24 gigs of VRAM and almost 3x the bandwidth &lt;/p&gt;\n\n&lt;p&gt;Cons: risky, will it handle our loads well? Thermal failure? Higher TDP for sff? What if i get handed a bad card ( mining etc )&lt;/p&gt;\n\n&lt;p&gt;Please help me i am so confused 😕 \nThis community is awesome 🙏&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxwpqp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Alpine_Privacy",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxwpqp/rtx_5060ti_16gb_vs_rtx_3090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxwpqp/rtx_5060ti_16gb_vs_rtx_3090/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752314043,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "What am I looking at for something that can run DeepSeek R1 Q8 w/ full 128K context window?  \nI know an Epyc setup can do this, I am not sure about if it can hit 20 tokens/second.\n\nI suspect it will need 1024G ram, potentially more?\n\nAnyone have a CPU system running full DeepSeek R1 (ideally Q8) at 20+ tokens/second?\n\nFrom what I understand, a handful of GPUs won't improve the performance that much?  \n",
          "author_fullname": "t2_ijzb7",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best setup for ~20 tokens/sec DeepSeek R1 671B Q8 w/ 128K context window",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxwodv",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752313888,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What am I looking at for something that can run DeepSeek R1 Q8 w/ full 128K context window?&lt;br/&gt;\nI know an Epyc setup can do this, I am not sure about if it can hit 20 tokens/second.&lt;/p&gt;\n\n&lt;p&gt;I suspect it will need 1024G ram, potentially more?&lt;/p&gt;\n\n&lt;p&gt;Anyone have a CPU system running full DeepSeek R1 (ideally Q8) at 20+ tokens/second?&lt;/p&gt;\n\n&lt;p&gt;From what I understand, a handful of GPUs won&amp;#39;t improve the performance that much?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxwodv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MidnightProgrammer",
          "discussion_type": null,
          "num_comments": 56,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752313888,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I’m after a decently sized - by which I mean 50B+ parameters - text-focused foundation model I can fine-tune for a specific use case. I have the dataset, I have the hardware. What I don’t have is a suitable LLM to use as a base. Something like Llama 3.3-70b would be perfect, but that’s only being distributed as an instruct model. And I don’t want to touch Chinese-originating models because there’s a reputational risk in using something that denies Tiananmen Square ever happened.  \nAny suggestions? ",
          "author_fullname": "t2_4sqhqdgr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Performant open weights foundation text-specific models are where now?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxwb4m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752312367,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m after a decently sized - by which I mean 50B+ parameters - text-focused foundation model I can fine-tune for a specific use case. I have the dataset, I have the hardware. What I don’t have is a suitable LLM to use as a base. Something like Llama 3.3-70b would be perfect, but that’s only being distributed as an instruct model. And I don’t want to touch Chinese-originating models because there’s a reputational risk in using something that denies Tiananmen Square ever happened.&lt;br/&gt;\nAny suggestions? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxwb4m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "psychonomy",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxwb4m/performant_open_weights_foundation_textspecific/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxwb4m/performant_open_weights_foundation_textspecific/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752312367,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone\n\nI'm currently building a new rig to get my feet wet with LLMs. There is a sale where I live and these 2 GPUs are pretty much the same price with 9070 XT beeing \\~40 USD more expensive.\n\nThe trade off would be those 4GB VRAM extra on 7900 XT vs PCIE 5 on the newer 9070 XT.\n\n7900 XTX is out of the question since is it about \\~220 USD more expensive and NVIDIA is out of the question because it is NVIDIA.\n\nI will be running Fedora on my box. Any thoughts ? ",
          "author_fullname": "t2_16ic2d2lup",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "New GPU 7900 XT vs 9070 XT where price difference is ~40 USD",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxw7es",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752311943,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently building a new rig to get my feet wet with LLMs. There is a sale where I live and these 2 GPUs are pretty much the same price with 9070 XT beeing ~40 USD more expensive.&lt;/p&gt;\n\n&lt;p&gt;The trade off would be those 4GB VRAM extra on 7900 XT vs PCIE 5 on the newer 9070 XT.&lt;/p&gt;\n\n&lt;p&gt;7900 XTX is out of the question since is it about ~220 USD more expensive and NVIDIA is out of the question because it is NVIDIA.&lt;/p&gt;\n\n&lt;p&gt;I will be running Fedora on my box. Any thoughts ? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxw7es",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "restless_forever",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxw7es/new_gpu_7900_xt_vs_9070_xt_where_price_difference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxw7es/new_gpu_7900_xt_vs_9070_xt_where_price_difference/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752311943,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Medical triage means determining whether symptoms require emergency care, urgent care, or can be managed with self-care. This matters because LLMs are increasingly becoming the \"digital front door\" for health concerns—replacing the instinct to just Google it.\n\nGetting triage wrong can be dangerous (missed emergencies) or costly (unnecessary ER visits).\n\nWe've open-sourced **TriageBench**, a reproducible framework for evaluating LLM triage accuracy. It includes:\n\n* Standard clinical dataset (Semigran vignettes)\n* Paired McNemar's test to detect model performance differences on small datasets\n* Full methodology and evaluation code\n\nGitHub: [https://github.com/medaks/medask-benchmark](https://github.com/medaks/medask-benchmark)\n\nAs a demonstration, we benchmarked our own model (MedAsk) against several OpenAI models:\n\n* MedAsk: **87.6% accuracy**\n* o3: **75.6%**\n* GPT‑4.5: **68.9%**\n\nThe main limitation is dataset size (45 vignettes). We're looking for collaborators to help expand this—the field needs larger, more diverse clinical datasets.\n\nBlog post with full results: [https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/](https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/)",
          "author_fullname": "t2_lp2ivten",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We built an open-source medical triage benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxw3zz",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 116,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 116,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752311546,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Medical triage means determining whether symptoms require emergency care, urgent care, or can be managed with self-care. This matters because LLMs are increasingly becoming the &amp;quot;digital front door&amp;quot; for health concerns—replacing the instinct to just Google it.&lt;/p&gt;\n\n&lt;p&gt;Getting triage wrong can be dangerous (missed emergencies) or costly (unnecessary ER visits).&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;ve open-sourced &lt;strong&gt;TriageBench&lt;/strong&gt;, a reproducible framework for evaluating LLM triage accuracy. It includes:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Standard clinical dataset (Semigran vignettes)&lt;/li&gt;\n&lt;li&gt;Paired McNemar&amp;#39;s test to detect model performance differences on small datasets&lt;/li&gt;\n&lt;li&gt;Full methodology and evaluation code&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;GitHub: &lt;a href=\"https://github.com/medaks/medask-benchmark\"&gt;https://github.com/medaks/medask-benchmark&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;As a demonstration, we benchmarked our own model (MedAsk) against several OpenAI models:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;MedAsk: &lt;strong&gt;87.6% accuracy&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;o3: &lt;strong&gt;75.6%&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;GPT‑4.5: &lt;strong&gt;68.9%&lt;/strong&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;The main limitation is dataset size (45 vignettes). We&amp;#39;re looking for collaborators to help expand this—the field needs larger, more diverse clinical datasets.&lt;/p&gt;\n\n&lt;p&gt;Blog post with full results: &lt;a href=\"https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/\"&gt;https://medask.tech/blogs/medical-ai-triage-accuracy-2025-medask-beats-openais-o3-gpt-4-5/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?auto=webp&amp;s=41ebf4298905eb26f4cef7c264a930eaa2aa2c5c",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b9c7e10a1a4f6aeffdd4ad9ec00fba71b13e9850",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=12ae853d79e9684e4fdd32bea3af05e334b73b38",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b194f182a8f6d7fa069cdedc5685ff5d9aecc59",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=861e796a8d8ac8b7c2b73c785d2751b59de40d1c",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f068abe4f3993070d3e9aefaedd824a71bb88c7",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39b1861c4e75006d81e7f80388e64198bfc249fa",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "YGRuXIPLJmfx-HMMUWxIo3PT1Eu1Kllj_TeA0JBYYtI"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1lxw3zz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Significant-Pair-275",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxw3zz/we_built_an_opensource_medical_triage_benchmark/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752311546,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I just checked the monthly LLM API costs at my firm, and it's insanely high. I don’t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.\n\nAnd honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.\n",
          "author_fullname": "t2_h1zso7cq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Traditional Data Science work is going to be back",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxvrjm",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 37,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 37,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752310104,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I just checked the monthly LLM API costs at my firm, and it&amp;#39;s insanely high. I don’t see this being sustainable for much longer. Eventually, senior management will realize it and start cutting down on these expenses. Companies will likely shift towards hosting smaller LLMs internally for agentic use cases instead of relying on external APIs.&lt;/p&gt;\n\n&lt;p&gt;And honestly, who better to understand the nitty-gritty details of an ML model than data scientists? For the past two years, it felt like ML engineers were contributing more than data scientists, but I think that trend is going to slowly reverse.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxvrjm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Competitive_Push5407",
          "discussion_type": null,
          "num_comments": 42,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxvrjm/traditional_data_science_work_is_going_to_be_back/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752310104,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Please tell if you can limit your 5090 down to 300W or below and the driver version. I think I've seen reports that it could be limited to 300W and below but now the lower limit is 400W, it seems that the Jacket is jacking with us.",
          "author_fullname": "t2_1eex9ug5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "5090 minimum power limit = 400W ?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxvh5t",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.81,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752308949,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Please tell if you can limit your 5090 down to 300W or below and the driver version. I think I&amp;#39;ve seen reports that it could be limited to 300W and below but now the lower limit is 400W, it seems that the Jacket is jacking with us.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxvh5t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MelodicRecognition7",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxvh5t/5090_minimum_power_limit_400w/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxvh5t/5090_minimum_power_limit_400w/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752308949,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "## Edit 1\nI want to reiterate this is not using llama cpp. This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers. \n\n\n## Background &amp; Brief Setup\nWe need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. \n\nI am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].\n\n**Model** : Qwen 3 Embeddings 0.6B [should not matter but _downloaded locally_]\n\nNot using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.\n\n## Problem\n \nLike I don't know how to put this, but the embeddings feel really.. 'bad'? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can't understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.\n\n## Questions\n\nIs there something obvious I am missing here?\n\nHas someone else faced similar issues with Qwen3 Embeddings?\n\nAre embeddings tuned for instructions fundamentally different from 'normal' embedding models in any way? \n\nAre there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?",
          "author_fullname": "t2_alrxvbt1",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen 3 Embeddings 0.6B faring really poorly inspite of high score on benchmarks",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxvf0j",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1752409332,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752308700,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h2&gt;Edit 1&lt;/h2&gt;\n\n&lt;p&gt;I want to reiterate this is not using llama cpp. This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers. &lt;/p&gt;\n\n&lt;h2&gt;Background &amp;amp; Brief Setup&lt;/h2&gt;\n\n&lt;p&gt;We need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. &lt;/p&gt;\n\n&lt;p&gt;I am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt; : Qwen 3 Embeddings 0.6B [should not matter but &lt;em&gt;downloaded locally&lt;/em&gt;]&lt;/p&gt;\n\n&lt;p&gt;Not using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.&lt;/p&gt;\n\n&lt;h2&gt;Problem&lt;/h2&gt;\n\n&lt;p&gt;Like I don&amp;#39;t know how to put this, but the embeddings feel really.. &amp;#39;bad&amp;#39;? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can&amp;#39;t understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.&lt;/p&gt;\n\n&lt;h2&gt;Questions&lt;/h2&gt;\n\n&lt;p&gt;Is there something obvious I am missing here?&lt;/p&gt;\n\n&lt;p&gt;Has someone else faced similar issues with Qwen3 Embeddings?&lt;/p&gt;\n\n&lt;p&gt;Are embeddings tuned for instructions fundamentally different from &amp;#39;normal&amp;#39; embedding models in any way? &lt;/p&gt;\n\n&lt;p&gt;Are there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxvf0j",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "i4858i",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752308700,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am assuming most LLMs today use more or less a similar architecture. I am also assuming the initial training data is mostly the same (i.e. books, wikipedia etc), and probably close to being exhausted already?\n\nSo what would make a future major version of an LLM much better than the previous one?\n\nI get post training and finetuning. But in terms of general intelligence and performance, are we slowing down until the next breakthroughs?",
          "author_fullname": "t2_1441omqx4c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What drives progress in newer LLMs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxv6a5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.91,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 22,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 22,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752307718,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am assuming most LLMs today use more or less a similar architecture. I am also assuming the initial training data is mostly the same (i.e. books, wikipedia etc), and probably close to being exhausted already?&lt;/p&gt;\n\n&lt;p&gt;So what would make a future major version of an LLM much better than the previous one?&lt;/p&gt;\n\n&lt;p&gt;I get post training and finetuning. But in terms of general intelligence and performance, are we slowing down until the next breakthroughs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxv6a5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cangaroo_hamam",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxv6a5/what_drives_progress_in_newer_llms/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxv6a5/what_drives_progress_in_newer_llms/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752307718,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "As per the title. I want to run a model for dnd, the plan is to use Gemma 3 27b and max out the context length so that the model can remember things. Once the context fills up, I plan to ask the model to summarise the session and paste it into a new instance to continue. I have tried it with Gemini 2.5 Pro and the method works well enough.\n\nThe issue I mainly want to ask about is what impacts the filled up context length would have. From my understanding, I will need a stronger gpu chip for the prompt processing, but the vram will get filled up as usual.\n\nWill this just be the same as running a model that progressively gets larger the more I use it?\n\nHow does this work with multiple gpus? \n\nWhat prompt processing speeds can I expect with an mi50 32gb?\n\nHow does prompt processing work actually, each portion loaded into vram is processed by that vram’s corresponding gpu chip right?\n\nSo many questions, I’ll probably ask further clarifying questions in the comments",
          "author_fullname": "t2_rn6co7q5m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does having a very long context window impact performance?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxuu5m",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752306412,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per the title. I want to run a model for dnd, the plan is to use Gemma 3 27b and max out the context length so that the model can remember things. Once the context fills up, I plan to ask the model to summarise the session and paste it into a new instance to continue. I have tried it with Gemini 2.5 Pro and the method works well enough.&lt;/p&gt;\n\n&lt;p&gt;The issue I mainly want to ask about is what impacts the filled up context length would have. From my understanding, I will need a stronger gpu chip for the prompt processing, but the vram will get filled up as usual.&lt;/p&gt;\n\n&lt;p&gt;Will this just be the same as running a model that progressively gets larger the more I use it?&lt;/p&gt;\n\n&lt;p&gt;How does this work with multiple gpus? &lt;/p&gt;\n\n&lt;p&gt;What prompt processing speeds can I expect with an mi50 32gb?&lt;/p&gt;\n\n&lt;p&gt;How does prompt processing work actually, each portion loaded into vram is processed by that vram’s corresponding gpu chip right?&lt;/p&gt;\n\n&lt;p&gt;So many questions, I’ll probably ask further clarifying questions in the comments&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxuu5m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "opoot_",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752306412,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey there, I got an M3 Max 96GB, which model do you guys think is the best for my hardware? For context, I mostly do light coding and agentic workflows that use MCP for data analytics. Thanks!",
          "author_fullname": "t2_2f6s556",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best model for M3 Max 96GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxufzz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.69,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752304885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, I got an M3 Max 96GB, which model do you guys think is the best for my hardware? For context, I mostly do light coding and agentic workflows that use MCP for data analytics. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxufzz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gaztrab",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxufzz/best_model_for_m3_max_96gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxufzz/best_model_for_m3_max_96gb/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752304885,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone, I'm trying to write some simple demo which uses an AI agent to play N-puzzle. I envision that the AI would use: move\\\\\\_up, move\\\\\\_down, move\\\\\\_right, move\\\\\\_left to move the game state, and also a print\\\\\\_state tool to print the current state. Here is my code:\n\n\n\nfrom pdb import set\\_trace\n\nimport os\n\nimport json\n\nfrom copy import deepcopy\n\nimport requests\n\nimport math\n\nimport inspect\n\nfrom inspect import signature\n\nimport numpy as np\n\nfrom pprint import pprint\n\nimport hashlib\n\nfrom collections import deque, defaultdict\n\nimport time\n\nimport random\n\nimport re\n\n\n\nfrom typing import Annotated, Sequence, TypedDict\n\nfrom pydantic import BaseModel, Field\n\n\n\nfrom pydantic\\_ai import Agent, RunContext\n\nfrom pydantic\\_ai.models.openai import OpenAIModel\n\nfrom pydantic\\_ai.providers.openai import OpenAIProvider\n\n\n\nollama\\_model = OpenAIModel(\n\nmodel\\_name='qwen3:latest', provider=OpenAIProvider(base\\_url='http://localhost:11434/v1')\n\n)\n\nagent = Agent(ollama\\_model,\n\n\\# output\\_type=CityLocation\n\n)\n\n\n\ndef get\\_n\\_digit(num):\n\nif num &gt; 0:\n\ndigits = int(math.log10(num))+1\n\nelif num == 0:\n\ndigits = 1\n\nelse:\n\ndigits = int(math.log10(-num))+2 # +1 if you don't count the '-' \n\nreturn digits\n\n\n\n\n\nclass GameState:\n\ndef \\_\\_init\\_\\_(self, start, goal):\n\nself.start = start\n\nself.goal = goal\n\n\n\nself.size = start.shape\\[0\\]\n\nself.state = deepcopy(start)\n\n\n\n\n\ndef get\\_state(self):\n\nreturn self.state\n\n\n\n\n\ndef finished(self):\n\nis\\_finished = (self.state==self.goal).all()\n\nif is\\_finished:\n\nprint(\"FINISHED!\")\n\nset\\_trace()\n\nreturn is\\_finished       \n\n\n\n\n\ndef print\\_state(self, no\\_print=False):\n\nmax\\_elem = np.max(self.state)\n\nn\\_digit = get\\_n\\_digit(max\\_elem)\n\n\n\nstate\\_text = \"\"\n\n\n\nfor row\\_idx in range(self.size):\n\nfor col\\_idx in range(self.size):\n\nif int(self.state\\[row\\_idx, col\\_idx\\]) != 0:\n\ntext = '{num:0{width}} '.format(num=self.state\\[row\\_idx, col\\_idx\\], width=n\\_digit)\n\nelse:                    \n\ntext = \"\\_\" \\* (n\\_digit) + \" \"\n\nstate\\_text += text\n\nstate\\_text += \"\\\\n\"\n\nif no\\_print is False:\n\nprint(state\\_text)\n\n\n\nreturn state\\_text\n\n\n\n\n\ndef create\\_diff\\_view(self):\n\n\"\"\"Show which tiles are out of place\"\"\"\n\ndiff\\_state = \"\"\n\nfor i in range(self.size):\n\nfor j in range(self.size):\n\ncurrent = self.state\\[i, j\\]\n\ntarget = self.goal\\[i, j\\]\n\nif current == target:\n\ndiff\\_state += f\"✓{current} \"\n\nelse:\n\ndiff\\_state += f\"✗{current} \"\n\ndiff\\_state += \"\\\\n\"\n\nreturn diff\\_state\n\n\n\n\n\n\n\ndef move\\_up(self):\n\nitemindex = np.where(self.state == 0)\n\npos\\_row = int(itemindex\\[0\\]\\[0\\])\n\npos\\_col = int(itemindex\\[1\\]\\[0\\])\n\n\n\nif (pos\\_row == 0):\n\nreturn\n\n\n\ntemp = self.state\\[pos\\_row, pos\\_col\\]\n\nself.state\\[pos\\_row, pos\\_col\\] = self.state\\[pos\\_row-1, pos\\_col\\]\n\nself.state\\[pos\\_row-1, pos\\_col\\] = temp\n\n\n\n\n\ndef move\\_down(self):\n\nitemindex = np.where(self.state == 0)\n\npos\\_row = int(itemindex\\[0\\]\\[0\\])\n\npos\\_col = int(itemindex\\[1\\]\\[0\\])\n\n\n\nif (pos\\_row == (self.size-1)):\n\nreturn\n\n\n\ntemp = self.state\\[pos\\_row, pos\\_col\\]\n\nself.state\\[pos\\_row, pos\\_col\\] = self.state\\[pos\\_row+1, pos\\_col\\]\n\nself.state\\[pos\\_row+1, pos\\_col\\] = temp\n\n\n\n\n\ndef move\\_left(self):\n\nitemindex = np.where(self.state == 0)\n\npos\\_row = int(itemindex\\[0\\]\\[0\\])\n\npos\\_col = int(itemindex\\[1\\]\\[0\\])\n\n\n\nif (pos\\_col == 0):\n\nreturn\n\n\n\ntemp = self.state\\[pos\\_row, pos\\_col\\]\n\nself.state\\[pos\\_row, pos\\_col\\] = self.state\\[pos\\_row, pos\\_col-1\\]\n\nself.state\\[pos\\_row, pos\\_col-1\\] = temp\n\n\n\n\n\ndef move\\_right(self):\n\nitemindex = np.where(self.state == 0)\n\npos\\_row = int(itemindex\\[0\\]\\[0\\])\n\npos\\_col = int(itemindex\\[1\\]\\[0\\])\n\n\n\nif (pos\\_col == (self.size-1)):\n\nreturn\n\n\n\ntemp = self.state\\[pos\\_row, pos\\_col\\]\n\nself.state\\[pos\\_row, pos\\_col\\] = self.state\\[pos\\_row, pos\\_col+1\\]\n\nself.state\\[pos\\_row, pos\\_col+1\\] = temp\n\n\n\n\\# 8-puzzle\n\n\\# start = np.array(\\[\n\n\\# \\[0, 1, 3\\],\n\n\\# \\[4, 2, 5\\],\n\n\\# \\[7, 8, 6\\],\n\n\\# \\])\n\n\n\n\\# goal = np.array(\\[\n\n\\# \\[1, 2, 3\\],\n\n\\# \\[4, 5, 6\\],\n\n\\# \\[7, 8, 0\\],\n\n\\# \\])\n\n\n\n\\# 15-puzzle\n\nstart = np.array(\\[\n\n\\[ 6, 13,  7, 10\\],\n\n\\[ 8,  9, 11,  0\\],\n\n\\[15,  2, 12,  5\\],\n\n\\[14,  3,  1,  4\\],\n\n\\])\n\n\n\ngoal = np.array(\\[\n\n\\[ 1,  2,  3,  4\\],\n\n\\[ 5,  6,  7,  8\\],\n\n\\[ 9, 10, 11, 12\\],\n\n\\[13, 14, 15,  0\\],\n\n\\])\n\n\n\n\n\ngame\\_state = GameState(start, goal)\n\n\n\n\\# u/agent.tool\\_plain\n\n\\# def check\\_finished() -&gt; bool:\n\n\\# \"\"\"Check whether or not the game state has reached the goal. Returns a boolean value\"\"\"\n\n\\# print(f\"CALL TOOL: {inspect.currentframe().f\\_code.co\\_name}\")\n\n\\# return game\\_state.finished()\n\n\n\nu/agent.tool\\_plain\n\ndef move\\_up():\n\n\"\"\"Move the '\\_' tile up by one block, swapping the tile with the number above. Returns the text describing the new game state after moving up.\"\"\"\n\nprint(f\"CALL TOOL: {inspect.currentframe().f\\_code.co\\_name}\")\n\ngame\\_state.move\\_up()\n\nreturn game\\_state.print\\_state(no\\_print=True)\n\n\n\n\n\nu/agent.tool\\_plain    \n\ndef move\\_down():\n\n\"\"\"Move the '\\_' tile down by one block, swapping the tile with the number below. Returns the text describing the new game state after moving down.\"\"\"\n\nprint(f\"CALL TOOL: {inspect.currentframe().f\\_code.co\\_name}\")\n\ngame\\_state.move\\_down()\n\nreturn game\\_state.print\\_state(no\\_print=True)\n\n\n\n\n\nu/agent.tool\\_plain    \n\ndef move\\_left():\n\n\"\"\"Move the '\\_' tile left by one block, swapping the tile with the number to the left. Returns the text describing the new game state after moving left.\"\"\"\n\nprint(f\"CALL TOOL: {inspect.currentframe().f\\_code.co\\_name}\")\n\ngame\\_state.move\\_left()\n\nreturn game\\_state.print\\_state(no\\_print=True)\n\n\n\n\n\nu/agent.tool\\_plain   \n\ndef move\\_right():\n\n\"\"\"Move the '\\_' tile right by one block, swapping the tile with the number to the right. Returns the text describing the new game state after moving right.\"\"\"\n\nprint(f\"CALL TOOL: {inspect.currentframe().f\\_code.co\\_name}\")\n\ngame\\_state.move\\_right()\n\nreturn game\\_state.print\\_state(no\\_print=True)\n\n\n\nu/agent.tool\\_plain\n\ndef print\\_state():\n\n\"\"\"Print the current game state.\"\"\"\n\nprint(f\"CALL TOOL: {inspect.currentframe().f\\_code.co\\_name}\")\n\nreturn game\\_state.print\\_state(no\\_print=True)\n\n\n\n\n\ndef main():\n\nmax\\_elem = np.max(goal)\n\nn\\_digit = get\\_n\\_digit(max\\_elem)\n\nsize = goal.shape\\[0\\]\n\ngoal\\_text = \"\"\n\n\n\n\\# tool\\_list = \\[move\\_up, move\\_down, move\\_left, move\\_right\\]\n\n\n\nfor row\\_idx in range(size):\n\nfor col\\_idx in range(size):\n\nif int(goal\\[row\\_idx, col\\_idx\\]) != 0:\n\ntext = '{num:0{width}} '.format(num=goal\\[row\\_idx, col\\_idx\\], width=n\\_digit)\n\nelse:                    \n\ntext = \"\\_\" \\* (n\\_digit) + \" \"\n\ngoal\\_text += text\n\ngoal\\_text += \"\\\\n\"\n\n\n\nstate\\_text = game\\_state.print\\_state()\n\n\n\ndice\\_result = agent.run\\_sync(f\"\"\"\n\nYou are an N-puzzle solver. \n\nYou need to find moves to go from the current state to the goal, such that all positions in current state are the same as the goal. At each turn, you can either move up, move down, move left, or move right. \n\nWhen you move the tile, the position of the tile will be swapped with the number at the place where you move to. \n\nIn the final answer, output the LIST OF MOVES, which should be either: move\\_left, move\\_right, move\\_up or move\\_down.\n\n\n\nCURRENT STATE:\n\n{state\\_text}\n\n\n\nGOAL STATE:\n\n{goal\\_text}\n\n\n\nEXAMPLE\\_OUTPUT (the \"FINAL ANSWER\" section):\n\nmove\\_left, move\\_right, move\\_up, move\\_down\n\n\n\n\"\"\", \n\ndeps='Anne')\n\npprint(dice\\_result.output)\n\npprint(dice\\_result.all\\_messages())\n\n\n\nif \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n\nmain()\n\n\n\nWhen I tried on 8-puzzle (N=3), then the agent worked well. An example is here:\n\n\n\n\\# 8-puzzle\n\nstart = np.array(\\[\n\n\\[0, 1, 3\\],\n\n\\[4, 2, 5\\],\n\n\\[7, 8, 6\\],\n\n\\])\n\n\n\ngoal = np.array(\\[\n\n\\[1, 2, 3\\],\n\n\\[4, 5, 6\\],\n\n\\[7, 8, 0\\],\n\n\\])\n\n\n\nI used Qwen3:latest from Ollama as the LLM, on my laptop with 8GB GPU. I tried other models such as Gemma3 but the performance wasn't good (I tried on a separate code which doesn't use Pydantic AI but instead uses LLM to answer in predetermined format and from that call the functions in that format, because I was trying to learn how AI agents work under the hood, thing is each model had different outputs so really hard to do that). The outputs showed that the agent did call tools:\n\n\n\n\\[https://pastebin.com/m0U2E66w\\](https://pastebin.com/m0U2E66w)  \n\nHowever, on 15-puzzle (N=3), the agent could not work at all, it completely failed to call any tool whatsoever.\n\n\n\n\\[https://pastebin.com/yqM6YZuq\\](https://pastebin.com/yqM6YZuq)\n\n\n\nDoes anyone know how to fix this ? I am still learning to would appreciate any resources, papers, tutorials, etc. which you guys point to. Thank you!",
          "author_fullname": "t2_t6pzf08p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Trying to use AI agent to play N-puzzle but the agent could only solve 8-puzzle but completely failed on 15-puzzle.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxtivp",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752301446,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone, I&amp;#39;m trying to write some simple demo which uses an AI agent to play N-puzzle. I envision that the AI would use: move\\_up, move\\_down, move\\_right, move\\_left to move the game state, and also a print\\_state tool to print the current state. Here is my code:&lt;/p&gt;\n\n&lt;p&gt;from pdb import set_trace&lt;/p&gt;\n\n&lt;p&gt;import os&lt;/p&gt;\n\n&lt;p&gt;import json&lt;/p&gt;\n\n&lt;p&gt;from copy import deepcopy&lt;/p&gt;\n\n&lt;p&gt;import requests&lt;/p&gt;\n\n&lt;p&gt;import math&lt;/p&gt;\n\n&lt;p&gt;import inspect&lt;/p&gt;\n\n&lt;p&gt;from inspect import signature&lt;/p&gt;\n\n&lt;p&gt;import numpy as np&lt;/p&gt;\n\n&lt;p&gt;from pprint import pprint&lt;/p&gt;\n\n&lt;p&gt;import hashlib&lt;/p&gt;\n\n&lt;p&gt;from collections import deque, defaultdict&lt;/p&gt;\n\n&lt;p&gt;import time&lt;/p&gt;\n\n&lt;p&gt;import random&lt;/p&gt;\n\n&lt;p&gt;import re&lt;/p&gt;\n\n&lt;p&gt;from typing import Annotated, Sequence, TypedDict&lt;/p&gt;\n\n&lt;p&gt;from pydantic import BaseModel, Field&lt;/p&gt;\n\n&lt;p&gt;from pydantic_ai import Agent, RunContext&lt;/p&gt;\n\n&lt;p&gt;from pydantic_ai.models.openai import OpenAIModel&lt;/p&gt;\n\n&lt;p&gt;from pydantic_ai.providers.openai import OpenAIProvider&lt;/p&gt;\n\n&lt;p&gt;ollama_model = OpenAIModel(&lt;/p&gt;\n\n&lt;p&gt;model_name=&amp;#39;qwen3:latest&amp;#39;, provider=OpenAIProvider(base_url=&amp;#39;http://localhost:11434/v1&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;agent = Agent(ollama_model,&lt;/p&gt;\n\n&lt;p&gt;# output_type=CityLocation&lt;/p&gt;\n\n&lt;p&gt;)&lt;/p&gt;\n\n&lt;p&gt;def get_n_digit(num):&lt;/p&gt;\n\n&lt;p&gt;if num &amp;gt; 0:&lt;/p&gt;\n\n&lt;p&gt;digits = int(math.log10(num))+1&lt;/p&gt;\n\n&lt;p&gt;elif num == 0:&lt;/p&gt;\n\n&lt;p&gt;digits = 1&lt;/p&gt;\n\n&lt;p&gt;else:&lt;/p&gt;\n\n&lt;p&gt;digits = int(math.log10(-num))+2 # +1 if you don&amp;#39;t count the &amp;#39;-&amp;#39; &lt;/p&gt;\n\n&lt;p&gt;return digits&lt;/p&gt;\n\n&lt;p&gt;class GameState:&lt;/p&gt;\n\n&lt;p&gt;def __init__(self, start, goal):&lt;/p&gt;\n\n&lt;p&gt;self.start = start&lt;/p&gt;\n\n&lt;p&gt;self.goal = goal&lt;/p&gt;\n\n&lt;p&gt;self.size = start.shape[0]&lt;/p&gt;\n\n&lt;p&gt;self.state = deepcopy(start)&lt;/p&gt;\n\n&lt;p&gt;def get_state(self):&lt;/p&gt;\n\n&lt;p&gt;return self.state&lt;/p&gt;\n\n&lt;p&gt;def finished(self):&lt;/p&gt;\n\n&lt;p&gt;is_finished = (self.state==self.goal).all()&lt;/p&gt;\n\n&lt;p&gt;if is_finished:&lt;/p&gt;\n\n&lt;p&gt;print(&amp;quot;FINISHED!&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;set_trace()&lt;/p&gt;\n\n&lt;p&gt;return is_finished       &lt;/p&gt;\n\n&lt;p&gt;def print_state(self, no_print=False):&lt;/p&gt;\n\n&lt;p&gt;max_elem = np.max(self.state)&lt;/p&gt;\n\n&lt;p&gt;n_digit = get_n_digit(max_elem)&lt;/p&gt;\n\n&lt;p&gt;state_text = &amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;for row_idx in range(self.size):&lt;/p&gt;\n\n&lt;p&gt;for col_idx in range(self.size):&lt;/p&gt;\n\n&lt;p&gt;if int(self.state[row_idx, col_idx]) != 0:&lt;/p&gt;\n\n&lt;p&gt;text = &amp;#39;{num:0{width}} &amp;#39;.format(num=self.state[row_idx, col_idx], width=n_digit)&lt;/p&gt;\n\n&lt;p&gt;else:                    &lt;/p&gt;\n\n&lt;p&gt;text = &amp;quot;_&amp;quot; * (n_digit) + &amp;quot; &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;state_text += text&lt;/p&gt;\n\n&lt;p&gt;state_text += &amp;quot;\\n&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;if no_print is False:&lt;/p&gt;\n\n&lt;p&gt;print(state_text)&lt;/p&gt;\n\n&lt;p&gt;return state_text&lt;/p&gt;\n\n&lt;p&gt;def create_diff_view(self):&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Show which tiles are out of place&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;diff_state = &amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;for i in range(self.size):&lt;/p&gt;\n\n&lt;p&gt;for j in range(self.size):&lt;/p&gt;\n\n&lt;p&gt;current = self.state[i, j]&lt;/p&gt;\n\n&lt;p&gt;target = self.goal[i, j]&lt;/p&gt;\n\n&lt;p&gt;if current == target:&lt;/p&gt;\n\n&lt;p&gt;diff_state += f&amp;quot;✓{current} &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;else:&lt;/p&gt;\n\n&lt;p&gt;diff_state += f&amp;quot;✗{current} &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;diff_state += &amp;quot;\\n&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;return diff_state&lt;/p&gt;\n\n&lt;p&gt;def move_up(self):&lt;/p&gt;\n\n&lt;p&gt;itemindex = np.where(self.state == 0)&lt;/p&gt;\n\n&lt;p&gt;pos_row = int(itemindex[0][0])&lt;/p&gt;\n\n&lt;p&gt;pos_col = int(itemindex[1][0])&lt;/p&gt;\n\n&lt;p&gt;if (pos_row == 0):&lt;/p&gt;\n\n&lt;p&gt;return&lt;/p&gt;\n\n&lt;p&gt;temp = self.state[pos_row, pos_col]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row, pos_col] = self.state[pos_row-1, pos_col]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row-1, pos_col] = temp&lt;/p&gt;\n\n&lt;p&gt;def move_down(self):&lt;/p&gt;\n\n&lt;p&gt;itemindex = np.where(self.state == 0)&lt;/p&gt;\n\n&lt;p&gt;pos_row = int(itemindex[0][0])&lt;/p&gt;\n\n&lt;p&gt;pos_col = int(itemindex[1][0])&lt;/p&gt;\n\n&lt;p&gt;if (pos_row == (self.size-1)):&lt;/p&gt;\n\n&lt;p&gt;return&lt;/p&gt;\n\n&lt;p&gt;temp = self.state[pos_row, pos_col]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row, pos_col] = self.state[pos_row+1, pos_col]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row+1, pos_col] = temp&lt;/p&gt;\n\n&lt;p&gt;def move_left(self):&lt;/p&gt;\n\n&lt;p&gt;itemindex = np.where(self.state == 0)&lt;/p&gt;\n\n&lt;p&gt;pos_row = int(itemindex[0][0])&lt;/p&gt;\n\n&lt;p&gt;pos_col = int(itemindex[1][0])&lt;/p&gt;\n\n&lt;p&gt;if (pos_col == 0):&lt;/p&gt;\n\n&lt;p&gt;return&lt;/p&gt;\n\n&lt;p&gt;temp = self.state[pos_row, pos_col]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row, pos_col] = self.state[pos_row, pos_col-1]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row, pos_col-1] = temp&lt;/p&gt;\n\n&lt;p&gt;def move_right(self):&lt;/p&gt;\n\n&lt;p&gt;itemindex = np.where(self.state == 0)&lt;/p&gt;\n\n&lt;p&gt;pos_row = int(itemindex[0][0])&lt;/p&gt;\n\n&lt;p&gt;pos_col = int(itemindex[1][0])&lt;/p&gt;\n\n&lt;p&gt;if (pos_col == (self.size-1)):&lt;/p&gt;\n\n&lt;p&gt;return&lt;/p&gt;\n\n&lt;p&gt;temp = self.state[pos_row, pos_col]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row, pos_col] = self.state[pos_row, pos_col+1]&lt;/p&gt;\n\n&lt;p&gt;self.state[pos_row, pos_col+1] = temp&lt;/p&gt;\n\n&lt;p&gt;# 8-puzzle&lt;/p&gt;\n\n&lt;p&gt;# start = np.array([&lt;/p&gt;\n\n&lt;p&gt;# [0, 1, 3],&lt;/p&gt;\n\n&lt;p&gt;# [4, 2, 5],&lt;/p&gt;\n\n&lt;p&gt;# [7, 8, 6],&lt;/p&gt;\n\n&lt;p&gt;# ])&lt;/p&gt;\n\n&lt;p&gt;# goal = np.array([&lt;/p&gt;\n\n&lt;p&gt;# [1, 2, 3],&lt;/p&gt;\n\n&lt;p&gt;# [4, 5, 6],&lt;/p&gt;\n\n&lt;p&gt;# [7, 8, 0],&lt;/p&gt;\n\n&lt;p&gt;# ])&lt;/p&gt;\n\n&lt;p&gt;# 15-puzzle&lt;/p&gt;\n\n&lt;p&gt;start = np.array([&lt;/p&gt;\n\n&lt;p&gt;[ 6, 13,  7, 10],&lt;/p&gt;\n\n&lt;p&gt;[ 8,  9, 11,  0],&lt;/p&gt;\n\n&lt;p&gt;[15,  2, 12,  5],&lt;/p&gt;\n\n&lt;p&gt;[14,  3,  1,  4],&lt;/p&gt;\n\n&lt;p&gt;])&lt;/p&gt;\n\n&lt;p&gt;goal = np.array([&lt;/p&gt;\n\n&lt;p&gt;[ 1,  2,  3,  4],&lt;/p&gt;\n\n&lt;p&gt;[ 5,  6,  7,  8],&lt;/p&gt;\n\n&lt;p&gt;[ 9, 10, 11, 12],&lt;/p&gt;\n\n&lt;p&gt;[13, 14, 15,  0],&lt;/p&gt;\n\n&lt;p&gt;])&lt;/p&gt;\n\n&lt;p&gt;game_state = GameState(start, goal)&lt;/p&gt;\n\n&lt;p&gt;# &lt;a href=\"/u/agent\"&gt;u/agent&lt;/a&gt;.tool_plain&lt;/p&gt;\n\n&lt;p&gt;# def check_finished() -&amp;gt; bool:&lt;/p&gt;\n\n&lt;p&gt;# &amp;quot;&amp;quot;&amp;quot;Check whether or not the game state has reached the goal. Returns a boolean value&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;# print(f&amp;quot;CALL TOOL: {inspect.currentframe().f_code.co_name}&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;# return game_state.finished()&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/agent\"&gt;u/agent&lt;/a&gt;.tool_plain&lt;/p&gt;\n\n&lt;p&gt;def move_up():&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Move the &amp;#39;_&amp;#39; tile up by one block, swapping the tile with the number above. Returns the text describing the new game state after moving up.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;print(f&amp;quot;CALL TOOL: {inspect.currentframe().f_code.co_name}&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;game_state.move_up()&lt;/p&gt;\n\n&lt;p&gt;return game_state.print_state(no_print=True)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/agent\"&gt;u/agent&lt;/a&gt;.tool_plain    &lt;/p&gt;\n\n&lt;p&gt;def move_down():&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Move the &amp;#39;_&amp;#39; tile down by one block, swapping the tile with the number below. Returns the text describing the new game state after moving down.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;print(f&amp;quot;CALL TOOL: {inspect.currentframe().f_code.co_name}&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;game_state.move_down()&lt;/p&gt;\n\n&lt;p&gt;return game_state.print_state(no_print=True)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/agent\"&gt;u/agent&lt;/a&gt;.tool_plain    &lt;/p&gt;\n\n&lt;p&gt;def move_left():&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Move the &amp;#39;_&amp;#39; tile left by one block, swapping the tile with the number to the left. Returns the text describing the new game state after moving left.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;print(f&amp;quot;CALL TOOL: {inspect.currentframe().f_code.co_name}&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;game_state.move_left()&lt;/p&gt;\n\n&lt;p&gt;return game_state.print_state(no_print=True)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/agent\"&gt;u/agent&lt;/a&gt;.tool_plain   &lt;/p&gt;\n\n&lt;p&gt;def move_right():&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Move the &amp;#39;_&amp;#39; tile right by one block, swapping the tile with the number to the right. Returns the text describing the new game state after moving right.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;print(f&amp;quot;CALL TOOL: {inspect.currentframe().f_code.co_name}&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;game_state.move_right()&lt;/p&gt;\n\n&lt;p&gt;return game_state.print_state(no_print=True)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/agent\"&gt;u/agent&lt;/a&gt;.tool_plain&lt;/p&gt;\n\n&lt;p&gt;def print_state():&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;Print the current game state.&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;print(f&amp;quot;CALL TOOL: {inspect.currentframe().f_code.co_name}&amp;quot;)&lt;/p&gt;\n\n&lt;p&gt;return game_state.print_state(no_print=True)&lt;/p&gt;\n\n&lt;p&gt;def main():&lt;/p&gt;\n\n&lt;p&gt;max_elem = np.max(goal)&lt;/p&gt;\n\n&lt;p&gt;n_digit = get_n_digit(max_elem)&lt;/p&gt;\n\n&lt;p&gt;size = goal.shape[0]&lt;/p&gt;\n\n&lt;p&gt;goal_text = &amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;# tool_list = [move_up, move_down, move_left, move_right]&lt;/p&gt;\n\n&lt;p&gt;for row_idx in range(size):&lt;/p&gt;\n\n&lt;p&gt;for col_idx in range(size):&lt;/p&gt;\n\n&lt;p&gt;if int(goal[row_idx, col_idx]) != 0:&lt;/p&gt;\n\n&lt;p&gt;text = &amp;#39;{num:0{width}} &amp;#39;.format(num=goal[row_idx, col_idx], width=n_digit)&lt;/p&gt;\n\n&lt;p&gt;else:                    &lt;/p&gt;\n\n&lt;p&gt;text = &amp;quot;_&amp;quot; * (n_digit) + &amp;quot; &amp;quot;&lt;/p&gt;\n\n&lt;p&gt;goal_text += text&lt;/p&gt;\n\n&lt;p&gt;goal_text += &amp;quot;\\n&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;state_text = game_state.print_state()&lt;/p&gt;\n\n&lt;p&gt;dice_result = agent.run_sync(f&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;You are an N-puzzle solver. &lt;/p&gt;\n\n&lt;p&gt;You need to find moves to go from the current state to the goal, such that all positions in current state are the same as the goal. At each turn, you can either move up, move down, move left, or move right. &lt;/p&gt;\n\n&lt;p&gt;When you move the tile, the position of the tile will be swapped with the number at the place where you move to. &lt;/p&gt;\n\n&lt;p&gt;In the final answer, output the LIST OF MOVES, which should be either: move_left, move_right, move_up or move_down.&lt;/p&gt;\n\n&lt;p&gt;CURRENT STATE:&lt;/p&gt;\n\n&lt;p&gt;{state_text}&lt;/p&gt;\n\n&lt;p&gt;GOAL STATE:&lt;/p&gt;\n\n&lt;p&gt;{goal_text}&lt;/p&gt;\n\n&lt;p&gt;EXAMPLE_OUTPUT (the &amp;quot;FINAL ANSWER&amp;quot; section):&lt;/p&gt;\n\n&lt;p&gt;move_left, move_right, move_up, move_down&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;&amp;quot;&amp;quot;, &lt;/p&gt;\n\n&lt;p&gt;deps=&amp;#39;Anne&amp;#39;)&lt;/p&gt;\n\n&lt;p&gt;pprint(dice_result.output)&lt;/p&gt;\n\n&lt;p&gt;pprint(dice_result.all_messages())&lt;/p&gt;\n\n&lt;p&gt;if __name__ == &amp;quot;__main__&amp;quot;:&lt;/p&gt;\n\n&lt;p&gt;main()&lt;/p&gt;\n\n&lt;p&gt;When I tried on 8-puzzle (N=3), then the agent worked well. An example is here:&lt;/p&gt;\n\n&lt;p&gt;# 8-puzzle&lt;/p&gt;\n\n&lt;p&gt;start = np.array([&lt;/p&gt;\n\n&lt;p&gt;[0, 1, 3],&lt;/p&gt;\n\n&lt;p&gt;[4, 2, 5],&lt;/p&gt;\n\n&lt;p&gt;[7, 8, 6],&lt;/p&gt;\n\n&lt;p&gt;])&lt;/p&gt;\n\n&lt;p&gt;goal = np.array([&lt;/p&gt;\n\n&lt;p&gt;[1, 2, 3],&lt;/p&gt;\n\n&lt;p&gt;[4, 5, 6],&lt;/p&gt;\n\n&lt;p&gt;[7, 8, 0],&lt;/p&gt;\n\n&lt;p&gt;])&lt;/p&gt;\n\n&lt;p&gt;I used Qwen3:latest from Ollama as the LLM, on my laptop with 8GB GPU. I tried other models such as Gemma3 but the performance wasn&amp;#39;t good (I tried on a separate code which doesn&amp;#39;t use Pydantic AI but instead uses LLM to answer in predetermined format and from that call the functions in that format, because I was trying to learn how AI agents work under the hood, thing is each model had different outputs so really hard to do that). The outputs showed that the agent did call tools:&lt;/p&gt;\n\n&lt;p&gt;[&lt;a href=\"https://pastebin.com/m0U2E66w%5C%5D(https://pastebin.com/m0U2E66w)\"&gt;https://pastebin.com/m0U2E66w\\](https://pastebin.com/m0U2E66w)&lt;/a&gt;  &lt;/p&gt;\n\n&lt;p&gt;However, on 15-puzzle (N=3), the agent could not work at all, it completely failed to call any tool whatsoever.&lt;/p&gt;\n\n&lt;p&gt;[&lt;a href=\"https://pastebin.com/yqM6YZuq%5C%5D(https://pastebin.com/yqM6YZuq)\"&gt;https://pastebin.com/yqM6YZuq\\](https://pastebin.com/yqM6YZuq)&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Does anyone know how to fix this ? I am still learning to would appreciate any resources, papers, tutorials, etc. which you guys point to. Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxtivp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "CommunityOpposite645",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxtivp/trying_to_use_ai_agent_to_play_npuzzle_but_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxtivp/trying_to_use_ai_agent_to_play_npuzzle_but_the/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752301446,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Read this post for [context](https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/). Here are some updates:\n\n1. We've added a [changelog](https://www.designarena.ai/changelog) of when each model was added or deactivated from the arena. System prompts can be found in [methodology](https://www.designarena.ai/about) or [this page](https://www.designarena.ai/system-prompts). The system prompts were meant to be very simple, but feel free to provide your critiques on them (we acknowledge they're not the best).   \n  \n2. Devstral Medium, Devstral Small 1.1, Qwen3 30B-A3B, Mistral Small 3.2, and kimi-k2 were added to the area. Note that the temperature of kimi-k2 is set to be low right now since we're using the public api (0.3 instead of 0.8 for the other models) but we will modify that when we switch to better hosting.   \n  \n3. Working on adding more models suggested [in this thread](https://www.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/) such as GLM-4, Gemma, more moonshot models, and more open source / smaller models. It's actually been quite interesting to see that many of the [OS models / smaller ones are holding their weight](https://www.designarena.ai/leaderboard) against the giants. \n\n4. Grok 4 might be crushing every benchmark left and right, but for coding (specifically frontend dev and UI/UX), people haven't found the model to be all that impressive. xAI didn't appear to intend for Grok 4 to be a 100X developer, but we'll see how it's coding model will fare in August (or maybe September). \n\nThose are the major updates. One food for thought is how will Open AI's open source model do on here, given that none of its flagships are even in the top 10.   \n  \nAs always let us know what we can do better and what else you'd like to see!  ",
          "author_fullname": "t2_c3b3edv5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "7/11 Update on Design Arena: Added Devstral, Qwen, and kimi-k2, Grok 4 struggling but coding model coming out later?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxth6s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "ups": 45,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 45,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/oU1rLTsu37J9jGtw8_snnfgVJ4E523tKR5kPdv065cQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1752301281,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Read this post for &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lu7lsi/uiux_benchmark_update_and_response_more_models/\"&gt;context&lt;/a&gt;. Here are some updates:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;We&amp;#39;ve added a &lt;a href=\"https://www.designarena.ai/changelog\"&gt;changelog&lt;/a&gt; of when each model was added or deactivated from the arena. System prompts can be found in &lt;a href=\"https://www.designarena.ai/about\"&gt;methodology&lt;/a&gt; or &lt;a href=\"https://www.designarena.ai/system-prompts\"&gt;this page&lt;/a&gt;. The system prompts were meant to be very simple, but feel free to provide your critiques on them (we acknowledge they&amp;#39;re not the best).   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Devstral Medium, Devstral Small 1.1, Qwen3 30B-A3B, Mistral Small 3.2, and kimi-k2 were added to the area. Note that the temperature of kimi-k2 is set to be low right now since we&amp;#39;re using the public api (0.3 instead of 0.8 for the other models) but we will modify that when we switch to better hosting.   &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Working on adding more models suggested &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lwxr2l/what_other_models_would_you_like_to_see_on_design/\"&gt;in this thread&lt;/a&gt; such as GLM-4, Gemma, more moonshot models, and more open source / smaller models. It&amp;#39;s actually been quite interesting to see that many of the &lt;a href=\"https://www.designarena.ai/leaderboard\"&gt;OS models / smaller ones are holding their weight&lt;/a&gt; against the giants. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Grok 4 might be crushing every benchmark left and right, but for coding (specifically frontend dev and UI/UX), people haven&amp;#39;t found the model to be all that impressive. xAI didn&amp;#39;t appear to intend for Grok 4 to be a 100X developer, but we&amp;#39;ll see how it&amp;#39;s coding model will fare in August (or maybe September). &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Those are the major updates. One food for thought is how will Open AI&amp;#39;s open source model do on here, given that none of its flagships are even in the top 10.   &lt;/p&gt;\n\n&lt;p&gt;As always let us know what we can do better and what else you&amp;#39;d like to see!  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/y1r7gm6xydcf1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/y1r7gm6xydcf1.png?auto=webp&amp;s=1c52aa327f434c6160b3711835eae997289394a4",
                  "width": 2018,
                  "height": 1472
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6337a58a55ba1b51ed8a34aff7d5ea0f2f0b508e",
                    "width": 108,
                    "height": 78
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=152626aa8037f64587029ff294434331c386a808",
                    "width": 216,
                    "height": 157
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=949898834f12fb685e196906350da59569edae91",
                    "width": 320,
                    "height": 233
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b09a16e07e1536b269458ffdcbaf1811010956c8",
                    "width": 640,
                    "height": 466
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7430dcc89d24f5db72621a15f3f73de354100cf7",
                    "width": 960,
                    "height": 700
                  },
                  {
                    "url": "https://preview.redd.it/y1r7gm6xydcf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=67913b2a0fd5d059345ed23c5dbdbadb59201e90",
                    "width": 1080,
                    "height": 787
                  }
                ],
                "variants": {},
                "id": "vIql9W4rnq3HE0yIHD9GcmKasPlbv-MPOSjabuSuy_c"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1lxth6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "adviceguru25",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxth6s/711_update_on_design_arena_added_devstral_qwen/",
          "stickied": false,
          "url": "https://i.redd.it/y1r7gm6xydcf1.png",
          "subreddit_subscribers": 498343,
          "created_utc": 1752301281,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone!\n\nI didn’t specifically buy my MacBook Pro (M3 Max, 36GB unified memory) to run LLMs, but now that I’m working in tech, I’m curious what kinds of models I can realistically run locally.\n\nI know 36GB might be a bit limiting for some larger models, but I’d love to hear your experience or suggestions on what LLMs this setup can handle — both for casual play and practical use.\n\nAny recommendations for models or tools (Ollama, LM Studio, etc.) are also appreciated!",
          "author_fullname": "t2_aunc6vo9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MBP M3 Max 36 GB Memory - what can I run?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1lxseu8",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1752297457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;I didn’t specifically buy my MacBook Pro (M3 Max, 36GB unified memory) to run LLMs, but now that I’m working in tech, I’m curious what kinds of models I can realistically run locally.&lt;/p&gt;\n\n&lt;p&gt;I know 36GB might be a bit limiting for some larger models, but I’d love to hear your experience or suggestions on what LLMs this setup can handle — both for casual play and practical use.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations for models or tools (Ollama, LM Studio, etc.) are also appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1lxseu8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DeepTarget8436",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1lxseu8/mbp_m3_max_36_gb_memory_what_can_i_run/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxseu8/mbp_m3_max_36_gb_memory_what_can_i_run/",
          "subreddit_subscribers": 498343,
          "created_utc": 1752297457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}