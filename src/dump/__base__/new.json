{
  "kind": "Listing",
  "data": {
    "after": "t3_1m52h8x",
    "dist": 100,
    "modhash": "",
    "geo_filter": "",
    "children": [
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Unfortunately it's on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It'll be an interesting piece of localllama HW.",
          "author_fullname": "t2_bjeo1gwy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Used A100 40GB just dropped below $2000, for those who care with caveat",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m60ahf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.83,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753145404,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately it&amp;#39;s on SXM4, you will need a $600 adapter for this. but I am sure someone with enough motivation will figure out a way to drop it into a PCIe adapter to sell it as a complete package. It&amp;#39;ll be an interesting piece of localllama HW.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m60ahf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "--dany--",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m60ahf/used_a100_40gb_just_dropped_below_2000_for_those/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753145404,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The first thing we noticed is that the size estimates in Qwen's answer are incorrect: a 4-bit GPTQ will obviously not be 59GB in size. For reference, the 4-bit w4a16 quant of 235B we are currently testing consumes 88GB VRAM per GPU. \n\nThus we are suspicious of the rest of Qwen's answer, but lack the domain-specific expertise to validate or even assess the quality of the rest of the information. \n\nWe defer to you, dear LocalLLaMa community, for any analysis you can offer in this regard.\n\nOur prompt: \n\n&gt; Qwen3 235B A22 2507 (which is you, by the way) is available in BF16. I‚Äôd like to quantize you to 4-bits in order to fit into the 192GB VRAM I have available while maintaining your integrity the best I can. Please help me design a quantization process for you. Your server has two NVidia Blackwell PRO 6000 Workstation 96GB GPUs on an AMD EPYC 9745 CPU-based system with 768GB RAM. The server runs Linux. We will be using vLLM for inference. Please come up with the best possible quantization recipe for you that matches this spec. Format your answer so that I can copy/paste it into a Reddit post and maintain beautiful formatting.\n\nOne last point we enjoyed from the last line of the answer:\n\n&gt; P.S. I'm Qwen3. Yes, this is me helping you quantize myself. Meta, huh?\n\nEdit #1: It looks like Qwen renamed our Blackwell PRO 6000s to \"B1000 PRO\", which is quite odd.\n\n---\n\n# üöÄ How to Quantize Qwen3-235B to 4-bit GPTQ for vLLM (Dual B1000 PRO Setup)\n\nWant to run **Qwen3-235B** on **two NVIDIA Blackwell B1000 PRO GPUs (96GB each)**?  \nHere‚Äôs the **ultimate 4-bit GPTQ recipe** to fit it in **192GB VRAM**, preserve accuracy, and deploy via **vLLM**.\n\nYou‚Äôll get near-BF16 quality at **1/8th the memory cost** ‚Äî perfect for local inference.\n\n---\n\n## üîß Hardware &amp; Goal\n\n| Component | Spec |\n|--------|------|\n| GPUs | 2√ó **NVIDIA Blackwell B1000 PRO** (96GB) ‚Üí **192GB total VRAM** |\n| CPU | AMD EPYC 9745 (96 cores) |\n| RAM | 768GB DDR5 |\n| Storage | Fast NVMe (&gt;1TB free) |\n\nüéØ **Goal**: Run **Qwen3-235B-A2.2** in **4-bit GPTQ** via **vLLM**  \n‚úÖ Fits in VRAM  \n‚úÖ High throughput  \n‚úÖ Minimal accuracy loss\n\n&gt; ‚ùóÔ∏èBF16 model = ~470GB ‚Üí too big  \n&gt; ‚úÖ 4-bit GPTQ = ~**59GB** ‚Üí fits perfectly!\n\n---\n\n## ‚öôÔ∏è Step 1: Environment Setup\n\n```bash\n# Create conda env\nconda create -n qwen-gptq python=3.10\nconda activate qwen-gptq\n\n# Install PyTorch + CUDA 12.4\npip install torch==2.4.0+cu124 torchvision --extra-index-url https://download.pytorch.org/whl/cu124\n\n# Install Hugging Face &amp; AutoGPTQ\npip install transformers accelerate datasets\npip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/\n\n# Later: Install vLLM for inference\npip install vllm\n```\n\n&gt; üí° Use `--extra-index-url` for latest AutoGPTQ builds.\n\n---\n\n## üìö Step 2: Calibration Dataset\n\nUse **c4 (en)** for calibration ‚Äî matches pretraining data.\n\n```python\nfrom datasets import load_dataset\n\ncalib_dataset = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\ncalib_dataset = calib_dataset.take(1024)  # First 1024 samples\n```\n\n&gt; ‚úÖ Why c4? High-quality web text, diverse, standard for LLMs.\n\nOptional: Mix in Wikipedia for broader coverage.\n\n---\n\n## üî§ Step 3: Quantize to 4-bit GPTQ (NF4)\n\nRun this script to quantize offline using **AutoGPTQ**:\n\n```python\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport torch\nfrom transformers import AutoTokenizer\n\n# Load model &amp; tokenizer\nmodel_name = \"Qwen/Qwen3-235B-A2.2\"  # or local path\nquant_path = \"./qwen3-235b-4bit-gptq\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Quantization config\nquantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.01,\n    sym=True,\n    true_sequential=True,\n    model_name_or_path=model_name,\n    quantize_method=\"gptq\",\n    use_cuda_fp16=False\n)\n\n# Load model in FP16 (needs ~470GB RAM)\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_name,\n    quantize_config=quantize_config,\n    trust_remote_code=True\n)\n\n# Calibrate &amp; quantize\nmodel.quantize(calib_dataset, tokenizer)\n\n# Save\nmodel.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)\n\nprint(f\"‚úÖ Quantized model saved to {quant_path}\")\n```\n\n&gt; ‚è≥ Runtime: 4‚Äì8 hours (depends on disk/CPU)  \n&gt; üì¶ Output: ~59GB 4-bit model\n\n---\n\n## üîå Step 4: Deploy with vLLM\n\nvLLM supports GPTQ natively. Just launch:\n\n```bash\npython -m vllm.entrypoints.api_server \\\n  --model ./qwen3-235b-4bit-gptq \\\n  --tensor-parallel-size 2 \\\n  --dtype half \\\n  --quantization gptq \\\n  --gpu-memory-utilization 0.95 \\\n  --max-model-len 32768 \\\n  --port 8000 \\\n  --host 0.0.0.0\n```\n\n### Key Args:\n- `--tensor-parallel-size 2`: Splits model across both GPUs\n- `--quantization gptq`: Enables 4-bit kernels\n- `--gpu-memory-utilization 0.95`: Maximizes VRAM use\n- `--max-model-len 32768`: Full context support\n\n---\n\n## üìà Performance Expectations\n\n| Metric | Estimate |\n|-------|----------|\n| Model Size | ~59 GB |\n| VRAM Usage | ~90GB per GPU |\n| Load Time | 30‚Äì60 sec |\n| Throughput | 150‚Äì250 tokens/sec (batch 1‚Äì4) |\n| Accuracy | ~97% of BF16 on MMLU, GSM8K |\n\n&gt; üéØ Perfect for long-context reasoning, coding, RAG.\n\n---\n\n## ‚úÖ Why This Recipe Wins\n\n| Choice | Why It‚Äôs Best |\n|-------|---------------|\n| **GPTQ over AWQ** | Better vLLM support, faster kernels |\n| **NF4 + sym=True** | Higher precision than INT4 |\n| **group_size=128** | Optimal speed/accuracy balance |\n| **desc_act=False** | Faster inference, minor quality drop |\n| **c4 calibration** | Matches pretraining distribution |\n\n&gt; üîÑ Alternative: Try `desc_act=True` if you want +1% accuracy (slower).\n\n---\n\n## üí° Pro Tips\n\n- **Monitor VRAM**: `nvidia-smi -l 1`\n- **Use fast storage**: NVMe required for smooth loading\n- **Warm up**: Send test prompts after startup\n- **Enable batching**: Boosts throughput\n- **Update vLLM monthly**: GPTQ perf keeps improving\n\n---\n\n## üß© Final Notes\n\nWhile **AWQ** exists, **vLLM‚Äôs GPTQ support is more mature** for 200B+ models. Stick with GPTQ for now.\n\nAlso: No public evidence that AWQ beats GPTQ on Qwen3 at 4-bit.\n\n---\n\n## üèÅ Summary: Your 4-bit Dream Setup\n\n```yaml\nModel:       Qwen3-235B-A2.2\nPrecision:   4-bit GPTQ (NF4)\nCalibration: c4 [1024 samples]\nGroup Size:  128\nSymmetric:   Yes\nInference:   vLLM + tensor_parallel=2\nResult:      59GB model, blazing fast, fits like magic\n```\n\nWith this setup, you‚Äôre running one of the most powerful open models in the world ‚Äî **locally, efficiently, and privately**.\n\nLet me know if you want the Colab/Slurm scripts! üí¨üî•\n\n‚Äî  \n*P.S. I'm Qwen3. Yes, this is me helping you quantize myself. Meta, huh?* üòâ",
          "author_fullname": "t2_1t7r9dkpud",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "We asked Qwen3-235B-A22-Instruct-2507 for advice on how best to quantize itself to 4-bits for vLLM. Anyone who understands these things care to comment on its recommendations?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m5ynit",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.15,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753140885,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The first thing we noticed is that the size estimates in Qwen&amp;#39;s answer are incorrect: a 4-bit GPTQ will obviously not be 59GB in size. For reference, the 4-bit w4a16 quant of 235B we are currently testing consumes 88GB VRAM per GPU. &lt;/p&gt;\n\n&lt;p&gt;Thus we are suspicious of the rest of Qwen&amp;#39;s answer, but lack the domain-specific expertise to validate or even assess the quality of the rest of the information. &lt;/p&gt;\n\n&lt;p&gt;We defer to you, dear LocalLLaMa community, for any analysis you can offer in this regard.&lt;/p&gt;\n\n&lt;p&gt;Our prompt: &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Qwen3 235B A22 2507 (which is you, by the way) is available in BF16. I‚Äôd like to quantize you to 4-bits in order to fit into the 192GB VRAM I have available while maintaining your integrity the best I can. Please help me design a quantization process for you. Your server has two NVidia Blackwell PRO 6000 Workstation 96GB GPUs on an AMD EPYC 9745 CPU-based system with 768GB RAM. The server runs Linux. We will be using vLLM for inference. Please come up with the best possible quantization recipe for you that matches this spec. Format your answer so that I can copy/paste it into a Reddit post and maintain beautiful formatting.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;One last point we enjoyed from the last line of the answer:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;P.S. I&amp;#39;m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Edit #1: It looks like Qwen renamed our Blackwell PRO 6000s to &amp;quot;B1000 PRO&amp;quot;, which is quite odd.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h1&gt;üöÄ How to Quantize Qwen3-235B to 4-bit GPTQ for vLLM (Dual B1000 PRO Setup)&lt;/h1&gt;\n\n&lt;p&gt;Want to run &lt;strong&gt;Qwen3-235B&lt;/strong&gt; on &lt;strong&gt;two NVIDIA Blackwell B1000 PRO GPUs (96GB each)&lt;/strong&gt;?&lt;br/&gt;\nHere‚Äôs the &lt;strong&gt;ultimate 4-bit GPTQ recipe&lt;/strong&gt; to fit it in &lt;strong&gt;192GB VRAM&lt;/strong&gt;, preserve accuracy, and deploy via &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;You‚Äôll get near-BF16 quality at &lt;strong&gt;1/8th the memory cost&lt;/strong&gt; ‚Äî perfect for local inference.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üîß Hardware &amp;amp; Goal&lt;/h2&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Component&lt;/th&gt;\n&lt;th&gt;Spec&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;GPUs&lt;/td&gt;\n&lt;td&gt;2√ó &lt;strong&gt;NVIDIA Blackwell B1000 PRO&lt;/strong&gt; (96GB) ‚Üí &lt;strong&gt;192GB total VRAM&lt;/strong&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td&gt;AMD EPYC 9745 (96 cores)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;RAM&lt;/td&gt;\n&lt;td&gt;768GB DDR5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Storage&lt;/td&gt;\n&lt;td&gt;Fast NVMe (&amp;gt;1TB free)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;üéØ &lt;strong&gt;Goal&lt;/strong&gt;: Run &lt;strong&gt;Qwen3-235B-A2.2&lt;/strong&gt; in &lt;strong&gt;4-bit GPTQ&lt;/strong&gt; via &lt;strong&gt;vLLM&lt;/strong&gt;&lt;br/&gt;\n‚úÖ Fits in VRAM&lt;br/&gt;\n‚úÖ High throughput&lt;br/&gt;\n‚úÖ Minimal accuracy loss&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;‚ùóÔ∏èBF16 model = ~470GB ‚Üí too big&lt;br/&gt;\n‚úÖ 4-bit GPTQ = ~&lt;strong&gt;59GB&lt;/strong&gt; ‚Üí fits perfectly!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;‚öôÔ∏è Step 1: Environment Setup&lt;/h2&gt;\n\n&lt;p&gt;```bash&lt;/p&gt;\n\n&lt;h1&gt;Create conda env&lt;/h1&gt;\n\n&lt;p&gt;conda create -n qwen-gptq python=3.10\nconda activate qwen-gptq&lt;/p&gt;\n\n&lt;h1&gt;Install PyTorch + CUDA 12.4&lt;/h1&gt;\n\n&lt;p&gt;pip install torch==2.4.0+cu124 torchvision --extra-index-url &lt;a href=\"https://download.pytorch.org/whl/cu124\"&gt;https://download.pytorch.org/whl/cu124&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Install Hugging Face &amp;amp; AutoGPTQ&lt;/h1&gt;\n\n&lt;p&gt;pip install transformers accelerate datasets\npip install auto-gptq --extra-index-url &lt;a href=\"https://huggingface.github.io/autogptq-index/whl/\"&gt;https://huggingface.github.io/autogptq-index/whl/&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Later: Install vLLM for inference&lt;/h1&gt;\n\n&lt;p&gt;pip install vllm\n```&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;üí° Use &lt;code&gt;--extra-index-url&lt;/code&gt; for latest AutoGPTQ builds.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üìö Step 2: Calibration Dataset&lt;/h2&gt;\n\n&lt;p&gt;Use &lt;strong&gt;c4 (en)&lt;/strong&gt; for calibration ‚Äî matches pretraining data.&lt;/p&gt;\n\n&lt;p&gt;```python\nfrom datasets import load_dataset&lt;/p&gt;\n\n&lt;p&gt;calib_dataset = load_dataset(&amp;quot;allenai/c4&amp;quot;, &amp;quot;en&amp;quot;, split=&amp;quot;train&amp;quot;, streaming=True)\ncalib_dataset = calib_dataset.take(1024)  # First 1024 samples\n```&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;‚úÖ Why c4? High-quality web text, diverse, standard for LLMs.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Optional: Mix in Wikipedia for broader coverage.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üî§ Step 3: Quantize to 4-bit GPTQ (NF4)&lt;/h2&gt;\n\n&lt;p&gt;Run this script to quantize offline using &lt;strong&gt;AutoGPTQ&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;```python\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport torch\nfrom transformers import AutoTokenizer&lt;/p&gt;\n\n&lt;h1&gt;Load model &amp;amp; tokenizer&lt;/h1&gt;\n\n&lt;p&gt;model_name = &amp;quot;Qwen/Qwen3-235B-A2.2&amp;quot;  # or local path\nquant_path = &amp;quot;./qwen3-235b-4bit-gptq&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&lt;/p&gt;\n\n&lt;h1&gt;Quantization config&lt;/h1&gt;\n\n&lt;p&gt;quantize_config = BaseQuantizeConfig(\n    bits=4,\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.01,\n    sym=True,\n    true_sequential=True,\n    model_name_or_path=model_name,\n    quantize_method=&amp;quot;gptq&amp;quot;,\n    use_cuda_fp16=False\n)&lt;/p&gt;\n\n&lt;h1&gt;Load model in FP16 (needs ~470GB RAM)&lt;/h1&gt;\n\n&lt;p&gt;model = AutoGPTQForCausalLM.from_pretrained(\n    model_name,\n    quantize_config=quantize_config,\n    trust_remote_code=True\n)&lt;/p&gt;\n\n&lt;h1&gt;Calibrate &amp;amp; quantize&lt;/h1&gt;\n\n&lt;p&gt;model.quantize(calib_dataset, tokenizer)&lt;/p&gt;\n\n&lt;h1&gt;Save&lt;/h1&gt;\n\n&lt;p&gt;model.save_quantized(quant_path)\ntokenizer.save_pretrained(quant_path)&lt;/p&gt;\n\n&lt;p&gt;print(f&amp;quot;‚úÖ Quantized model saved to {quant_path}&amp;quot;)\n```&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;‚è≥ Runtime: 4‚Äì8 hours (depends on disk/CPU)&lt;br/&gt;\nüì¶ Output: ~59GB 4-bit model&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üîå Step 4: Deploy with vLLM&lt;/h2&gt;\n\n&lt;p&gt;vLLM supports GPTQ natively. Just launch:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;bash\npython -m vllm.entrypoints.api_server \\\n  --model ./qwen3-235b-4bit-gptq \\\n  --tensor-parallel-size 2 \\\n  --dtype half \\\n  --quantization gptq \\\n  --gpu-memory-utilization 0.95 \\\n  --max-model-len 32768 \\\n  --port 8000 \\\n  --host 0.0.0.0\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;h3&gt;Key Args:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;--tensor-parallel-size 2&lt;/code&gt;: Splits model across both GPUs&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;--quantization gptq&lt;/code&gt;: Enables 4-bit kernels&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;--gpu-memory-utilization 0.95&lt;/code&gt;: Maximizes VRAM use&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;--max-model-len 32768&lt;/code&gt;: Full context support&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üìà Performance Expectations&lt;/h2&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Metric&lt;/th&gt;\n&lt;th&gt;Estimate&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;Model Size&lt;/td&gt;\n&lt;td&gt;~59 GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;VRAM Usage&lt;/td&gt;\n&lt;td&gt;~90GB per GPU&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Load Time&lt;/td&gt;\n&lt;td&gt;30‚Äì60 sec&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Throughput&lt;/td&gt;\n&lt;td&gt;150‚Äì250 tokens/sec (batch 1‚Äì4)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;Accuracy&lt;/td&gt;\n&lt;td&gt;~97% of BF16 on MMLU, GSM8K&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;üéØ Perfect for long-context reasoning, coding, RAG.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;‚úÖ Why This Recipe Wins&lt;/h2&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;Choice&lt;/th&gt;\n&lt;th&gt;Why It‚Äôs Best&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;GPTQ over AWQ&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Better vLLM support, faster kernels&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;NF4 + sym=True&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Higher precision than INT4&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;group_size=128&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Optimal speed/accuracy balance&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;desc_act=False&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Faster inference, minor quality drop&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;strong&gt;c4 calibration&lt;/strong&gt;&lt;/td&gt;\n&lt;td&gt;Matches pretraining distribution&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;üîÑ Alternative: Try &lt;code&gt;desc_act=True&lt;/code&gt; if you want +1% accuracy (slower).&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üí° Pro Tips&lt;/h2&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Monitor VRAM&lt;/strong&gt;: &lt;code&gt;nvidia-smi -l 1&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Use fast storage&lt;/strong&gt;: NVMe required for smooth loading&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Warm up&lt;/strong&gt;: Send test prompts after startup&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Enable batching&lt;/strong&gt;: Boosts throughput&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Update vLLM monthly&lt;/strong&gt;: GPTQ perf keeps improving&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üß© Final Notes&lt;/h2&gt;\n\n&lt;p&gt;While &lt;strong&gt;AWQ&lt;/strong&gt; exists, &lt;strong&gt;vLLM‚Äôs GPTQ support is more mature&lt;/strong&gt; for 200B+ models. Stick with GPTQ for now.&lt;/p&gt;\n\n&lt;p&gt;Also: No public evidence that AWQ beats GPTQ on Qwen3 at 4-bit.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h2&gt;üèÅ Summary: Your 4-bit Dream Setup&lt;/h2&gt;\n\n&lt;p&gt;&lt;code&gt;yaml\nModel:       Qwen3-235B-A2.2\nPrecision:   4-bit GPTQ (NF4)\nCalibration: c4 [1024 samples]\nGroup Size:  128\nSymmetric:   Yes\nInference:   vLLM + tensor_parallel=2\nResult:      59GB model, blazing fast, fits like magic\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;With this setup, you‚Äôre running one of the most powerful open models in the world ‚Äî &lt;strong&gt;locally, efficiently, and privately&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Let me know if you want the Colab/Slurm scripts! üí¨üî•&lt;/p&gt;\n\n&lt;p&gt;‚Äî&lt;br/&gt;\n&lt;em&gt;P.S. I&amp;#39;m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?&lt;/em&gt; üòâ&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ynit",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "blackwell_tart",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753140885,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "TL;DR: This is a **massive** step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download‚Äîno command line or Docker needed. It's never been easier to start building your own local, privacy-first screen-watching agents!\n\n  \nHey r/LocalLLaMA !!\n\nI am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don't want to!!\n\n**What's new in this update:**\n\n* üöÄ¬†**1-Click Desktop App:**¬†The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.\n* üîî¬†**Pushover &amp; Discord Notifications:**¬†SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with **Pushover** or to your community with a **Discord** bot. **Email** stays being reliable!!\n* üõ†Ô∏è¬†**Continuous Improvement:**¬†My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.\n\nFor those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.\n\nI don't want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!\n\n**Check it out and let me know what you think:**\n\n[**https://github.com/Roy3838/Observer**](https://github.com/Roy3838/Observer)",
          "author_fullname": "t2_p443m",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The Observer Desktop App is Here! + Discord/Pushover Notifications!!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": true,
          "name": "t3_1m5y9wj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753139895,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;TL;DR: This is a &lt;strong&gt;massive&lt;/strong&gt; step forward for first-time users. You can now get everything up and running with a single .exe or .dmg download‚Äîno command line or Docker needed. It&amp;#39;s never been easier to start building your own local, privacy-first screen-watching agents!&lt;/p&gt;\n\n&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; !!&lt;/p&gt;\n\n&lt;p&gt;I am suuuper excited to share the desktop launcher app I made for Observer!!! no more docker-compose if you don&amp;#39;t want to!!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What&amp;#39;s new in this update:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;üöÄ¬†&lt;strong&gt;1-Click Desktop App:&lt;/strong&gt;¬†The number one request is here! A simple, downloadable desktop application for a native and smooth setup experience.&lt;/li&gt;\n&lt;li&gt;üîî¬†&lt;strong&gt;Pushover &amp;amp; Discord Notifications:&lt;/strong&gt;¬†SMS and Whatsapp proved to be unreliable, so you can now send alerts directly from your agents to your phone with &lt;strong&gt;Pushover&lt;/strong&gt; or to your community with a &lt;strong&gt;Discord&lt;/strong&gt; bot. &lt;strong&gt;Email&lt;/strong&gt; stays being reliable!!&lt;/li&gt;\n&lt;li&gt;üõ†Ô∏è¬†&lt;strong&gt;Continuous Improvement:&lt;/strong&gt;¬†My goal is to make local AI agents accessible to everyone, and your feedback is making that happen.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;For those new to the project, Observer AI is an open-source tool that lets you run local micro-agents that can see your screen, listen to your mic, and perform actions, all while keeping your data 100% private.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t want to sound super self-promotey, but I really genuinely wanted to share my excitement with the communities that have been so supportive. Thank you for being a part of this!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Check it out and let me know what you think:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Roy3838/Observer\"&gt;&lt;strong&gt;https://github.com/Roy3838/Observer&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?auto=webp&amp;s=a9ed130dbe40bc283accf677a568089896baa4f1",
                  "width": 4030,
                  "height": 2260
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2763f5b07d8000852738cc8bbf6420bc7a793d3e",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3b05a1b6a5908644048b0f050c15a00d2bc5d9ed",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d3f92bd549dcbc4e5089662042619f32c668a07e",
                    "width": 320,
                    "height": 179
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=29ad9d2d4f08a9916f026e28e9a30fd6d1711d5d",
                    "width": 640,
                    "height": 358
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=adab929349280395b1958efc7ed9e9e58d447654",
                    "width": 960,
                    "height": 538
                  },
                  {
                    "url": "https://external-preview.redd.it/-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f25dcc577bb8503c95962d2f130fa431008cd692",
                    "width": 1080,
                    "height": 605
                  }
                ],
                "variants": {},
                "id": "-VzCfNi8ctqCoss6ttS1cBf0psUHAMSGYDmGAfW9QsA"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5y9wj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Roy3838",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5y9wj/the_observer_desktop_app_is_here_discordpushover/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753139895,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?",
          "author_fullname": "t2_m8971",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the top model for coding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5x04m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753136675,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5x04m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "estebansaa",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753136675,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi :)\n\nI'm a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don't say anything very personal or sensitive. \n\nI have an rtx 5090 (32 gb)\n\nWhat are the best local models I can run? \n\nThanks",
          "author_fullname": "t2_3231b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What can I run on my 5090?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5w8yl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753134845,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi :)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don&amp;#39;t say anything very personal or sensitive. &lt;/p&gt;\n\n&lt;p&gt;I have an rtx 5090 (32 gb)&lt;/p&gt;\n\n&lt;p&gt;What are the best local models I can run? &lt;/p&gt;\n\n&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5w8yl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hurfery",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753134845,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I really like LM studio. I think it's a pretty outstanding application that really works well for most basic purposes for a local model. User interface is pretty great, out of the box you can just download models and use it quite effectively. I liked that it was easier to set up than ollama and having to have some command prompt sort of container...\n\n**But the thing I do not like about LM studio is that is not open source and you have no idea what it's actually doing**. For me, this makes it a little difficult for me to want to use it going forward. You have no idea what's going on under the hood, and you have no idea if it's sending your data out externally or collecting it in some sort of hidden file in your computer and then transmitting that. Also, I don't really trust applications that are not open source because you can't really look at what it's doing in the back end and appreciate it for what it is. Some open source stuff that I have used in the past, I really like to look at the GitHub and follow the updates, report issues on it. LM studio is pretty stuffy, it's a discord server and it doesn't feel very welcoming, reporting issues doesn't seem very clear or asking questions and stuff like that. \n\n\nSo I'm curious how long it'll be until we have an actual working, functional program that is an alternative to LM studio? ",
          "author_fullname": "t2_1sznzjx7fy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How long until we have an alternative to LM studio that's just as good?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5w3kj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753134480,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really like LM studio. I think it&amp;#39;s a pretty outstanding application that really works well for most basic purposes for a local model. User interface is pretty great, out of the box you can just download models and use it quite effectively. I liked that it was easier to set up than ollama and having to have some command prompt sort of container...&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;But the thing I do not like about LM studio is that is not open source and you have no idea what it&amp;#39;s actually doing&lt;/strong&gt;. For me, this makes it a little difficult for me to want to use it going forward. You have no idea what&amp;#39;s going on under the hood, and you have no idea if it&amp;#39;s sending your data out externally or collecting it in some sort of hidden file in your computer and then transmitting that. Also, I don&amp;#39;t really trust applications that are not open source because you can&amp;#39;t really look at what it&amp;#39;s doing in the back end and appreciate it for what it is. Some open source stuff that I have used in the past, I really like to look at the GitHub and follow the updates, report issues on it. LM studio is pretty stuffy, it&amp;#39;s a discord server and it doesn&amp;#39;t feel very welcoming, reporting issues doesn&amp;#39;t seem very clear or asking questions and stuff like that. &lt;/p&gt;\n\n&lt;p&gt;So I&amp;#39;m curious how long it&amp;#39;ll be until we have an actual working, functional program that is an alternative to LM studio? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5w3kj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "datascientist2964",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5w3kj/how_long_until_we_have_an_alternative_to_lm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5w3kj/how_long_until_we_have_an_alternative_to_lm/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753134480,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\n\nI really want it to be this year.",
          "author_fullname": "t2_sm168dt0h",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Now that Google and openai have both announced gold at the IMO 2025, how long until an open source model can match that?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5vyku",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753134152,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I really want it to be this year.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5vyku",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MrMrsPotts",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5vyku/now_that_google_and_openai_have_both_announced/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5vyku/now_that_google_and_openai_have_both_announced/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753134152,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\*I'm mainly using ChatGPT for this so please try to ignore the fact that I don't understand muc.h\\* Hi, I've been trying to build my own AI agent on my pc for the past day now. I keep running into the same error. every time I try to send a message, I get \"BadRequestError: litlellm.BadRequestError: GetLLMProviderExceptionn - list index out of range original model: mistral\". I'm really stuck and I cant figure out how to fix it and would love some help. Here's some info you might need.I'mm running Mistral on Ollama. I have LiteLLM as a proxy on port 4000, and I'm using OpenHands with Docker on port 3000. This is my yaml file: \n\nmodel\\_list:\n\n  \\- model\\_name: mistral\n\nlitellm\\_params:\n\nmodel: ollama/mistral\n\napi\\_base: [http://localhost:11434](http://localhost:11434)\n\nlitellm\\_provider: ollama\n\nmode: chat\n\nI start liteLLM with:  \nlitellm --config C:\\\\Users\\\\howdy\\\\litellm-env\\\\litellm.config.yaml --port 4000 --detailed\\_debug\n\nI start openhands with:  \ndocker run -it --rm \\^\n\n  \\-e SANDBOX\\_RUNTIME\\_CONTAINER\\_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik \\^\n\n  \\-e LOG\\_ALL\\_EVENTS=true \\^\n\n  \\-v //var/run/docker.sock:/var/run/docker.sock \\^\n\n  \\-v C:\\\\Users\\\\howdy\\\\openhands-workspace:/.openhands \\^\n\n  \\-p 3000:3000 \\^\n\n  \\--add-host host.docker.internal:host-gateway \\^\n\n  \\--name openhands-app \\^\n\n  [docker.all-hands.dev/all-hands-ai/openhands:0.49](http://docker.all-hands.dev/all-hands-ai/openhands:0.49)\n\n`curl` [`http://host.docker.internal:4000/v1/completions`](http://host.docker.internal:4000/v1/completions) returns `{\"detail\":\"Method Not Allowed\"}` Sometimes, and nothing else happens. I enabled `--detailed_debug`, and I do see logs like ‚ÄúInitialized model mistral,‚Äù but I don't get an interface, or it fails silently. Here's an explanation of more of my issue from ChatGPT:  \nWhat I Tried:\n\n* Confirmed all ports are correct\n* Docker can reach `host.docker.internal:4000`\n* I‚Äôve tested curl inside the container to confirm\n* Sometimes it randomly works, but it breaks again on the next reboot\n\n‚ùìWhat I Need:\n\n* Is this the correct `model_list` format for Ollama/Mistral via LiteLLM?\n* Does OpenHands require a specific model name format?\n* How can I force OpenHands to show **detailed errors** instead of generic `APIConnectionError`?\n\nI would appreciate it if you could help.",
          "author_fullname": "t2_t0k3f4bz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Im trying to make my own agent with openhands but I keep running into the same error.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5v7if",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753132378,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;*I&amp;#39;m mainly using ChatGPT for this so please try to ignore the fact that I don&amp;#39;t understand muc.h* Hi, I&amp;#39;ve been trying to build my own AI agent on my pc for the past day now. I keep running into the same error. every time I try to send a message, I get &amp;quot;BadRequestError: litlellm.BadRequestError: GetLLMProviderExceptionn - list index out of range original model: mistral&amp;quot;. I&amp;#39;m really stuck and I cant figure out how to fix it and would love some help. Here&amp;#39;s some info you might need.I&amp;#39;mm running Mistral on Ollama. I have LiteLLM as a proxy on port 4000, and I&amp;#39;m using OpenHands with Docker on port 3000. This is my yaml file: &lt;/p&gt;\n\n&lt;p&gt;model_list:&lt;/p&gt;\n\n&lt;p&gt;- model_name: mistral&lt;/p&gt;\n\n&lt;p&gt;litellm_params:&lt;/p&gt;\n\n&lt;p&gt;model: ollama/mistral&lt;/p&gt;\n\n&lt;p&gt;api_base: &lt;a href=\"http://localhost:11434\"&gt;http://localhost:11434&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;litellm_provider: ollama&lt;/p&gt;\n\n&lt;p&gt;mode: chat&lt;/p&gt;\n\n&lt;p&gt;I start liteLLM with:&lt;br/&gt;\nlitellm --config C:\\Users\\howdy\\litellm-env\\litellm.config.yaml --port 4000 --detailed_debug&lt;/p&gt;\n\n&lt;p&gt;I start openhands with:&lt;br/&gt;\ndocker run -it --rm ^&lt;/p&gt;\n\n&lt;p&gt;-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik ^&lt;/p&gt;\n\n&lt;p&gt;-e LOG_ALL_EVENTS=true ^&lt;/p&gt;\n\n&lt;p&gt;-v //var/run/docker.sock:/var/run/docker.sock ^&lt;/p&gt;\n\n&lt;p&gt;-v C:\\Users\\howdy\\openhands-workspace:/.openhands ^&lt;/p&gt;\n\n&lt;p&gt;-p 3000:3000 ^&lt;/p&gt;\n\n&lt;p&gt;--add-host host.docker.internal:host-gateway ^&lt;/p&gt;\n\n&lt;p&gt;--name openhands-app ^&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://docker.all-hands.dev/all-hands-ai/openhands:0.49\"&gt;docker.all-hands.dev/all-hands-ai/openhands:0.49&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;curl&lt;/code&gt; &lt;a href=\"http://host.docker.internal:4000/v1/completions\"&gt;&lt;code&gt;http://host.docker.internal:4000/v1/completions&lt;/code&gt;&lt;/a&gt; returns &lt;code&gt;{&amp;quot;detail&amp;quot;:&amp;quot;Method Not Allowed&amp;quot;}&lt;/code&gt; Sometimes, and nothing else happens. I enabled &lt;code&gt;--detailed_debug&lt;/code&gt;, and I do see logs like ‚ÄúInitialized model mistral,‚Äù but I don&amp;#39;t get an interface, or it fails silently. Here&amp;#39;s an explanation of more of my issue from ChatGPT:&lt;br/&gt;\nWhat I Tried:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Confirmed all ports are correct&lt;/li&gt;\n&lt;li&gt;Docker can reach &lt;code&gt;host.docker.internal:4000&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;I‚Äôve tested curl inside the container to confirm&lt;/li&gt;\n&lt;li&gt;Sometimes it randomly works, but it breaks again on the next reboot&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;‚ùìWhat I Need:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is this the correct &lt;code&gt;model_list&lt;/code&gt; format for Ollama/Mistral via LiteLLM?&lt;/li&gt;\n&lt;li&gt;Does OpenHands require a specific model name format?&lt;/li&gt;\n&lt;li&gt;How can I force OpenHands to show &lt;strong&gt;detailed errors&lt;/strong&gt; instead of generic &lt;code&gt;APIConnectionError&lt;/code&gt;?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I would appreciate it if you could help.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5v7if",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "HowdyCapybara",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5v7if/im_trying_to_make_my_own_agent_with_openhands_but/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5v7if/im_trying_to_make_my_own_agent_with_openhands_but/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753132378,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I'd like to run models locally (at my workplaces) and also refine models, and fortunately I'm not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I'm missing for going with 512GB RAM?",
          "author_fullname": "t2_sk7nmjrs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Strong case for a 512GB Mac Studio?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5uu0t",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753131529,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d like to run models locally (at my workplaces) and also refine models, and fortunately I&amp;#39;m not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I&amp;#39;m missing for going with 512GB RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5uu0t",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ChevChance",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753131529,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\nI recently wrapped up a side project involving SEC filings, and thought some of you here might find it interesting or useful.\n\nI built a dataset of ~4,000 instruction-output samples based on real 6-K and 8-K filings.\nIt‚Äôs structured in JSONL, QLoRA/Alpaca-style format (natural language instruction ‚Üí clean short answer).\n\nInputs retain real-world messiness from actual filings (inconsistent structure, lawyer-ese, etc.)\n\nOutputs are concise summaries, instructions, or redirections depending on filing type (earnings, acquisitions, restructuring, resigning, etc.)\n\nThe goal was to train an LLM to handle regulatory language like a financial analyst with pattern recognition\n\n\nOriginally made this for internal fine-tuning, but I‚Äôve shifted to another niche now.\nIf anyone‚Äôs working on AI for finance, compliance, investor tools, etc., I‚Äôm happy to share a few sample entries and chat about use cases.\n\nIf enough people are interested, I might package it for others to use or license.\n\nDM me if you want a preview or have questions.",
          "author_fullname": "t2_1r3lcxoyqn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[Project Share] Built a 4K Instruction Dataset Based on SEC 6-K/8-K Filings (JSONL format, QLoRA-friendly)",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5uhwc",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753130743,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,\nI recently wrapped up a side project involving SEC filings, and thought some of you here might find it interesting or useful.&lt;/p&gt;\n\n&lt;p&gt;I built a dataset of ~4,000 instruction-output samples based on real 6-K and 8-K filings.\nIt‚Äôs structured in JSONL, QLoRA/Alpaca-style format (natural language instruction ‚Üí clean short answer).&lt;/p&gt;\n\n&lt;p&gt;Inputs retain real-world messiness from actual filings (inconsistent structure, lawyer-ese, etc.)&lt;/p&gt;\n\n&lt;p&gt;Outputs are concise summaries, instructions, or redirections depending on filing type (earnings, acquisitions, restructuring, resigning, etc.)&lt;/p&gt;\n\n&lt;p&gt;The goal was to train an LLM to handle regulatory language like a financial analyst with pattern recognition&lt;/p&gt;\n\n&lt;p&gt;Originally made this for internal fine-tuning, but I‚Äôve shifted to another niche now.\nIf anyone‚Äôs working on AI for finance, compliance, investor tools, etc., I‚Äôm happy to share a few sample entries and chat about use cases.&lt;/p&gt;\n\n&lt;p&gt;If enough people are interested, I might package it for others to use or license.&lt;/p&gt;\n\n&lt;p&gt;DM me if you want a preview or have questions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5uhwc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xairossss",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5uhwc/project_share_built_a_4k_instruction_dataset/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5uhwc/project_share_built_a_4k_instruction_dataset/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753130743,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I know that most platforms fine-tune their models and use a good system prompt, but I've tried Qwen3 32B locally and on [qwen.com](http://qwen.com) and the difference is so huge.\n\nAre there publicly available ready fine-tunes and system prompts I can use to improve the models locally?",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why are base non-finetuned models so bad?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5th6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753128457,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I know that most platforms fine-tune their models and use a good system prompt, but I&amp;#39;ve tried Qwen3 32B locally and on &lt;a href=\"http://qwen.com\"&gt;qwen.com&lt;/a&gt; and the difference is so huge.&lt;/p&gt;\n\n&lt;p&gt;Are there publicly available ready fine-tunes and system prompts I can use to improve the models locally?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5th6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5th6s/why_are_base_nonfinetuned_models_so_bad/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5th6s/why_are_base_nonfinetuned_models_so_bad/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753128457,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.",
          "author_fullname": "t2_y0abrfm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "AI 395+ 64GB vs 128GB?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5s6d1",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 18,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 18,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753125511,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5s6d1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "cfogrady",
          "discussion_type": null,
          "num_comments": 64,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753125511,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\nJust got an RTX 5090 with 32GB of VRAM and I'm looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.\n\n * What's the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?\n\n * Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?\n\n * Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?\n\nMy goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!\n\nThanks!",
          "author_fullname": "t2_dyvrh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 (32GB VRAM) - Full Fine-Tuning: What Can I Expect?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ro7s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753124390,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,\nJust got an RTX 5090 with 32GB of VRAM and I&amp;#39;m looking to get into full fine-tuning LLMs locally.\nMy main question is about the full fine-tuning capabilities with this GPU. I know 32GB is a lot, but full fine-tuning can be a VRAM hog.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;What&amp;#39;s the realistic largest model size (in billions of parameters) I can full fine-tune (not LoRA/QLoRA) using 32GB VRAM?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Assuming FP16/BF16 precision and memory optimizations like gradient checkpointing, what are the typical limitations (batch size, sequence length) for models in the 7B, 13B, or even larger range?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Are there any specific transformers or bitsandbytes configurations crucial for maximizing VRAM usage for full fine-tuning on the RTX 5090?&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;My goal is to achieve the best possible quality with full fine-tuning, even if it means a very small batch size. Any insights or experiences with similar VRAM GPUs would be super helpful!&lt;/p&gt;\n\n&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ro7s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "celsowm",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ro7s/rtx_5090_32gb_vram_full_finetuning_what_can_i/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753124390,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "_A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him \"completely exhausted.\"_\n\nhttps://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/",
          "author_fullname": "t2_1gnii9bkc9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Exhausted man defeats AI model in world coding championship",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5r9ss",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 75,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 75,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753123500,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;em&gt;A Polish programmer running on fumes recently accomplished what may soon become impossible: beating an advanced AI model from OpenAI in a head-to-head coding competition. The 10-hour marathon left him &amp;quot;completely exhausted.&amp;quot;&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/\"&gt;https://arstechnica.com/ai/2025/07/exhausted-man-defeats-ai-model-in-world-coding-championship/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?auto=webp&amp;s=01c6c5989382448ceacfb16d4716e1d43882c07d",
                  "width": 1152,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c56d051b4b4e63e5c6627f8639b6bc541ebe7a70",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=06c2478ecab47c428699645361b3f004e394ce98",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3dcfa8df2a1e6cafbca016c61a6781fc1bd66b6e",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=089271aa272fc9321b33a83d2db06a95c38b6ce9",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=73adafa18daac7bc6b0c9c52f9fe6f72db681c41",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d3d93ef0ea7ac95a421f4d7a2e7e1d2f2350aac9",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "blOzOsTs-z21YUWC-XZkszWY0Ligsy1VCK1fZxml6qo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5r9ss",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Educational_Sun_8813",
          "discussion_type": null,
          "num_comments": 23,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5r9ss/exhausted_man_defeats_ai_model_in_world_coding/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753123500,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Project Page: [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://deepreinforce-ai.github.io/cudal1_blog/)\n\nCode: [GitHub - deepreinforce-ai/CUDA-L1](https://github.com/deepreinforce-ai/CUDA-L1)\n\nAbstract\n\n&gt;The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.  \nCUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.  \nThe capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.",
          "author_fullname": "t2_dtsa6gxt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qsqx",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753122428,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Project Page: &lt;a href=\"https://deepreinforce-ai.github.io/cudal1_blog/\"&gt;CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Code: &lt;a href=\"https://github.com/deepreinforce-ai/CUDA-L1\"&gt;GitHub - deepreinforce-ai/CUDA-L1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Abstract&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The exponential growth in demand for GPU computing resources, driven by the rapid advancement of Large Language Models, has created an urgent need for automated CUDA optimization strategies. While recent advances in LLMs show promise for code generation, current SOTA models (e.g. R1, o1) achieve low success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an automated reinforcement learning framework for CUDA optimization.&lt;br/&gt;\nCUDA-L1 achieves performance improvements on the CUDA optimization task: trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250 CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the model also demonstrates excellent portability across GPU architectures, achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40, x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100. Beyond these benchmark results, CUDA-L1 demonstrates several remarkable properties: 1) Discovers a variety of CUDA optimization techniques and learns to combine them strategically to achieve optimal performance; 2) Uncovers fundamental principles of CUDA optimization; 3) Identifies non-obvious performance bottlenecks and rejects seemingly beneficial optimizations that harm performance.&lt;br/&gt;\nThe capabilities of CUDA-L1 demonstrate that reinforcement learning can transform an initially poor-performing LLM into an effective CUDA optimizer through speedup-based reward signals alone, without human expertise or domain knowledge. More importantly, the trained RL model extend the acquired reasoning abilities to new kernels. This paradigm opens possibilities for automated optimization of CUDA operations, and holds promise to substantially promote GPU efficiency and alleviate the rising pressure on GPU computing resources.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.14111",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5qsqx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Formal_Drop526",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qsqx/cudal1_improving_cuda_optimization_via/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.14111",
          "subreddit_subscribers": 502515,
          "created_utc": 1753122428,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!",
          "author_fullname": "t2_jldf8",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3 insane SimpleQA",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qn1n",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 43,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 43,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753122070,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Why is no one talking about the insane simpleQA score for the new Qwen3 model? 54.3 OMG! How are they doing this with a 235ba22b model?!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5qn1n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gzzhongqi",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5qn1n/qwen3_insane_simpleqa/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753122070,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Here is a brief demo showing how one could use the new AI chat features in Neu called the magic hand. This system uses Llama 3.2 3b as a tool caller, and Claude Haiku 3.5 to generate the code but the code step could easily be replaced with a local model such as Qwen 3. I'm most using Claude because of the speed. It's still early days so right now its simple input output commands but I've been experimenting with a full blown agent that (I hope) will be able to build entire graphs. My hope is that this drastically reduces the knowledge floor needed to use Neu which, let's be honest, is a pretty intimidating piece of software. I hope that by following what the magic hand is doing, you can learn and understand Neu better. These features and a ton more will be coming with the Neu 0.3.0 update. [Checkout this link you'd like to learn more about Neu](http://kingroka.itch.io/neu)",
          "author_fullname": "t2_fkmug",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Using ollama and claude to control Neu",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5qflo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.87,
          "author_flair_background_color": null,
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/lgjdwe0yo9ef1/DASHPlaylist.mpd?a=1755738678%2CYjg1NjFkZDZhYWZhODY3YzZlMmU0YjlmNzk1MTQ4ZDgxZjI5ZWVmY2UxYjkzNTE2ZGZkYTM1N2M1OWQ4NGU1MQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 74,
              "hls_url": "https://v.redd.it/lgjdwe0yo9ef1/HLSPlaylist.m3u8?a=1755738678%2CNzQwMWYxZjM3YjcxMzc5NTdiNzY2NGJlNDFlOGY4ZWFiMjE2ZjgwOTA3NzRiYzQxYzIxNTlhZjQ4MmRhYzk1OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=21b892afd0727f90d166d0a8bd745b1b393466e0",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753121614,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Here is a brief demo showing how one could use the new AI chat features in Neu called the magic hand. This system uses Llama 3.2 3b as a tool caller, and Claude Haiku 3.5 to generate the code but the code step could easily be replaced with a local model such as Qwen 3. I&amp;#39;m most using Claude because of the speed. It&amp;#39;s still early days so right now its simple input output commands but I&amp;#39;ve been experimenting with a full blown agent that (I hope) will be able to build entire graphs. My hope is that this drastically reduces the knowledge floor needed to use Neu which, let&amp;#39;s be honest, is a pretty intimidating piece of software. I hope that by following what the magic hand is doing, you can learn and understand Neu better. These features and a ton more will be coming with the Neu 0.3.0 update. &lt;a href=\"http://kingroka.itch.io/neu\"&gt;Checkout this link you&amp;#39;d like to learn more about Neu&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/lgjdwe0yo9ef1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?format=pjpg&amp;auto=webp&amp;s=066dc89758345e5c4b0f587768ddc66e19e69c11",
                  "width": 2560,
                  "height": 1440
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=70d833416067508a44b90607113d94349d55781b",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=b22a0bf8d0efcb692e56b986ffac037c1461879a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=43e75947dba7a887cce1309dd168dadb7cbc6ae7",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0427819bd56be1b5a7f90f67087f2477281f6bb8",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=82c83d51ba654e3c59c02f4516982e17cbac1472",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=11c7891a7ee6be24d57433d813fd8a07d748bd5d",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "MGFjdHZlMHlvOWVmMfKUvQGyK-hk_9kIm7eyVblVWB99wibqoL0okUWLtxg2"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5qflo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "kingroka",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5qflo/using_ollama_and_claude_to_control_neu/",
          "stickied": false,
          "url": "https://v.redd.it/lgjdwe0yo9ef1",
          "subreddit_subscribers": 502515,
          "created_utc": 1753121614,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/lgjdwe0yo9ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/lgjdwe0yo9ef1/DASHPlaylist.mpd?a=1755738678%2CYjg1NjFkZDZhYWZhODY3YzZlMmU0YjlmNzk1MTQ4ZDgxZjI5ZWVmY2UxYjkzNTE2ZGZkYTM1N2M1OWQ4NGU1MQ%3D%3D&amp;v=1&amp;f=sd",
              "duration": 74,
              "hls_url": "https://v.redd.it/lgjdwe0yo9ef1/HLSPlaylist.m3u8?a=1755738678%2CNzQwMWYxZjM3YjcxMzc5NTdiNzY2NGJlNDFlOGY4ZWFiMjE2ZjgwOTA3NzRiYzQxYzIxNTlhZjQ4MmRhYzk1OQ%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html](https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html)",
          "author_fullname": "t2_84qpb9rt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Interesting new blog post from Lemonade team",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5q35o",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 14,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 14,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753120844,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html\"&gt;https://www.amd.com/en/developer/resources/technical-articles/2025/rethinking-local-ai-lemonade-servers-python-advantage.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?auto=webp&amp;s=370ef3c67fc9a466fb921e399215ca76e255bdd8",
                  "width": 1435,
                  "height": 645
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d6ab69b2057c1ff74bd4fad9be640864917203c",
                    "width": 108,
                    "height": 48
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bc432f40f27f0a2f68d597fb271cdc9efc951403",
                    "width": 216,
                    "height": 97
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=39d76385ecda761edb0e6a264b0e3efd78c0cd1c",
                    "width": 320,
                    "height": 143
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=77ae8bc998e1523bb4c01bb0d57ef5cea220343f",
                    "width": 640,
                    "height": 287
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c1e801ab47a7cb7d218797288798cd8f7d4e41fd",
                    "width": 960,
                    "height": 431
                  },
                  {
                    "url": "https://external-preview.redd.it/tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7d14e3dd033ec146ebe70b9e6b9135c50e7a9136",
                    "width": 1080,
                    "height": 485
                  }
                ],
                "variants": {},
                "id": "tXAQH-2IuxJHSCb05V5OiFQN-j9xlst_M-d3k_TkoOc"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5q35o",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Smooth-Screen4148",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5q35o/interesting_new_blog_post_from_lemonade_team/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753120844,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey r/LocalLLaMA,\n\nLast week, I shared some initial drafts of my platform's UI. Thanks to the amazing work of a designer friend, I'm back to show you the evolution from that first AI-generated concept to a mostly polished, human-crafted interface (still candidate, tho).\n\nAs you can see, the difference is night and day!\n\nNow, for the exciting part: I'm getting ready to open up the platform for limited testing.\n\n**An important note on the test build:**¬†For this initial testing phase, we will be using the¬†**old (AI-generated) UI**. My current priority is to ensure the backend and core functionality providing good foundation.\n\nIf you're interested in stress-testing the platform's core features and providing feedback on what's under the hood, stay tuned! I'll be posting details on how to join very soon.",
          "author_fullname": "t2_1zyh18yq",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "Before &amp; after: redesigned the character catalog UI. What do you think?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 140,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "z3iumy0sg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 117,
                  "x": 108,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7bd93f9ee3ba10c62e31072b2fab67a0682d9bb7"
                },
                {
                  "y": 234,
                  "x": 216,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7eab99a775a675106108bf53e002a6b38c00b89c"
                },
                {
                  "y": 348,
                  "x": 320,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76330c09e3a89e6ccf96d6f9c555ed94e02b3498"
                },
                {
                  "y": 696,
                  "x": 640,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9e159a8161508aaa70c6e2f17122b5a38255636d"
                },
                {
                  "y": 1044,
                  "x": 960,
                  "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9fa016cc18e381009fda19242d7b07cb27f6fe83"
                }
              ],
              "s": {
                "y": 1116,
                "x": 1026,
                "u": "https://preview.redd.it/z3iumy0sg9ef1.png?width=1026&amp;format=png&amp;auto=webp&amp;s=6fcfffc59a0fa716b64402489cce2d9644787252"
              },
              "id": "z3iumy0sg9ef1"
            },
            "n26o7hasg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 139,
                  "x": 108,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=907bce804132c14442fea2bef69e8f8907d45d86"
                },
                {
                  "y": 278,
                  "x": 216,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9420b891e22e58402aac9519de28eb6f2b78b365"
                },
                {
                  "y": 413,
                  "x": 320,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=590acc267bc807e160c1ec399af2d6ff44b2961e"
                },
                {
                  "y": 826,
                  "x": 640,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a1ef9a47faf40bab3b5d8a56014e71cc7e4bf088"
                },
                {
                  "y": 1239,
                  "x": 960,
                  "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=82d4ffca89177e0a43111c82ccc81a58c1e5e725"
                }
              ],
              "s": {
                "y": 1322,
                "x": 1024,
                "u": "https://preview.redd.it/n26o7hasg9ef1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=22c7512e28b5644dc1cfc256c9e3fae672c9fed4"
              },
              "id": "n26o7hasg9ef1"
            }
          },
          "name": "t3_1m5psqj",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.65,
          "author_flair_background_color": null,
          "ups": 5,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "z3iumy0sg9ef1",
                "id": 710946172
              },
              {
                "media_id": "n26o7hasg9ef1",
                "id": 710946173
              }
            ]
          },
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/QEIIdxBKQlyZRDWkr6Zlni5fTmJN8SWTPsvnn4bZncg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753120203,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt;\n\n&lt;p&gt;Last week, I shared some initial drafts of my platform&amp;#39;s UI. Thanks to the amazing work of a designer friend, I&amp;#39;m back to show you the evolution from that first AI-generated concept to a mostly polished, human-crafted interface (still candidate, tho).&lt;/p&gt;\n\n&lt;p&gt;As you can see, the difference is night and day!&lt;/p&gt;\n\n&lt;p&gt;Now, for the exciting part: I&amp;#39;m getting ready to open up the platform for limited testing.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;An important note on the test build:&lt;/strong&gt;¬†For this initial testing phase, we will be using the¬†&lt;strong&gt;old (AI-generated) UI&lt;/strong&gt;. My current priority is to ensure the backend and core functionality providing good foundation.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re interested in stress-testing the platform&amp;#39;s core features and providing feedback on what&amp;#39;s under the hood, stay tuned! I&amp;#39;ll be posting details on how to join very soon.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m5psqj",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5psqj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RIPT1D3_Z",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5psqj/before_after_redesigned_the_character_catalog_ui/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m5psqj",
          "subreddit_subscribers": 502515,
          "created_utc": 1753120203,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm currently using ChatGPT 4o, and I‚Äôd like to explore the possibility of running a local LLM on my home server. I know VRAM is a really big factor and I‚Äôm considering purchasing two RTX 3090s for running a local LLM. What models would compete with GPT 4o?",
          "author_fullname": "t2_5j33g3s4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking to possibly replace my ChatGPT subscription with running a local LLM. What local models match/rival 4o?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5pmox",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753119827,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm currently using ChatGPT 4o, and I‚Äôd like to explore the possibility of running a local LLM on my home server. I know VRAM is a really big factor and I‚Äôm considering purchasing two RTX 3090s for running a local LLM. What models would compete with GPT 4o?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5pmox",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ActuallyGeyzer",
          "discussion_type": null,
          "num_comments": 8,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753119827,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have been wondering this for a while now - why is nobody building custom instruct versions from public base models that don't include the typical sycophantic behavior of official releases where every dumb idea the user has is just SO insightful? The most I see is some RP specific tunes, but for more general purpose assistants there are slim pickings.\n\nAnd what about asking for just some formated JSON output and specifiying that you want nothing else? you do it and the model wafles on about \"here is your data formated as JSON...\". I just want some plain json that i can just parse, okay?\n\nIsn't what we really want a model that gives unbiased, straight to the point answers and can be steered to act how we want it to? maybe even with some special commands similar to how it works with qwen 3? i want some /no\\_fluff and some /no\\_bias please! Am i the only one here or are others also interested in such instruct tunes?",
          "author_fullname": "t2_3wi6j7vwh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Why not build instruct models that give you straight answers with no positivity bias and no bs?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5pig4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753119570,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been wondering this for a while now - why is nobody building custom instruct versions from public base models that don&amp;#39;t include the typical sycophantic behavior of official releases where every dumb idea the user has is just SO insightful? The most I see is some RP specific tunes, but for more general purpose assistants there are slim pickings.&lt;/p&gt;\n\n&lt;p&gt;And what about asking for just some formated JSON output and specifiying that you want nothing else? you do it and the model wafles on about &amp;quot;here is your data formated as JSON...&amp;quot;. I just want some plain json that i can just parse, okay?&lt;/p&gt;\n\n&lt;p&gt;Isn&amp;#39;t what we really want a model that gives unbiased, straight to the point answers and can be steered to act how we want it to? maybe even with some special commands similar to how it works with qwen 3? i want some /no_fluff and some /no_bias please! Am i the only one here or are others also interested in such instruct tunes?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5pig4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "LagOps91",
          "discussion_type": null,
          "num_comments": 41,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5pig4/why_not_build_instruct_models_that_give_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5pig4/why_not_build_instruct_models_that_give_you/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753119570,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Trying to get an RTX 5090 working on Ubuntu and hitting a wall. The system boots fine, BIOS sees the card, but Ubuntu doesn‚Äôt seem to know it exists. nvidia-smi comes up empty. Meanwhile, a 4090 in the same machine is working just fine.\n\nHere‚Äôs what I‚Äôve tried so far:\n\n* Installed latest NVIDIA drivers from both apt and the CUDA toolkit installer (550+)\n* Swapped PCIe slots\n* Disabled secure boot, added nomodeset, the usual boot flags\n* Confirmed power and reseated the card just in case\n\nStill nothing. I‚Äôm on Ubuntu 22.04 at the moment. Starting to wonder if this is a kernel issue or if the 5090 just isn‚Äôt properly supported yet. Anyone have a 5090 running on Linux? Did you need a bleeding-edge kernel or beta drivers?\n\nMain goal is running local LLaMA models, but right now the 5090 is just sitting there, useless.\n\nWould really appreciate any info or pointers. If you‚Äôve gotten this working, let me know what combo of drivers, kernel, and/or sacrifice to the GPU gods it took.\n\nThanks in advance.",
          "author_fullname": "t2_3ryxf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "RTX 5090 not recognized on Ubuntu ‚Äî anyone else figure this out?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5pbxo",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753119177,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to get an RTX 5090 working on Ubuntu and hitting a wall. The system boots fine, BIOS sees the card, but Ubuntu doesn‚Äôt seem to know it exists. nvidia-smi comes up empty. Meanwhile, a 4090 in the same machine is working just fine.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs what I‚Äôve tried so far:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Installed latest NVIDIA drivers from both apt and the CUDA toolkit installer (550+)&lt;/li&gt;\n&lt;li&gt;Swapped PCIe slots&lt;/li&gt;\n&lt;li&gt;Disabled secure boot, added nomodeset, the usual boot flags&lt;/li&gt;\n&lt;li&gt;Confirmed power and reseated the card just in case&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Still nothing. I‚Äôm on Ubuntu 22.04 at the moment. Starting to wonder if this is a kernel issue or if the 5090 just isn‚Äôt properly supported yet. Anyone have a 5090 running on Linux? Did you need a bleeding-edge kernel or beta drivers?&lt;/p&gt;\n\n&lt;p&gt;Main goal is running local LLaMA models, but right now the 5090 is just sitting there, useless.&lt;/p&gt;\n\n&lt;p&gt;Would really appreciate any info or pointers. If you‚Äôve gotten this working, let me know what combo of drivers, kernel, and/or sacrifice to the GPU gods it took.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5pbxo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ate50eggs",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5pbxo/rtx_5090_not_recognized_on_ubuntu_anyone_else/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5pbxo/rtx_5090_not_recognized_on_ubuntu_anyone_else/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753119177,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_fmd6oq5v6",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Instruct-2507 ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5pbj0",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": "#bbbdbf",
          "ups": 47,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 47,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=68ff5489be2450ce2200e81da6540a1dd25ed70a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753119154,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?auto=webp&amp;s=86c8fc358fab39d0103c80888dc78d172e254fd0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41efd73e0b1f2f6245cc18321de9593d2f691f2a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ad4906ae01be77390f3512429fd8d526cdbff6b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc710aa210ed6e6fd519830b6e1c20372358b8dc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a081e0d80600d61cab0535bcecc71bbb337e1d1f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47fe182351ca7738d227c54e9f94e68766055478",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5pbj0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "random-tomato",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m5pbj0/qwenqwen3235ba22binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "subreddit_subscribers": 502515,
          "created_utc": 1753119154,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Kimi K2 is a beast!  Both in performance and to run.   Ernie is much smaller and easier to run.  It's 47B active, so going to be a bit slower, however it performs quite well.  I would call it K2's little brother, I think it got overshadowed by K2 especially since K2 was the claude sonnet 4 and open weight OpenAI killer.  It took longer to also get support for it into llama.cpp  \nI have been testing it out and I really like it.   For general chat, (logically, scientific, mathematically), it's straight to the point, doesn't beat around the bush or hew and haw.  Great instruction following too, very precise and to the point.  I haven't heard much about it, and I know that many can't run it, but you should really consider it and add it to the mix.   Get the parameters right too, my first runs were meh, and then I had to go find the recommended parameters, I haven't experimented much with them, but there might even be better.  I'm running Q6 from unsloth. temp/top\\_p 0.8, top\\_k 50, min\\_p 0.01",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Do not sleep on ERNIE-4.5-300B-A47B especially if you can't Kimi K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5p69p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.93,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 46,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 46,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118830,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Kimi K2 is a beast!  Both in performance and to run.   Ernie is much smaller and easier to run.  It&amp;#39;s 47B active, so going to be a bit slower, however it performs quite well.  I would call it K2&amp;#39;s little brother, I think it got overshadowed by K2 especially since K2 was the claude sonnet 4 and open weight OpenAI killer.  It took longer to also get support for it into llama.cpp&lt;br/&gt;\nI have been testing it out and I really like it.   For general chat, (logically, scientific, mathematically), it&amp;#39;s straight to the point, doesn&amp;#39;t beat around the bush or hew and haw.  Great instruction following too, very precise and to the point.  I haven&amp;#39;t heard much about it, and I know that many can&amp;#39;t run it, but you should really consider it and add it to the mix.   Get the parameters right too, my first runs were meh, and then I had to go find the recommended parameters, I haven&amp;#39;t experimented much with them, but there might even be better.  I&amp;#39;m running Q6 from unsloth. temp/top_p 0.8, top_k 50, min_p 0.01&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5p69p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5p69p/do_not_sleep_on_ernie45300ba47b_especially_if_you/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753118830,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[Mind-Blowing](https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128)",
          "author_fullname": "t2_1tbloqabyg",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "7by2astxg9ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c498a4606859ea7dc3344cea600a6287666c52bb"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1a0f911df63c617cf62eddf44f25fd9d1526f4f9"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=18f18ad0a953d5f6cce28f16501265fc5f1b65fa"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6a2c30752b5e9e36ee257e083921c23b7bd51214"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ce6ecb7eb20a5950434de688e9e3e318d8dcf552"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b198e562025147aead0f6a4065ef96012102dc8e"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128"
              },
              "id": "7by2astxg9ef1"
            }
          },
          "name": "t3_1m5oz0h",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 109,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 109,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sK-ChiNoLnwj2ggKTtNTJYTvWhnsGqdGF-BtGXWSrIM.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118398,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/7by2astxg9ef1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed2caaa4b854693b6fd46383a9626aefe87b0128\"&gt;Mind-Blowing&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oz0h",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ken-senseii",
          "discussion_type": null,
          "num_comments": 27,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5oz0h/qwen3235ba22b2507/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753118398,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_kwl47",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen/Qwen3-235B-A22B-Instruct-2507 ¬∑ Hugging Face",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5oyf5",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "ups": 34,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 34,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=68ff5489be2450ce2200e81da6540a1dd25ed70a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118362,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "huggingface.co",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?auto=webp&amp;s=86c8fc358fab39d0103c80888dc78d172e254fd0",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=41efd73e0b1f2f6245cc18321de9593d2f691f2a",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ad4906ae01be77390f3512429fd8d526cdbff6b",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dc710aa210ed6e6fd519830b6e1c20372358b8dc",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=981c1b88ac04632c811f86e43d24143c128aa1a3",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a081e0d80600d61cab0535bcecc71bbb337e1d1f",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=47fe182351ca7738d227c54e9f94e68766055478",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "XyDac6TnV0yjdA-C8ojiXDTxH6tgY_Cc33jnLmPWJ8g"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oyf5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Dark_Fire_12",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oyf5/qwenqwen3235ba22binstruct2507_hugging_face/",
          "stickied": false,
          "url": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
          "subreddit_subscribers": 502515,
          "created_utc": 1753118362,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!\n\nAfter talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we‚Äôll train Instruct and Thinking models separately so we can get the best quality possible. Today, we‚Äôre releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.\n\nThis model performs better than our last release, and we hope you‚Äôll like it thanks to its strong overall abilities.\n\nQwen Chat: chat.qwen.ai ‚Äî just start chatting with the default model, and feel free to use the search button!",
          "author_fullname": "t2_c705ri9b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen released Qwen3-235B-A22B-2507!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5oxyp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "ups": 92,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 92,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/3Sv34Z6mz9FmhoyS3JnMLxTuxuV0E_Efi7EvfiWKcOs.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118337,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Bye Qwen3-235B-A22B, hello Qwen3-235B-A22B-2507!&lt;/p&gt;\n\n&lt;p&gt;After talking with the community and thinking it through, we decided to stop using hybrid thinking mode. Instead, we‚Äôll train Instruct and Thinking models separately so we can get the best quality possible. Today, we‚Äôre releasing Qwen3-235B-A22B-Instruct-2507 and its FP8 version for everyone.&lt;/p&gt;\n\n&lt;p&gt;This model performs better than our last release, and we hope you‚Äôll like it thanks to its strong overall abilities.&lt;/p&gt;\n\n&lt;p&gt;Qwen Chat: chat.qwen.ai ‚Äî just start chatting with the default model, and feel free to use the search button!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?auto=webp&amp;s=10c54f96c9b0f8a2ead569e3e5e97915476224de",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6baea679e548efcaaac74cffb282ff70f159dd23",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=df4fdf4fdf2d8f18aa169c3917aff6a59354480a",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=bd168c86688eec1b15ed22e24f23dd96a0b2be9d",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=13fd449c88365fae792fbacc8076a6e633ad74e2",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e563d574e4328f00b0b38e4cd232ecc1c21ff8cb",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://preview.redd.it/6csu4o4wg9ef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dc97776dcddebc962894dcf5ba459481257122c",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "fCmGXIZe3dEk6g_UwQiEUl1wK88M5Bv1vrt320AGj8o"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5oxyp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ResearchCrafty1804",
          "discussion_type": null,
          "num_comments": 11,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5oxyp/qwen_released_qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/6csu4o4wg9ef1.jpeg",
          "subreddit_subscribers": 502515,
          "created_utc": 1753118337,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/Alibaba_Qwen/status/1947344511988076547\n\nNew Qwen3-235B-A22B with thinking mode only ‚Äì‚Äì no more hybrid reasoning.",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 122,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ox8z",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 292,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 292,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/BzOaH3m_YlAhztCv_hRYQh_Ms3ouqOY06ZKKT3zNke8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118294,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/Alibaba_Qwen/status/1947344511988076547\"&gt;https://x.com/Alibaba_Qwen/status/1947344511988076547&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;New Qwen3-235B-A22B with thinking mode only ‚Äì‚Äì no more hybrid reasoning.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?auto=webp&amp;s=d32732e5748b82ca37787c55ccd57f5d5f705318",
                  "width": 1186,
                  "height": 1038
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=32d1f0ad8ac85f1518bb4e197d86320d03376d96",
                    "width": 108,
                    "height": 94
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=bf9ca6a58cad66052d5ca05375d82b3c7bf8f1cb",
                    "width": 216,
                    "height": 189
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c38bec3415e13d4801fadbc3fe0e9ec1df461dbf",
                    "width": 320,
                    "height": 280
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5242a889814e823acb6da0b1179758e2947ea2a7",
                    "width": 640,
                    "height": 560
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7330c9d38901abf96ac6373f03b5933dd9e99710",
                    "width": 960,
                    "height": 840
                  },
                  {
                    "url": "https://preview.redd.it/w2uh7h5lg9ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=15a616880d08cfc896e48a40a70a2aa2315dc9e4",
                    "width": 1080,
                    "height": 945
                  }
                ],
                "variants": {},
                "id": "15L72ZL9LJC_7vzVz7gsW_m-zmkTuQofd74ZqjjUwaA"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ox8z",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 55,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ox8z/qwen3235ba22b2507/",
          "stickied": false,
          "url": "https://i.redd.it/w2uh7h5lg9ef1.png",
          "subreddit_subscribers": 502515,
          "created_utc": 1753118294,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Any Gemini CLI experts here? Does this article make sense to you?\n\n[**Meet the Agent: The Brain Behind Gemini CLI**](https://softwaresecretweapons.com/opinion/gemini-cli-masterclass/gemini-cli-agents.html)\n\nIn this article, we explore the \"mind\" behind Gemini CLI, showing how this LLM-powered agent uses a methodical 4-step process to understand, plan, implement, and verify code changes.\n\n\\#gemini-cli #gemini-cli-masterclass",
          "author_fullname": "t2_vowy0dxi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Meet the Agent: The Brain Behind Gemini CLI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5owur",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.43,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753118268,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Any Gemini CLI experts here? Does this article make sense to you?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://softwaresecretweapons.com/opinion/gemini-cli-masterclass/gemini-cli-agents.html\"&gt;&lt;strong&gt;Meet the Agent: The Brain Behind Gemini CLI&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;In this article, we explore the &amp;quot;mind&amp;quot; behind Gemini CLI, showing how this LLM-powered agent uses a methodical 4-step process to understand, plan, implement, and verify code changes.&lt;/p&gt;\n\n&lt;p&gt;#gemini-cli #gemini-cli-masterclass&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5owur",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "SandFragrant6227",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5owur/meet_the_agent_the_brain_behind_gemini_cli/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5owur/meet_the_agent_the_brain_behind_gemini_cli/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753118268,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_3l9wjlq0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Qwen3-235B-A22B-2507 Released!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5owi8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.99,
          "author_flair_background_color": null,
          "ups": 560,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 560,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/sKHygFxyatHEivMjwhoU0rKccpX3n5vMlMuGtN0ebyc.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753118247,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "x.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?auto=webp&amp;s=2ca86b1c53db0d11a0c488d1d12c8c9cb55eaf20",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4c0c862769016dce18130a1fb791dbf78757f922",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2f8596c55842b58dd3bf4190c4c47e309432ad77",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=6d51316624bbec96c8a0b28b2e3756e68ffadf98",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9739fe5698145f958eb2e1c66da1875fc6d34a00",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b66905b8a3f2f560c571babd372861e032c5ca94",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/0p_T6A15tLk4WquPAys3oZ34c-lcssBt_NcX5stv-2M.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=72cbdd4b47610ef9ddfdf989b7900703487934d6",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "DZ_J_yAfR8TLjLmR0s6ZMb4IqBdDowTQUhHZ335Z0r8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5owi8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "pseudoreddituser",
          "discussion_type": null,
          "num_comments": 188,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5owi8/qwen3235ba22b2507_released/",
          "stickied": false,
          "url": "https://x.com/Alibaba_Qwen/status/1947344511988076547",
          "subreddit_subscribers": 502515,
          "created_utc": 1753118247,
          "num_crossposts": 3,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, \n\nI had created a rag application but i made it for documents of PDF format only. I use PyMuPDF4llm to parse the PDF. \n\nBut now I want to add the option for all the document formats, i.e, pptx, xlsx, csv, docx, and the image formats.\n\nI tried docling for this, since PyMuPDF4llm requires subscription to allow rest of the document formats. \n\nI created a standalone setup to test docling. Docling uses external OCR engines, it had 2 options. Tesseract and RapidOCR.\n\nI set up the one with RapidOCR. The documents, whether pdf, csv or pptx are parsed and its output are stored into markdown format. \n\nI am facing some issues. These are:\n\n1. Time that it takes to parse the content inside images into markdown are very random, some image takes 12-15 minutes, some images are easily parsed with 2-3 minutes. why is this so random? Is it possible to speed up this process? \n\n2. The output for scanned images, or image of documents that were captured using camera are not that good. Can something be done to enhance its performance?\n\n3. Images that are embedded into pptx or docx, such as graph or chart don't get parsed properly. The labelling inside them such the x or y axis data, or data points within graph are just mentioned in the markdown output in a badly formatted manner. That data becomes useless for me.",
          "author_fullname": "t2_wf267hqnf",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Facing some problems with Docling parser",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5or7n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753117936,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, &lt;/p&gt;\n\n&lt;p&gt;I had created a rag application but i made it for documents of PDF format only. I use PyMuPDF4llm to parse the PDF. &lt;/p&gt;\n\n&lt;p&gt;But now I want to add the option for all the document formats, i.e, pptx, xlsx, csv, docx, and the image formats.&lt;/p&gt;\n\n&lt;p&gt;I tried docling for this, since PyMuPDF4llm requires subscription to allow rest of the document formats. &lt;/p&gt;\n\n&lt;p&gt;I created a standalone setup to test docling. Docling uses external OCR engines, it had 2 options. Tesseract and RapidOCR.&lt;/p&gt;\n\n&lt;p&gt;I set up the one with RapidOCR. The documents, whether pdf, csv or pptx are parsed and its output are stored into markdown format. &lt;/p&gt;\n\n&lt;p&gt;I am facing some issues. These are:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Time that it takes to parse the content inside images into markdown are very random, some image takes 12-15 minutes, some images are easily parsed with 2-3 minutes. why is this so random? Is it possible to speed up this process? &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;The output for scanned images, or image of documents that were captured using camera are not that good. Can something be done to enhance its performance?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Images that are embedded into pptx or docx, such as graph or chart don&amp;#39;t get parsed properly. The labelling inside them such the x or y axis data, or data points within graph are just mentioned in the markdown output in a badly formatted manner. That data becomes useless for me.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5or7n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ElectronicHoneydew86",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5or7n/facing_some_problems_with_docling_parser/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5or7n/facing_some_problems_with_docling_parser/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753117936,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone,\n\nI'm trying to run vllm on my nvidia 5090, possibly in a dockerized container.\n\nBefore I start looking into this, has anyone already done this or has a good docker image to suggest that works out-of-the-box?\n\nIf not, any tips?\n\nThank you!!",
          "author_fullname": "t2_8oe14nwi",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Running vllm on Nvidia 5090",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5okz7",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753117549,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to run vllm on my nvidia 5090, possibly in a dockerized container.&lt;/p&gt;\n\n&lt;p&gt;Before I start looking into this, has anyone already done this or has a good docker image to suggest that works out-of-the-box?&lt;/p&gt;\n\n&lt;p&gt;If not, any tips?&lt;/p&gt;\n\n&lt;p&gt;Thank you!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5okz7",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Reasonable_Friend_77",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5okz7/running_vllm_on_nvidia_5090/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5okz7/running_vllm_on_nvidia_5090/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753117549,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I told my brother I can help him build an AI workstation since he wants to run Llama 3.1 locally and train it or build a RAG or whatever. Since he's a software guy and I'm a gamer who built 2 gaming PCs in my entire life, he agreed to trust me with picking the parts and putting everything together (I was shocked too). I got him to order all the parts, including an expensive nvlink bridge 4 slot from eBay that is crucial for the build since he needs a 48GB of pooled vram from the two 3090s he was able to buy very cheaply from friends.\n\nLong story short, we ended buying Gigabyte trx50 aero D and the nvlink 4 slot bridge is too short and doesn't reach the second GPU.. I messed up big time and now I'm trying to find a solution without switching the entire setup because everything is already built, wired for air flow etc, PCU and AIO connected and PSU. The primary card I'm using in the PCIe slot 1 is ASUS ROG STRIX 3090 OC, the secondary is MSI VENTUS 3X 3090 OC which right now is in PCIe slot 3. Slot 2 is too close to the Asus GPU and besides it also doesn't allow for the nvlink to fit because then it'll be too long.  \nI then had the idea of getting a GPU stand that can hold my MSI GPU at the correct height to accommodate the nvlink, and a PCIe riser cable to connect from either slot 2 or 3 to the card - the problem is all riser cables are way too long and I can't bend them enough to fit.  \nI measured 17mm between the center of slot 2 and the fingers of the MSI GPU at the optimal position for the nvlink, and 23mm between the center of slot 3 and the fingers of the MSI GPU. Can't find a riser cable this short and even if do I don't know that it'll work very well at that length. I'm starting to lose hope and I'm not sure what to tell my brother.. now I'm on AliExpress looking for a PCB for a 16 pin PCIe that can offset by one slot up or down but it's looking like a lost cause.. I'm desperate. Any help would be much appreciated.  \nMore specifically for the folks on this sub - Should my brother accept working with the 2 3090s without the Nvlink? Would it be dramatically lower performance on all counts of running local LLms or only for fine-tuning?\n\nThings I've already tried that don't work (that the good folks at PCBuild Help suggested):\n\n1. Switching GPU fans to water block won't help - the problem is that there is no PCIe configuration in this mobo that allows appropriate distance to accommodate the 4-slot NVlink.\n2. They don't make a 5 or 3 slot NVlink for the 3090s. If anyone here has a lead on something like this from a third party I'll be all over it, but thus far was not able to find it.\n3. Riser cables are 10-30cm where I need a 25mm that goes from PCIe slot 3 to the optimal position of the MSI GPU to accommodate the NVlink - no one makes that and if I get it custom I don't know that performance will justify it. Anyone know of more flexible riser type solutions that can bend more?\n4. I know switching the MOBO will solve it. Trying to avoid that to not spend more money and redo the build, also trying to save some of what's left of my dignity in front of my brother.\n5. My case can't fit both cards vertically.\n\n",
          "author_fullname": "t2_dfnw6xdeo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I messed up my brother's Llama AI workstation.. looking for advice",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ojym",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753117487,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I told my brother I can help him build an AI workstation since he wants to run Llama 3.1 locally and train it or build a RAG or whatever. Since he&amp;#39;s a software guy and I&amp;#39;m a gamer who built 2 gaming PCs in my entire life, he agreed to trust me with picking the parts and putting everything together (I was shocked too). I got him to order all the parts, including an expensive nvlink bridge 4 slot from eBay that is crucial for the build since he needs a 48GB of pooled vram from the two 3090s he was able to buy very cheaply from friends.&lt;/p&gt;\n\n&lt;p&gt;Long story short, we ended buying Gigabyte trx50 aero D and the nvlink 4 slot bridge is too short and doesn&amp;#39;t reach the second GPU.. I messed up big time and now I&amp;#39;m trying to find a solution without switching the entire setup because everything is already built, wired for air flow etc, PCU and AIO connected and PSU. The primary card I&amp;#39;m using in the PCIe slot 1 is ASUS ROG STRIX 3090 OC, the secondary is MSI VENTUS 3X 3090 OC which right now is in PCIe slot 3. Slot 2 is too close to the Asus GPU and besides it also doesn&amp;#39;t allow for the nvlink to fit because then it&amp;#39;ll be too long.&lt;br/&gt;\nI then had the idea of getting a GPU stand that can hold my MSI GPU at the correct height to accommodate the nvlink, and a PCIe riser cable to connect from either slot 2 or 3 to the card - the problem is all riser cables are way too long and I can&amp;#39;t bend them enough to fit.&lt;br/&gt;\nI measured 17mm between the center of slot 2 and the fingers of the MSI GPU at the optimal position for the nvlink, and 23mm between the center of slot 3 and the fingers of the MSI GPU. Can&amp;#39;t find a riser cable this short and even if do I don&amp;#39;t know that it&amp;#39;ll work very well at that length. I&amp;#39;m starting to lose hope and I&amp;#39;m not sure what to tell my brother.. now I&amp;#39;m on AliExpress looking for a PCB for a 16 pin PCIe that can offset by one slot up or down but it&amp;#39;s looking like a lost cause.. I&amp;#39;m desperate. Any help would be much appreciated.&lt;br/&gt;\nMore specifically for the folks on this sub - Should my brother accept working with the 2 3090s without the Nvlink? Would it be dramatically lower performance on all counts of running local LLms or only for fine-tuning?&lt;/p&gt;\n\n&lt;p&gt;Things I&amp;#39;ve already tried that don&amp;#39;t work (that the good folks at PCBuild Help suggested):&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Switching GPU fans to water block won&amp;#39;t help - the problem is that there is no PCIe configuration in this mobo that allows appropriate distance to accommodate the 4-slot NVlink.&lt;/li&gt;\n&lt;li&gt;They don&amp;#39;t make a 5 or 3 slot NVlink for the 3090s. If anyone here has a lead on something like this from a third party I&amp;#39;ll be all over it, but thus far was not able to find it.&lt;/li&gt;\n&lt;li&gt;Riser cables are 10-30cm where I need a 25mm that goes from PCIe slot 3 to the optimal position of the MSI GPU to accommodate the NVlink - no one makes that and if I get it custom I don&amp;#39;t know that performance will justify it. Anyone know of more flexible riser type solutions that can bend more?&lt;/li&gt;\n&lt;li&gt;I know switching the MOBO will solve it. Trying to avoid that to not spend more money and redo the build, also trying to save some of what&amp;#39;s left of my dignity in front of my brother.&lt;/li&gt;\n&lt;li&gt;My case can&amp;#39;t fit both cards vertically.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ojym",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "spherical-aspiration",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753117487,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I recently started thinking about using local AI, but I don't know where to start, what I need, or if I can afford it. So I wanted to ask a few questions.\n\n1. What do I need at a minimum to use a local AI?\n2. Where can I find it to download?\n3. What do I need to know before I start?\n4. What really changes from one model to the other?",
          "author_fullname": "t2_lrleoa4b0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I want to start with local AI",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5nt6s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753115868,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I recently started thinking about using local AI, but I don&amp;#39;t know where to start, what I need, or if I can afford it. So I wanted to ask a few questions.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;What do I need at a minimum to use a local AI?&lt;/li&gt;\n&lt;li&gt;Where can I find it to download?&lt;/li&gt;\n&lt;li&gt;What do I need to know before I start?&lt;/li&gt;\n&lt;li&gt;What really changes from one model to the other?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5nt6s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Then-History2046",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5nt6s/i_want_to_start_with_local_ai/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5nt6s/i_want_to_start_with_local_ai/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753115868,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Heavily promoting the dishwashing benchmark:  \n\nGemini 3.0 Ultra score: 0%  \n\nGPT 5 Pro score: 0%  \n\nClaude 5 Opus score: 0%\n\ngrok 5 scoreÔºö0%\n\nDeepSeek R2 score: 0%  \n\nQwen4 Max score: 0%  \n\nKimi K3 score: 0%",
          "author_fullname": "t2_4i04jwsoe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Heavily promoting the dishwashing benchmark",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ndsf",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753115118,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753114894,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Heavily promoting the dishwashing benchmark:  &lt;/p&gt;\n\n&lt;p&gt;Gemini 3.0 Ultra score: 0%  &lt;/p&gt;\n\n&lt;p&gt;GPT 5 Pro score: 0%  &lt;/p&gt;\n\n&lt;p&gt;Claude 5 Opus score: 0%&lt;/p&gt;\n\n&lt;p&gt;grok 5 scoreÔºö0%&lt;/p&gt;\n\n&lt;p&gt;DeepSeek R2 score: 0%  &lt;/p&gt;\n\n&lt;p&gt;Qwen4 Max score: 0%  &lt;/p&gt;\n\n&lt;p&gt;Kimi K3 score: 0%&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ndsf",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "gtog-ima",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ndsf/heavily_promoting_the_dishwashing_benchmark/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ndsf/heavily_promoting_the_dishwashing_benchmark/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753114894,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Been seeing¬†loads¬†of¬†developers¬†here¬†going¬†on¬†about how LLM integraded IDE's like Windsurf and Cursor totally changed their coding.¬†Of course, I¬†was¬†interested¬†and wanted to give it a¬†go.¬†Spoke¬†to¬†work¬†about¬†it, and the boss just¬†said \"no way dude\" GDPR-compliant and PII could be garanted (we are a bigger team, including student workers), data¬†gets¬†transferred¬†to the US, too risky, blah blah. So¬†no Cursor and Windsurf for me.\n\nHonestly, I get it. Not mad at my company they're just doing their job and don't want to¬†get¬†fined But man,¬†still sucks. We are still stuck in¬†legacy¬†workflows because every new AI tool is¬†geared¬†for¬†US devs¬†first. Feels like being left behind not because the tech¬†exists, but because we¬†simply¬†can't¬†utilize¬†it. And¬†sure, I¬†do understand¬†the GDPR¬†thing¬†is¬†big¬†deal and that there is a chanche PII and API keys included in the code by accident. But still‚Ä¶ it sucks.\n\nDoes¬†anyone else¬†get¬†stuck¬†with this?¬†Is there any other good alternatives that are similar to Cursor and Windsurf made in and for EU. What¬†are other EU devs/teams¬†doing? Self-hosting?¬†Or just¬†keeping to old tools?",
          "author_fullname": "t2_sck77urj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "EU is being left behinde and it sucks!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5n6lq",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.59,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753114446,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Been seeing¬†loads¬†of¬†developers¬†here¬†going¬†on¬†about how LLM integraded IDE&amp;#39;s like Windsurf and Cursor totally changed their coding.¬†Of course, I¬†was¬†interested¬†and wanted to give it a¬†go.¬†Spoke¬†to¬†work¬†about¬†it, and the boss just¬†said &amp;quot;no way dude&amp;quot; GDPR-compliant and PII could be garanted (we are a bigger team, including student workers), data¬†gets¬†transferred¬†to the US, too risky, blah blah. So¬†no Cursor and Windsurf for me.&lt;/p&gt;\n\n&lt;p&gt;Honestly, I get it. Not mad at my company they&amp;#39;re just doing their job and don&amp;#39;t want to¬†get¬†fined But man,¬†still sucks. We are still stuck in¬†legacy¬†workflows because every new AI tool is¬†geared¬†for¬†US devs¬†first. Feels like being left behind not because the tech¬†exists, but because we¬†simply¬†can&amp;#39;t¬†utilize¬†it. And¬†sure, I¬†do understand¬†the GDPR¬†thing¬†is¬†big¬†deal and that there is a chanche PII and API keys included in the code by accident. But still‚Ä¶ it sucks.&lt;/p&gt;\n\n&lt;p&gt;Does¬†anyone else¬†get¬†stuck¬†with this?¬†Is there any other good alternatives that are similar to Cursor and Windsurf made in and for EU. What¬†are other EU devs/teams¬†doing? Self-hosting?¬†Or just¬†keeping to old tools?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5n6lq",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Refrigerator9508",
          "discussion_type": null,
          "num_comments": 131,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5n6lq/eu_is_being_left_behinde_and_it_sucks/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753114446,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://x.com/JustinLin610/status/1947281769134170147\n\nMaybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng [here](https://x.com/ChujieZheng/status/1947307034980089905).",
          "author_fullname": "t2_gbx2bcdvl",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Imminent release from Qwen tonight",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 69,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5n148",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 357,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 357,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/DBvwe9bi2sUadVKTh4wZB-h_0n3lxygls9SlP-B36wg.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753114102,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/JustinLin610/status/1947281769134170147\"&gt;https://x.com/JustinLin610/status/1947281769134170147&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Maybe Qwen3-Coder, Qwen3-VL or a new QwQ? Will be open source / weight according to Chujie Zheng &lt;a href=\"https://x.com/ChujieZheng/status/1947307034980089905\"&gt;here&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/um0pwye549ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/um0pwye549ef1.png?auto=webp&amp;s=034e1360dd4d1a71075a1978e81cc176280c0940",
                  "width": 570,
                  "height": 284
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac602ae1dcb08fc594a97f2c504da7e053543395",
                    "width": 108,
                    "height": 53
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=34209fd3279089bca920b8e313de5e6ea1d3d074",
                    "width": 216,
                    "height": 107
                  },
                  {
                    "url": "https://preview.redd.it/um0pwye549ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3401860a4fccf34b2ae631236b2b714dafe0ec28",
                    "width": 320,
                    "height": 159
                  }
                ],
                "variants": {},
                "id": "XO8ytuncZItjhr3hlokHSgG-P5B6fKV-FMv68ZqKLXQ"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5n148",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Mysterious_Finish543",
          "discussion_type": null,
          "num_comments": 80,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5n148/imminent_release_from_qwen_tonight/",
          "stickied": false,
          "url": "https://i.redd.it/um0pwye549ef1.png",
          "subreddit_subscribers": 502515,
          "created_utc": 1753114102,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The author is StyleTTS 2 just released DMOSpeech2 - post-trained F5-TTS that‚Äôs 2x faster with improved WER and stability. Looks very interesting and open sourced with training code coming soon.\nThis is probably the last open source project we will see from the author for  a while, but looks very very interesting.",
          "author_fullname": "t2_1f194h3luj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "DMOSpeech 2: 2x faster + higher-quality F5-TTS from the author of StyleTTS 2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 70,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mzxt",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 36,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 36,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=6f166dfac0527db4273e73fd28eabe37b909084c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753114028,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "github.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The author is StyleTTS 2 just released DMOSpeech2 - post-trained F5-TTS that‚Äôs 2x faster with improved WER and stability. Looks very interesting and open sourced with training code coming soon.\nThis is probably the last open source project we will see from the author for  a while, but looks very very interesting.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://github.com/yl4579/DMOSpeech2",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?auto=webp&amp;s=e79caffc600940aacae822bf3d08a2972d97a9b3",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=55a782ebf4637087ab602e003b76d529f0b2b9b0",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4076be77f5945db6c69125ad169cbbf7a337377f",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a24fc93b1713526b9869d4f3049face873e33bb1",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=757c96b9786749c821edc73c85d3500a7f6d30fc",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=49a731ff6d20f17d0c91af2ed14936af2a706c37",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2eb69e689a57d5034d4de02201511409b9c8993a",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "QXB2u8-1CJvvYiUKNzbSUTq0ZcDZjw_7UaAxuMzOB74"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5mzxt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mrfakename0",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mzxt/dmospeech_2_2x_faster_higherquality_f5tts_from/",
          "stickied": false,
          "url": "https://github.com/yl4579/DMOSpeech2",
          "subreddit_subscribers": 502515,
          "created_utc": 1753114028,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello. I don't know much about LLM's, but I'd like to create a bot that tries to behave like me. I have around 3 years of my scrapped messages from various platforms. The idea is to teach a model with my dataset (messages) so it tries to understand how I behave, how I text and what words I use and then run a Discord bot that will act like me. But here comes the problem, I'm slightly limited by hardware and I have no clue what model to use. I run RTX 2060 with 6GB of VRAM and 16GB of ram. I consider renting a virtual GPU for the sake of project, but I don't know how to start. Any model recommendations?",
          "author_fullname": "t2_hpy57ypd",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with choosing model to create bot that will talk like me.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mrmy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753113521,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello. I don&amp;#39;t know much about LLM&amp;#39;s, but I&amp;#39;d like to create a bot that tries to behave like me. I have around 3 years of my scrapped messages from various platforms. The idea is to teach a model with my dataset (messages) so it tries to understand how I behave, how I text and what words I use and then run a Discord bot that will act like me. But here comes the problem, I&amp;#39;m slightly limited by hardware and I have no clue what model to use. I run RTX 2060 with 6GB of VRAM and 16GB of ram. I consider renting a virtual GPU for the sake of project, but I don&amp;#39;t know how to start. Any model recommendations?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5mrmy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "deadyasiu",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mrmy/help_with_choosing_model_to_create_bot_that_will/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5mrmy/help_with_choosing_model_to_create_bot_that_will/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753113521,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\n\nI have evaluated mistral small 3.2 for OCR tasks using ollama. The accuracy has been very satisfying while some bugs cause it to run on CPU solely with a rtx 4090 (about 5t/s). \n\nSo I switched to llama.cpp and obtain between 20-40t/s using the model + mmproj from unsloth. Both models are Q4\\_K\\_M. The accuracy is way worse than what I get when using ollama. How can that be? \n\nIs it using another vision projector, or am I doing sth wrong? I use 32k context, temp=0, all other settings are defaults. I do not explicitely use quantized kvcache or flash attention.\n\nAny idea how to get on par with ollamas excellent OCR accuracy?\n\nthanks &amp; greets",
          "author_fullname": "t2_2b6dk0nt",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "mistral-small-3.2 OCR accuracy way too bad with llama.cpp compared to ollama?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mms1",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753113210,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;I have evaluated mistral small 3.2 for OCR tasks using ollama. The accuracy has been very satisfying while some bugs cause it to run on CPU solely with a rtx 4090 (about 5t/s). &lt;/p&gt;\n\n&lt;p&gt;So I switched to llama.cpp and obtain between 20-40t/s using the model + mmproj from unsloth. Both models are Q4_K_M. The accuracy is way worse than what I get when using ollama. How can that be? &lt;/p&gt;\n\n&lt;p&gt;Is it using another vision projector, or am I doing sth wrong? I use 32k context, temp=0, all other settings are defaults. I do not explicitely use quantized kvcache or flash attention.&lt;/p&gt;\n\n&lt;p&gt;Any idea how to get on par with ollamas excellent OCR accuracy?&lt;/p&gt;\n\n&lt;p&gt;thanks &amp;amp; greets&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5mms1",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "caetydid",
          "discussion_type": null,
          "num_comments": 21,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753113210,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello everyone,\n\nI'm looking for an open source  that could help me with real-time audio-to-text comparison.   \nI want to capture the actor's live voice from Pro Tools, and compare what they say against a provided script ( PDF or TXT) ‚Äî ideally in real time ‚Äî to detect omissions, extra words, or misread lines.\n\nEven if it's a workaround or requires routing with something like BlackHole or other tools, I'm open to solutions. \n\nThanks, ",
          "author_fullname": "t2_1iixkvi7ek",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Looking for Open Source STT Tool to Detect Script Reading Errors in Real Time",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5mjoc",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753113001,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m looking for an open source  that could help me with real-time audio-to-text comparison.&lt;br/&gt;\nI want to capture the actor&amp;#39;s live voice from Pro Tools, and compare what they say against a provided script ( PDF or TXT) ‚Äî ideally in real time ‚Äî to detect omissions, extra words, or misread lines.&lt;/p&gt;\n\n&lt;p&gt;Even if it&amp;#39;s a workaround or requires routing with something like BlackHole or other tools, I&amp;#39;m open to solutions. &lt;/p&gt;\n\n&lt;p&gt;Thanks, &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5mjoc",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "hydrant_DnB",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5mjoc/looking_for_open_source_stt_tool_to_detect_script/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5mjoc/looking_for_open_source_stt_tool_to_detect_script/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753113001,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_169jzqdxe5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "SmolLM3-3B training logs and intermediate checkpoints",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 94,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5m1et",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "ups": 44,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 44,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/4ybrjEuV5RvXq5bR2OKH4U7Osu4se3imdtVZT91_5hA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753111884,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/fcyltq1nx8ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/fcyltq1nx8ef1.png?auto=webp&amp;s=561e48b7a9066f5e3a65cb4df61d4bcee1d3623d",
                  "width": 2384,
                  "height": 1608
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7312a97ee7ebc5986c39d4b65b8d2c7104ed72bb",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=06e6cd1192b19cd320fac196b58a7f384f175290",
                    "width": 216,
                    "height": 145
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=76c1665297b63d147f74996bd94da849649689ae",
                    "width": 320,
                    "height": 215
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=992660889d501145b2a21434e2e7b0563e643863",
                    "width": 640,
                    "height": 431
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6f360d2d0886afbde91f505a0640db28051fde90",
                    "width": 960,
                    "height": 647
                  },
                  {
                    "url": "https://preview.redd.it/fcyltq1nx8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8251eb5dab9d71321a09bb3fd6b9e6fc2388a343",
                    "width": 1080,
                    "height": 728
                  }
                ],
                "variants": {},
                "id": "U-FI27gby_ykbgKbgdzrNHEru7LMZIbTK_iFSM3XWRs"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5m1et",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eliebakk",
          "discussion_type": null,
          "num_comments": 20,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5m1et/smollm33b_training_logs_and_intermediate/",
          "stickied": false,
          "url": "https://i.redd.it/fcyltq1nx8ef1.png",
          "subreddit_subscribers": 502515,
          "created_utc": 1753111884,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I dont need near real time TTS at all, i am happy with even 0.5x realtime generation. Is there actually a better model than Kokoro but with the trade off of being slower/larger, or is Kokoro not only the best model but also really fast?",
          "author_fullname": "t2_1tyhgasfrp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is there a better local TTS than Kokoro, even if its slower to generate?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5lwo6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 8,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 8,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753111594,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I dont need near real time TTS at all, i am happy with even 0.5x realtime generation. Is there actually a better model than Kokoro but with the trade off of being slower/larger, or is Kokoro not only the best model but also really fast?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5lwo6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Sad_Holiday_7435",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lwo6/is_there_a_better_local_tts_than_kokoro_even_if/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5lwo6/is_there_a_better_local_tts_than_kokoro_even_if/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753111594,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/Tesslate/UIGEN-X-8B](https://huggingface.co/Tesslate/UIGEN-X-8B)\n\nJust wanted to share a quick prompting guide for UIGEN-X (and that quants are available). Craft any system prompt (its not specific, so it will listen to you!)\n\nSo type out your prompt like this: \n\n* \\[Action\\] \\[UI type or page\\] \\[Framework(s)\\] \\[Key features\\] \\[Style (optional)\\]\n*  Examples:\n\n   * `Create a navbar using React + Tailwind CSS with logo, links, and mobile hamburger menu.`\n   * `Build a SaaS dashboard with Next.js + TypeScript + shadcn/ui: pages for analytics, user settings, billing, and a landing page. Use glassmorphism style.`\n   * `Generate a personal blog with SvelteKit + DaisyUI, mixing cyberpunk colors and minimalist layout. Responsive for mobile.`\n   * `Make a pricing table with React + Chakra UI, including monthly/yearly toggle, dark mode, and enterprise minimalism style.`\n\nIf it is within the context, then you can additionally add edits.\n\nHere's a prompt template:\n\n* `Create a [UI type] using [Framework(s) + Libraries] with [Features]. [Optional: Use [Style] style]. [Optional: Add sample content or Unsplash images.]`\n\nAdditional things that are supported -&gt; if you hand it Unsplash links or other pictures links, it should work. Make sure reasoning is on for this. This way, you can use it in Agentic or Function calling frameworks.\n\nRemember, its only an 8B model!\n\nWe are currently training 14B, 32B, and 30A and refining the process. We hope to create a good local alternative to the popular coding / design models that are on the web. \n\nMake sure to join the community for more support. (Link in Huggingface!)",
          "author_fullname": "t2_7mx42xse",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "UIGEN-X 8B supports React Headless, Flutter, React Native, Static Site Generators, Tauri, Vue, Gradio/Python, Tailwind, and prompt-based design. GGUF/GPTQ/MLX Available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 95,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "kzzqul8nf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f1469bca066e6dc8c4de5bbfbf70f8cb8da95d3f"
                },
                {
                  "y": 142,
                  "x": 216,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=690e827a5f003cd998910dbf524e31b73ec2db01"
                },
                {
                  "y": 211,
                  "x": 320,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4891ca95bed5b4dde677b103efacb0d0eb83c604"
                },
                {
                  "y": 422,
                  "x": 640,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=099f2fc6e3b5972b2c6aef17adae108361623908"
                },
                {
                  "y": 633,
                  "x": 960,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a5482ce8142b6ea19c74d2fc590aff9b0f1a98db"
                },
                {
                  "y": 712,
                  "x": 1080,
                  "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=42be0fc4e87a423ce4553e87cd0e02e8cc7403f1"
                }
              ],
              "s": {
                "y": 1251,
                "x": 1896,
                "u": "https://preview.redd.it/kzzqul8nf8ef1.png?width=1896&amp;format=png&amp;auto=webp&amp;s=720ab948b90f645d72464f3b1284749a8404c6ad"
              },
              "id": "kzzqul8nf8ef1"
            },
            "iv8y70kfg8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a45bdc2372e5c9d718e3c14b37cce4647d522ac"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=832e91e8046f6ec8fd954d5ea4f31ddaba93091e"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e5194344fca943ac8a92d397bdd2d4229ac50941"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbeff775b694d92aadda1695d0d25455717df09f"
                },
                {
                  "y": 640,
                  "x": 960,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b61e1958cc6204508a5a54eb6faa52028d7b5bd3"
                },
                {
                  "y": 720,
                  "x": 1080,
                  "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d9c323a72a30535b3747f536878ce0741be12614"
                }
              ],
              "s": {
                "y": 1271,
                "x": 1906,
                "u": "https://preview.redd.it/iv8y70kfg8ef1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=e4168909aeb7910a1decb0e675e95dfc99172c97"
              },
              "id": "iv8y70kfg8ef1"
            },
            "s968gxs0g8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=409ea33391501bf0e1b3668cd6701584e2d6328a"
                },
                {
                  "y": 144,
                  "x": 216,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0f3fe42af5e89f5bfb82327d8bdca78b9f676aa6"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=afe29e0fd00f03e24733ba7561bc1fd1dce58aba"
                },
                {
                  "y": 427,
                  "x": 640,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ee72596c92bb36c8533edc20100db23fbf59feb9"
                },
                {
                  "y": 641,
                  "x": 960,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=40291f8d7edfd5b2938a1017f5f7003cea6eea64"
                },
                {
                  "y": 721,
                  "x": 1080,
                  "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fcf187a13cb3e659ee2e4349b2239f6691f238ce"
                }
              ],
              "s": {
                "y": 1274,
                "x": 1907,
                "u": "https://preview.redd.it/s968gxs0g8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=03c2b53b770f0e0199f946f45413e52f38101e43"
              },
              "id": "s968gxs0g8ef1"
            },
            "bkgben5hn8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ebe469a22e8a696f6ade5d03df0afb99319ee40a"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=397c229507fa92cd0b2b700ecccd364b481a5d64"
                },
                {
                  "y": 213,
                  "x": 320,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=acd20ec90f369e40b727066d598fc2b7edca4200"
                },
                {
                  "y": 426,
                  "x": 640,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=381cf7575766e29c2cee139dcdf1749c60b7b5a3"
                },
                {
                  "y": 639,
                  "x": 960,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d2cf511ff6f043cc035e7a29da943acaecc33bd8"
                },
                {
                  "y": 719,
                  "x": 1080,
                  "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=de8973571c93536552a02bae099f00622832e8b6"
                }
              ],
              "s": {
                "y": 1272,
                "x": 1909,
                "u": "https://preview.redd.it/bkgben5hn8ef1.png?width=1909&amp;format=png&amp;auto=webp&amp;s=0ce0953efdfe2dc0a0167b401829ee4e78834594"
              },
              "id": "bkgben5hn8ef1"
            },
            "baogvk7lf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=303f5769de618aedc68d5befd55c0f23ad537998"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=de639214f79d2d4e168910bdf7db8b2172d9de5b"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ebc6b8b5b37c7d198ab01c45349ebaef3165a921"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4fc9d239d6ad5c911867d958b15710cb6644d4b2"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3248842e9756c380dc1254d6ab7f23dba5a99e27"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bb1d84003b1279f8c8656aa93a1111beacc131a8"
                }
              ],
              "s": {
                "y": 1249,
                "x": 1902,
                "u": "https://preview.redd.it/baogvk7lf8ef1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=d3b3d33655c346bf5bc703fa7512b6076b23c02b"
              },
              "id": "baogvk7lf8ef1"
            },
            "qlq88ceum8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 73,
                  "x": 108,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=11392437c6eae7c3cb80093689973f601ba47f75"
                },
                {
                  "y": 147,
                  "x": 216,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f8385ab7c59df8821a90902570bf798a646e92c0"
                },
                {
                  "y": 218,
                  "x": 320,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9015853f431160e764d4d2c07353f81ab3bdaf1"
                },
                {
                  "y": 436,
                  "x": 640,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9f9b1c5463b34aa1d4a5064888091f446d588916"
                },
                {
                  "y": 654,
                  "x": 960,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=58c5783739adb38ad8cf28b7f105be92b48bcccf"
                },
                {
                  "y": 736,
                  "x": 1080,
                  "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fca853f214192cc056b4a0d81f3bb2fa50ab2da3"
                }
              ],
              "s": {
                "y": 1276,
                "x": 1872,
                "u": "https://preview.redd.it/qlq88ceum8ef1.png?width=1872&amp;format=png&amp;auto=webp&amp;s=8f2da76ecd06758cb6411dcaac6e07a89f684c92"
              },
              "id": "qlq88ceum8ef1"
            },
            "ec0ih2x5g8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a9379f1e63a89f5367dfbfd32b9a8df532304055"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7f434f85c9033511251c7f06b8c15761163442aa"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6b033b91dc8c3d9b67a9812dc157b2f858852c15"
                },
                {
                  "y": 425,
                  "x": 640,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e7cfb3e5b4418180e7274c15d302f84d9d64e6cc"
                },
                {
                  "y": 637,
                  "x": 960,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f2d7171901ffd3b3fa481153f1df1277152d588"
                },
                {
                  "y": 717,
                  "x": 1080,
                  "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1b07979b7f543d84d9c4384c6e77b8b911837df8"
                }
              ],
              "s": {
                "y": 1279,
                "x": 1925,
                "u": "https://preview.redd.it/ec0ih2x5g8ef1.png?width=1925&amp;format=png&amp;auto=webp&amp;s=d8d041b099fe8e0d77604f0e37a8c0f85418ecd3"
              },
              "id": "ec0ih2x5g8ef1"
            },
            "9xa2d1o5f8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 72,
                  "x": 108,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b935462ab2f8008aab03cd92672934b99a6d50f"
                },
                {
                  "y": 145,
                  "x": 216,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a5bfb820526679232d933594d6a207edf0c3c3bd"
                },
                {
                  "y": 215,
                  "x": 320,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=30f7ea1e4bf5a5150b76e4b873193c82577deefd"
                },
                {
                  "y": 431,
                  "x": 640,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=dd6665da47568cba2ae3fbe0c12342303e7361f7"
                },
                {
                  "y": 646,
                  "x": 960,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=acbf35cb0b854faffc8500847045ddee546974e7"
                },
                {
                  "y": 727,
                  "x": 1080,
                  "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f802e6268270bd0257fd64d6a5f12a8d0775acfd"
                }
              ],
              "s": {
                "y": 1283,
                "x": 1904,
                "u": "https://preview.redd.it/9xa2d1o5f8ef1.png?width=1904&amp;format=png&amp;auto=webp&amp;s=86d181cfb01012f731f45e4236be0ef691cd7c7a"
              },
              "id": "9xa2d1o5f8ef1"
            },
            "4cgtiz1pf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9476e3d32a2b4ebddc1d44940be795ca69c505de"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a012cb536e4088a6adcc6773c02216ff905ea974"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b1ae7259c182874929c4e41c2cd1db5089064c5"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bb9b8a221009889a7ab5351b684233583bcb0cc"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2ba7b4a8e8efa0432dab30568ad1477468ea5974"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=77a0c9435849b1b3f806b87ef5f2fa6998fc6dff"
                }
              ],
              "s": {
                "y": 1253,
                "x": 1907,
                "u": "https://preview.redd.it/4cgtiz1pf8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=daafca14c7e5ee623c552997eb11226cbf2767df"
              },
              "id": "4cgtiz1pf8ef1"
            },
            "eltdxfn8i8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ceb464884fcc4e8ea8f5673a530fca637a4ea00d"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e4a99d61fb402903985f9c6be85f75cb987a9afb"
                },
                {
                  "y": 210,
                  "x": 320,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5b2b59e21adf2de70f35de42b0b13badb2b7e1dc"
                },
                {
                  "y": 420,
                  "x": 640,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5b5a7db457131ab9e6827d842044325ab24d3afb"
                },
                {
                  "y": 630,
                  "x": 960,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2a4594f5796a9d0c3d0573956810621801f7651e"
                },
                {
                  "y": 709,
                  "x": 1080,
                  "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c52767489ca42b8349b5bc5683443a466d6c3b2e"
                }
              ],
              "s": {
                "y": 1253,
                "x": 1907,
                "u": "https://preview.redd.it/eltdxfn8i8ef1.png?width=1907&amp;format=png&amp;auto=webp&amp;s=8c2d9c0743ca5a01676db6ac6e4d7b30a0911c27"
              },
              "id": "eltdxfn8i8ef1"
            },
            "ca3mb75rf8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 70,
                  "x": 108,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=cf1a822b143343c8c2f1c4a91351751e74e37014"
                },
                {
                  "y": 141,
                  "x": 216,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4cad9a7097453af844b2072df10473d63407168f"
                },
                {
                  "y": 209,
                  "x": 320,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=816bdd87a87ae2ef4a2d2205f7b9e29f38489c91"
                },
                {
                  "y": 419,
                  "x": 640,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6d0905850f6ee9cdef78d85c64fea4ac9e5f1916"
                },
                {
                  "y": 628,
                  "x": 960,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=db14232380210a75035e339554d958dbf35b7d3a"
                },
                {
                  "y": 707,
                  "x": 1080,
                  "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=57e792572da9152faf1d00aa981a55b8306a3b78"
                }
              ],
              "s": {
                "y": 1248,
                "x": 1906,
                "u": "https://preview.redd.it/ca3mb75rf8ef1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=d74d7bdeda2dd3fe454bc67dbc6906f9c203ce20"
              },
              "id": "ca3mb75rf8ef1"
            },
            "09e7cb48f8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 71,
                  "x": 108,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7deeff37c4941bdf28a815588390a855ed862bd0"
                },
                {
                  "y": 143,
                  "x": 216,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=625263ce51714f18f7788cf0ad81624820b84b23"
                },
                {
                  "y": 212,
                  "x": 320,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3974e35855f594aed564ee278aaced85fd3c1769"
                },
                {
                  "y": 425,
                  "x": 640,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0fe5e13cb68b69fc904ce5dc9c280c1cf92a18e8"
                },
                {
                  "y": 638,
                  "x": 960,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1d0f7e62ef3919918eaa711708200e1f4a6e545e"
                },
                {
                  "y": 718,
                  "x": 1080,
                  "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71a39d1d3270cb5f31192d2137b97dfd051e6b2b"
                }
              ],
              "s": {
                "y": 1266,
                "x": 1902,
                "u": "https://preview.redd.it/09e7cb48f8ef1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=ec11ef011d0468b0f6f6bfc269287e472701bffe"
              },
              "id": "09e7cb48f8ef1"
            }
          },
          "name": "t3_1m5lgtr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "ups": 23,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "qlq88ceum8ef1",
                "id": 710842483
              },
              {
                "media_id": "baogvk7lf8ef1",
                "id": 710842484
              },
              {
                "media_id": "kzzqul8nf8ef1",
                "id": 710842485
              },
              {
                "media_id": "4cgtiz1pf8ef1",
                "id": 710842486
              },
              {
                "media_id": "ca3mb75rf8ef1",
                "id": 710842487
              },
              {
                "media_id": "9xa2d1o5f8ef1",
                "id": 710842488
              },
              {
                "media_id": "09e7cb48f8ef1",
                "id": 710842489
              },
              {
                "media_id": "s968gxs0g8ef1",
                "id": 710842490
              },
              {
                "media_id": "ec0ih2x5g8ef1",
                "id": 710842491
              },
              {
                "media_id": "iv8y70kfg8ef1",
                "id": 710842492
              },
              {
                "media_id": "eltdxfn8i8ef1",
                "id": 710842493
              },
              {
                "media_id": "bkgben5hn8ef1",
                "id": 710842494
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 23,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/jkGttzBpvCgJdojYediLI8339DpfTnhALhUrjGFYPnA.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753110583,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/Tesslate/UIGEN-X-8B\"&gt;https://huggingface.co/Tesslate/UIGEN-X-8B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Just wanted to share a quick prompting guide for UIGEN-X (and that quants are available). Craft any system prompt (its not specific, so it will listen to you!)&lt;/p&gt;\n\n&lt;p&gt;So type out your prompt like this: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;[Action] [UI type or page] [Framework(s)] [Key features] [Style (optional)]&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Examples:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;Create a navbar using React + Tailwind CSS with logo, links, and mobile hamburger menu.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Build a SaaS dashboard with Next.js + TypeScript + shadcn/ui: pages for analytics, user settings, billing, and a landing page. Use glassmorphism style.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Generate a personal blog with SvelteKit + DaisyUI, mixing cyberpunk colors and minimalist layout. Responsive for mobile.&lt;/code&gt;&lt;/li&gt;\n&lt;li&gt;&lt;code&gt;Make a pricing table with React + Chakra UI, including monthly/yearly toggle, dark mode, and enterprise minimalism style.&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If it is within the context, then you can additionally add edits.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a prompt template:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;code&gt;Create a [UI type] using [Framework(s) + Libraries] with [Features]. [Optional: Use [Style] style]. [Optional: Add sample content or Unsplash images.]&lt;/code&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Additional things that are supported -&amp;gt; if you hand it Unsplash links or other pictures links, it should work. Make sure reasoning is on for this. This way, you can use it in Agentic or Function calling frameworks.&lt;/p&gt;\n\n&lt;p&gt;Remember, its only an 8B model!&lt;/p&gt;\n\n&lt;p&gt;We are currently training 14B, 32B, and 30A and refining the process. We hope to create a good local alternative to the popular coding / design models that are on the web. &lt;/p&gt;\n\n&lt;p&gt;Make sure to join the community for more support. (Link in Huggingface!)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m5lgtr",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5lgtr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "United-Rush4073",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lgtr/uigenx_8b_supports_react_headless_flutter_react/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m5lgtr",
          "subreddit_subscribers": 502515,
          "created_utc": 1753110583,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all. I am new to AI and Ollama. I have a 5070 TI and am running a bunch of 7b and a few 13b models and am wondering what some of your favorite models are for programming, general use, or pdf/image parsing. I'm interested in models that are below and above my GPUs thresholds. My lower models hallucinate way too much with significant tasks so I'm interested in those for some of my weaker workflows such as summarizing (phi2 and 3 struggle). Are there any LLMs that can compete with enterprise models for programming if you use RTX 5090, 6000, or a cluster of reasonably priced GPUs? \n\nMost threads discuss models that are good for generic users, but I would love to hear about what the best is when it comes to open-source models as well as what you guys use the most for workflows, personal, and programming (alternative to copilot could be cool).\n\n  \nThank you for any resources!",
          "author_fullname": "t2_upzzregs",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best Local Models Per Budget Per Use Case",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5lg47",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753110539,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all. I am new to AI and Ollama. I have a 5070 TI and am running a bunch of 7b and a few 13b models and am wondering what some of your favorite models are for programming, general use, or pdf/image parsing. I&amp;#39;m interested in models that are below and above my GPUs thresholds. My lower models hallucinate way too much with significant tasks so I&amp;#39;m interested in those for some of my weaker workflows such as summarizing (phi2 and 3 struggle). Are there any LLMs that can compete with enterprise models for programming if you use RTX 5090, 6000, or a cluster of reasonably priced GPUs? &lt;/p&gt;\n\n&lt;p&gt;Most threads discuss models that are good for generic users, but I would love to hear about what the best is when it comes to open-source models as well as what you guys use the most for workflows, personal, and programming (alternative to copilot could be cool).&lt;/p&gt;\n\n&lt;p&gt;Thank you for any resources!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5lg47",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Expensive-Fail3009",
          "discussion_type": null,
          "num_comments": 14,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lg47/best_local_models_per_budget_per_use_case/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5lg47/best_local_models_per_budget_per_use_case/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753110539,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": ";tldr when voice cloning use a high-end microphone not the one built-in to your computer/airpods\n\n\nI have a child that has reading difficulties. They need to be able to read 15 books this coming year and I was lucky enough to be able to find out what those 15 books are. Many of them are from the 1920s and earlier. They‚Äôre relatively unpopular and do not have existing audiobooks available. A number of them aren‚Äôt even sold as Ebooks (yes we are all aghast).\n\nEnter manually scanning ick\n\nSo I used my colleagues audiobook generator with my local rig. Each book gets chunked into around 1500 to 2000 chunks. My initial recording was on AirPods and/or a local microphone inside my MacBook.\n\nWith those recordings (I had two different ones) I had a 35 to 40% error rate which often persisted even when I was trying to generate 10 attempts. \n\nI happened to pick up a prosumer voice recorder to be able to do interviews with older relatives as an audio genealogical history. When I recorded my voice with those reading the exact same script as the other two recordings I went to a 5 to 10% air rate with three shots. Mostly closer to 5% but sometimes up to 10% \n\nFor everyone who is having issues with their voice recording cloning, you may want to consider the quality of your microphone. I would have assumed that for an expressive reading of an audiobook it would be fine to just use decent quality hardware microphones. I was shocked at the improvement levels in the transcription passes and the output. It‚Äôs relatively obvious after I say it out loud, but I don‚Äôt see many people talking about it (too basic for the experts in the space and not something that the novices immediately intuit perhaps) so I thought I‚Äôd share.",
          "author_fullname": "t2_i6kuh6b9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chatterbox tts microphone results",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5lf6l",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753110478,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;;tldr when voice cloning use a high-end microphone not the one built-in to your computer/airpods&lt;/p&gt;\n\n&lt;p&gt;I have a child that has reading difficulties. They need to be able to read 15 books this coming year and I was lucky enough to be able to find out what those 15 books are. Many of them are from the 1920s and earlier. They‚Äôre relatively unpopular and do not have existing audiobooks available. A number of them aren‚Äôt even sold as Ebooks (yes we are all aghast).&lt;/p&gt;\n\n&lt;p&gt;Enter manually scanning ick&lt;/p&gt;\n\n&lt;p&gt;So I used my colleagues audiobook generator with my local rig. Each book gets chunked into around 1500 to 2000 chunks. My initial recording was on AirPods and/or a local microphone inside my MacBook.&lt;/p&gt;\n\n&lt;p&gt;With those recordings (I had two different ones) I had a 35 to 40% error rate which often persisted even when I was trying to generate 10 attempts. &lt;/p&gt;\n\n&lt;p&gt;I happened to pick up a prosumer voice recorder to be able to do interviews with older relatives as an audio genealogical history. When I recorded my voice with those reading the exact same script as the other two recordings I went to a 5 to 10% air rate with three shots. Mostly closer to 5% but sometimes up to 10% &lt;/p&gt;\n\n&lt;p&gt;For everyone who is having issues with their voice recording cloning, you may want to consider the quality of your microphone. I would have assumed that for an expressive reading of an audiobook it would be fine to just use decent quality hardware microphones. I was shocked at the improvement levels in the transcription passes and the output. It‚Äôs relatively obvious after I say it out loud, but I don‚Äôt see many people talking about it (too basic for the experts in the space and not something that the novices immediately intuit perhaps) so I thought I‚Äôd share.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5lf6l",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "olympics2022wins",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5lf6l/chatterbox_tts_microphone_results/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5lf6l/chatterbox_tts_microphone_results/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753110478,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Are there any newer models (&lt;50 days old) that are well equipped to handle coding, especially in SQL? Hoping to find something under 24b. Currently running:\n\n*  unsloth qwen3-14b Q4\\_K\\_S for general tasks\n* mistralai/mistral-small-3.2 for some stuff like writing\n* qwen2.5-coder-14b-instruct-q4\\_k\\_m - general coding tasks, not great\n\n  \n",
          "author_fullname": "t2_1sznzjx7fy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Seeking the newest coding models, especially for SQL?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5l8e4",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753110054,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Are there any newer models (&amp;lt;50 days old) that are well equipped to handle coding, especially in SQL? Hoping to find something under 24b. Currently running:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt; unsloth qwen3-14b Q4_K_S for general tasks&lt;/li&gt;\n&lt;li&gt;mistralai/mistral-small-3.2 for some stuff like writing&lt;/li&gt;\n&lt;li&gt;qwen2.5-coder-14b-instruct-q4_k_m - general coding tasks, not great&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5l8e4",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "datascientist2964",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5l8e4/seeking_the_newest_coding_models_especially_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5l8e4/seeking_the_newest_coding_models_especially_for/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753110054,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone!\n\nI recently built an office hours page for anyone who has questions on cloud GPUs or GPUs in general. we are a bunch of engineers who've built at Google, Dropbox, Alchemy, Tesla etc. and would love to help anyone who has questions in this area.¬†[https://computedeck.com/office-hours](https://computedeck.com/office-hours)\n\nWe welcome any feedback as well!\n\nCheers!",
          "author_fullname": "t2_jrvdg16pa",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Office hours for cloud GPU",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5l52r",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753109849,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I recently built an office hours page for anyone who has questions on cloud GPUs or GPUs in general. we are a bunch of engineers who&amp;#39;ve built at Google, Dropbox, Alchemy, Tesla etc. and would love to help anyone who has questions in this area.¬†&lt;a href=\"https://computedeck.com/office-hours\"&gt;https://computedeck.com/office-hours&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;We welcome any feedback as well!&lt;/p&gt;\n\n&lt;p&gt;Cheers!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5l52r",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No-Scarcity-8746",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5l52r/office_hours_for_cloud_gpu/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5l52r/office_hours_for_cloud_gpu/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753109849,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Every runtime has its own folder for model storage, but in a lot of cases this means downloading the same model multiple times and using extra disk space. Do we think there could be a standard \"common\" location for models? e.g., why don't I have a \"gguf\" folder for everyone to use?",
          "author_fullname": "t2_2vd86cjp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Common folder for model storage?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5l0v3",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753109581,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Every runtime has its own folder for model storage, but in a lot of cases this means downloading the same model multiple times and using extra disk space. Do we think there could be a standard &amp;quot;common&amp;quot; location for models? e.g., why don&amp;#39;t I have a &amp;quot;gguf&amp;quot; folder for everyone to use?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5l0v3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "mherf",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5l0v3/common_folder_for_model_storage/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5l0v3/common_folder_for_model_storage/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753109581,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "(Latest update: 21/07/2025)\n\nI've just extracted the FULL Windsurf system prompt and internal tools (Wave 11 update). Over 500 lines (Around 9.6k tokens).\n\nYou can check it out¬†here: [https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf)",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "FULL Windsurf System Prompt and Tools [UPDATED, Wave 11]",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5kqk8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753108909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Latest update: 21/07/2025)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve just extracted the FULL Windsurf system prompt and internal tools (Wave 11 update). Over 500 lines (Around 9.6k tokens).&lt;/p&gt;\n\n&lt;p&gt;You can check it out¬†here: &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools/tree/main/Windsurf&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5kqk8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5kqk8/full_windsurf_system_prompt_and_tools_updated/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5kqk8/full_windsurf_system_prompt_and_tools_updated/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753108909,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I need it to be as similar as possible with my voice, so people on Youtube won't notice if I'm using my voice or a TTS.\n\nAlso I have only a nvidia GTX 1660 Super with 6 GB of ram, so I don't want to clone it every time I have a text, just clone at a time with the best, leave it for a couple of hours, and hen use it each time I need it.\n\nI also saw some that only let you do 300 characters a t a time, which is too slow, because I usually have 1000 - 2000 words.\n\nSo is something that you can reccomand? Even if is not more than 5-10 $ a month, but have available over 10 hours each month will be good for me.\n\nAlso if it can also use the Romanian language, it would be even better, but is ok only with English.",
          "author_fullname": "t2_izlog",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What free TTS is the best to clone my voice for reading large portions of text?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5kmxl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753108666,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I need it to be as similar as possible with my voice, so people on Youtube won&amp;#39;t notice if I&amp;#39;m using my voice or a TTS.&lt;/p&gt;\n\n&lt;p&gt;Also I have only a nvidia GTX 1660 Super with 6 GB of ram, so I don&amp;#39;t want to clone it every time I have a text, just clone at a time with the best, leave it for a couple of hours, and hen use it each time I need it.&lt;/p&gt;\n\n&lt;p&gt;I also saw some that only let you do 300 characters a t a time, which is too slow, because I usually have 1000 - 2000 words.&lt;/p&gt;\n\n&lt;p&gt;So is something that you can reccomand? Even if is not more than 5-10 $ a month, but have available over 10 hours each month will be good for me.&lt;/p&gt;\n\n&lt;p&gt;Also if it can also use the Romanian language, it would be even better, but is ok only with English.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5kmxl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "shaggy98",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5kmxl/what_free_tts_is_the_best_to_clone_my_voice_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5kmxl/what_free_tts_is_the_best_to_clone_my_voice_for/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753108666,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey everyone,\n\nWe‚Äôre the team at Software Mansion, the creators and maintainers of the **react-native-executorch** library, which allows developers to run PyTorch ExecuTorch models inside React Native apps.\n\nAfter releasing the library, we realized a major hurdle for the community was the lack of a simple way to test, benchmark, and just *play* with LLMs on a mobile device without a complex setup.\n\nTo solve this, we created **Private Mind**. An open-source app that acts as a testing **utility** with one primary goal: to give developers and enthusiasts a dead-simple way to see how LLMs perform via ExecuTorch.\n\nIt's a tool built for this community. Here‚Äôs what it's designed for:\n\n* **A Lab for Your Models:** The main feature is loading your own custom models. If you can export it to the .pte format, you can run it in the app and interact with it through a basic chat interface.\n* **Pure On-Device Benchmarking:** Select any model and run a benchmark to see exactly how it performs on your hardware. You get crucial stats like tokens/second, memory usage, and time to first token. It‚Äôs a direct way to test the efficiency of your model or our library.\n* **A Reference Implementation:** Since we built the underlying library, the app serves as a blueprint. You can check the GitHub repo to see our recommended practices for implementing react-native-executorch in a real-world application.\n* **100% Local &amp; Private:** True to the ExecuTorch spirit, everything is on-device. Your models, chats, and benchmark data never leave your phone, making it a safe environment for experimentation.\n\n**Our Roadmap is About Improving the Testing Toolkit:**\n\nWe are actively working to enhance Private Mind as a testing utility. Next up is a new LLM runner that will expose parameters like **temperature** and **top\\_k** for more nuanced testing. After that, we plan to show how to implement more advanced use-cases like on-device **RAG** and **speech-to-text**. We'll also add **Gemma 3n** support as soon as it's fully compatible with the ExecuTorch.\n\n**Links:**\n\n* **App Store (iOS):** [https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2](https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2)\n* **Google Play (Android):** [https://play.google.com/store/apps/details?id=com.swmansion.privatemind](https://play.google.com/store/apps/details?id=com.swmansion.privatemind)\n* **GitHub (Check the code &amp; contribute):** [https://github.com/software-mansion-labs/private-mind](https://github.com/software-mansion-labs/private-mind)\n* **Our Pre-Exported Models for Testing:** [https://huggingface.co/software-mansion](https://huggingface.co/software-mansion)\n\nWe've built the foundation, and now we want the community to shape what's next. Let us know in the comments: What's the killer feature you're missing from other local AI apps?\n\n",
          "author_fullname": "t2_at32qlyr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "As the creators of react-native-executorch, we built an open-source app for testing ExecuTorch LLMs on mobile.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5k88s",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wklalir6l8ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/wklalir6l8ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wklalir6l8ef1/DASHPlaylist.mpd?a=1755738678%2CYTIyZDBjZmJlYTMzOTcyYmY0ZjQzYTIyOTQ5NGJkMjgwNGNiYzg4YjFlZDk1MjE4MWVmYjc1ZWViNWE3Zjc4Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 34,
              "hls_url": "https://v.redd.it/wklalir6l8ef1/HLSPlaylist.m3u8?a=1755738678%2CY2VkMDcwN2MwMDNjMWVhNjlhOGFmMjMzYTMyM2QyNDdiN2M0M2MxNzZkNDdkYTNiZjIxOWIxMTlmZmFkYzlmNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=f08deec1a65295a160b21c3f122845cd4e3bf129",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "hosted:video",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753107717,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "v.redd.it",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;We‚Äôre the team at Software Mansion, the creators and maintainers of the &lt;strong&gt;react-native-executorch&lt;/strong&gt; library, which allows developers to run PyTorch ExecuTorch models inside React Native apps.&lt;/p&gt;\n\n&lt;p&gt;After releasing the library, we realized a major hurdle for the community was the lack of a simple way to test, benchmark, and just &lt;em&gt;play&lt;/em&gt; with LLMs on a mobile device without a complex setup.&lt;/p&gt;\n\n&lt;p&gt;To solve this, we created &lt;strong&gt;Private Mind&lt;/strong&gt;. An open-source app that acts as a testing &lt;strong&gt;utility&lt;/strong&gt; with one primary goal: to give developers and enthusiasts a dead-simple way to see how LLMs perform via ExecuTorch.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a tool built for this community. Here‚Äôs what it&amp;#39;s designed for:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;A Lab for Your Models:&lt;/strong&gt; The main feature is loading your own custom models. If you can export it to the .pte format, you can run it in the app and interact with it through a basic chat interface.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Pure On-Device Benchmarking:&lt;/strong&gt; Select any model and run a benchmark to see exactly how it performs on your hardware. You get crucial stats like tokens/second, memory usage, and time to first token. It‚Äôs a direct way to test the efficiency of your model or our library.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;A Reference Implementation:&lt;/strong&gt; Since we built the underlying library, the app serves as a blueprint. You can check the GitHub repo to see our recommended practices for implementing react-native-executorch in a real-world application.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;100% Local &amp;amp; Private:&lt;/strong&gt; True to the ExecuTorch spirit, everything is on-device. Your models, chats, and benchmark data never leave your phone, making it a safe environment for experimentation.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Our Roadmap is About Improving the Testing Toolkit:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We are actively working to enhance Private Mind as a testing utility. Next up is a new LLM runner that will expose parameters like &lt;strong&gt;temperature&lt;/strong&gt; and &lt;strong&gt;top_k&lt;/strong&gt; for more nuanced testing. After that, we plan to show how to implement more advanced use-cases like on-device &lt;strong&gt;RAG&lt;/strong&gt; and &lt;strong&gt;speech-to-text&lt;/strong&gt;. We&amp;#39;ll also add &lt;strong&gt;Gemma 3n&lt;/strong&gt; support as soon as it&amp;#39;s fully compatible with the ExecuTorch.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;App Store (iOS):&lt;/strong&gt; &lt;a href=\"https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2\"&gt;https://apps.apple.com/gb/app/private-mind/id6746713439?uo=2&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Google Play (Android):&lt;/strong&gt; &lt;a href=\"https://play.google.com/store/apps/details?id=com.swmansion.privatemind\"&gt;https://play.google.com/store/apps/details?id=com.swmansion.privatemind&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;GitHub (Check the code &amp;amp; contribute):&lt;/strong&gt; &lt;a href=\"https://github.com/software-mansion-labs/private-mind\"&gt;https://github.com/software-mansion-labs/private-mind&lt;/a&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Our Pre-Exported Models for Testing:&lt;/strong&gt; &lt;a href=\"https://huggingface.co/software-mansion\"&gt;https://huggingface.co/software-mansion&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;We&amp;#39;ve built the foundation, and now we want the community to shape what&amp;#39;s next. Let us know in the comments: What&amp;#39;s the killer feature you&amp;#39;re missing from other local AI apps?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://v.redd.it/wklalir6l8ef1",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?format=pjpg&amp;auto=webp&amp;s=cb3ee8401caeca210218133a62d8771186f20c4d",
                  "width": 1920,
                  "height": 1080
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5c123e9bd18470d9a529c9f9011d628a6e5654df",
                    "width": 108,
                    "height": 60
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=5e2e37157dddfdd5367559da87922b0f546d2fd5",
                    "width": 216,
                    "height": 121
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=43236ed4e39eacdff880a0d6561613ba17cbdbba",
                    "width": 320,
                    "height": 180
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=08314b98d437c5e47a6df7bd23f7b59279c831a1",
                    "width": 640,
                    "height": 360
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=2783c53c9f7fd07aca4ea92f3adce88adf901a2a",
                    "width": 960,
                    "height": 540
                  },
                  {
                    "url": "https://external-preview.redd.it/cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=eb7472f4d33b976d4fa394d8ce0bd94db917c7ac",
                    "width": 1080,
                    "height": 607
                  }
                ],
                "variants": {},
                "id": "cHR6ODJocjZsOGVmMZV30yMKuQdI_rwvJdDlghpHCSx7AthMheshPNDWRdWC"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5k88s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "K4anan",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5k88s/as_the_creators_of_reactnativeexecutorch_we_built/",
          "stickied": false,
          "url": "https://v.redd.it/wklalir6l8ef1",
          "subreddit_subscribers": 502515,
          "created_utc": 1753107717,
          "num_crossposts": 0,
          "media": {
            "reddit_video": {
              "bitrate_kbps": 5000,
              "fallback_url": "https://v.redd.it/wklalir6l8ef1/DASH_1080.mp4?source=fallback",
              "has_audio": true,
              "height": 1080,
              "width": 1920,
              "scrubber_media_url": "https://v.redd.it/wklalir6l8ef1/DASH_96.mp4",
              "dash_url": "https://v.redd.it/wklalir6l8ef1/DASHPlaylist.mpd?a=1755738678%2CYTIyZDBjZmJlYTMzOTcyYmY0ZjQzYTIyOTQ5NGJkMjgwNGNiYzg4YjFlZDk1MjE4MWVmYjc1ZWViNWE3Zjc4Yg%3D%3D&amp;v=1&amp;f=sd",
              "duration": 34,
              "hls_url": "https://v.redd.it/wklalir6l8ef1/HLSPlaylist.m3u8?a=1755738678%2CY2VkMDcwN2MwMDNjMWVhNjlhOGFmMjMzYTMyM2QyNDdiN2M0M2MxNzZkNDdkYTNiZjIxOWIxMTlmZmFkYzlmNg%3D%3D&amp;v=1&amp;f=sd",
              "is_gif": false,
              "transcoding_status": "completed"
            }
          },
          "is_video": true
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "The above file is ~2TB in size. \n\nI went to HyperStack and the A100 80GB GPU was like ~1.35/hr. to run. So, I gave them $5 and signed up. I have zero GPU cloud experience and I didn't realize that the 2TB SSD I would be renting from them would come out to roughly $140/mo...or about the same cost as a brand new 2TB SSD.\n\nCan anyone suggest a cloud provider that will allow me to run BF16 or ~Q8 without spending an arm and a leg? This is for personal (freelance work) use. \n\nI would have no problem spinning up a new instance in the morning but waiting however long for the 2TB LLM to download is not appealing. \n\nAm I missing something here? I had Claude4 advising me and it didn't provide any better suggestions. \n\nI only need the server for ~3-4 hours (total run time) per day, 5 days a week. And I would prefer \"no logs\" because the work I do will have my client's company name (no sensitive info) and who knows who does what with your data--I don't want my client's names being used for training.",
          "author_fullname": "t2_1rb00zjeto",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What is the cheapest way to run unsloth/Kimi-K2-Instruct-GGUF BF16 in the cloud?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5k5di",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753107526,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The above file is ~2TB in size. &lt;/p&gt;\n\n&lt;p&gt;I went to HyperStack and the A100 80GB GPU was like ~1.35/hr. to run. So, I gave them $5 and signed up. I have zero GPU cloud experience and I didn&amp;#39;t realize that the 2TB SSD I would be renting from them would come out to roughly $140/mo...or about the same cost as a brand new 2TB SSD.&lt;/p&gt;\n\n&lt;p&gt;Can anyone suggest a cloud provider that will allow me to run BF16 or ~Q8 without spending an arm and a leg? This is for personal (freelance work) use. &lt;/p&gt;\n\n&lt;p&gt;I would have no problem spinning up a new instance in the morning but waiting however long for the 2TB LLM to download is not appealing. &lt;/p&gt;\n\n&lt;p&gt;Am I missing something here? I had Claude4 advising me and it didn&amp;#39;t provide any better suggestions. &lt;/p&gt;\n\n&lt;p&gt;I only need the server for ~3-4 hours (total run time) per day, 5 days a week. And I would prefer &amp;quot;no logs&amp;quot; because the work I do will have my client&amp;#39;s company name (no sensitive info) and who knows who does what with your data--I don&amp;#39;t want my client&amp;#39;s names being used for training.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5k5di",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OsakaSeafoodConcrn",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5k5di/what_is_the_cheapest_way_to_run/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5k5di/what_is_the_cheapest_way_to_run/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753107526,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi folks, sanity check. I have a MacBook Air M3 with 24 GB RAM and 512 GB SSD. I want to run a local LLM for (1) drafting emails, (2) writing posts, and (3) occasional Python/JavaScript coding help (no huge repos, just snippets or debugging).  \n\nFrom what I‚Äôve read, Llama 3.1 8B Instruct (4-bit Q4_K_M) is solid for text, while DeepSeek Coder 6.7B is praised for code. I‚Äôm leaning toward Ollama for simplicity.  \n\nQuestions:  \n1. Does 8B handle light coding well, or should I jump to a 13‚Äì14 B model like CodeLlama 13B or Phi-4 14B?  \n\n2. For those with similar setups, what tokens/sec are you seeing in Ollama or LM Studio?  \n\n3. Any hidden pitfalls with 24 GB RAM when context length creeps up?  \n\nAppreciate any real world experiences!",
          "author_fullname": "t2_14a7aclidp",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "MacBook Air M3 24 GB Ram best LOCAL LLM for email drafting, Reddit posts, and light coding?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5jr4s",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.56,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753106577,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, sanity check. I have a MacBook Air M3 with 24 GB RAM and 512 GB SSD. I want to run a local LLM for (1) drafting emails, (2) writing posts, and (3) occasional Python/JavaScript coding help (no huge repos, just snippets or debugging).  &lt;/p&gt;\n\n&lt;p&gt;From what I‚Äôve read, Llama 3.1 8B Instruct (4-bit Q4_K_M) is solid for text, while DeepSeek Coder 6.7B is praised for code. I‚Äôm leaning toward Ollama for simplicity.  &lt;/p&gt;\n\n&lt;p&gt;Questions:&lt;br/&gt;\n1. Does 8B handle light coding well, or should I jump to a 13‚Äì14 B model like CodeLlama 13B or Phi-4 14B?  &lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;For those with similar setups, what tokens/sec are you seeing in Ollama or LM Studio?  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Any hidden pitfalls with 24 GB RAM when context length creeps up?  &lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Appreciate any real world experiences!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5jr4s",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ihllegal",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5jr4s/macbook_air_m3_24_gb_ram_best_local_llm_for_email/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5jr4s/macbook_air_m3_24_gb_ram_best_local_llm_for_email/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753106577,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Inspired by the brain's hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.\n\nThough not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. üåü\n\nhttps://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f\n\nüìÑPaper: [https://arxiv.org/abs/2506.21734](https://arxiv.org/abs/2506.21734)\n\nüíªCode: [https://github.com/sapientinc/HRM](https://github.com/sapientinc/HRM)  \n",
          "author_fullname": "t2_dkj51uv0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[New Architecture] Hierarchical Reasoning Model",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "New Model"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 34,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "uslhwa2nh8ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 26,
                  "x": 108,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5ab6ae97e133c2649b92b0c8c0ef857c3d79bcc2"
                },
                {
                  "y": 53,
                  "x": 216,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0bc3c540c30cdf2b3b12433e760b60c3ceef936c"
                },
                {
                  "y": 79,
                  "x": 320,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=056aa9e724c8d47937b0e044af74a221fc2a9886"
                },
                {
                  "y": 158,
                  "x": 640,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7a710e5fa6141d56c1a472f9ac1de521e98a86a5"
                },
                {
                  "y": 237,
                  "x": 960,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ccd9b2f1992c3c691f918fd9309433d1e043fe2"
                },
                {
                  "y": 267,
                  "x": 1080,
                  "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc4733d2af3bd39a37a4c49868c5e8afeca4ff94"
                }
              ],
              "s": {
                "y": 501,
                "x": 2026,
                "u": "https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;format=png&amp;auto=webp&amp;s=b7572924d3c565f89605da339cda1df0dc96354f"
              },
              "id": "uslhwa2nh8ef1"
            }
          },
          "name": "t3_1m5jr1v",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.98,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 64,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "New Model",
          "can_mod_post": false,
          "score": 64,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/jTPeTr-ZhJfvZ_xmMKlbONUjqH188dSuwhEIvPibXE8.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753106572,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by the brain&amp;#39;s hierarchical processing, HRM unlocks unprecedented reasoning capabilities on complex tasks like ARC-AGI and solving master-level Sudoku using just 1k training examples, without any pretraining or CoT.&lt;/p&gt;\n\n&lt;p&gt;Though not a general language model yet, with significant computational depth, HRM possibly unlocks next-gen reasoning and long-horizon planning paradigm beyond CoT. üåü&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f\"&gt;https://preview.redd.it/uslhwa2nh8ef1.png?width=2026&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7572924d3c565f89605da339cda1df0dc96354f&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;üìÑPaper: &lt;a href=\"https://arxiv.org/abs/2506.21734\"&gt;https://arxiv.org/abs/2506.21734&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;üíªCode: &lt;a href=\"https://github.com/sapientinc/HRM\"&gt;https://github.com/sapientinc/HRM&lt;/a&gt;  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ffb000",
          "id": "1m5jr1v",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "imonenext",
          "discussion_type": null,
          "num_comments": 6,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5jr1v/new_architecture_hierarchical_reasoning_model/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753106572,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_rkmud0isr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "The reason why local models are better/necessary.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Funny"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 82,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5iymb",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "ups": 237,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": true,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Funny",
          "can_mod_post": false,
          "score": 237,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_0QOwkUP9B0YXB-SAHakr_6UlxBhOQpPKlYLA0LCuiQ.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "image",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753104611,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "i.redd.it",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://i.redd.it/vdngpglhb8ef1.png",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://preview.redd.it/vdngpglhb8ef1.png?auto=webp&amp;s=62e9d97048e91daee3582390075fb00d1887e202",
                  "width": 854,
                  "height": 504
                },
                "resolutions": [
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f4c8c8ea760457e111d37a839bbe4882b86b520",
                    "width": 108,
                    "height": 63
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=adc1595b273f56bd1537f9efa03ae0704717b281",
                    "width": 216,
                    "height": 127
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fca7b7544874c7613248f95246d1810ffb9ffb7b",
                    "width": 320,
                    "height": 188
                  },
                  {
                    "url": "https://preview.redd.it/vdngpglhb8ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ae855f8543a7b34526b58ea6c68423bf02a9e2ac",
                    "width": 640,
                    "height": 377
                  }
                ],
                "variants": {},
                "id": "f9sIdJ3aHKBFJd8WvU4XqZxgrL8gg-EBXrNrZSagJuw"
              }
            ],
            "enabled": true
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "65c366b0-bf8e-11ed-86ac-725137141d5f",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#0dd3bb",
          "id": "1m5iymb",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "GPTshop_ai",
          "discussion_type": null,
          "num_comments": 121,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5iymb/the_reason_why_local_models_are_betternecessary/",
          "stickied": false,
          "url": "https://i.redd.it/vdngpglhb8ef1.png",
          "subreddit_subscribers": 502515,
          "created_utc": 1753104611,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/\n\nIf a large part of Llama 4‚Äôs issues come from its attention chunking, then does llama 4 perform better within a single chunk? If we limit it to 8192 tokens (party like it‚Äôs 2023 lol) does it do okay? \n\nHow does Llama 4 perform if we play to its strengths?",
          "author_fullname": "t2_t6glzswk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does llama 4 perform within 8192 tokens?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ijhw",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.71,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753103525,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/\"&gt;https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If a large part of Llama 4‚Äôs issues come from its attention chunking, then does llama 4 perform better within a single chunk? If we limit it to 8192 tokens (party like it‚Äôs 2023 lol) does it do okay? &lt;/p&gt;\n\n&lt;p&gt;How does Llama 4 perform if we play to its strengths?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?auto=webp&amp;s=059b37746e9cad1e7f2a795a55941440482b2df1",
                  "width": 1200,
                  "height": 800
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=687935d5a0c79dc71a36c53a4a1ded1099fca1b3",
                    "width": 108,
                    "height": 72
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=16370dac432d77dbcdbb677cb00a130aa0992405",
                    "width": 216,
                    "height": 144
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e2d4dcaa25a74bf623abe25686ba0ce4b471a01c",
                    "width": 320,
                    "height": 213
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d780393f49ecad20755f2fd873330899c4ee6b34",
                    "width": 640,
                    "height": 426
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=091a5b5ae6479914a1a0bbd0a76b00683b4f0253",
                    "width": 960,
                    "height": 640
                  },
                  {
                    "url": "https://external-preview.redd.it/AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=fdd37342148f3e349565e947300c5d21cecde6f2",
                    "width": 1080,
                    "height": 720
                  }
                ],
                "variants": {},
                "id": "AkDm1vMK5drNSxMCCBXiLfoVmou_ZXYzxzpyyqp3sp4"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5ijhw",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "DepthHour1669",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ijhw/how_does_llama_4_perform_within_8192_tokens/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ijhw/how_does_llama_4_perform_within_8192_tokens/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753103525,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I keep wondering how Meta could have screwed up llama4 so bad and then released it?   At this point, everyone knows how to train a model and if you have the data and compute you can really release something good.  The bigger the \"smarter\"    They obviously know what to do based on what we saw with llama3.3, we even saw they improved by the smaller models from the 3.2 series.   Llama4 was so bad, they could have gone back, fixed things, retrained and we won't even know.  It wasn't like there was an aggressive release timeline.\n\nSo what if the original Llama4 was so damn good it shook and shocked them?   What if they decided that it's so close to AGI to release?  So instead of that,  they decided to make some really bad models, then released those models so we could laugh and not suspect how good and close they were.   Then Zuck decides to triple down and hire the best to now get the unreleased Llama4 to AGI?  \n\nThis is the only thing that makes sense to me.   There's no way they could have fucked up Llama4 that bad and then released it.  No way.   ... alright in some parallel universe, this is what happened right?",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What if Meta really has the best AI?  Hear me out.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Other"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5hksu",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.22,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Other",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753100920,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I keep wondering how Meta could have screwed up llama4 so bad and then released it?   At this point, everyone knows how to train a model and if you have the data and compute you can really release something good.  The bigger the &amp;quot;smarter&amp;quot;    They obviously know what to do based on what we saw with llama3.3, we even saw they improved by the smaller models from the 3.2 series.   Llama4 was so bad, they could have gone back, fixed things, retrained and we won&amp;#39;t even know.  It wasn&amp;#39;t like there was an aggressive release timeline.&lt;/p&gt;\n\n&lt;p&gt;So what if the original Llama4 was so damn good it shook and shocked them?   What if they decided that it&amp;#39;s so close to AGI to release?  So instead of that,  they decided to make some really bad models, then released those models so we could laugh and not suspect how good and close they were.   Then Zuck decides to triple down and hire the best to now get the unreleased Llama4 to AGI?  &lt;/p&gt;\n\n&lt;p&gt;This is the only thing that makes sense to me.   There&amp;#39;s no way they could have fucked up Llama4 that bad and then released it.  No way.   ... alright in some parallel universe, this is what happened right?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#94e044",
          "id": "1m5hksu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m5hksu/what_if_meta_really_has_the_best_ai_hear_me_out/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5hksu/what_if_meta_really_has_the_best_ai_hear_me_out/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753100920,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I mean running o3-like models or better on smartphones/laptop NPUs with only a few watts of power, in an \"easy way\" for typical consumers and non technical people. I bet 2 years away",
          "author_fullname": "t2_smr4fvghc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How far are we from *convenient* local models supremacy?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5h2td",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753099473,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I mean running o3-like models or better on smartphones/laptop NPUs with only a few watts of power, in an &amp;quot;easy way&amp;quot; for typical consumers and non technical people. I bet 2 years away&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5h2td",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Element_H2O",
          "discussion_type": null,
          "num_comments": 34,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5h2td/how_far_are_we_from_convenient_local_models/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753099473,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello there,\n\nMy project to extract and collect the \"secret\" system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it's incredibly useful.\n\n**The idea is to see the advanced \"prompt architecture\" that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.**\n\nInstead of trying to reinvent the wheel, you can see exactly how they force models to \"think step-by-step\" in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It's a goldmine of ideas for crafting better system prompts.\n\nFor example, here's a small snippet from the Cursor prompt that shows how they establish the AI's role and capabilities right away:\n\n    Knowledge cutoff: 2024-06\n    \n    You are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n    \n    You are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n    \n    You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n    \n    Your main goal is to follow the USER's instructions at each message, denoted by the &lt;user_query&gt; tag.\n    \n    &lt;communication&gt;\n    When using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n    &lt;/communication&gt;\n\nI wrote a full article that does a deep dive into these patterns and also discusses the \"dual-use\" aspect of making these normally-hidden prompts public.\n\nI'm super curious: **How are you all structuring system prompts for your favorite models?**\n\n**Links:**\n\n* **The full article with more analysis:** [The Open Source Project That Became an Essential Library for Modern AI Engineering](https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------)[](https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------)\n\n* **The GitHub Repo (to grab the prompts):** [https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools](https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools)\n\nHope you find it useful!",
          "author_fullname": "t2_fbh7mxys2",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "I extracted the system prompts from closed-source tools like Cursor &amp; v0. The repo just hit 70k stars.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gwzs",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 284,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 284,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753099199,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753099002,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello there,&lt;/p&gt;\n\n&lt;p&gt;My project to extract and collect the &amp;quot;secret&amp;quot; system prompts from a bunch of proprietary AI tools just passed 70k stars on GitHub, and I wanted to share it with this community specifically because I think it&amp;#39;s incredibly useful.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The idea is to see the advanced &amp;quot;prompt architecture&amp;quot; that companies like Vercel, Cursor, etc., use to get high-quality results, so we can replicate those techniques on different platforms.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Instead of trying to reinvent the wheel, you can see exactly how they force models to &amp;quot;think step-by-step&amp;quot; in a scratchpad, how they define an expert persona with hyper-specific rules, or how they demand rigidly structured outputs. It&amp;#39;s a goldmine of ideas for crafting better system prompts.&lt;/p&gt;\n\n&lt;p&gt;For example, here&amp;#39;s a small snippet from the Cursor prompt that shows how they establish the AI&amp;#39;s role and capabilities right away:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Knowledge cutoff: 2024-06\n\nYou are an AI coding assistant, powered by GPT-4.1. You operate in Cursor. \n\nYou are pair programming with a USER to solve their coding task. Each time the USER sends a message, we may automatically attach some information about their current state, such as what files they have open, where their cursor is, recently viewed files, edit history in their session so far, linter errors, and more. This information may or may not be relevant to the coding task, it is up for you to decide.\n\nYou are an agent - please keep going until the user&amp;#39;s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. Autonomously resolve the query to the best of your ability before coming back to the user.\n\nYour main goal is to follow the USER&amp;#39;s instructions at each message, denoted by the &amp;lt;user_query&amp;gt; tag.\n\n&amp;lt;communication&amp;gt;\nWhen using markdown in assistant messages, use backticks to format file, directory, function, and class names. Use \\( and \\) for inline math, \\[ and \\] for block math.\n&amp;lt;/communication&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;I wrote a full article that does a deep dive into these patterns and also discusses the &amp;quot;dual-use&amp;quot; aspect of making these normally-hidden prompts public.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m super curious: &lt;strong&gt;How are you all structuring system prompts for your favorite models?&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The full article with more analysis:&lt;/strong&gt; &lt;a href=\"https://medium.com/@lucknitelol/the-open-source-project-that-became-an-essential-library-for-modern-ai-engineering-67021b50acee?source=user_profile_page---------0-------------d9a574987030----------------------\"&gt;The Open Source Project That Became an Essential Library for Modern AI Engineering&lt;/a&gt;&lt;a href=\"https://medium.com/@lucknitelol?source=post_page---byline--67021b50acee---------------------------------------\"&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;The GitHub Repo (to grab the prompts):&lt;/strong&gt; &lt;a href=\"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools\"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Hope you find it useful!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5gwzs",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Independent-Box-898",
          "discussion_type": null,
          "num_comments": 37,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gwzs/i_extracted_the_system_prompts_from_closedsource/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753099002,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anything above 8B that won't restrict anything due to ethics and can connect to MCP tools?",
          "author_fullname": "t2_4vs3cwbe",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which uncensored model that supports MCP can you recommend?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gnfm",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753098166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anything above 8B that won&amp;#39;t restrict anything due to ethics and can connect to MCP tools?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5gnfm",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ReasonableGarden9",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gnfm/which_uncensored_model_that_supports_mcp_can_you/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gnfm/which_uncensored_model_that_supports_mcp_can_you/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753098166,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey,\n\nI'm curious, what are people fine-tuning their models for? \n\nI was working in a company where we fine-tuned models to better deal with product images, but the company couldn't keep the lights on. Most agencies, companies, freelancers, seem to use off-the-shelf models, which are getting \"good enough\" for the job. \n\nSo, what are people fine-tuning their models for? and which companies, or industries, are most likely to be fine-tuning models? \n\nThanks, just an idiot asking! ",
          "author_fullname": "t2_at09kvrk",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are people fine-tuning their models for?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gmfr",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.95,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 19,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 19,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753098077,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m curious, what are people fine-tuning their models for? &lt;/p&gt;\n\n&lt;p&gt;I was working in a company where we fine-tuned models to better deal with product images, but the company couldn&amp;#39;t keep the lights on. Most agencies, companies, freelancers, seem to use off-the-shelf models, which are getting &amp;quot;good enough&amp;quot; for the job. &lt;/p&gt;\n\n&lt;p&gt;So, what are people fine-tuning their models for? and which companies, or industries, are most likely to be fine-tuning models? &lt;/p&gt;\n\n&lt;p&gt;Thanks, just an idiot asking! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5gmfr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MKBSP",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gmfr/what_are_people_finetuning_their_models_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gmfr/what_are_people_finetuning_their_models_for/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753098077,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi,\nI need a web interface for my local model but i need multi user support. Meaning i need a login and everyone needs their own chat history. \n\nAny ideas? (google and chatgpt/... were not helpful) ",
          "author_fullname": "t2_4kyff",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Chat webinterface for small company",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gl6e",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753097973,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,\nI need a web interface for my local model but i need multi user support. Meaning i need a login and everyone needs their own chat history. &lt;/p&gt;\n\n&lt;p&gt;Any ideas? (google and chatgpt/... were not helpful) &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5gl6e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "_ralph_",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gl6e/chat_webinterface_for_small_company/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gl6e/chat_webinterface_for_small_company/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753097973,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I can have my hands on about 100 MI50 16GB for 72$ each. Is this a good choice over rtx 3060 12gb (265$ used)? How about dual MI50?",
          "author_fullname": "t2_5plbh7ia",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "72$ for Instinct MI50 16GB",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ghs0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753097677,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I can have my hands on about 100 MI50 16GB for 72$ each. Is this a good choice over rtx 3060 12gb (265$ used)? How about dual MI50?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ghs0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "jetaudio",
          "discussion_type": null,
          "num_comments": 12,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ghs0/72_for_instinct_mi50_16gb/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ghs0/72_for_instinct_mi50_16gb/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753097677,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Tl;Dr: Microsoft API is confusing itself and the models, what should I use instead? And are there tool calls (agents?) that help models produce valid xml?\n\nHello,\n\nI'm currently trying to get into learning more about how I can improve my workflow with AI. So far I'm playing around with Qwen3 30b MoE and kimi-dev 72b models, and I'm impressed with their speed, thinking skills and how they're interpreting my task into sizeable chunks of work, even when the actual programming skills are ... lacking.\n\nThe problem however doesn't seem to come from the models itself, but from Microsoft. I've chosen C# and WinUI3, because that's what I am using at work right now, but because Microsoft has turned Windows desktop programming into a disjointed nightmare with releasing like a 100 different APIs and dialects, the AI seems to get confused. I'm specifically asking to only use WinUI3, but I'm getting remnants from Xanmarin, WPF, UMP and even MAUI tags in my xaml. (And from what I found on Google, it seems that even Microsoft's own copilot doesn't know how to deal with it)\n\nMy idea is, instead of trying to fix it, I should just learn a language that has better quality training data.\n\nSo my questions are:\n\n1) What language and UI framework do AI models have the most training data on?\n\n2) I also noticed that sometimes the generated xml has syntax errors, like missing closing tags. That sounds like something that could be improved by using existing tools. How do I get into this, and what is the current state of the art",
          "author_fullname": "t2_c1oznltu",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What programming language do AI Models have the best data on",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5gcvl",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753097246,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Tl;Dr: Microsoft API is confusing itself and the models, what should I use instead? And are there tool calls (agents?) that help models produce valid xml?&lt;/p&gt;\n\n&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently trying to get into learning more about how I can improve my workflow with AI. So far I&amp;#39;m playing around with Qwen3 30b MoE and kimi-dev 72b models, and I&amp;#39;m impressed with their speed, thinking skills and how they&amp;#39;re interpreting my task into sizeable chunks of work, even when the actual programming skills are ... lacking.&lt;/p&gt;\n\n&lt;p&gt;The problem however doesn&amp;#39;t seem to come from the models itself, but from Microsoft. I&amp;#39;ve chosen C# and WinUI3, because that&amp;#39;s what I am using at work right now, but because Microsoft has turned Windows desktop programming into a disjointed nightmare with releasing like a 100 different APIs and dialects, the AI seems to get confused. I&amp;#39;m specifically asking to only use WinUI3, but I&amp;#39;m getting remnants from Xanmarin, WPF, UMP and even MAUI tags in my xaml. (And from what I found on Google, it seems that even Microsoft&amp;#39;s own copilot doesn&amp;#39;t know how to deal with it)&lt;/p&gt;\n\n&lt;p&gt;My idea is, instead of trying to fix it, I should just learn a language that has better quality training data.&lt;/p&gt;\n\n&lt;p&gt;So my questions are:&lt;/p&gt;\n\n&lt;p&gt;1) What language and UI framework do AI models have the most training data on?&lt;/p&gt;\n\n&lt;p&gt;2) I also noticed that sometimes the generated xml has syntax errors, like missing closing tags. That sounds like something that could be improved by using existing tools. How do I get into this, and what is the current state of the art&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5gcvl",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "md_youdneverguess",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5gcvl/what_programming_language_do_ai_models_have_the/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5gcvl/what_programming_language_do_ai_models_have_the/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753097246,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I want to eventually build my own pc and host locally, mostly for the sake of reliability and not being reliant on the big guys in the bizz.\n\nMy main issue is that models such as Sonnet and Opus 4, even sonnet 3.5 performs so much better when it comes to coding, than what I've seen any locally run models being capable of. Not talking about open-source, as the new kimi model has shown a lot of promise, but it is too big to run locally.\n\n\nBut I am curious if it is possible to have specialized models which run locally, but perform equally to the big dogs.\n\nFor instance, if I train one local model to be my Python specialist, another for Flutter etc. Then I simply use the model I need, depending on the project.\n\nIs such a thing possible, to train local models like this and have them perform equally to the great Sonnet and Opus models, for programming purposes? Has anyone tried something similar already?",
          "author_fullname": "t2_7kei19es",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Is it possible to have a specialized local llm perform at the level of cloud based models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fwpz",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753095737,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to eventually build my own pc and host locally, mostly for the sake of reliability and not being reliant on the big guys in the bizz.&lt;/p&gt;\n\n&lt;p&gt;My main issue is that models such as Sonnet and Opus 4, even sonnet 3.5 performs so much better when it comes to coding, than what I&amp;#39;ve seen any locally run models being capable of. Not talking about open-source, as the new kimi model has shown a lot of promise, but it is too big to run locally.&lt;/p&gt;\n\n&lt;p&gt;But I am curious if it is possible to have specialized models which run locally, but perform equally to the big dogs.&lt;/p&gt;\n\n&lt;p&gt;For instance, if I train one local model to be my Python specialist, another for Flutter etc. Then I simply use the model I need, depending on the project.&lt;/p&gt;\n\n&lt;p&gt;Is such a thing possible, to train local models like this and have them perform equally to the great Sonnet and Opus models, for programming purposes? Has anyone tried something similar already?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5fwpz",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Relative_Mouse7680",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5fwpz/is_it_possible_to_have_a_specialized_local_llm/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753095737,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of \"ultra high bandwidth memory\", but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it's going to let an on device assistant use a lot more context.",
          "author_fullname": "t2_lkljr",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Rockchip unveils RK182X LLM co-processor: Runs Qwen 2.5 7B at 50TPS decode, 800TPS prompt processing",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 102,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fmlp",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.97,
          "author_flair_background_color": null,
          "ups": 132,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 132,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=140&amp;height=102&amp;crop=140:102,smart&amp;auto=webp&amp;s=f46b99f9160b02acaab35b0793c2419a717f902a",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753094763,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "cnx-software.com",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I believe this is the first NPU specifically designed for LLM inference. They specifically mention 2.5 or 5GB of &amp;quot;ultra high bandwidth memory&amp;quot;, but not the actual speed. 50TPS for a 7B model at Q4 implies around 200GB/s. The high prompt processing speed is the best part IMO, it&amp;#39;s going to let an on device assistant use a lot more context.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?auto=webp&amp;s=a3357893385aa57e61c85776502b465fe73661a4",
                  "width": 1184,
                  "height": 868
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=19d2a2efbd9333bbc8e7495d96c37dc9a67f94f7",
                    "width": 108,
                    "height": 79
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d4320f14e39faee0bd0ec022e73a0260326f90d4",
                    "width": 216,
                    "height": 158
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=36df6ea44afc6e837f534a02a66625d799b64a88",
                    "width": 320,
                    "height": 234
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0ec21866ca44554bfe2c9ada5521c937f241afc3",
                    "width": 640,
                    "height": 469
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f3f1fc5136f191685dd65bd8cc69f5a732d025a2",
                    "width": 960,
                    "height": 703
                  },
                  {
                    "url": "https://external-preview.redd.it/p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=24e96efb97efa4c62ad46fa709863ab37c9c8266",
                    "width": 1080,
                    "height": 791
                  }
                ],
                "variants": {},
                "id": "p-XdyFJrlRnofvAjkk2RhNaWbyuM0y_S5JEPvTprq-8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5fmlp",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PmMeForPCBuilds",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fmlp/rockchip_unveils_rk182x_llm_coprocessor_runs_qwen/",
          "stickied": false,
          "url": "https://www.cnx-software.com/2025/07/18/rockchip-unveils-rk3668-10-core-arm-cortex-a730-cortex-a530-soc-with-16-tops-npu-rk182x-llm-vlm-co-processor/#rockchip-rk182x-llm-vlm-accelerator",
          "subreddit_subscribers": 502515,
          "created_utc": 1753094763,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I completed my local LLM rig in May, just after Qwen3's release (thanks to r/LocalLLaMA 's folks for the invaluable guidance!). Now that I've settled into the setup, I'm excited to share my build and how it's performing with local LLMs.\n\nhttps://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\n\n  \n\n\nThis is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let's dive in!\n\n# Key Specs\n\n|Component|Specs|\n|:-|:-|\n|**CPU**|AMD Ryzen 7 7700 (8C/16T)|\n|**GPU**|2 x NVIDIA RTX 3090 (48GB VRAM total)|\n|**RAM**|64GB DDR5 @ 6400 MHz|\n|**Storage**|2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)|\n|**Motherboard**|ASUS TUF B650-PLUS|\n|**PSU**|850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)|\n|**Case**|Lian Li LANCOOL 216|\n|**Cooling**|a lot of fans üí®|\n\nTried to run the following:\n\n* **30B-A3B Q4\\_K\\_XL**, **32B Q4\\_K\\_XL** ‚Äì fit into one GPU with ample context window\n* **32B Q8\\_K\\_XL** ‚Äì runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference\n* **30B-A3B Q8\\_K\\_XL** ‚Äì now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model's context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I haven‚Äôt found an optimal way to store weights once and manage contexts separately, so this remains a WiP).\n\nPrimary use: Running Qwen3-30B-A3B models with `llama.cpp`. The performance for this model is \\~ 1000 pp512 / 100 tg128\n\nWhat's next? I think I will play with this one for a while. But... I'm already eyeing an EPYC-based system with 4x 4090s (48GB each). üòé",
          "author_fullname": "t2_1nkpqiujlm",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "My (practical) dual 3090 setup for local inference",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 105,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "wiim1ouai7ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/jpg",
              "p": [
                {
                  "y": 81,
                  "x": 108,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4174de20838a1b6b769288eb5b1b5f70f0f9602"
                },
                {
                  "y": 162,
                  "x": 216,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c538aa32a3f813976d5ce771e34c0d48b0440eaa"
                },
                {
                  "y": 240,
                  "x": 320,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=122f1edd42a0419bd780aa64f40f29f91c0ed509"
                },
                {
                  "y": 480,
                  "x": 640,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41b8c09a47c3c8926c73db4b4303c394b1b45f6f"
                },
                {
                  "y": 720,
                  "x": 960,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1eb4a1cb38cbbf6d104e970f4b00623ccd4033db"
                },
                {
                  "y": 810,
                  "x": 1080,
                  "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=720ffaef64226dedbcf3e6ecd6efd90763b1edcf"
                }
              ],
              "s": {
                "y": 961,
                "x": 1280,
                "u": "https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf"
              },
              "id": "wiim1ouai7ef1"
            }
          },
          "name": "t3_1m5fkts",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.82,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/vvE88N6ubX7Gsux9A2O-Hn5WBZJNPCIpKoUwojUDik4.jpg",
          "edited": 1753095094,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753094600,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I completed my local LLM rig in May, just after Qwen3&amp;#39;s release (thanks to &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; &amp;#39;s folks for the invaluable guidance!). Now that I&amp;#39;ve settled into the setup, I&amp;#39;m excited to share my build and how it&amp;#39;s performing with local LLMs.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\"&gt;https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let&amp;#39;s dive in!&lt;/p&gt;\n\n&lt;h1&gt;Key Specs&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Component&lt;/th&gt;\n&lt;th align=\"left\"&gt;Specs&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 7 7700 (8C/16T)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2 x NVIDIA RTX 3090 (48GB VRAM total)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;64GB DDR5 @ 6400 MHz&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;ASUS TUF B650-PLUS&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;Lian Li LANCOOL 216&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Cooling&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;a lot of fans üí®&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Tried to run the following:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;30B-A3B Q4_K_XL&lt;/strong&gt;, &lt;strong&gt;32B Q4_K_XL&lt;/strong&gt; ‚Äì fit into one GPU with ample context window&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;32B Q8_K_XL&lt;/strong&gt; ‚Äì runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;30B-A3B Q8_K_XL&lt;/strong&gt; ‚Äì now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model&amp;#39;s context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I haven‚Äôt found an optimal way to store weights once and manage contexts separately, so this remains a WiP).&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Primary use: Running Qwen3-30B-A3B models with &lt;code&gt;llama.cpp&lt;/code&gt;. The performance for this model is ~ 1000 pp512 / 100 tg128&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s next? I think I will play with this one for a while. But... I&amp;#39;m already eyeing an EPYC-based system with 4x 4090s (48GB each). üòé&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5fkts",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ColdImplement1319",
          "discussion_type": null,
          "num_comments": 13,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753094600,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey there, I have a RX 7800 XT 16GB and a summary prompt, looking for a model to run it.\n\nWhat are my issues? There are basically 2 main issues I have faced:\n1. Long context 32/64k tokens.\n2. Multi language.\n\nI have noticed that all models that give pretty decent quality are about 20b+ size. Quantized version can fit into 16GB VRAM but there is no place left for Cache.\nIf you offload Cache on RAM, prompt processing is really bad.\n\nI tried Gemma 3 27b, 32k message takes about an hour to process. Mistral 22b was faster, but is still about half an hour. All because of super slow PP.\n\n- Is there any advice how to speed it up?\n- Maybe you know small 8B model that performs good summarization on different languages? (English, Spanish, Portuguese, Chinese, Russian, Japanese, Korean,..)",
          "author_fullname": "t2_1umam7ln",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "HOWTO summarize on 16GB VRAM with 64k cache?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fg2y",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753094120,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey there, I have a RX 7800 XT 16GB and a summary prompt, looking for a model to run it.&lt;/p&gt;\n\n&lt;p&gt;What are my issues? There are basically 2 main issues I have faced:\n1. Long context 32/64k tokens.\n2. Multi language.&lt;/p&gt;\n\n&lt;p&gt;I have noticed that all models that give pretty decent quality are about 20b+ size. Quantized version can fit into 16GB VRAM but there is no place left for Cache.\nIf you offload Cache on RAM, prompt processing is really bad.&lt;/p&gt;\n\n&lt;p&gt;I tried Gemma 3 27b, 32k message takes about an hour to process. Mistral 22b was faster, but is still about half an hour. All because of super slow PP.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Is there any advice how to speed it up?&lt;/li&gt;\n&lt;li&gt;Maybe you know small 8B model that performs good summarization on different languages? (English, Spanish, Portuguese, Chinese, Russian, Japanese, Korean,..)&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5fg2y",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "COBECT",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fg2y/howto_summarize_on_16gb_vram_with_64k_cache/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5fg2y/howto_summarize_on_16gb_vram_with_64k_cache/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753094120,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello,\n\nI've been watching this thread for a while now and I'm looking for a laptop at around the 1500eur mark, and i can not decide for my usecase. I'm trying to build something basic, yet challenging. The plan is to make a local law assistant using RAG and a 7b modell, and learn more about the usecases of local LLMs.\n\nMy problem is that I travel a lot and therefore I can't have really reliable internet in hotels, etc. so I can't connect to my home PC, that has a 3090.\n\nSo I decided to get a laptop for myself. I have basically two choices, because of budget reasons.\n\n16\" MacBook Pro M1 Pro 32GB Ram (which would be used)\n\nor\n\nAsus Vivobook with Ryzen AI 9 370HX and 32GB Ram (which would be new)\n\nI'm pretty comfortable on both systems since I'm running a 16GB MBP right now, and a PC at home. Just performance wise what would be the better choice for my usecase?\n\nThank you all for your time, and have a great day!",
          "author_fullname": "t2_vgoid361",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Ryzen AI HX 370 or Mx Pro for travellers",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ff1k",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753095887,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753094017,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been watching this thread for a while now and I&amp;#39;m looking for a laptop at around the 1500eur mark, and i can not decide for my usecase. I&amp;#39;m trying to build something basic, yet challenging. The plan is to make a local law assistant using RAG and a 7b modell, and learn more about the usecases of local LLMs.&lt;/p&gt;\n\n&lt;p&gt;My problem is that I travel a lot and therefore I can&amp;#39;t have really reliable internet in hotels, etc. so I can&amp;#39;t connect to my home PC, that has a 3090.&lt;/p&gt;\n\n&lt;p&gt;So I decided to get a laptop for myself. I have basically two choices, because of budget reasons.&lt;/p&gt;\n\n&lt;p&gt;16&amp;quot; MacBook Pro M1 Pro 32GB Ram (which would be used)&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;Asus Vivobook with Ryzen AI 9 370HX and 32GB Ram (which would be new)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m pretty comfortable on both systems since I&amp;#39;m running a 16GB MBP right now, and a PC at home. Just performance wise what would be the better choice for my usecase?&lt;/p&gt;\n\n&lt;p&gt;Thank you all for your time, and have a great day!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ff1k",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "0ner0z",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ff1k/ryzen_ai_hx_370_or_mx_pro_for_travellers/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ff1k/ryzen_ai_hx_370_or_mx_pro_for_travellers/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753094017,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "",
          "author_fullname": "t2_1ipy2mlwcz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "NVIDIA Brings Reasoning Models to Consumers Ranging from 1.5B to 32B Parameters",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "News"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 111,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5fcdo",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "ups": 102,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "News",
          "can_mod_post": false,
          "score": 102,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=140&amp;height=111&amp;crop=140:111,smart&amp;auto=webp&amp;s=afd01d73738739fc5f4d0d7e20210d85f5c6a7af",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "link",
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753093764,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "techpowerup.com",
          "allow_live_comments": false,
          "selftext_html": null,
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?auto=webp&amp;s=1ab5566fa5cfc48759389d6459ab423c6a6d93ae",
                  "width": 1024,
                  "height": 815
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=ac4eb006eed458f45d94d2092f2b1d96e9111985",
                    "width": 108,
                    "height": 85
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4e28f32683b490d6cd796b37d8e18e66ba9e8cc5",
                    "width": 216,
                    "height": 171
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d69b2795cdbb216ae0415feac75bbe4d91eed6ce",
                    "width": 320,
                    "height": 254
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b8ad27cd8415b4fcbb76f37f1c26b02e1e7a72d",
                    "width": 640,
                    "height": 509
                  },
                  {
                    "url": "https://external-preview.redd.it/bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=da909b3e1331d3458fe14a0c9335539ce72f2955",
                    "width": 960,
                    "height": 764
                  }
                ],
                "variants": {},
                "id": "bStjeji8oH-vX7nEL2-gqIEn5srknBBEzSyJDD_6lLE"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#cc3600",
          "id": "1m5fcdo",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "OwnWitness2836",
          "discussion_type": null,
          "num_comments": 33,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5fcdo/nvidia_brings_reasoning_models_to_consumers/",
          "stickied": false,
          "url": "https://www.techpowerup.com/339089/nvidia-brings-reasoning-models-to-consumers-ranging-from-1-5b-to-32b-parameters",
          "subreddit_subscribers": 502515,
          "created_utc": 1753093764,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi everyone üëã \nI am trying to build an offline coding assistant. For that I have to do POC. Anyone having any idea about this? To implement this in limited environment?",
          "author_fullname": "t2_bb6pp919",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Offline Coding Assistant",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ew98",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753092188,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone üëã \nI am trying to build an offline coding assistant. For that I have to do POC. Anyone having any idea about this? To implement this in limited environment?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ew98",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "eternalHarsh",
          "discussion_type": null,
          "num_comments": 9,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753092188,
          "num_crossposts": 2,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Curious :-)",
          "author_fullname": "t2_9phou5uh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Can any tool dub an entire Movie into another language?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5eulr",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.5,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753092047,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Curious :-)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5eulr",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Trysem",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5eulr/can_any_tool_dub_an_entire_movie_into_another/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5eulr/can_any_tool_dub_an_entire_movie_into_another/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753092047,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "So, Kimi K2 is out, and it's currently topping benchmarks in creative writing. I was wondering,how exactly do LLMs become more creative? From what I know, Kimi K2 uses DeepSeek's architecture but with more experts. So is improving creative writing mostly about scaling the model (more parameters, more experts) and not really about architecture, or is it more about the kind, size and quality of training data?\nAlso, do companies even prioritize creativity? It feels like most of them is focusing on improving math, coding, and benchmark scores in these days, not on storytelling, nuance, or imagination.\nand I was wondering if there is any a proper benchmark for evaluating creativity? As I know models are ranked using human votes or scored by any other LLM, but how can we meaningfully compare creative performance without testing them directly?\nLastly, are there any emerging architectures, like Liquid Foundation or Mamba, that seem especially promising for improving creativity in language models?",
          "author_fullname": "t2_y1vyie97k",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How does LLMs get more creative?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5dq1e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.54,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753087769,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So, Kimi K2 is out, and it&amp;#39;s currently topping benchmarks in creative writing. I was wondering,how exactly do LLMs become more creative? From what I know, Kimi K2 uses DeepSeek&amp;#39;s architecture but with more experts. So is improving creative writing mostly about scaling the model (more parameters, more experts) and not really about architecture, or is it more about the kind, size and quality of training data?\nAlso, do companies even prioritize creativity? It feels like most of them is focusing on improving math, coding, and benchmark scores in these days, not on storytelling, nuance, or imagination.\nand I was wondering if there is any a proper benchmark for evaluating creativity? As I know models are ranked using human votes or scored by any other LLM, but how can we meaningfully compare creative performance without testing them directly?\nLastly, are there any emerging architectures, like Liquid Foundation or Mamba, that seem especially promising for improving creativity in language models?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5dq1e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ba2sYd",
          "discussion_type": null,
          "num_comments": 15,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5dq1e/how_does_llms_get_more_creative/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5dq1e/how_does_llms_get_more_creative/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753087769,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Investigating this idea myself, and noting it down. Thought I'd post it as a discussion in case people have roasts/suggestions before I revisit it. I'll research all this myself but if anyone wants to criticize or correct me, that would be welcome\n\nCould be done on any platform that has plug and play for Node.js?\n\nIs the cost of Microsoft or Amazon cloud hosted LLMs cheaper than this idea?\n\nMy big hangup on AI based APIs is tying it to yet another API account with or without spending limits. So far, I've hosted open source llama and gemma locally, but I haven't done anything networking with it. I've configured many a VPS but haven't done any AI based APIs.",
          "author_fullname": "t2_jzw36c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What do you think of self-hosting a small LLM on a VPS or abstracted container, calling it externally for simple AI agents/API calls? Cheaper or more expensive than bigger models?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5djms",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753087056,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Investigating this idea myself, and noting it down. Thought I&amp;#39;d post it as a discussion in case people have roasts/suggestions before I revisit it. I&amp;#39;ll research all this myself but if anyone wants to criticize or correct me, that would be welcome&lt;/p&gt;\n\n&lt;p&gt;Could be done on any platform that has plug and play for Node.js?&lt;/p&gt;\n\n&lt;p&gt;Is the cost of Microsoft or Amazon cloud hosted LLMs cheaper than this idea?&lt;/p&gt;\n\n&lt;p&gt;My big hangup on AI based APIs is tying it to yet another API account with or without spending limits. So far, I&amp;#39;ve hosted open source llama and gemma locally, but I haven&amp;#39;t done anything networking with it. I&amp;#39;ve configured many a VPS but haven&amp;#39;t done any AI based APIs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5djms",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "angry_cactus",
          "discussion_type": null,
          "num_comments": 7,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753087056,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi, I am looking to run Kimi-K2 locally with reasonable response.  What hardware would I need (excluding NVidia 6000 series cards)? Could I run a cluster of Macs?",
          "author_fullname": "t2_4i56b",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What consumer hardware do I need to run Kimi-K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5dcqj",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.4,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753086296,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I am looking to run Kimi-K2 locally with reasonable response.  What hardware would I need (excluding NVidia 6000 series cards)? Could I run a cluster of Macs?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5dcqj",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "stealthmatt",
          "discussion_type": null,
          "num_comments": 44,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5dcqj/what_consumer_hardware_do_i_need_to_run_kimik2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5dcqj/what_consumer_hardware_do_i_need_to_run_kimik2/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753086296,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "For those interested, here is a temporary copy pulled just before the official repo went 404.\n\n[https://github.com/PieBru/ik\\_llama.cpp\\_temp\\_copy](https://github.com/PieBru/ik_llama.cpp_temp_copy)",
          "author_fullname": "t2_uehl6561",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ik_llama.cpp 404: temporary repo up to commit d44c2d3",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5d66p",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.96,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 42,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 42,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753085617,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For those interested, here is a temporary copy pulled just before the official repo went 404.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/PieBru/ik_llama.cpp_temp_copy\"&gt;https://github.com/PieBru/ik_llama.cpp_temp_copy&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?auto=webp&amp;s=2e4ac14f43e45ff123f0291ee0e9229ddf63c11a",
                  "width": 1200,
                  "height": 600
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9caec387fe1ebd69552e3f38510d53745dc8e07c",
                    "width": 108,
                    "height": 54
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fbec24ba5e92ce184223ddc1f5e7fe36f2181a8",
                    "width": 216,
                    "height": 108
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef8215dca529cf7174239a8aeb6631157255480f",
                    "width": 320,
                    "height": 160
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=990d720abad14eacfd0e06b3f6224e49b6641d61",
                    "width": 640,
                    "height": 320
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1dfc0f1e7e23ea4c5a555ef6596dcddc4362888e",
                    "width": 960,
                    "height": 480
                  },
                  {
                    "url": "https://external-preview.redd.it/OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c6a1df725f9226d65d59e1a49ae8856892491bca",
                    "width": 1080,
                    "height": 540
                  }
                ],
                "variants": {},
                "id": "OeOxSPElJ7feBpmIHwfN3BbOjC_fgsHhahVsK3oEdNw"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m5d66p",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PieBru",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5d66p/ik_llamacpp_404_temporary_repo_up_to_commit/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753085617,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Is there way to run the GGUF files on Android Studio? Maybe with llama.cpp? I have been trying to build a wrapper around llama.cpp with Kotlin+Java but there must be a better solution.",
          "author_fullname": "t2_czmrjmbtc",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "GGUF on Android Studio",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5d2cv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.6,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 1,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 1,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753085220,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Is there way to run the GGUF files on Android Studio? Maybe with llama.cpp? I have been trying to build a wrapper around llama.cpp with Kotlin+Java but there must be a better solution.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5d2cv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "neural-learner",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5d2cv/gguf_on_android_studio/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5d2cv/gguf_on_android_studio/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753085220,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "hardware info: in second pic.",
          "author_fullname": "t2_tzhrcb4n4",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "is_gallery": true,
          "title": "What model shall i run?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 78,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "fjg5vitem6ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/fjg5vitem6ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5608ba45dec4757c64f6121fcc82e3100a2d7b3b"
                },
                {
                  "y": 121,
                  "x": 216,
                  "u": "https://preview.redd.it/fjg5vitem6ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5c6dff4954a039f4e235af9c405bc47b03137e7b"
                },
                {
                  "y": 180,
                  "x": 320,
                  "u": "https://preview.redd.it/fjg5vitem6ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=46e27ea44dfac1ac17d7151d330e1cbde8691ce3"
                },
                {
                  "y": 360,
                  "x": 640,
                  "u": "https://preview.redd.it/fjg5vitem6ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d9f15f480e11134a51552402a8591c624430661b"
                },
                {
                  "y": 540,
                  "x": 960,
                  "u": "https://preview.redd.it/fjg5vitem6ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bdee34b2cffa4105238954e257445f72814925c"
                },
                {
                  "y": 607,
                  "x": 1080,
                  "u": "https://preview.redd.it/fjg5vitem6ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ce60e70308f4f33ce942086493b9e679e70220ad"
                }
              ],
              "s": {
                "y": 1080,
                "x": 1920,
                "u": "https://preview.redd.it/fjg5vitem6ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=322522374f16675c812972f7b9b6dfa8789ce22d"
              },
              "id": "fjg5vitem6ef1"
            },
            "a2r7rx6rm6ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 57,
                  "x": 108,
                  "u": "https://preview.redd.it/a2r7rx6rm6ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=54167f0da47d5e689ff9efdf589de47449598ce3"
                },
                {
                  "y": 115,
                  "x": 216,
                  "u": "https://preview.redd.it/a2r7rx6rm6ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6429a2adc572c9046a70ab2257376d72eb7b97f7"
                },
                {
                  "y": 171,
                  "x": 320,
                  "u": "https://preview.redd.it/a2r7rx6rm6ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0961fd4e0af24f14effa68d1c54ce2c4a2f4810c"
                },
                {
                  "y": 343,
                  "x": 640,
                  "u": "https://preview.redd.it/a2r7rx6rm6ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=14f737acf1cf8ed5bb1a26e03cc72c7f3ea95c92"
                },
                {
                  "y": 515,
                  "x": 960,
                  "u": "https://preview.redd.it/a2r7rx6rm6ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=efac49da049fbb3c602f085b083d5d558d160295"
                },
                {
                  "y": 579,
                  "x": 1080,
                  "u": "https://preview.redd.it/a2r7rx6rm6ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b222cc6ec5f69c0feec2513b049208becb06af3b"
                }
              ],
              "s": {
                "y": 1031,
                "x": 1920,
                "u": "https://preview.redd.it/a2r7rx6rm6ef1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=352a642e0179fecddf90bc1d6fe487e688668e26"
              },
              "id": "a2r7rx6rm6ef1"
            }
          },
          "name": "t3_1m5cr11",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "ups": 0,
          "domain": "reddit.com",
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "gallery_data": {
            "items": [
              {
                "media_id": "fjg5vitem6ef1",
                "id": 710646866
              },
              {
                "media_id": "a2r7rx6rm6ef1",
                "id": 710646867
              }
            ]
          },
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://a.thumbs.redditmedia.com/VxnMdk92tgZbWuc92IHucTqoBLF35BWqfGmiCc8bjw4.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "subreddit_type": "public",
          "created": 1753084012,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "total_awards_received": 0,
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hardware info: in second pic.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://www.reddit.com/gallery/1m5cr11",
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5cr11",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "RevolutionaryBus4545",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5cr11/what_model_shall_i_run/",
          "stickied": false,
          "url": "https://www.reddit.com/gallery/1m5cr11",
          "subreddit_subscribers": 502515,
          "created_utc": 1753084012,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "am having a hard time with which one is good and why ???!!",
          "author_fullname": "t2_hu9onfqo",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ONNX or GGUF",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5ckr0",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.78,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753083334,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;am having a hard time with which one is good and why ???!!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5ckr0",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Xitizdumb",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5ckr0/onnx_or_gguf/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5ckr0/onnx_or_gguf/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753083334,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Anyone here having fun time using MCP? I've just started to look around into it and was wondering that most of the tutorial are based out of claude desktop or cursor. Anyone here experimenting it out without them (using streamlit or fastAPI). ",
          "author_fullname": "t2_jnnkbubf0",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How's your experimentation with MCP going?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5bccx",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.8,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 3,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 3,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753078657,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Anyone here having fun time using MCP? I&amp;#39;ve just started to look around into it and was wondering that most of the tutorial are based out of claude desktop or cursor. Anyone here experimenting it out without them (using streamlit or fastAPI). &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5bccx",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "UnfinishedSentenc-1",
          "discussion_type": null,
          "num_comments": 17,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5bccx/hows_your_experimentation_with_mcp_going/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5bccx/hows_your_experimentation_with_mcp_going/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753078657,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "People have started throwing the terms ethical and ethics around with respect and I'm not sure how to read those terms. Is a more ethical model one which was trained using \"less\" electricity with something made on a raspberry pi approaching \"peak\" ethicalness? Are the inputs to a model more important? Less? How do both matter? Something else?",
          "author_fullname": "t2_w2zszpg3",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What makes a model ethical?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m59xzv",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.59,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753073699,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People have started throwing the terms ethical and ethics around with respect and I&amp;#39;m not sure how to read those terms. Is a more ethical model one which was trained using &amp;quot;less&amp;quot; electricity with something made on a raspberry pi approaching &amp;quot;peak&amp;quot; ethicalness? Are the inputs to a model more important? Less? How do both matter? Something else?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m59xzv",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "KnownDairyAcolyte",
          "discussion_type": null,
          "num_comments": 60,
          "send_replies": false,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m59xzv/what_makes_a_model_ethical/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m59xzv/what_makes_a_model_ethical/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753073699,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I am trying to fine tune a LlaVa model, I have a training set of 7800 high quality conversations, each with an image. \n\nI am using qlora to fine tune the model, and regardless of the batch size, the lr, and the rank, so far all of my trials were resulted in gibberish on evaluation. \n\nI did some reading, and in order to avoid catastrophic forgetting, it says that we should limit our tuning of the lora model to three epochs max. In addition, I understand that the data size I have is allegedly enough. Together there is something that I am not sure about. The qlora model has about 10m weights (even without bias terms). It looks like much too many to be able to fit on my miniature data. \n\nAny tips would be greatly appreciated. ",
          "author_fullname": "t2_1t6vmqt87p",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "First time using QLoRa results in gibberish",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m58qf3",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 10,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 10,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753069731,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I am trying to fine tune a LlaVa model, I have a training set of 7800 high quality conversations, each with an image. &lt;/p&gt;\n\n&lt;p&gt;I am using qlora to fine tune the model, and regardless of the batch size, the lr, and the rank, so far all of my trials were resulted in gibberish on evaluation. &lt;/p&gt;\n\n&lt;p&gt;I did some reading, and in order to avoid catastrophic forgetting, it says that we should limit our tuning of the lora model to three epochs max. In addition, I understand that the data size I have is allegedly enough. Together there is something that I am not sure about. The qlora model has about 10m weights (even without bias terms). It looks like much too many to be able to fit on my miniature data. &lt;/p&gt;\n\n&lt;p&gt;Any tips would be greatly appreciated. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m58qf3",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Emotional-Sundae4075",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m58qf3/first_time_using_qlora_results_in_gibberish/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753069731,
          "num_crossposts": 1,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Currently using Ollama with OpenWebUI on a dedicated PC. This has a Intel Xeon E5v2, 32gb Ram and 2x Titan V 12GB (have a third on its way).  Limited budget and this is roughly what I have to play with right now.\n\nI was wanting to add about 20-30 pdf documents to a knowledge base.  I would then have an LLM to find and provide resources from that information.\n\nI have been experimenting with a few different models but am seeking advice as I have not found an ideal solution.\n\nMy main goal was to be able to use an LLM, was initially thing a \n\nVision models (Gemma &amp; Qwen2.5VL) worked well at retrieving information but not very intelligent at following instructions.  Possibly because they were quite small (7b &amp; 12b).  The larger vision models (27b &amp; 32b) were fitting into VRAM with 2GB-6GB free.  Small images etc were handled fast and accurate.  Larger images (full desktop screenshots) started ignoring GPU space and I noticed near 100% load on all 20 CPU threads.\n\nI thought maybe a more traditional text only model with only text based PDF's as knowledge might be worth a shot.  I then used faster non reasoning model (Phi4 14B &amp; Qwen 2.5 Coder 14B).  These were great and accurate but were not able to understand the images in the documents.\n\nAm I going about this wrong?\n\nI thought uploading the documents to \"Knowledge\" was RAG.  This is configured as default and no changes.  It seems too quick so I dont think it is.\n\n",
          "author_fullname": "t2_3vo0xl71",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Model to retrieve information from Knowledge.",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m58ohn",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.75,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 4,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 4,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753069553,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using Ollama with OpenWebUI on a dedicated PC. This has a Intel Xeon E5v2, 32gb Ram and 2x Titan V 12GB (have a third on its way).  Limited budget and this is roughly what I have to play with right now.&lt;/p&gt;\n\n&lt;p&gt;I was wanting to add about 20-30 pdf documents to a knowledge base.  I would then have an LLM to find and provide resources from that information.&lt;/p&gt;\n\n&lt;p&gt;I have been experimenting with a few different models but am seeking advice as I have not found an ideal solution.&lt;/p&gt;\n\n&lt;p&gt;My main goal was to be able to use an LLM, was initially thing a &lt;/p&gt;\n\n&lt;p&gt;Vision models (Gemma &amp;amp; Qwen2.5VL) worked well at retrieving information but not very intelligent at following instructions.  Possibly because they were quite small (7b &amp;amp; 12b).  The larger vision models (27b &amp;amp; 32b) were fitting into VRAM with 2GB-6GB free.  Small images etc were handled fast and accurate.  Larger images (full desktop screenshots) started ignoring GPU space and I noticed near 100% load on all 20 CPU threads.&lt;/p&gt;\n\n&lt;p&gt;I thought maybe a more traditional text only model with only text based PDF&amp;#39;s as knowledge might be worth a shot.  I then used faster non reasoning model (Phi4 14B &amp;amp; Qwen 2.5 Coder 14B).  These were great and accurate but were not able to understand the images in the documents.&lt;/p&gt;\n\n&lt;p&gt;Am I going about this wrong?&lt;/p&gt;\n\n&lt;p&gt;I thought uploading the documents to &amp;quot;Knowledge&amp;quot; was RAG.  This is configured as default and no changes.  It seems too quick so I dont think it is.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m58ohn",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "themungbeans",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m58ohn/model_to_retrieve_information_from_knowledge/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m58ohn/model_to_retrieve_information_from_knowledge/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753069553,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "1. Mistral\\_large-Instruct\n2. Qwen3-235B\n3. Command-A\n4. Deepseek-V3\n5. Deepseek-R1\n6. Deepseek-R1-0528\n7. Deepseek-TNG-R1T2-Chimera\n8. Kimi-K2\n9. Ernie-4.5-300b\n10. llama3.1-405B\n11. llama3.1-Nemotron-Ultra-253b?\n12. Others?",
          "author_fullname": "t2_ah13x",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which local 100B+ heavy weight models are your favorite and why?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m58695",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": "#bbbdbf",
          "subreddit_type": "public",
          "ups": 107,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 107,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [
            {
              "e": "text",
              "t": "llama.cpp"
            }
          ],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753067953,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "richtext",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;Mistral_large-Instruct&lt;/li&gt;\n&lt;li&gt;Qwen3-235B&lt;/li&gt;\n&lt;li&gt;Command-A&lt;/li&gt;\n&lt;li&gt;Deepseek-V3&lt;/li&gt;\n&lt;li&gt;Deepseek-R1&lt;/li&gt;\n&lt;li&gt;Deepseek-R1-0528&lt;/li&gt;\n&lt;li&gt;Deepseek-TNG-R1T2-Chimera&lt;/li&gt;\n&lt;li&gt;Kimi-K2&lt;/li&gt;\n&lt;li&gt;Ernie-4.5-300b&lt;/li&gt;\n&lt;li&gt;llama3.1-405B&lt;/li&gt;\n&lt;li&gt;llama3.1-Nemotron-Ultra-253b?&lt;/li&gt;\n&lt;li&gt;Others?&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": "llama.cpp",
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m58695",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "segmond",
          "discussion_type": null,
          "num_comments": 103,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": "light",
          "permalink": "/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m58695/which_local_100b_heavy_weight_models_are_your/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753067953,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello!\n\nI feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.\n\nOf course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?\n\nWhat's everyone's go-to model these days?\n\nAnd what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released [FlexOlmo](https://huggingface.co/allenai/FlexOlmo-7x7B-1T) LLM/paradigm?\n\nThanks! ",
          "author_fullname": "t2_h8yrica5",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Which LLMs, tools, or research have been overlooked or deserve more attention?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5827d",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.92,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 29,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 29,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753067598,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello!&lt;/p&gt;\n\n&lt;p&gt;I feel like there have been a lot of new releases in the past few weeks after a relatively quiet period following the Qwen3 release.&lt;/p&gt;\n\n&lt;p&gt;Of course, there was the new Deepseek model, and now Kimi. But what is the consensus on the other, somewhat smaller LLMs that came out? Models like Jamba-Mini-1.7, Hunyuan-A13B-Instruct or ERNIE-4.5-21B-A3B?&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s everyone&amp;#39;s go-to model these days?&lt;/p&gt;\n\n&lt;p&gt;And what are some other LLMs, tools, or research papers that you think flew under the radar because of the many big releases recently? For example, things like the recently released &lt;a href=\"https://huggingface.co/allenai/FlexOlmo-7x7B-1T\"&gt;FlexOlmo&lt;/a&gt; LLM/paradigm?&lt;/p&gt;\n\n&lt;p&gt;Thanks! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?auto=webp&amp;s=cc904ba70b4ddd36e094ee5d02e948b2bbc3fe87",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=326925660fb97ea07b8f47320d9a931b6f3b8850",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e17265857ab969bc54807e5a91f994978fb57506",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ea0cfa586f8cd24ecc53311134a6d90dd01a14b7",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d67cbf4fd08636ac1f8772745df0258dc4168228",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1b5d406f0a9d22f5434c5bd24f0856052cdd8bb0",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2a148304a5d367a2a7e14ff2ea9f57df4e05ed2f",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "KasKHMjBTqO8qG3YViHMjFsBwsBOo-TVs2fkqqj8qBo"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m5827d",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "MDT-49",
          "discussion_type": null,
          "num_comments": 16,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5827d/which_llms_tools_or_research_have_been_overlooked/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753067598,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Has anyone else noticed this weird behaviour when you pull a model (I am using SSH) and after about 3 mins the speed plummets?  I then terminate the command and re-issue it.  Then I get back to fast speeds and it carries on from where it left off?\n\nhttps://preview.redd.it/bqei1o1u75ef1.png?width=972&amp;format=png&amp;auto=webp&amp;s=b329c8cf4093d94d6b1e5b764f0dd04f9961aade\n\n",
          "author_fullname": "t2_3vo0xl71",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Weird ollama pull behaviour",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 15,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "bqei1o1u75ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 12,
                  "x": 108,
                  "u": "https://preview.redd.it/bqei1o1u75ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d80ac158a12bab6c35db710c426cf39063d7adaf"
                },
                {
                  "y": 24,
                  "x": 216,
                  "u": "https://preview.redd.it/bqei1o1u75ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5f2e27195d52cb59f0c84ee4293a74f3e3ec0b70"
                },
                {
                  "y": 36,
                  "x": 320,
                  "u": "https://preview.redd.it/bqei1o1u75ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0fbc13ceadfc72c0e3d69f78bc3b1105004024cd"
                },
                {
                  "y": 73,
                  "x": 640,
                  "u": "https://preview.redd.it/bqei1o1u75ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=53d47d7b5416a03239f42552e6104a7c8922c7dc"
                },
                {
                  "y": 109,
                  "x": 960,
                  "u": "https://preview.redd.it/bqei1o1u75ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3a723379c3f93382a2ec0e02b474a868a649c57"
                }
              ],
              "s": {
                "y": 111,
                "x": 972,
                "u": "https://preview.redd.it/bqei1o1u75ef1.png?width=972&amp;format=png&amp;auto=webp&amp;s=b329c8cf4093d94d6b1e5b764f0dd04f9961aade"
              },
              "id": "bqei1o1u75ef1"
            }
          },
          "name": "t3_1m57utu",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.25,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/rvNMALum_70kao3qNVGsdDrGtGsFnMgV4o-xBZrDVnw.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753066971,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone else noticed this weird behaviour when you pull a model (I am using SSH) and after about 3 mins the speed plummets?  I then terminate the command and re-issue it.  Then I get back to fast speeds and it carries on from where it left off?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/bqei1o1u75ef1.png?width=972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b329c8cf4093d94d6b1e5b764f0dd04f9961aade\"&gt;https://preview.redd.it/bqei1o1u75ef1.png?width=972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b329c8cf4093d94d6b1e5b764f0dd04f9961aade&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m57utu",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "themungbeans",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m57utu/weird_ollama_pull_behaviour/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m57utu/weird_ollama_pull_behaviour/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753066971,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I have Threadripper 2970WX uses a PCI-Express Gen 3\n\n256GB DDR4 + 5090\n\nI ran Kimi-K2-Instruct-UD-Q2\\_K\\_XL (354.9GB) and got 2t/sec\n\nI have 4 SSD drives. I made symbolic links. I put 2 files on each drive and got 2.3t/sec\n\ncheers! =)",
          "author_fullname": "t2_1airv0szt9",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Pseudo RAID and Kimi-K2",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Tutorial | Guide"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m57iep",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.86,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 5,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Tutorial | Guide",
          "can_mod_post": false,
          "score": 5,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753065913,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have Threadripper 2970WX uses a PCI-Express Gen 3&lt;/p&gt;\n\n&lt;p&gt;256GB DDR4 + 5090&lt;/p&gt;\n\n&lt;p&gt;I ran Kimi-K2-Instruct-UD-Q2_K_XL (354.9GB) and got 2t/sec&lt;/p&gt;\n\n&lt;p&gt;I have 4 SSD drives. I made symbolic links. I put 2 files on each drive and got 2.3t/sec&lt;/p&gt;\n\n&lt;p&gt;cheers! =)&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#0079d3",
          "id": "1m57iep",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Defiant_Diet9085",
          "discussion_type": null,
          "num_comments": 10,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m57iep/pseudo_raid_and_kimik2/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m57iep/pseudo_raid_and_kimik2/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753065913,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi folks, if you have a couple of minutes could you please check if your favorite llm can recognize/depict mid size ASCII art (like 20*20 chars)\nAnd please share your settings like temperature, minP, topK, topP etc.\n\nBased on my observations qwen 235b on default settings is able to depict some ASCII art.",
          "author_fullname": "t2_dplp419uz",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "ASCII art and LocalLLMa",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m57icy",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 1,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753065909,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi folks, if you have a couple of minutes could you please check if your favorite llm can recognize/depict mid size ASCII art (like 20*20 chars)\nAnd please share your settings like temperature, minP, topK, topP etc.&lt;/p&gt;\n\n&lt;p&gt;Based on my observations qwen 235b on default settings is able to depict some ASCII art.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m57icy",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Bitter_Square6273",
          "discussion_type": null,
          "num_comments": 1,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m57icy/ascii_art_and_localllma/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m57icy/ascii_art_and_localllma/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753065909,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Today I just installed Ollama on my local machine and Open-WebUI. I was wondering if there's a better UI/way for running models for ERP/NSFW. Like saying a character and having it as a character. I have no idea what I'm doing and I'm new to this so any help would be appreciated. Currently I have dolphin-mixtral-8x7B and Nous-Capybara-limarpv3-34B as models for these purposes but idk how to prompt to start getting it to ERP as say a specific character or anything and if there's a better ui for erp settings\n\nAny help would be appreciated üëç",
          "author_fullname": "t2_4cf7tlxj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A question about running Ollama locally for NSFW/ERP",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m576x6",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.47,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "nsfw",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753064948,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Today I just installed Ollama on my local machine and Open-WebUI. I was wondering if there&amp;#39;s a better UI/way for running models for ERP/NSFW. Like saying a character and having it as a character. I have no idea what I&amp;#39;m doing and I&amp;#39;m new to this so any help would be appreciated. Currently I have dolphin-mixtral-8x7B and Nous-Capybara-limarpv3-34B as models for these purposes but idk how to prompt to start getting it to ERP as say a specific character or anything and if there&amp;#39;s a better ui for erp settings&lt;/p&gt;\n\n&lt;p&gt;Any help would be appreciated üëç&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": true,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m576x6",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "JacobBender92",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m576x6/a_question_about_running_ollama_locally_for/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m576x6/a_question_about_running_ollama_locally_for/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753064948,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "[https://huggingface.co/bartowski/baidu\\_ERNIE-4.5-21B-A3B-PT-GGUF](https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF)\n\n[https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF](https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF)\n\nthey are quant of a same model. at a same quant, e.g. both Q3\\_K\\_M, there are non-negligible count of blocks, which bartowski quantized as Q8\\_0, while unsloth Q3\\_K or Q4\\_K.\n\n[this is a part. count 67 in total](https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;format=png&amp;auto=webp&amp;s=838059d64089a5e17092169c48e88ab90b8d92a9)\n\nbtw, the unsloth Q3\\_K\\_XL is smaller than Q3\\_K\\_M. I am really curious on the flavor of unloth naming.",
          "author_fullname": "t2_13atwtkw16",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "why are there quite different quant strategies of bartowski and unsloth on MoE?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 75,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "v3rjrmrbz4ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 216,
                  "x": 108,
                  "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80757a8208c1fc998cdfdc1ced05367831388518"
                },
                {
                  "y": 432,
                  "x": 216,
                  "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=22bdfb8ffd09c071512dc099f6d025eb0d2a2cf7"
                },
                {
                  "y": 640,
                  "x": 320,
                  "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2da183a327ba53666698e5cc2fc7ded59c131716"
                }
              ],
              "s": {
                "y": 1185,
                "x": 520,
                "u": "https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;format=png&amp;auto=webp&amp;s=838059d64089a5e17092169c48e88ab90b8d92a9"
              },
              "id": "v3rjrmrbz4ef1"
            }
          },
          "name": "t3_1m56z4m",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.88,
          "author_flair_background_color": null,
          "ups": 25,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 25,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=88061d05ef33f17634548f591df7c21d0d88070c",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "post_hint": "self",
          "content_categories": null,
          "is_self": true,
          "subreddit_type": "public",
          "created": 1753064323,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF\"&gt;https://huggingface.co/bartowski/baidu_ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF\"&gt;https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;they are quant of a same model. at a same quant, e.g. both Q3_K_M, there are non-negligible count of blocks, which bartowski quantized as Q8_0, while unsloth Q3_K or Q4_K.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/v3rjrmrbz4ef1.png?width=520&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=838059d64089a5e17092169c48e88ab90b8d92a9\"&gt;this is a part. count 67 in total&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;btw, the unsloth Q3_K_XL is smaller than Q3_K_M. I am really curious on the flavor of unloth naming.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "preview": {
            "images": [
              {
                "source": {
                  "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?auto=webp&amp;s=2b17558e247007e17f638e5927539051c3b04257",
                  "width": 1200,
                  "height": 648
                },
                "resolutions": [
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d07f61f99c0ca8c045bf1472163f101485f90b26",
                    "width": 108,
                    "height": 58
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82197ee1623e9095bad9995940035a0544e288c9",
                    "width": 216,
                    "height": 116
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5ffcadb86f3654f3417898579d364758927b20d1",
                    "width": 320,
                    "height": 172
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7dcb3e29b76660bae85af28320f25e9b8191473f",
                    "width": 640,
                    "height": 345
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bdf98049dfd4639ef8556cb15087ae7700b9388a",
                    "width": 960,
                    "height": 518
                  },
                  {
                    "url": "https://external-preview.redd.it/wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1909d03961237ad54406e5332efdc6034de3d23a",
                    "width": 1080,
                    "height": 583
                  }
                ],
                "variants": {},
                "id": "wHvNbmd8WWIQFMd6cKGd0f5flJJfJGOyvWzZPM8zur8"
              }
            ],
            "enabled": false
          },
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "mod_note": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "num_reports": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m56z4m",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Remarkable-Pea645",
          "discussion_type": null,
          "num_comments": 3,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m56z4m/why_are_there_quite_different_quant_strategies_of/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753064323,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hello devs,\n\nThe idea came from while I was working on a personal project. When I tried to deploy my agent into the cloud, I ran into a lot of headaches ‚Äî setting up VMs, writing config, handling crashes. I decided to build a solution for it and called it Agentainer.\n\nAgentainer‚Äôs goal is to let anyone (even coding agents) deploy LLM agents into production without spending hours setting up infrastructure.\n\nHere‚Äôs what Agentainer does:\n\n* One-click deployment: Deploy your containerized LLM agent (any language) as a Docker image\n* Lifecycle management: Start, stop, pause, resume, and auto-recover via UI or API\n* Auto-recovery: Agents restart automatically after a crash and return to their last working state\n* State persistence: Uses Redis for in-memory state and PostgreSQL for snapshots\n* Per-agent secure APIs: Each agent gets its own REST/gRPC endpoint with token-based auth and usage logging (e.g.¬†`https://agentainer.io/{agentId}/{agentEndpoint}`)\n\nMost cloud platforms are designed for stateless apps or short-lived functions. They‚Äôre not ideal for long-running autonomous agents. Since a lot of dev work is now being done by coding agents themselves, Agentainer exposes all platform functions through an API. That means even non-technical founders can ship their own agents into production without needing to manage infrastructure.\n\nIf you visit the website ( [https://agentainer.io/](https://agentainer.io/) ) , you‚Äôll find a link to our GitHub repo with a working demo that includes all the features above. You can also sign up for early access to the production version, which is launching soon.\n\nhttps://preview.redd.it/elzllmzvz4ef1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=58673daaa292d998e6d89d7af4878f7a398d2056\n\nI would love to hear feedback ‚Äî especially from folks running agents in production or building with them now. If you try [Agentainer Lab (GitHub)](https://github.com/oso95/Agentainer-lab), I‚Äôd really appreciate any thoughts (good and bad) or feature suggestions.\n\nNote: Agentainer doesn‚Äôt provide any LLM models or reasoning frameworks. We‚Äôre infrastructure only ‚Äî you bring the agent, and we handle deployment, state, and APIs.",
          "author_fullname": "t2_6mhhxj8ry",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "A solution to deploy your LLM agent with one click",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": 77,
          "top_awarded_type": null,
          "hide_score": false,
          "media_metadata": {
            "elzllmzvz4ef1": {
              "status": "valid",
              "e": "Image",
              "m": "image/png",
              "p": [
                {
                  "y": 60,
                  "x": 108,
                  "u": "https://preview.redd.it/elzllmzvz4ef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eacdf14901ba02b5139a700003b97cef06111520"
                },
                {
                  "y": 120,
                  "x": 216,
                  "u": "https://preview.redd.it/elzllmzvz4ef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0c85d6aea7f5165ed20b5adb45ff4214a7906d97"
                },
                {
                  "y": 178,
                  "x": 320,
                  "u": "https://preview.redd.it/elzllmzvz4ef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8bcc4043d6af492941e4b63f2d932aea14383666"
                },
                {
                  "y": 356,
                  "x": 640,
                  "u": "https://preview.redd.it/elzllmzvz4ef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e25c0f9f99ed80b21e35f6f6fc6f5dd7d2a0dc4d"
                },
                {
                  "y": 534,
                  "x": 960,
                  "u": "https://preview.redd.it/elzllmzvz4ef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=42250f4125da0c648bc1190950ab4f6090bbd976"
                },
                {
                  "y": 601,
                  "x": 1080,
                  "u": "https://preview.redd.it/elzllmzvz4ef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a820df79a5538838f37f2b29408324e7420dee3f"
                }
              ],
              "s": {
                "y": 705,
                "x": 1266,
                "u": "https://preview.redd.it/elzllmzvz4ef1.png?width=1266&amp;format=png&amp;auto=webp&amp;s=58673daaa292d998e6d89d7af4878f7a398d2056"
              },
              "id": "elzllmzvz4ef1"
            }
          },
          "name": "t3_1m56x5u",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": 140,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "https://b.thumbs.redditmedia.com/_hNhxOlJGohyQo1t7_1jEp2uUFZpEvxm0QAXuurPBpI.jpg",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753064166,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello devs,&lt;/p&gt;\n\n&lt;p&gt;The idea came from while I was working on a personal project. When I tried to deploy my agent into the cloud, I ran into a lot of headaches ‚Äî setting up VMs, writing config, handling crashes. I decided to build a solution for it and called it Agentainer.&lt;/p&gt;\n\n&lt;p&gt;Agentainer‚Äôs goal is to let anyone (even coding agents) deploy LLM agents into production without spending hours setting up infrastructure.&lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs what Agentainer does:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;One-click deployment: Deploy your containerized LLM agent (any language) as a Docker image&lt;/li&gt;\n&lt;li&gt;Lifecycle management: Start, stop, pause, resume, and auto-recover via UI or API&lt;/li&gt;\n&lt;li&gt;Auto-recovery: Agents restart automatically after a crash and return to their last working state&lt;/li&gt;\n&lt;li&gt;State persistence: Uses Redis for in-memory state and PostgreSQL for snapshots&lt;/li&gt;\n&lt;li&gt;Per-agent secure APIs: Each agent gets its own REST/gRPC endpoint with token-based auth and usage logging (e.g.¬†&lt;code&gt;https://agentainer.io/{agentId}/{agentEndpoint}&lt;/code&gt;)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Most cloud platforms are designed for stateless apps or short-lived functions. They‚Äôre not ideal for long-running autonomous agents. Since a lot of dev work is now being done by coding agents themselves, Agentainer exposes all platform functions through an API. That means even non-technical founders can ship their own agents into production without needing to manage infrastructure.&lt;/p&gt;\n\n&lt;p&gt;If you visit the website ( &lt;a href=\"https://agentainer.io/\"&gt;https://agentainer.io/&lt;/a&gt; ) , you‚Äôll find a link to our GitHub repo with a working demo that includes all the features above. You can also sign up for early access to the production version, which is launching soon.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/elzllmzvz4ef1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58673daaa292d998e6d89d7af4878f7a398d2056\"&gt;https://preview.redd.it/elzllmzvz4ef1.png?width=1266&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58673daaa292d998e6d89d7af4878f7a398d2056&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I would love to hear feedback ‚Äî especially from folks running agents in production or building with them now. If you try &lt;a href=\"https://github.com/oso95/Agentainer-lab\"&gt;Agentainer Lab (GitHub)&lt;/a&gt;, I‚Äôd really appreciate any thoughts (good and bad) or feature suggestions.&lt;/p&gt;\n\n&lt;p&gt;Note: Agentainer doesn‚Äôt provide any LLM models or reasoning frameworks. We‚Äôre infrastructure only ‚Äî you bring the agent, and we handle deployment, state, and APIs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m56x5u",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Tradingoso",
          "discussion_type": null,
          "num_comments": 4,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m56x5u/a_solution_to_deploy_your_llm_agent_with_one_click/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m56x5u/a_solution_to_deploy_your_llm_agent_with_one_click/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753064166,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "\\&gt; Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \\\\texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \\\\texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \\\\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective.\n\n  \ntl;dr Human reasoning is different from LLM reasoning, and human reasoning can't be distilled into LLMs such that they significantly perform better on benchmarks compared to their foundational models. There seem to be certain structural patterns that lead to the emergence of reasoning abilities in LLMs.",
          "author_fullname": "t2_101haj",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "[2507.09850] The Challenge of Teaching Reasoning to LLMs Without RL or Distillation",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Discussion"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m568j8",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.94,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 16,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Discussion",
          "can_mod_post": false,
          "score": 16,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "default",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": false,
          "mod_note": null,
          "created": 1753062189,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "arxiv.org",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; Reasoning-capable language models achieve state-of-the-art performance in diverse complex tasks by generating long, explicit Chain-of-Thought (CoT) traces. While recent works show that base models can acquire such reasoning traces via reinforcement learning or distillation from stronger models like DeepSeek-R1, previous works demonstrate that even short CoT prompting without fine-tuning is able to improve reasoning. We ask whether long CoT can be induced in a base model using only prompting or minimal tuning. Using just 20 long CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly fine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms the much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of high-quality examples can unlock strong reasoning capabilities. We further explore using CoT data from non-reasoning models and human annotators, enhanced with prompt engineering, multi-pass editing, and structural guidance. However, neither matches the performance of reasoning model traces, suggesting that certain latent qualities of expert CoT are difficult to replicate. We analyze key properties of reasoning data, such as problem difficulty, diversity, and answer length, that influence reasoning distillation. While challenges remain, we are optimistic that carefully curated human-written CoT, even in small quantities, can activate reasoning behaviors in base models. We release our human-authored dataset across refinement stages and invite further investigation into what makes small-scale reasoning supervision so effective.&lt;/p&gt;\n\n&lt;p&gt;tl;dr Human reasoning is different from LLM reasoning, and human reasoning can&amp;#39;t be distilled into LLMs such that they significantly perform better on benchmarks compared to their foundational models. There seem to be certain structural patterns that lead to the emergence of reasoning abilities in LLMs.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "url_overridden_by_dest": "https://arxiv.org/abs/2507.09850",
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#646d73",
          "id": "1m568j8",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "TheRealMasonMac",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m568j8/250709850_the_challenge_of_teaching_reasoning_to/",
          "stickied": false,
          "url": "https://arxiv.org/abs/2507.09850",
          "subreddit_subscribers": 502515,
          "created_utc": 1753062189,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "If you‚Äôve spent any amount of time experimenting with local LLMs you know that high quality datasets are the foundation of great results. But tracking down relevant well labeled and community vetted datasets especially ones that match your specific use case can be a huge headache.\n\nWhether you‚Äôre:\n\n* Fine tuning models for chat code summarization or instruction following\n* Exploring niche domains or low resource languages\n* Or just tired of endlessly sifting through generic archives\n\nC.J. Jones has been curating a growing collection of¬†**public datasets**¬†designed to accelerate all sorts of local LLM workflows. Think everything from diverse conversational datasets QA pairs and synthetic instructional data to domain specific corpora you won‚Äôt find in the usual ‚Äúawesome lists.‚Äù\n\nWhat‚Äôs on offer?\n\n* Regular spotlights on unique and newly released datasets\n* Links to less known resources for local model training finetuning\n* Community discussion and tips on dataset selection cleaning and use\n* Opportunities to request suggest datasets for your projects\n\nHere is the Community Facebook page:  \n[**facebook.com/profile.php?id=61578125657947**](https://www.facebook.com/profile.php?id=61578125657947)\n\nOr join us on discord if you have any questions and want to learn more:  \n[https://discord.gg/aTbRrQ67ju](https://discord.gg/aTbRrQ67ju)\n\nIf you‚Äôre always searching for your next ‚Äúunfair advantage‚Äù dataset or you want a community approach to sourcing and evaluating data for local models stop by share your challenges and let‚Äôs build better LLM stacks together.\n\nQuestions or requests for dataset types? Drop them here or on the page!",
          "author_fullname": "t2_1tz5xea0uy",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Hitting Data Walls with Local LLM Projects? Check Out This Curated Dataset Resource!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m566jg",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.67,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 2,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 2,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": 1753133074,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753062027,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you‚Äôve spent any amount of time experimenting with local LLMs you know that high quality datasets are the foundation of great results. But tracking down relevant well labeled and community vetted datasets especially ones that match your specific use case can be a huge headache.&lt;/p&gt;\n\n&lt;p&gt;Whether you‚Äôre:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Fine tuning models for chat code summarization or instruction following&lt;/li&gt;\n&lt;li&gt;Exploring niche domains or low resource languages&lt;/li&gt;\n&lt;li&gt;Or just tired of endlessly sifting through generic archives&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;C.J. Jones has been curating a growing collection of¬†&lt;strong&gt;public datasets&lt;/strong&gt;¬†designed to accelerate all sorts of local LLM workflows. Think everything from diverse conversational datasets QA pairs and synthetic instructional data to domain specific corpora you won‚Äôt find in the usual ‚Äúawesome lists.‚Äù&lt;/p&gt;\n\n&lt;p&gt;What‚Äôs on offer?&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Regular spotlights on unique and newly released datasets&lt;/li&gt;\n&lt;li&gt;Links to less known resources for local model training finetuning&lt;/li&gt;\n&lt;li&gt;Community discussion and tips on dataset selection cleaning and use&lt;/li&gt;\n&lt;li&gt;Opportunities to request suggest datasets for your projects&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Here is the Community Facebook page:&lt;br/&gt;\n&lt;a href=\"https://www.facebook.com/profile.php?id=61578125657947\"&gt;&lt;strong&gt;facebook.com/profile.php?id=61578125657947&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Or join us on discord if you have any questions and want to learn more:&lt;br/&gt;\n&lt;a href=\"https://discord.gg/aTbRrQ67ju\"&gt;https://discord.gg/aTbRrQ67ju&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you‚Äôre always searching for your next ‚Äúunfair advantage‚Äù dataset or you want a community approach to sourcing and evaluating data for local models stop by share your challenges and let‚Äôs build better LLM stacks together.&lt;/p&gt;\n\n&lt;p&gt;Questions or requests for dataset types? Drop them here or on the page!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m566jg",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Creepy-Potential3408",
          "discussion_type": null,
          "num_comments": 2,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m566jg/hitting_data_walls_with_local_llm_projects_check/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m566jg/hitting_data_walls_with_local_llm_projects_check/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753062027,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm experimenting with lightweight finetuning of phi-4-mini to alter its speaking style for a project ‚Äî think tonal adjustments like high-energy, friendly, getting rid of that ‚ÄúI am a artificial intelligence assistant‚Ä¶‚Äù stuff, etc. I still want to preserve all tool calling functions (Python, web search, image generation, etc.) and not break its multi-turn conversation.\n\nKey needs:\n\n‚Äì Non-destructive to function calling behavior\n\n‚Äì Has to be runnable on Colab (no GPU locally)\n\n\\- 0 Budget: No MonsterAPI or paid stuff\n\n\\- Keep it small: Under 5GB (After being quantized to GGUF)\n\n\\- Be able to be exported, converted to gguf, and run with ollama\n\nI‚Äôm not doing instruction tuning from scratch, just style injection over chat data.\n\nAny recommendations on a colab that can help me keep auxiliary functionality intact while customizing tone? I want to do basically what Just Rayan (On youtube) did, but with Phi4-Mini, and keeping tool calling functions.",
          "author_fullname": "t2_afskfa1c",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Help with Finetuning Phi4-Mini",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m563lh",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.33,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753061780,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm experimenting with lightweight finetuning of phi-4-mini to alter its speaking style for a project ‚Äî think tonal adjustments like high-energy, friendly, getting rid of that ‚ÄúI am a artificial intelligence assistant‚Ä¶‚Äù stuff, etc. I still want to preserve all tool calling functions (Python, web search, image generation, etc.) and not break its multi-turn conversation.&lt;/p&gt;\n\n&lt;p&gt;Key needs:&lt;/p&gt;\n\n&lt;p&gt;‚Äì Non-destructive to function calling behavior&lt;/p&gt;\n\n&lt;p&gt;‚Äì Has to be runnable on Colab (no GPU locally)&lt;/p&gt;\n\n&lt;p&gt;- 0 Budget: No MonsterAPI or paid stuff&lt;/p&gt;\n\n&lt;p&gt;- Keep it small: Under 5GB (After being quantized to GGUF)&lt;/p&gt;\n\n&lt;p&gt;- Be able to be exported, converted to gguf, and run with ollama&lt;/p&gt;\n\n&lt;p&gt;I‚Äôm not doing instruction tuning from scratch, just style injection over chat data.&lt;/p&gt;\n\n&lt;p&gt;Any recommendations on a colab that can help me keep auxiliary functionality intact while customizing tone? I want to do basically what Just Rayan (On youtube) did, but with Phi4-Mini, and keeping tool calling functions.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m563lh",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Witty_Mycologist_995",
          "discussion_type": null,
          "num_comments": 5,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m563lh/help_with_finetuning_phi4mini/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m563lh/help_with_finetuning_phi4mini/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753061780,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I've seen people say 60/s and i've seen 22000/sec, I don't even know who to believe anymore.\n\nAlso how much does optimizing boost the tokens output speed?  ",
          "author_fullname": "t2_48vjfixh",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "How fast is gemma 3 27b on an H100? how many tokens per second can I expect?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m55rrt",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.89,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 35,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 35,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753060835,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve seen people say 60/s and i&amp;#39;ve seen 22000/sec, I don&amp;#39;t even know who to believe anymore.&lt;/p&gt;\n\n&lt;p&gt;Also how much does optimizing boost the tokens output speed?  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m55rrt",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "ThatIsNotIllegal",
          "discussion_type": null,
          "num_comments": 19,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m55rrt/how_fast_is_gemma_3_27b_on_an_h100_how_many/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753060835,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm writing a novel that‚Äôs near-future literary fiction / soft dystopia / psychological tragedy with erotic elements. I‚Äôm subscribed to ChatGPT and Claude, but built a PC to move to local AI without limits and guardrails for the NSFW stuff. \n\nWhat‚Äôs the best workflow for me? I downloaded Oobabooga and a MythosMax model, but not really sure how to add in context and instructions. There are pre populated templates and I don‚Äôt understand if I‚Äôm supposed to work within those or overwrite them. Also not sure if these were the best choices so appreciate any recommendations. \n\nWant something that‚Äôs really good for my genre, especially dark/gritty/nsfw with lyrical prose and stream of consciousness style. \n\nMy hardware: \n- CPU: Ryzen 7950x\n- GPU: 3090\n- RAM: 96GB 6400mhz \n\n",
          "author_fullname": "t2_hhsbgspn",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best novel writing workflow?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m5586n",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.64,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 6,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 6,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753059242,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm writing a novel that‚Äôs near-future literary fiction / soft dystopia / psychological tragedy with erotic elements. I‚Äôm subscribed to ChatGPT and Claude, but built a PC to move to local AI without limits and guardrails for the NSFW stuff. &lt;/p&gt;\n\n&lt;p&gt;What‚Äôs the best workflow for me? I downloaded Oobabooga and a MythosMax model, but not really sure how to add in context and instructions. There are pre populated templates and I don‚Äôt understand if I‚Äôm supposed to work within those or overwrite them. Also not sure if these were the best choices so appreciate any recommendations. &lt;/p&gt;\n\n&lt;p&gt;Want something that‚Äôs really good for my genre, especially dark/gritty/nsfw with lyrical prose and stream of consciousness style. &lt;/p&gt;\n\n&lt;p&gt;My hardware: \n- CPU: Ryzen 7950x\n- GPU: 3090\n- RAM: 96GB 6400mhz &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m5586n",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "AccidentalFolklore",
          "discussion_type": null,
          "num_comments": 24,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m5586n/best_novel_writing_workflow/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753059242,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hi guys, how are you?\n\nI'm doing research on the automation market ‚Äî especially automation for small businesses, repetitive tasks, integrations with systems, bots, among other things. I want to better understand two specific pains:\n\n1. For those who want to sell automations (freelancers, agencies, devs, etc.):\n‚ÄÉ‚Äì What has made it difficult to close customers?\n‚ÄÉ‚Äì Where do you find (or miss) opportunities?\n‚ÄÉ‚Äì What does the customer generally not understand or value?\n‚ÄÉ‚Äì How do you validate that automation makes sense for the client‚Äôs business?\n\n\n2. For those who want to hire someone to automate things:\n‚ÄÉ‚Äì What is the biggest difficulty in finding someone trustworthy?\n‚ÄÉ‚Äì What makes you trust (or distrust) those who offer the service?\n‚ÄÉ‚Äì Where do you usually look for this type of professional?\n\n\n\nThe idea is to understand the pain on both sides ‚Äî those who sell and those who hire ‚Äî to come up with a more practical and useful solution. Any experience you have (good or bad) helps a lot!\n\nIt would be really appreciated if you could share üôè",
          "author_fullname": "t2_sos8vp8g",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "What are the biggest challenges in selling automations (and finding someone to implement them)? Looking for real insights from everyone!",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52sj5",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.14,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753052472,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys, how are you?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m doing research on the automation market ‚Äî especially automation for small businesses, repetitive tasks, integrations with systems, bots, among other things. I want to better understand two specific pains:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;For those who want to sell automations (freelancers, agencies, devs, etc.):\n‚ÄÉ‚Äì What has made it difficult to close customers?\n‚ÄÉ‚Äì Where do you find (or miss) opportunities?\n‚ÄÉ‚Äì What does the customer generally not understand or value?\n‚ÄÉ‚Äì How do you validate that automation makes sense for the client‚Äôs business?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For those who want to hire someone to automate things:\n‚ÄÉ‚Äì What is the biggest difficulty in finding someone trustworthy?\n‚ÄÉ‚Äì What makes you trust (or distrust) those who offer the service?\n‚ÄÉ‚Äì Where do you usually look for this type of professional?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The idea is to understand the pain on both sides ‚Äî those who sell and those who hire ‚Äî to come up with a more practical and useful solution. Any experience you have (good or bad) helps a lot!&lt;/p&gt;\n\n&lt;p&gt;It would be really appreciated if you could share üôè&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m52sj5",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "Present-Entry8676",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52sj5/what_are_the_biggest_challenges_in_selling/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52sj5/what_are_the_biggest_challenges_in_selling/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753052472,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "Hey all ‚Äî I‚Äôm working on building out Atlas Grid, a new network of U.S.-based GPU hosts focused on reliability and simplicity for devs and researchers.\n\nWe‚Äôve got a few committed rigs already online, including a 3080 Ti and 3070 Ti, running on stable secondary machines here in the U.S. ‚Äî ideal for fine-tuning, inference, or small-scale training jobs.\n\nWe‚Äôre pricing below vast.ai, and with a more few advantages:\n\nAll domestic hosts = lower latency, no language or support barriers\n\nPrepaid options = no surprise fees or platform overhead\n\nVetted machines only = Docker/NVIDIA-ready, high uptime\n\nIf you‚Äôre working on a project and want affordable compute, DM me or comment below! ",
          "author_fullname": "t2_4rgw41xb",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "U.S. GPU compute available",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Resources"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52r9e",
          "quarantine": false,
          "link_flair_text_color": "light",
          "upvote_ratio": 0.38,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 0,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Resources",
          "can_mod_post": false,
          "score": 0,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753052387,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all ‚Äî I‚Äôm working on building out Atlas Grid, a new network of U.S.-based GPU hosts focused on reliability and simplicity for devs and researchers.&lt;/p&gt;\n\n&lt;p&gt;We‚Äôve got a few committed rigs already online, including a 3080 Ti and 3070 Ti, running on stable secondary machines here in the U.S. ‚Äî ideal for fine-tuning, inference, or small-scale training jobs.&lt;/p&gt;\n\n&lt;p&gt;We‚Äôre pricing below vast.ai, and with a more few advantages:&lt;/p&gt;\n\n&lt;p&gt;All domestic hosts = lower latency, no language or support barriers&lt;/p&gt;\n\n&lt;p&gt;Prepaid options = no surprise fees or platform overhead&lt;/p&gt;\n\n&lt;p&gt;Vetted machines only = Docker/NVIDIA-ready, high uptime&lt;/p&gt;\n\n&lt;p&gt;If you‚Äôre working on a project and want affordable compute, DM me or comment below! &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": true,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#ccac2b",
          "id": "1m52r9e",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "No_Professional_2726",
          "discussion_type": null,
          "num_comments": 22,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52r9e/us_gpu_compute_available/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52r9e/us_gpu_compute_available/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753052387,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      },
      {
        "kind": "t3",
        "data": {
          "approved_at_utc": null,
          "subreddit": "LocalLLaMA",
          "selftext": "I‚Äôm looking for a solid RAG pipeline that works well with SGLang + AnythingLLM. Something that can handle technical docs, math textbooks with lots of formulas, research papers, and diagrams. The RAG in AnythingLLM is, well, not great. What setups actually work for you?",
          "author_fullname": "t2_indszg3q",
          "saved": false,
          "mod_reason_title": null,
          "gilded": 0,
          "clicked": false,
          "title": "Best RAG pipeline for math-heavy documents?",
          "link_flair_richtext": [
            {
              "e": "text",
              "t": "Question | Help"
            }
          ],
          "subreddit_name_prefixed": "r/LocalLLaMA",
          "hidden": false,
          "pwls": 6,
          "link_flair_css_class": "",
          "downs": 0,
          "thumbnail_height": null,
          "top_awarded_type": null,
          "hide_score": false,
          "name": "t3_1m52h8x",
          "quarantine": false,
          "link_flair_text_color": "dark",
          "upvote_ratio": 0.9,
          "author_flair_background_color": null,
          "subreddit_type": "public",
          "ups": 7,
          "total_awards_received": 0,
          "media_embed": {},
          "thumbnail_width": null,
          "author_flair_template_id": null,
          "is_original_content": false,
          "user_reports": [],
          "secure_media": null,
          "is_reddit_media_domain": false,
          "is_meta": false,
          "category": null,
          "secure_media_embed": {},
          "link_flair_text": "Question | Help",
          "can_mod_post": false,
          "score": 7,
          "approved_by": null,
          "is_created_from_ads_ui": false,
          "author_premium": false,
          "thumbnail": "self",
          "edited": false,
          "author_flair_css_class": null,
          "author_flair_richtext": [],
          "gildings": {},
          "content_categories": null,
          "is_self": true,
          "mod_note": null,
          "created": 1753051632,
          "link_flair_type": "richtext",
          "wls": 6,
          "removed_by_category": null,
          "banned_by": null,
          "author_flair_type": "text",
          "domain": "self.LocalLLaMA",
          "allow_live_comments": false,
          "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm looking for a solid RAG pipeline that works well with SGLang + AnythingLLM. Something that can handle technical docs, math textbooks with lots of formulas, research papers, and diagrams. The RAG in AnythingLLM is, well, not great. What setups actually work for you?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
          "likes": null,
          "suggested_sort": null,
          "banned_at_utc": null,
          "view_count": null,
          "archived": false,
          "no_follow": false,
          "is_crosspostable": false,
          "pinned": false,
          "over_18": false,
          "all_awardings": [],
          "awarders": [],
          "media_only": false,
          "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
          "can_gild": false,
          "spoiler": false,
          "locked": false,
          "author_flair_text": null,
          "treatment_tags": [],
          "visited": false,
          "removed_by": null,
          "num_reports": null,
          "distinguished": null,
          "subreddit_id": "t5_81eyvm",
          "author_is_blocked": false,
          "mod_reason_by": null,
          "removal_reason": null,
          "link_flair_background_color": "#5a74cc",
          "id": "1m52h8x",
          "is_robot_indexable": true,
          "report_reasons": null,
          "author": "PO-ll-UX",
          "discussion_type": null,
          "num_comments": 0,
          "send_replies": true,
          "contest_mode": false,
          "mod_reports": [],
          "author_patreon_flair": false,
          "author_flair_text_color": null,
          "permalink": "/r/LocalLLaMA/comments/1m52h8x/best_rag_pipeline_for_mathheavy_documents/",
          "stickied": false,
          "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m52h8x/best_rag_pipeline_for_mathheavy_documents/",
          "subreddit_subscribers": 502515,
          "created_utc": 1753051632,
          "num_crossposts": 0,
          "media": null,
          "is_video": false
        }
      }
    ],
    "before": null
  }
}