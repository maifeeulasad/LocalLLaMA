[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have been trying various LLMs running locally (on a 64GB DDR4 Threadripper + 5090 box, on llama.cpp) to try to arrive at a co-maintainer for my established FOSS project.  I would like it to see the code and propose patches in diff (or direct to git by MCP) form.\n\nMy current theory is that the pressure to run quantized models is a major cause of why I can't get any model to produce a diff / patch that will apply to my project, they are all broken or slide off into gibberish or forgetfulness.  It's like a kind of pervasive brain damage.  At least, that is my hope, it may get disproved at any time by slop diffs coming out of a BF16 model.\n\nI am wondering if anyone has been able to run a large BF16 model successfully locally, or even remotely as a service, so I can assess whether my theory is just copium and it's all trash out there.\n\nThe next reachable step up for me seems to be an 8480ES + 512GB DDR5, but even this seems too small if the goal is to avoid quantization.\n\nI am reluctant to rent a H100 machine because I can only spend part of my time on this and the costs rack up all the time.\n\nA related difficulty is the context size, I guess most of the related sources can fit in 128K context, but this magnifies the compute needs accordingly.\n\nOpinions and experience welcome!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Escaping quantization brain damage with BF16?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m3qg3w",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_z5dk4aa",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752910988,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have been trying various LLMs running locally (on a 64GB DDR4 Threadripper + 5090 box, on llama.cpp) to try to arrive at a co-maintainer for my established FOSS project.  I would like it to see the code and propose patches in diff (or direct to git by MCP) form.&lt;/p&gt;\n\n&lt;p&gt;My current theory is that the pressure to run quantized models is a major cause of why I can&amp;#39;t get any model to produce a diff / patch that will apply to my project, they are all broken or slide off into gibberish or forgetfulness.  It&amp;#39;s like a kind of pervasive brain damage.  At least, that is my hope, it may get disproved at any time by slop diffs coming out of a BF16 model.&lt;/p&gt;\n\n&lt;p&gt;I am wondering if anyone has been able to run a large BF16 model successfully locally, or even remotely as a service, so I can assess whether my theory is just copium and it&amp;#39;s all trash out there.&lt;/p&gt;\n\n&lt;p&gt;The next reachable step up for me seems to be an 8480ES + 512GB DDR5, but even this seems too small if the goal is to avoid quantization.&lt;/p&gt;\n\n&lt;p&gt;I am reluctant to rent a H100 machine because I can only spend part of my time on this and the costs rack up all the time.&lt;/p&gt;\n\n&lt;p&gt;A related difficulty is the context size, I guess most of the related sources can fit in 128K context, but this magnifies the compute needs accordingly.&lt;/p&gt;\n\n&lt;p&gt;Opinions and experience welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m3qg3w",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "bitrumpled",
            "discussion_type": null,
            "num_comments": 23,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/",
            "subreddit_subscribers": 501232,
            "created_utc": 1752910988,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n3ymd00",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bitrumpled",
                      "can_mod_post": false,
                      "created_utc": 1752912320,
                      "send_replies": true,
                      "parent_id": "t1_n3ylegp",
                      "score": -2,
                      "author_fullname": "t2_z5dk4aa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have tried a variety of models (llama-3, llama-4, some forms of Qwen, some forms of Gemini) at different quantizations, none of them could produce usable output, so I am open to recommendations from the angle of what I could run at BF16.\n\nSince eg Kimi-2 is 2TB at BF16, I am afraid trying BF 16 with a large model is going to need a different approach with the kind of hw.  So I am asking about how to come at that as much as choosing which model.\n\n(I am also guessing most of the services offering to talk to the models are actually quite heavily quantized rather than running at BF16, none of them seem to explain the quantization level of the model you are actually talking to.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n3ymd00",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have tried a variety of models (llama-3, llama-4, some forms of Qwen, some forms of Gemini) at different quantizations, none of them could produce usable output, so I am open to recommendations from the angle of what I could run at BF16.&lt;/p&gt;\n\n&lt;p&gt;Since eg Kimi-2 is 2TB at BF16, I am afraid trying BF 16 with a large model is going to need a different approach with the kind of hw.  So I am asking about how to come at that as much as choosing which model.&lt;/p&gt;\n\n&lt;p&gt;(I am also guessing most of the services offering to talk to the models are actually quite heavily quantized rather than running at BF16, none of them seem to explain the quantization level of the model you are actually talking to.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m3qg3w",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3ymd00/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752912320,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n3ylegp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "z_3454_pfk",
            "can_mod_post": false,
            "created_utc": 1752911789,
            "send_replies": true,
            "parent_id": "t3_1m3qg3w",
            "score": 6,
            "author_fullname": "t2_askwa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "bf16 produces very little degradation compared to full fp32 weights. some models are more sensitive to quantization then others. you never mentioned which model you’re using.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3ylegp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;bf16 produces very little degradation compared to full fp32 weights. some models are more sensitive to quantization then others. you never mentioned which model you’re using.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3ylegp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752911789,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3qg3w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n3z73wh",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "bitrumpled",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n3z5xkj",
                                                              "score": 0,
                                                              "author_fullname": "t2_z5dk4aa",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "\\&gt; That's assuming\n\nNo, \"twice\" is referring to having to pay two AI services and compare the results.\n\n\\&gt; Also, did you test if the models work correctly at full precision, or did you only assume they do?\n\n... the whole goal of this post is to find out what, if anything, I need to do to test models at BF16.  If they can't provide usable patches either, I will know to give up and try again in some months / years, because the problems do not in fact start with the quantizations.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n3z73wh",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; That&amp;#39;s assuming&lt;/p&gt;\n\n&lt;p&gt;No, &amp;quot;twice&amp;quot; is referring to having to pay two AI services and compare the results.&lt;/p&gt;\n\n&lt;p&gt;&amp;gt; Also, did you test if the models work correctly at full precision, or did you only assume they do?&lt;/p&gt;\n\n&lt;p&gt;... the whole goal of this post is to find out what, if anything, I need to do to test models at BF16.  If they can&amp;#39;t provide usable patches either, I will know to give up and try again in some months / years, because the problems do not in fact start with the quantizations.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1m3qg3w",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z73wh/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1752923694,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1752923694,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 0
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n3z5xkj",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Awwtifishal",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n3z2z8a",
                                                    "score": 1,
                                                    "author_fullname": "t2_1d96a8k10t",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "That's assuming the cost of local inference is comparable to data centers. For big models local inference can be much more expensive at the precisions you want.\n\nAlso, did you test if the models work correctly at full precision, or did you only assume they do?",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n3z5xkj",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s assuming the cost of local inference is comparable to data centers. For big models local inference can be much more expensive at the precisions you want.&lt;/p&gt;\n\n&lt;p&gt;Also, did you test if the models work correctly at full precision, or did you only assume they do?&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m3qg3w",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z5xkj/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1752923129,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1752923129,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n3z2z8a",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "bitrumpled",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n3z0n06",
                                          "score": 1,
                                          "author_fullname": "t2_z5dk4aa",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Sure, it sounds like it should detect malarky.  Or, I can run it locally and not pay double the cost, duplicate all my queries, and deal with syncing and checking both queries.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n3z2z8a",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sure, it sounds like it should detect malarky.  Or, I can run it locally and not pay double the cost, duplicate all my queries, and deal with syncing and checking both queries.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m3qg3w",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z2z8a/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1752921666,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1752921666,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n3z0n06",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Awwtifishal",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n3ymm3v",
                                "score": 1,
                                "author_fullname": "t2_1d96a8k10t",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If you can use the same seed and samplers on multiple providers (that uses the same inference software), you can make the request to two of them and it will be highly likely that they match, so when they don't it's an indication that the result has been probably messed up with.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n3z0n06",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you can use the same seed and samplers on multiple providers (that uses the same inference software), you can make the request to two of them and it will be highly likely that they match, so when they don&amp;#39;t it&amp;#39;s an indication that the result has been probably messed up with.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m3qg3w",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z0n06/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752920416,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752920416,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n3ys406",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "lakySK",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n3yp5va",
                                                    "score": 0,
                                                    "author_fullname": "t2_y9y2q",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "No worries, I’m looking for an excuse to build an 8040ES system with 1TB RAM as much as the next person here! Just like to play devil’s advocate as I struggle to find a sensible use case. \n\nFor you, the fact that this sounds like an async agent, you can probably tolerate the slowness that the system would bring in terms of the prompt processing. \n\nNot sure how big your project is, but you could preprocess large contexts and save the KV cache to eliminate the need to wait half an hour for every first token. ",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n3ys406",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No worries, I’m looking for an excuse to build an 8040ES system with 1TB RAM as much as the next person here! Just like to play devil’s advocate as I struggle to find a sensible use case. &lt;/p&gt;\n\n&lt;p&gt;For you, the fact that this sounds like an async agent, you can probably tolerate the slowness that the system would bring in terms of the prompt processing. &lt;/p&gt;\n\n&lt;p&gt;Not sure how big your project is, but you could preprocess large contexts and save the KV cache to eliminate the need to wait half an hour for every first token. &lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m3qg3w",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3ys406/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1752915589,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1752915589,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 0
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n3yp5va",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "bitrumpled",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n3ynn0m",
                                          "score": -1,
                                          "author_fullname": "t2_z5dk4aa",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "&gt; So I’m struggling to see a benefit of a local deployment for this use case to be honest.\n\nI apologize if it seemed like I was waiting for your opinion on that, and caused you to waste your time giving it.\n\nI carefully review PRs, but a \"co-maintainer\" if it is successful will end up with closer and more direct access to the project.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n3yp5va",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;So I’m struggling to see a benefit of a local deployment for this use case to be honest.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I apologize if it seemed like I was waiting for your opinion on that, and caused you to waste your time giving it.&lt;/p&gt;\n\n&lt;p&gt;I carefully review PRs, but a &amp;quot;co-maintainer&amp;quot; if it is successful will end up with closer and more direct access to the project.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m3qg3w",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3yp5va/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1752913917,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1752913917,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": -1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n3ynn0m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "lakySK",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n3ymm3v",
                                "score": 1,
                                "author_fullname": "t2_y9y2q",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That’s fair, however, it’s an open source project, if someone wants to mess with it, they can just submit a PR, no?\n\nAnd any AI-generated code at this point can definitely not be trusted any more than a random person submitting a PR and needs to be carefully reviewed. \n\nSo I’m struggling to see a benefit of a local deployment for this use case to be honest. ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n3ynn0m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s fair, however, it’s an open source project, if someone wants to mess with it, they can just submit a PR, no?&lt;/p&gt;\n\n&lt;p&gt;And any AI-generated code at this point can definitely not be trusted any more than a random person submitting a PR and needs to be carefully reviewed. &lt;/p&gt;\n\n&lt;p&gt;So I’m struggling to see a benefit of a local deployment for this use case to be honest. &lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m3qg3w",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3ynn0m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752913037,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752913037,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n3ymm3v",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bitrumpled",
                      "can_mod_post": false,
                      "created_utc": 1752912461,
                      "send_replies": true,
                      "parent_id": "t1_n3ym7gi",
                      "score": 0,
                      "author_fullname": "t2_z5dk4aa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I do not want external providers to be able to actively and maliciously mess with what is in the patches.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n3ymm3v",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I do not want external providers to be able to actively and maliciously mess with what is in the patches.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m3qg3w",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3ymm3v/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752912461,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n3ym7gi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lakySK",
            "can_mod_post": false,
            "created_utc": 1752912236,
            "send_replies": true,
            "parent_id": "t3_1m3qg3w",
            "score": 1,
            "author_fullname": "t2_y9y2q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just curious, what is your motivation for running this locally?\n\nUsually, the main reason is privacy concerns. But with an open source project it doesn’t make too much sense to me. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3ym7gi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just curious, what is your motivation for running this locally?&lt;/p&gt;\n\n&lt;p&gt;Usually, the main reason is privacy concerns. But with an open source project it doesn’t make too much sense to me. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3ym7gi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752912236,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3qg3w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n3z3lwf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bitrumpled",
                      "can_mod_post": false,
                      "created_utc": 1752921989,
                      "send_replies": true,
                      "parent_id": "t1_n3z02jk",
                      "score": 1,
                      "author_fullname": "t2_z5dk4aa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks.  But, this still requires me to be able to run the BF16 model to get outputs to use in the training, so as an answer to \"how can I run a BF16 model\" this isn't super useful.  And I need to rent a big rig to do the training IIUI.\n\nThe outer reason for trying to run BF16 at all is to answer the question, \"are the models usable for maintaining at all\"?  Because the casually quantized ones are not, to the extent they cannot produce a diff I can give to `patch` successfully, before even considering the quality of the changes in the patch.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n3z3lwf",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks.  But, this still requires me to be able to run the BF16 model to get outputs to use in the training, so as an answer to &amp;quot;how can I run a BF16 model&amp;quot; this isn&amp;#39;t super useful.  And I need to rent a big rig to do the training IIUI.&lt;/p&gt;\n\n&lt;p&gt;The outer reason for trying to run BF16 at all is to answer the question, &amp;quot;are the models usable for maintaining at all&amp;quot;?  Because the casually quantized ones are not, to the extent they cannot produce a diff I can give to &lt;code&gt;patch&lt;/code&gt; successfully, before even considering the quality of the changes in the patch.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m3qg3w",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z3lwf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752921989,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n3z02jk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Longjumpingfish0403",
            "can_mod_post": false,
            "created_utc": 1752920103,
            "send_replies": true,
            "parent_id": "t3_1m3qg3w",
            "score": 1,
            "author_fullname": "t2_jarttha4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's worth exploring the approach of model distillation for your needs. By training a smaller model to mimic the large BF16 model's outputs, you could potentially reduce hardware demands without sacrificing much in terms of output quality. Also, check if any hybrid quantization techniques might offer a balance between performance and hardware constraints.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3z02jk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s worth exploring the approach of model distillation for your needs. By training a smaller model to mimic the large BF16 model&amp;#39;s outputs, you could potentially reduce hardware demands without sacrificing much in terms of output quality. Also, check if any hybrid quantization techniques might offer a balance between performance and hardware constraints.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z02jk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752920103,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3qg3w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n3z9tyg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ravage382",
            "can_mod_post": false,
            "created_utc": 1752924938,
            "send_replies": true,
            "parent_id": "t3_1m3qg3w",
            "score": 1,
            "author_fullname": "t2_9sf41",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Devstal in q8 has done the best for local coding tasks for me.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3z9tyg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Devstal in q8 has done the best for local coding tasks for me.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z9tyg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752924938,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3qg3w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n3zgesl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bitrumpled",
                      "can_mod_post": false,
                      "created_utc": 1752927726,
                      "send_replies": true,
                      "parent_id": "t1_n3zf7da",
                      "score": 1,
                      "author_fullname": "t2_z5dk4aa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks, today, I would say the same.  But I am guessing, none of the models you tried were in BF16?  I am wondering if the problems with diffs are coming from the quantization and not the model, that is why I want to try the same tests on a BF16 model.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n3zgesl",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, today, I would say the same.  But I am guessing, none of the models you tried were in BF16?  I am wondering if the problems with diffs are coming from the quantization and not the model, that is why I want to try the same tests on a BF16 model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m3qg3w",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3zgesl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752927726,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n3zf7da",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "And-Bee",
            "can_mod_post": false,
            "created_utc": 1752927223,
            "send_replies": true,
            "parent_id": "t3_1m3qg3w",
            "score": 1,
            "author_fullname": "t2_a81fjhk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I can’t get any model to produce any diff/patch that works. I think tools like cline get around this by getting the model to add tags in the code to do search and replace, otherwise you need to feed the model the code with line numbers so it knows which lines it can modify, adding a lot to the context length.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3zf7da",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I can’t get any model to produce any diff/patch that works. I think tools like cline get around this by getting the model to add tags in the code to do search and replace, otherwise you need to feed the model the code with line numbers so it knows which lines it can modify, adding a lot to the context length.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3zf7da/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752927223,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3qg3w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n3ymm1u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Guilty-History-9249",
            "can_mod_post": false,
            "created_utc": 1752912460,
            "send_replies": true,
            "parent_id": "t3_1m3qg3w",
            "score": 1,
            "author_fullname": "t2_rxk6hx4t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I literally received my new machine today, technically yesterday.  Just copied over 1 TB's of data from my old 4090 box to it.  Still getting things set up.  \nI got it to do AI training stuff as I've wasted my first 2.5 years in AI doing inference performance.  Time to dig into training and LLM's although I know all the basics and perhaps an expert on the performance side of inference.  \nIt has dual 5090's, a 7985WX threadripper, and 256 GB's of DDR5-6000 memory.\n\nI too am interested in studying fine tuning or even lora's(little used?) for LLM's.  \nTime to sleep...  1AM PST here.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3ymm1u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I literally received my new machine today, technically yesterday.  Just copied over 1 TB&amp;#39;s of data from my old 4090 box to it.  Still getting things set up.&lt;br/&gt;\nI got it to do AI training stuff as I&amp;#39;ve wasted my first 2.5 years in AI doing inference performance.  Time to dig into training and LLM&amp;#39;s although I know all the basics and perhaps an expert on the performance side of inference.&lt;br/&gt;\nIt has dual 5090&amp;#39;s, a 7985WX threadripper, and 256 GB&amp;#39;s of DDR5-6000 memory.&lt;/p&gt;\n\n&lt;p&gt;I too am interested in studying fine tuning or even lora&amp;#39;s(little used?) for LLM&amp;#39;s.&lt;br/&gt;\nTime to sleep...  1AM PST here.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3ymm1u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752912460,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3qg3w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n3zbc4e",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "bitrumpled",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n3z68gr",
                                                              "score": 1,
                                                              "author_fullname": "t2_z5dk4aa",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Thanks, this is the first actionable advice... I didn't disable swap before, I did that with `swapoff -a` and confirmed it with `swapon --show`.  For context, with that model it defaults to 40K, I forced it to 8K.\n\nMonitoring with top, I can see 1 x thread is at 99% for kswapd0 even so. llama-server is somewhere between 30xx % (ie, 30 threads maxxed) and 60xx (the actual max 60 threads) at the moment.  Despite that the load average is 60... I guess it means they are all trying to run but slowed down by the NVMe.  iostat shows 2300 tps / 176MB/s\n\nIt took ~1m to eat the 49 token prompt and then it's as above for 15m, I'll reply again when something changes.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n3zbc4e",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, this is the first actionable advice... I didn&amp;#39;t disable swap before, I did that with &lt;code&gt;swapoff -a&lt;/code&gt; and confirmed it with &lt;code&gt;swapon --show&lt;/code&gt;.  For context, with that model it defaults to 40K, I forced it to 8K.&lt;/p&gt;\n\n&lt;p&gt;Monitoring with top, I can see 1 x thread is at 99% for kswapd0 even so. llama-server is somewhere between 30xx % (ie, 30 threads maxxed) and 60xx (the actual max 60 threads) at the moment.  Despite that the load average is 60... I guess it means they are all trying to run but slowed down by the NVMe.  iostat shows 2300 tps / 176MB/s&lt;/p&gt;\n\n&lt;p&gt;It took ~1m to eat the 49 token prompt and then it&amp;#39;s as above for 15m, I&amp;#39;ll reply again when something changes.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1m3qg3w",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3zbc4e/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1752925604,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1752925604,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n3z68gr",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Aaaaaaaaaeeeee",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n3z2i1y",
                                                    "score": 2,
                                                    "author_fullname": "t2_el5pibmej",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Ok thanks, try a short prompt with the GPU backend disabled maybe it's a 5090 problem. Also disabling your swapfile.  you getting stuck at a certain point in Lama server binary or something? Have you lowered context to the minimum?",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n3z68gr",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ok thanks, try a short prompt with the GPU backend disabled maybe it&amp;#39;s a 5090 problem. Also disabling your swapfile.  you getting stuck at a certain point in Lama server binary or something? Have you lowered context to the minimum?&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m3qg3w",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z68gr/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1752923275,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1752923275,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n3z2i1y",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "bitrumpled",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n3z0t9c",
                                          "score": 0,
                                          "author_fullname": "t2_z5dk4aa",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yes this is all on Linux.  And the model is stored on a 2TB pcie gen 4 NVMe.\n\nI iteratively tried to maximize occupancy of my 5090 by looking at the situation on nvidia-smi.  I ended up with --gpu-layers 11 for Qwen3-235B-A22B-Q8_0.gguf.  But as I say: no output on my existing 64GB DDR4 box.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n3z2i1y",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes this is all on Linux.  And the model is stored on a 2TB pcie gen 4 NVMe.&lt;/p&gt;\n\n&lt;p&gt;I iteratively tried to maximize occupancy of my 5090 by looking at the situation on nvidia-smi.  I ended up with --gpu-layers 11 for Qwen3-235B-A22B-Q8_0.gguf.  But as I say: no output on my existing 64GB DDR4 box.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m3qg3w",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z2i1y/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1752921415,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1752921415,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 0
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n3z0t9c",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Aaaaaaaaaeeeee",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n3yru6i",
                                "score": 1,
                                "author_fullname": "t2_el5pibmej",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For me, I  use ubuntu Linux, downloading the .Safetensors models and see about conversion to gguf, to which I check the pull request to get an idea.\n\n\n\n\nOn Windows, I have tried falcon 180B and it took much longer and a lot of writing to disk. There was something wrong here, It might be that I have to disable the pagefile. \n\nI haven't actually heard others reports of people doing windows and the disc inference. Maybe you can abandon the idea for now.\n\n\n\n If you have a Linux box, Do you have a fast enough disk read speed? There are times the hard drive spinning disc is so slow That it has caused the device to overheat, It has happened when I tried to run float16 deepseekv2 from a USB drive. Please try with a solid state drive, I happen to have M2 PCIe Gen 4, with about 8GB/s but it's still possible with the others.\n\n\nMixture experts type models actually only have a certain amount of experts that are grabbed by the model router.\nThe rest of them are smartly inferenced in your machines vram or ram.\n\n\nHere's a good chart from a good man that shows some of this: \nhttps://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n3z0t9c",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For me, I  use ubuntu Linux, downloading the .Safetensors models and see about conversion to gguf, to which I check the pull request to get an idea.&lt;/p&gt;\n\n&lt;p&gt;On Windows, I have tried falcon 180B and it took much longer and a lot of writing to disk. There was something wrong here, It might be that I have to disable the pagefile. &lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t actually heard others reports of people doing windows and the disc inference. Maybe you can abandon the idea for now.&lt;/p&gt;\n\n&lt;p&gt; If you have a Linux box, Do you have a fast enough disk read speed? There are times the hard drive spinning disc is so slow That it has caused the device to overheat, It has happened when I tried to run float16 deepseekv2 from a USB drive. Please try with a solid state drive, I happen to have M2 PCIe Gen 4, with about 8GB/s but it&amp;#39;s still possible with the others.&lt;/p&gt;\n\n&lt;p&gt;Mixture experts type models actually only have a certain amount of experts that are grabbed by the model router.\nThe rest of them are smartly inferenced in your machines vram or ram.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s a good chart from a good man that shows some of this: \n&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1lzcuom/kimik2_is_a_deepseek_v3_with_more_experts/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m3qg3w",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3z0t9c/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752920510,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752920510,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n3yru6i",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bitrumpled",
                      "can_mod_post": false,
                      "created_utc": 1752915432,
                      "send_replies": true,
                      "parent_id": "t1_n3yq0r7",
                      "score": -1,
                      "author_fullname": "t2_z5dk4aa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I appreciate hearing that this is so in theory, but I downloaded one Q8 model 'Qwen3-235B-A22B-Q8_0.gguf' which totals to 250GB on disk, and left it running on a query for 9 hours before it stopped itself without any output, this is on a 64GB DDR4 box.\n\nI realize that is not an ideal platform to try it on, but clearly there is more to it than, \"All models even 2tb can be tested in bf16, regardless how little resources you may have.\".\n\nI am really asking what do I have to do on the hardware side, to be able to use a large Q8 (or much better, BF16) model.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n3yru6i",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I appreciate hearing that this is so in theory, but I downloaded one Q8 model &amp;#39;Qwen3-235B-A22B-Q8_0.gguf&amp;#39; which totals to 250GB on disk, and left it running on a query for 9 hours before it stopped itself without any output, this is on a 64GB DDR4 box.&lt;/p&gt;\n\n&lt;p&gt;I realize that is not an ideal platform to try it on, but clearly there is more to it than, &amp;quot;All models even 2tb can be tested in bf16, regardless how little resources you may have.&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;I am really asking what do I have to do on the hardware side, to be able to use a large Q8 (or much better, BF16) model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m3qg3w",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3yru6i/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752915432,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n3yq0r7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Aaaaaaaaaeeeee",
            "can_mod_post": false,
            "created_utc": 1752914407,
            "send_replies": true,
            "parent_id": "t3_1m3qg3w",
            "score": 0,
            "author_fullname": "t2_el5pibmej",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That is possible, for smaller models you can run the full transformers model with the fast gpu exllamav2 backend in text-gen-webui. Also, you can convert to gguf in order to get llama.cpp's backend which has offloading properties to your  RAM+vram and then from ssd disc storage. All models even 2tb can be tested in bf16, regardless how little resources you may have.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n3yq0r7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That is possible, for smaller models you can run the full transformers model with the fast gpu exllamav2 backend in text-gen-webui. Also, you can convert to gguf in order to get llama.cpp&amp;#39;s backend which has offloading properties to your  RAM+vram and then from ssd disc storage. All models even 2tb can be tested in bf16, regardless how little resources you may have.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m3qg3w/escaping_quantization_brain_damage_with_bf16/n3yq0r7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752914407,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m3qg3w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]