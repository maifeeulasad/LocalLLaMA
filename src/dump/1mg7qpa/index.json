[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "We've been working on an LLM proxy, balancer &amp; model unifier based on a few other projects we've created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. \n\nThis was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),\n\nSo some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.\n\nOlla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.\n\nAdd your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.\n\nThe attempt to unify across providers wasn't as successful - as in, both LMStudio &amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.\n\nGithub: [https://github.com/thushan/olla](https://github.com/thushan/olla) (golang)\n\nWould love to know your thoughts. \n\nOlla is still in its infancy, so we don't have auth implemented etc but there are plans in the future.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "is_gallery": true,
            "title": "Announcing Olla - LLM Load Balancer, Proxy &amp; Model Unifier for Ollama / LM Studio &amp; OpenAI Compatible backends",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 77,
            "top_awarded_type": null,
            "name": "t3_1mg7qpa",
            "media_metadata": {
              "5nw1vlfoqpgf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 108,
                    "x": 108,
                    "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb3a2dbb77b7c17f0c25c782f45d5f1145b6df22"
                  },
                  {
                    "y": 216,
                    "x": 216,
                    "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31ee39f327a201eb2bd4da0cab803303d2c7f1d4"
                  },
                  {
                    "y": 320,
                    "x": 320,
                    "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bb4a2254c5239c0b216fd14bb6c0f0e38e7b6f82"
                  },
                  {
                    "y": 640,
                    "x": 640,
                    "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=97c3ad10efc7e082eca21673151d8898dbe7206d"
                  }
                ],
                "s": {
                  "y": 648,
                  "x": 648,
                  "u": "https://preview.redd.it/5nw1vlfoqpgf1.png?width=648&amp;format=png&amp;auto=webp&amp;s=d12b43c56e8a84fc36c0b40b7f880085d0c6c4c4"
                },
                "id": "5nw1vlfoqpgf1"
              },
              "tfkhbanjqpgf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 151,
                    "x": 108,
                    "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=52ec679a9bd4d342bee02ef2f1165eb230e72cce"
                  },
                  {
                    "y": 302,
                    "x": 216,
                    "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d21c96bf9e46c48e0c9f056396472b42eebe4857"
                  },
                  {
                    "y": 448,
                    "x": 320,
                    "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3471f6cecc4f0ca3888500245adb1fa96e1a0776"
                  },
                  {
                    "y": 896,
                    "x": 640,
                    "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d04c10c876d43302f632b7e7171a15a08b4e8666"
                  }
                ],
                "s": {
                  "y": 1077,
                  "x": 769,
                  "u": "https://preview.redd.it/tfkhbanjqpgf1.png?width=769&amp;format=png&amp;auto=webp&amp;s=68776f8d4bf7c2b5ade021a1617e272f3a83f6f0"
                },
                "id": "tfkhbanjqpgf1"
              },
              "pvipbao9npgf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 59,
                    "x": 108,
                    "u": "https://preview.redd.it/pvipbao9npgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=08ec5455f30f170ffdb8fb10a28e530de3ffc87f"
                  },
                  {
                    "y": 119,
                    "x": 216,
                    "u": "https://preview.redd.it/pvipbao9npgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=983873e2a890b1d5b4d1549b6554d0d96e4c6147"
                  },
                  {
                    "y": 176,
                    "x": 320,
                    "u": "https://preview.redd.it/pvipbao9npgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b29670ef3ff3bf1e93b9734818e1ea388750f683"
                  }
                ],
                "s": {
                  "y": 249,
                  "x": 451,
                  "u": "https://preview.redd.it/pvipbao9npgf1.png?width=451&amp;format=png&amp;auto=webp&amp;s=de1da506433d6cdd0870b0b2a309a56defbfdaf9"
                },
                "id": "pvipbao9npgf1"
              }
            },
            "hide_score": false,
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.87,
            "author_flair_background_color": null,
            "ups": 64,
            "domain": "reddit.com",
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_116dje",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "gallery_data": {
              "items": [
                {
                  "media_id": "pvipbao9npgf1",
                  "id": 719953590
                },
                {
                  "media_id": "tfkhbanjqpgf1",
                  "id": 719953591
                },
                {
                  "media_id": "5nw1vlfoqpgf1",
                  "id": 719953592
                }
              ]
            },
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 64,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/aNDVZRvyIDhy6vwT1mm7Ch4ypRPRTf4JSNM_Np7grwg.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754187279,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;ve been working on an LLM proxy, balancer &amp;amp; model unifier based on a few other projects we&amp;#39;ve created in the past (scout, sherpa) to enable us to run several ollama / lmstudio backends and serve traffic for local-ai. &lt;/p&gt;\n\n&lt;p&gt;This was primarily after running into the same issues across several organisations - managing multiple LLM  backend instances &amp;amp; routing/failover etc. We use this currently across several organisations who self-host their AI workloads (one organisation, has a bunch of MacStudios, another has RTX 6000s in their onprem racks and another lets people use their laptops at home, their work infra onsite),&lt;/p&gt;\n\n&lt;p&gt;So some folks run the dockerised versions and point their tooling (like Junie for example) at Olla and use it between home / work.&lt;/p&gt;\n\n&lt;p&gt;Olla currently natively supports Ollama and LMStudio, with Lemonade, vLLM and a few others being added soon.&lt;/p&gt;\n\n&lt;p&gt;Add your LLM endpoints into a config file, Olla will discover the models (and unify per-provider), manage health updates and route based on the balancer you pick.&lt;/p&gt;\n\n&lt;p&gt;The attempt to unify across providers wasn&amp;#39;t as successful - as in, both LMStudio &amp;amp; Ollama, the nuances in naming causes more grief than its worth (right now). Maybe revisit later once other things have been implemented.&lt;/p&gt;\n\n&lt;p&gt;Github: &lt;a href=\"https://github.com/thushan/olla\"&gt;https://github.com/thushan/olla&lt;/a&gt; (golang)&lt;/p&gt;\n\n&lt;p&gt;Would love to know your thoughts. &lt;/p&gt;\n\n&lt;p&gt;Olla is still in its infancy, so we don&amp;#39;t have auth implemented etc but there are plans in the future.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://www.reddit.com/gallery/1mg7qpa",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mg7qpa",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "2shanigans",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/",
            "stickied": false,
            "url": "https://www.reddit.com/gallery/1mg7qpa",
            "subreddit_subscribers": 509919,
            "created_utc": 1754187279,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6moy29",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Tyme4Trouble",
            "can_mod_post": false,
            "created_utc": 1754188225,
            "send_replies": true,
            "parent_id": "t3_1mg7qpa",
            "score": 5,
            "author_fullname": "t2_973amyap",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Neat! A lot easier than learning Kubernetes or writing FastAPI wrappers for each and every endpoint you might be juggling.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6moy29",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Neat! A lot easier than learning Kubernetes or writing FastAPI wrappers for each and every endpoint you might be juggling.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6moy29/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754188225,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg7qpa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "7d1f04e6-4920-11ef-b2e1-2e580594e1a1",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6o08b2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "asankhs",
            "can_mod_post": false,
            "created_utc": 1754212155,
            "send_replies": true,
            "parent_id": "t3_1mg7qpa",
            "score": 2,
            "author_fullname": "t2_e0bph",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Great stuff, you can also check out OptiLLM - https://github.com/codelion/optillm you can add inference optimisation with it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6o08b2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3.1"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great stuff, you can also check out OptiLLM - &lt;a href=\"https://github.com/codelion/optillm\"&gt;https://github.com/codelion/optillm&lt;/a&gt; you can add inference optimisation with it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6o08b2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754212155,
            "author_flair_text": "Llama 3.1",
            "treatment_tags": [],
            "link_id": "t3_1mg7qpa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#93b1ba",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6sbfoi",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "2shanigans",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6queq8",
                                "score": 2,
                                "author_fullname": "t2_116dje",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "haha yes, we had a very enthusiastic bloke who'd always shorten Ollama to Olla when he talked, unfortunately he passed away after a motorbike accident so we named this after him.\n\nDid not know the Swedish angle, tip of the iceberg - that was enlightening :O",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6sbfoi",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;haha yes, we had a very enthusiastic bloke who&amp;#39;d always shorten Ollama to Olla when he talked, unfortunately he passed away after a motorbike accident so we named this after him.&lt;/p&gt;\n\n&lt;p&gt;Did not know the Swedish angle, tip of the iceberg - that was enlightening :O&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mg7qpa",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6sbfoi/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754266638,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754266638,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6queq8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "StandardPen9685",
                      "can_mod_post": false,
                      "created_utc": 1754249334,
                      "send_replies": true,
                      "parent_id": "t1_n6pk7rl",
                      "score": 3,
                      "author_fullname": "t2_1qz3zw1d6f",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "In swedish it’s something completely different… 😬",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6queq8",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In swedish it’s something completely different… 😬&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg7qpa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6queq8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754249334,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6pk7rl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Caffdy",
            "can_mod_post": false,
            "created_utc": 1754235250,
            "send_replies": true,
            "parent_id": "t3_1mg7qpa",
            "score": 2,
            "author_fullname": "t2_ql2vu0wz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Olla means **pot/saucepan** in Spanish (and probably other romance languages as well)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pk7rl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Olla means &lt;strong&gt;pot/saucepan&lt;/strong&gt; in Spanish (and probably other romance languages as well)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6pk7rl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754235250,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg7qpa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6mu417",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "2shanigans",
                      "can_mod_post": false,
                      "created_utc": 1754190315,
                      "send_replies": true,
                      "parent_id": "t1_n6msk82",
                      "score": 1,
                      "author_fullname": "t2_116dje",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, that's on the roadmap/backlog, feel free to open an issue &amp; mention your thoughts before I start on it. We went all out on Scout with key management but I'm trying to keep it simple first.\n\n\nThis way we can add openrouter &amp; other endpoints easily with auth too. \n\n\nAlso apologies for the massive image, didn't realise till I looked at the comments. Yikes. ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6mu417",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, that&amp;#39;s on the roadmap/backlog, feel free to open an issue &amp;amp; mention your thoughts before I start on it. We went all out on Scout with key management but I&amp;#39;m trying to keep it simple first.&lt;/p&gt;\n\n&lt;p&gt;This way we can add openrouter &amp;amp; other endpoints easily with auth too. &lt;/p&gt;\n\n&lt;p&gt;Also apologies for the massive image, didn&amp;#39;t realise till I looked at the comments. Yikes. &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg7qpa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6mu417/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754190315,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6msk82",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Character_Pie_5368",
            "can_mod_post": false,
            "created_utc": 1754189674,
            "send_replies": true,
            "parent_id": "t3_1mg7qpa",
            "score": 1,
            "author_fullname": "t2_nk8of363",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Any thoughts on adding api key authentication?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6msk82",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Any thoughts on adding api key authentication?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6msk82/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754189674,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg7qpa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6nk44t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "2shanigans",
                      "can_mod_post": false,
                      "created_utc": 1754202804,
                      "send_replies": true,
                      "parent_id": "t1_n6n38s2",
                      "score": 2,
                      "author_fullname": "t2_116dje",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks, made some mistakes in the docs, and is fixed in [this PR](https://github.com/thushan/olla/pull/38), i've also added [an example with Olla + OpenWebUI](https://github.com/thushan/olla/tree/main/examples/ollama-openwebui) for you to try.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6nk44t",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, made some mistakes in the docs, and is fixed in &lt;a href=\"https://github.com/thushan/olla/pull/38\"&gt;this PR&lt;/a&gt;, i&amp;#39;ve also added &lt;a href=\"https://github.com/thushan/olla/tree/main/examples/ollama-openwebui\"&gt;an example with Olla + OpenWebUI&lt;/a&gt; for you to try.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg7qpa",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6nk44t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754202804,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6n38s2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "vk3r",
            "can_mod_post": false,
            "created_utc": 1754194265,
            "send_replies": true,
            "parent_id": "t3_1mg7qpa",
            "score": 1,
            "author_fullname": "t2_hyklw8a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've been waiting for someone to do this. It's fantastic. I tried the following and it didn't work for me with OpenWebUI:\n\n\\`\\`\\`  \nolla:  \n    image: [ghcr.io/thushan/olla:${OLLA\\_VERSION}](http://ghcr.io/thushan/olla:${OLLA_VERSION})  \n    container\\_name: olla  \n    ports:  \n      \\- 40114:40114  \n    volumes:  \n      \\- ./olla.yaml:/config.yaml  \n\\`\\`\\`\n\nWith this configuration:\n\n\\`\\`\\`  \nserver:  \n  host: [0.0.0.0](http://0.0.0.0)  \n  port: 40114\n\nproxy:  \n  engine: \"olla\"          # or \"olla\" for high performance  \n  load\\_balancer: \"priority\" # or round-robin, least-connections\n\ndiscovery:  \n  endpoints:  \n    \\- name: \"server\"  \n      url: \"http://localhost:11434\"  \n      platform: \"ollama\"  \n      priority: 100         # Higher = preferred  \n      tags:  \n        models: \"hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:IQ2\\_XXS\"\n\n    \\- name: \"desktop\"  \n      url: \"http://other:11434\"  \n      platform: \"ollama\"  \n      priority: 50          # Lower priority fallback  \n      tags:  \n        models: \"hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:IQ2\\_XXS\"  \n\\`\\`\\`\n\nAnd in OpenWebUI conection: http://URL:PORT/olla/ollama\n\nAnd he does not recognize it ...",
            "edited": 1754194598,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6n38s2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been waiting for someone to do this. It&amp;#39;s fantastic. I tried the following and it didn&amp;#39;t work for me with OpenWebUI:&lt;/p&gt;\n\n&lt;p&gt;```&lt;br/&gt;\nolla:&lt;br/&gt;\n    image: &lt;a href=\"http://ghcr.io/thushan/olla:$%7BOLLA_VERSION%7D\"&gt;ghcr.io/thushan/olla:${OLLA_VERSION}&lt;/a&gt;&lt;br/&gt;\n    container_name: olla&lt;br/&gt;\n    ports:&lt;br/&gt;\n      - 40114:40114&lt;br/&gt;\n    volumes:&lt;br/&gt;\n      - ./olla.yaml:/config.yaml&lt;br/&gt;\n```&lt;/p&gt;\n\n&lt;p&gt;With this configuration:&lt;/p&gt;\n\n&lt;p&gt;```&lt;br/&gt;\nserver:&lt;br/&gt;\n  host: &lt;a href=\"http://0.0.0.0\"&gt;0.0.0.0&lt;/a&gt;&lt;br/&gt;\n  port: 40114&lt;/p&gt;\n\n&lt;p&gt;proxy:&lt;br/&gt;\n  engine: &amp;quot;olla&amp;quot;          # or &amp;quot;olla&amp;quot; for high performance&lt;br/&gt;\n  load_balancer: &amp;quot;priority&amp;quot; # or round-robin, least-connections&lt;/p&gt;\n\n&lt;p&gt;discovery:&lt;br/&gt;\n  endpoints:&lt;br/&gt;\n    - name: &amp;quot;server&amp;quot;&lt;br/&gt;\n      url: &amp;quot;http://localhost:11434&amp;quot;&lt;br/&gt;\n      platform: &amp;quot;ollama&amp;quot;&lt;br/&gt;\n      priority: 100         # Higher = preferred&lt;br/&gt;\n      tags:&lt;br/&gt;\n        models: &amp;quot;hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:IQ2_XXS&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;    - name: &amp;quot;desktop&amp;quot;&lt;br/&gt;\n      url: &amp;quot;http://other:11434&amp;quot;&lt;br/&gt;\n      platform: &amp;quot;ollama&amp;quot;&lt;br/&gt;\n      priority: 50          # Lower priority fallback&lt;br/&gt;\n      tags:&lt;br/&gt;\n        models: &amp;quot;hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:IQ2_XXS&amp;quot;&lt;br/&gt;\n```&lt;/p&gt;\n\n&lt;p&gt;And in OpenWebUI conection: http://URL:PORT/olla/ollama&lt;/p&gt;\n\n&lt;p&gt;And he does not recognize it ...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg7qpa/announcing_olla_llm_load_balancer_proxy_model/n6n38s2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754194265,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg7qpa",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]