[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey r/LocalLLaMA! I'm struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.\n\n**Hardware:**\n\n* RTX 4090 (24GB VRAM)\n* 64GB DDR4 RAM\n* Using latest llama.cpp build (6088 with clang 19.1.5)\n\n**Model:**\n\n* DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4\\_K\\_M, 6 shards, \\~67GB total)\n* 110B parameters, MoE with 128 experts (8 active)\n\n**Current working config:**\n\n    llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot \".ffn_.*_exps.=CPU\" --port 8081\n\n**Performance issues:**\n\n* Only getting **3.37 tokens/sec** generation\n* **1.53 tokens/sec** prompt processing\n* GPU utilization only **12%** (should be much higher!)\n* GPU memory usage: 7.7GB/24GB (tons of headroom)\n* System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)\n\n**What I've tried:**\n\n* Without expert offloading (`-ot` flag) → OOMs trying to allocate 66GB on 24GB GPU\n* Higher `-ngl` values without expert offloading → System freezes/crashes after 30min loading\n* Different batch sizes → No improvement\n* For comparison, 12B dense models get 40-50 TPS on this setup",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Need help optimizing GLM-4.5-Air 110B (Q4_K_M) on RTX 4090 + 64GB RAM - Getting only 3.37 TPS",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhsyv9",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_uptissiz",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754349244,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;! I&amp;#39;m struggling to get decent performance from the new GLM-4.5-Air model and could use some help finding the optimal config.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;RTX 4090 (24GB VRAM)&lt;/li&gt;\n&lt;li&gt;64GB DDR4 RAM&lt;/li&gt;\n&lt;li&gt;Using latest llama.cpp build (6088 with clang 19.1.5)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;DevQuasar/zai-org.GLM-4.5-Air-GGUF (Q4_K_M, 6 shards, ~67GB total)&lt;/li&gt;\n&lt;li&gt;110B parameters, MoE with 128 experts (8 active)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Current working config:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server -hf DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf -fa -c 4096 -ngl 99 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; --port 8081\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;Performance issues:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Only getting &lt;strong&gt;3.37 tokens/sec&lt;/strong&gt; generation&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;1.53 tokens/sec&lt;/strong&gt; prompt processing&lt;/li&gt;\n&lt;li&gt;GPU utilization only &lt;strong&gt;12%&lt;/strong&gt; (should be much higher!)&lt;/li&gt;\n&lt;li&gt;GPU memory usage: 7.7GB/24GB (tons of headroom)&lt;/li&gt;\n&lt;li&gt;System RAM: 62.9GB/64GB (98% full - this seems to be the bottleneck)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Without expert offloading (&lt;code&gt;-ot&lt;/code&gt; flag) → OOMs trying to allocate 66GB on 24GB GPU&lt;/li&gt;\n&lt;li&gt;Higher &lt;code&gt;-ngl&lt;/code&gt; values without expert offloading → System freezes/crashes after 30min loading&lt;/li&gt;\n&lt;li&gt;Different batch sizes → No improvement&lt;/li&gt;\n&lt;li&gt;For comparison, 12B dense models get 40-50 TPS on this setup&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhsyv9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Pro-editor-1105",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754349244,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yyi8g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1754354379,
                      "send_replies": true,
                      "parent_id": "t1_n6ynvet",
                      "score": 1,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'll add that while offloading a couple layers of a large MoE doesn't usually improve performance by much, here it will likely be critical... Since OP is running a 67GB model on system with 64GB of RAM they need to maximize the use of VRAM offload in order to prevent swapping (and their performance would indicate they are swapping).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yyi8g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll add that while offloading a couple layers of a large MoE doesn&amp;#39;t usually improve performance by much, here it will likely be critical... Since OP is running a 67GB model on system with 64GB of RAM they need to maximize the use of VRAM offload in order to prevent swapping (and their performance would indicate they are swapping).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yyi8g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754354379,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ynvet",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "segmond",
            "can_mod_post": false,
            "created_utc": 1754350684,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 5,
            "author_fullname": "t2_ah13x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Begin by loading the shared experts on the GPU, that should be about 4gb, then set your context and see how much free memory you have on your GPU, then load just 1 expert tensor.  \nblk.(\\[0\\]).fnn\\_.\\*\\_exps.=CUDA0, see how much memory it takes, let's say you notice you have  17gb free, and you load 1 expert and it uses 3gb, then you can see that you can put 5 more on there, then change that 0 to 0-4, blk.(\\[0-5\\]).fnn\\_.\\*\\_exps=.CUDA0, and for anyone that has more GPU, go to the next GPU and keep loading\n\n    --override-tensor \"blk.([0-9]).ffn.*shexp.=CUDA0,.ffn.*shexp.*=CUDA0,ffn_.*_exps.=CPU\"",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ynvet",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Begin by loading the shared experts on the GPU, that should be about 4gb, then set your context and see how much free memory you have on your GPU, then load just 1 expert tensor.&lt;br/&gt;\nblk.([0]).fnn_.*_exps.=CUDA0, see how much memory it takes, let&amp;#39;s say you notice you have  17gb free, and you load 1 expert and it uses 3gb, then you can see that you can put 5 more on there, then change that 0 to 0-4, blk.([0-5]).fnn_.*_exps=.CUDA0, and for anyone that has more GPU, go to the next GPU and keep loading&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;--override-tensor &amp;quot;blk.([0-9]).ffn.*shexp.=CUDA0,.ffn.*shexp.*=CUDA0,ffn_.*_exps.=CPU&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ynvet/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754350684,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6z42n3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DeProgrammer99",
                      "can_mod_post": false,
                      "created_utc": 1754356342,
                      "send_replies": true,
                      "parent_id": "t1_n6yy9bi",
                      "score": 1,
                      "author_fullname": "t2_w4j8t",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'd just like to add that earlier \"-ot\" arguments take precedence over later \"-ot\" arguments. Didn't see that spelled out anywhere, but I've tested it out to make sure.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6z42n3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d just like to add that earlier &amp;quot;-ot&amp;quot; arguments take precedence over later &amp;quot;-ot&amp;quot; arguments. Didn&amp;#39;t see that spelled out anywhere, but I&amp;#39;ve tested it out to make sure.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6z42n3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754356342,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yy9bi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "solidsnakeblue",
            "can_mod_post": false,
            "created_utc": 1754354291,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 2,
            "author_fullname": "t2_7zh6fslk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Now that I got it working I wanted to share what worked for me, slightly different system with 2 x 3090 and 128GB of ram.  This fills up the VRAM almost completely, looks like I am getting 107tk / sec PP and 4.5tk / sec generation\n\n    ./build/bin/llama-server \\\n        -m models/zai-org.GLM-4.5-Air-GGUF/zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf \\\n        --host 0.0.0.0 \\\n        --port 30000 \\\n        -c 100000 \\\n        --cache-type-k q8_0 \\\n        --cache-type-v q8_0 \\\n        -t 8 \\\n        -ngl 99  \\\n        -ot \"blk.[0-7].ffn_up_exps=CUDA0,blk.[0-7].ffn_gate_exps=CUDA0,blk.[0-7].ffn_down_exps=CUDA0\" \\\n        -ot \"blk.[8-9].ffn_up_exps=CUDA1,blk.[8-9].ffn_gate_exps=CUDA1,blk.[8-9].ffn_down_exps=CUDA1\" \\\n        -ot \"blk.1[0-9].ffn_up_exps=CUDA1,blk.1[0-9].ffn_gate_exps=CUDA1,blk.1[0-9].ffn_down_exps=CUDA1\" \\\n        -ot \".ffn_.*_exps.=CPU\" \\\n        --flash-attn\n\nYou could use a q4\\_0 cache and probably fit 32K context with your total amount of ram.  You can probably just use my first -ot exactly to offset the amount of blocks that will fit onto your GPU, if you OOM just lower the 7 in the \\[0-7\\] regex blocks.",
            "edited": 1754354853,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yy9bi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Now that I got it working I wanted to share what worked for me, slightly different system with 2 x 3090 and 128GB of ram.  This fills up the VRAM almost completely, looks like I am getting 107tk / sec PP and 4.5tk / sec generation&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./build/bin/llama-server \\\n    -m models/zai-org.GLM-4.5-Air-GGUF/zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf \\\n    --host 0.0.0.0 \\\n    --port 30000 \\\n    -c 100000 \\\n    --cache-type-k q8_0 \\\n    --cache-type-v q8_0 \\\n    -t 8 \\\n    -ngl 99  \\\n    -ot &amp;quot;blk.[0-7].ffn_up_exps=CUDA0,blk.[0-7].ffn_gate_exps=CUDA0,blk.[0-7].ffn_down_exps=CUDA0&amp;quot; \\\n    -ot &amp;quot;blk.[8-9].ffn_up_exps=CUDA1,blk.[8-9].ffn_gate_exps=CUDA1,blk.[8-9].ffn_down_exps=CUDA1&amp;quot; \\\n    -ot &amp;quot;blk.1[0-9].ffn_up_exps=CUDA1,blk.1[0-9].ffn_gate_exps=CUDA1,blk.1[0-9].ffn_down_exps=CUDA1&amp;quot; \\\n    -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; \\\n    --flash-attn\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You could use a q4_0 cache and probably fit 32K context with your total amount of ram.  You can probably just use my first -ot exactly to offset the amount of blocks that will fit onto your GPU, if you OOM just lower the 7 in the [0-7] regex blocks.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yy9bi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754354291,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ylviq",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Filo0104",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ykf07",
                                "score": 5,
                                "author_fullname": "t2_1qq10p5wjo",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "i was going to test that on my machine, but i see that hugging face marked it as unsafe\n\n```\nThe following viruses have been found: Pickle.Malware.SysAccess.sys.STACK\\_GLOBAL.UNOFFICIAL\n```",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ylviq",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i was going to test that on my machine, but i see that hugging face marked it as unsafe&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\nThe following viruses have been found: Pickle.Malware.SysAccess.sys.STACK\\_GLOBAL.UNOFFICIAL\n&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhsyv9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ylviq/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754350024,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754350024,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ylt1o",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pro-editor-1105",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ykf07",
                                "score": 3,
                                "author_fullname": "t2_uptissiz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Now mrradermacher has uploaded some, he seems a lot more legit so i will try that.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ylt1o",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Now mrradermacher has uploaded some, he seems a lot more legit so i will try that.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhsyv9",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ylt1o/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754350001,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754350001,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ykf07",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "solidsnakeblue",
                      "can_mod_post": false,
                      "created_utc": 1754349541,
                      "send_replies": true,
                      "parent_id": "t1_n6yjx7i",
                      "score": 1,
                      "author_fullname": "t2_7zh6fslk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Its in his current working config, \n\n    DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ykf07",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Its in his current working config, &lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;DevQuasar/zai-org.GLM-4.5-Air-GGUF:zai-org.GLM-4.5-Air.Q4_K_M-00001-of-00006.gguf\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6ykf07/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754349541,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yjx7i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Only_Situation_4713",
            "can_mod_post": false,
            "created_utc": 1754349376,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 1,
            "author_fullname": "t2_aafjsulg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Where did you get the gguf from? There's a lot of GGUFs floating around from when they were testing the PR.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yjx7i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Where did you get the gguf from? There&amp;#39;s a lot of GGUFs floating around from when they were testing the PR.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yjx7i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754349376,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yz884",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1754354629,
                      "send_replies": true,
                      "parent_id": "t1_n6yql46",
                      "score": 2,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Getting more tensors on the GPU I think should help prompt processing significantly, \n\nThis isn't particularly true on llama.cpp.  If you don't provide `--no-op-offload`  then it will stream the entire model to the GPU to process larger prompt batches (size &gt;=32 IIRC).  So you get the whole horsepower of the GPU but you're limited by PCIe speed vs prompt ubatch-size.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yz884",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Getting more tensors on the GPU I think should help prompt processing significantly, &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This isn&amp;#39;t particularly true on llama.cpp.  If you don&amp;#39;t provide &lt;code&gt;--no-op-offload&lt;/code&gt;  then it will stream the entire model to the GPU to process larger prompt batches (size &amp;gt;=32 IIRC).  So you get the whole horsepower of the GPU but you&amp;#39;re limited by PCIe speed vs prompt ubatch-size.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhsyv9",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yz884/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754354629,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yql46",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cristoper",
            "can_mod_post": false,
            "created_utc": 1754351599,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 1,
            "author_fullname": "t2_38xkk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; GPU memory usage: 7.7GB/24GB\n\nYou can fit more on your GPU. You're currently keeping all tensors that match \".ffn_.*_exps.\" on your CPU... have you tried keeping more experts on your GPU (see u/segmond's comment)?\n\nGetting more tensors on the GPU I think should help prompt processing significantly, but I'm not sure how much it will help token generation. Do you know how many tokens/s can be expected with a 4090 + RAM? I'm definitely interested in what kind of performance you end up getting out of it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yql46",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;GPU memory usage: 7.7GB/24GB&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You can fit more on your GPU. You&amp;#39;re currently keeping all tensors that match &amp;quot;.ffn_.*_exps.&amp;quot; on your CPU... have you tried keeping more experts on your GPU (see &lt;a href=\"/u/segmond\"&gt;u/segmond&lt;/a&gt;&amp;#39;s comment)?&lt;/p&gt;\n\n&lt;p&gt;Getting more tensors on the GPU I think should help prompt processing significantly, but I&amp;#39;m not sure how much it will help token generation. Do you know how many tokens/s can be expected with a 4090 + RAM? I&amp;#39;m definitely interested in what kind of performance you end up getting out of it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yql46/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754351599,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yy9tx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Muted-Celebration-47",
            "can_mod_post": false,
            "created_utc": 1754354296,
            "send_replies": true,
            "parent_id": "t3_1mhsyv9",
            "score": 1,
            "author_fullname": "t2_q2qi86l3f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I got 10t/s with my 3090 + 64gb ram but I use a complex override tensor",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yy9tx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I got 10t/s with my 3090 + 64gb ram but I use a complex override tensor&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhsyv9/need_help_optimizing_glm45air_110b_q4_k_m_on_rtx/n6yy9tx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754354296,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhsyv9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]