[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I've been debating the use of quantized KV cache with llama.cpp (no less than q8) for a long time, but I still can't tell if it's a good idea:\n\n* On one hand, the [original PR](https://github.com/ggml-org/llama.cpp/pull/7412) mentions that perplexity is more sensitive to model weight quants than to KV cache. Or in other words, quantizing kv cache is worth it if it frees memory for a slightly less quantized model (eg q4kL vs q4kM).\n* On the other hand, many commenters in this community seem to strongly suggest against it.\n* My own experience hasn't been conclusive one way or another. In theory, more quantization = less quality, but I don't have a concrete measure of the degradation introduced by quantized kv cache to rule out the idea entirely.\n\nSo I'm here to gauge everyone's experience with this, hopefully someone has a strong argument with or against that feature.\n\nThank you!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What's the verdict on using quantized KV cache?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhlj69",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 13,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_nc2u4f7",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 13,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754335757,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754332125,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been debating the use of quantized KV cache with llama.cpp (no less than q8) for a long time, but I still can&amp;#39;t tell if it&amp;#39;s a good idea:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;On one hand, the &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/7412\"&gt;original PR&lt;/a&gt; mentions that perplexity is more sensitive to model weight quants than to KV cache. Or in other words, quantizing kv cache is worth it if it frees memory for a slightly less quantized model (eg q4kL vs q4kM).&lt;/li&gt;\n&lt;li&gt;On the other hand, many commenters in this community seem to strongly suggest against it.&lt;/li&gt;\n&lt;li&gt;My own experience hasn&amp;#39;t been conclusive one way or another. In theory, more quantization = less quality, but I don&amp;#39;t have a concrete measure of the degradation introduced by quantized kv cache to rule out the idea entirely.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;So I&amp;#39;m here to gauge everyone&amp;#39;s experience with this, hopefully someone has a strong argument with or against that feature.&lt;/p&gt;\n\n&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?auto=webp&amp;s=9747fc98599981e69ef3dbabd18ac955767b61a1",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=719eefafd72ad9dfefe61307fd88e8316866de92",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9ae4f338ba2a27a6abf4d22dbfd20af9b475a6e6",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8782f872b86a598db0a3a6e2778f7fb6d2731edb",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6c1e124ade737ee359be91a0766310ce074cadd",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0453b80458e72b57f76e7907738a09419b3abbb2",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3a6f64546df415a5fb04416ad5ef8205b6fb933c",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "OvzkbSQGd0ValXuS9jVeaqGHriS1NS11UNljNQO8XGQ"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mhlj69",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ParaboloidalCrest",
            "discussion_type": null,
            "num_comments": 26,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754332125,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ygldg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ParaboloidalCrest",
                      "can_mod_post": false,
                      "created_utc": 1754348268,
                      "send_replies": true,
                      "parent_id": "t1_n6x9b13",
                      "score": 1,
                      "author_fullname": "t2_nc2u4f7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;But it's hard to really point the finger on what causes issues between inherent model intelligence, weight quant, KV quant, context degradation. Really anything could attribute to it.\n\nExactly! Let alone the hugely varying responses across multiple attempts.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ygldg",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;But it&amp;#39;s hard to really point the finger on what causes issues between inherent model intelligence, weight quant, KV quant, context degradation. Really anything could attribute to it.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Exactly! Let alone the hugely varying responses across multiple attempts.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlj69",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6ygldg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754348268,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6x9b13",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1754335104,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 8,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've been running V at Q8 all the time for V3/R1/K2 and don't really notice the degradation. I used to do K at Q8 for QwQ and I feel like I could attribute a lot of the random Aider failures to properly copy/paste diffs to minor token flips from the K quanting. Others said K gets impacted harder so, could be.\n\nBut it's hard to really point the finger on what causes issues between inherent model intelligence, weight quant, KV quant, context degradation. Really anything could attribute to it. Maybe someone could do some of those synthetic question/answer benches at different KV quants and try to see real impact that way.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6x9b13",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been running V at Q8 all the time for V3/R1/K2 and don&amp;#39;t really notice the degradation. I used to do K at Q8 for QwQ and I feel like I could attribute a lot of the random Aider failures to properly copy/paste diffs to minor token flips from the K quanting. Others said K gets impacted harder so, could be.&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s hard to really point the finger on what causes issues between inherent model intelligence, weight quant, KV quant, context degradation. Really anything could attribute to it. Maybe someone could do some of those synthetic question/answer benches at different KV quants and try to see real impact that way.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6x9b13/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754335104,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6xaqnk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Expensive-Apricot-25",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6x68h7",
                                "score": 2,
                                "author_fullname": "t2_idqkwio0",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yeah, vision models seem very sensitive to quantization. maybe the vision portions should be left unquantized?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6xaqnk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah, vision models seem very sensitive to quantization. maybe the vision portions should be left unquantized?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhlj69",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xaqnk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754335517,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754335517,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6x68h7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "segmond",
                      "can_mod_post": false,
                      "created_utc": 1754334222,
                      "send_replies": true,
                      "parent_id": "t1_n6x19mk",
                      "score": 3,
                      "author_fullname": "t2_ah13x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "yup, the first time I noticed how sensitive it can be, was last year while using a vision model.  Try as I may, I couldn't get it to accurately count objects in a picture.   The moment I had it as f16 it worked, q8 no go!   So as u/GL-AI mentioned, if precision is important to you then don't.  Coding, agents, tool calling, image, complex structured output, etc.  Only use as a last resort if you absolutely can't run the model without reducing it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6x68h7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yup, the first time I noticed how sensitive it can be, was last year while using a vision model.  Try as I may, I couldn&amp;#39;t get it to accurately count objects in a picture.   The moment I had it as f16 it worked, q8 no go!   So as &lt;a href=\"/u/GL-AI\"&gt;u/GL-AI&lt;/a&gt; mentioned, if precision is important to you then don&amp;#39;t.  Coding, agents, tool calling, image, complex structured output, etc.  Only use as a last resort if you absolutely can&amp;#39;t run the model without reducing it.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlj69",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6x68h7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754334222,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6x19mk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "GL-AI",
            "can_mod_post": false,
            "created_utc": 1754332800,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 10,
            "author_fullname": "t2_1sr5yw3yg0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "In my experience if you're coding or doing something that requires precision, quantizing the cache is definitely not worth it.\n\nIf you're just chatting or doing less precise things it's 1000% worth it going down to Q8 or sometimes even Q4",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6x19mk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In my experience if you&amp;#39;re coding or doing something that requires precision, quantizing the cache is definitely not worth it.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re just chatting or doing less precise things it&amp;#39;s 1000% worth it going down to Q8 or sometimes even Q4&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6x19mk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754332800,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yw31f",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "RedKnightRG",
                      "can_mod_post": false,
                      "created_utc": 1754353522,
                      "send_replies": true,
                      "parent_id": "t1_n6xac6o",
                      "score": 1,
                      "author_fullname": "t2_tlq31",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Are you going to put up a github repo with the code?  This is exactly the kind of thing I was begging for downthread...!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yw31f",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you going to put up a github repo with the code?  This is exactly the kind of thing I was begging for downthread...!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlj69",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6yw31f/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754353522,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xac6o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "no_witty_username",
            "can_mod_post": false,
            "created_utc": 1754335401,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 5,
            "author_fullname": "t2_4j2nc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My gut tells me its not a good idea but i have no evidence to back it up...yet. I am working on an automated benchmarking system that will test all possible server configs and all other hyperparameters and their correlation to reasoning tasks. Once that's running ill know what's good or not.  But so far from my own limited manual tests, anything cache related seems to degrades performance. This is specifically for llama.cpp... i havent tested vllm or other inferance engines yet.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xac6o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My gut tells me its not a good idea but i have no evidence to back it up...yet. I am working on an automated benchmarking system that will test all possible server configs and all other hyperparameters and their correlation to reasoning tasks. Once that&amp;#39;s running ill know what&amp;#39;s good or not.  But so far from my own limited manual tests, anything cache related seems to degrades performance. This is specifically for llama.cpp... i havent tested vllm or other inferance engines yet.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xac6o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754335401,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6yj5wv",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ParaboloidalCrest",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6yexvs",
                                "score": 2,
                                "author_fullname": "t2_nc2u4f7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That is very useful info given how strict the test is. Thanks!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6yj5wv",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That is very useful info given how strict the test is. Thanks!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhlj69",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6yj5wv/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754349125,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754349125,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yexvs",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "DeProgrammer99",
                      "can_mod_post": false,
                      "created_utc": 1754347718,
                      "send_replies": true,
                      "parent_id": "t1_n6xbfgp",
                      "score": 5,
                      "author_fullname": "t2_w4j8t",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Mistral Small 3.2's performance was obliterated by quantizing both K and V to Q8 when I ran my \"does this code match this tech spec and this design document\" test on it. [https://www.reddit.com/r/LocalLLaMA/comments/1ljp29d/comment/mzm84vk/](https://www.reddit.com/r/LocalLLaMA/comments/1ljp29d/comment/mzm84vk/)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yexvs",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mistral Small 3.2&amp;#39;s performance was obliterated by quantizing both K and V to Q8 when I ran my &amp;quot;does this code match this tech spec and this design document&amp;quot; test on it. &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ljp29d/comment/mzm84vk/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ljp29d/comment/mzm84vk/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlj69",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6yexvs/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754347718,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xbfgp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "cristoper",
            "can_mod_post": false,
            "created_utc": 1754335715,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 5,
            "author_fullname": "t2_38xkk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I use q8_0 (both K and V) occasionally if I need more context space (mostly for summarizing long documents). I haven't noticed quality issues with it, but I've not done any real benchmarks.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xbfgp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use q8_0 (both K and V) occasionally if I need more context space (mostly for summarizing long documents). I haven&amp;#39;t noticed quality issues with it, but I&amp;#39;ve not done any real benchmarks.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xbfgp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754335715,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6ycqmk",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Diligent-Direction95",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6y62z5",
                                          "score": 1,
                                          "author_fullname": "t2_f0rtcb0u",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "downloaded the gguf and yep, youre right, works great. \n\ni may move back to gguf because the cache makes such a big difference in multi turn, likely worth it even with the slower prompt processing",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6ycqmk",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;downloaded the gguf and yep, youre right, works great. &lt;/p&gt;\n\n&lt;p&gt;i may move back to gguf because the cache makes such a big difference in multi turn, likely worth it even with the slower prompt processing&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mhlj69",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6ycqmk/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754346973,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754346973,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y62z5",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pristine-Woodpecker",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y5fv6",
                                "score": 2,
                                "author_fullname": "t2_5b972ieo",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Might be worth checking against the MLX engine on the command line and then seeing what to report a bug against.\n\nNote that when I raged against the LM Studio UI marking it as experimental, I was talking about the GGUF support, because llama.cpp doesn't mark their support as experimental, and it's been there for quite a while, so it's unclear why LM Studio did that. MLX FA is much newer though.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y62z5",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Might be worth checking against the MLX engine on the command line and then seeing what to report a bug against.&lt;/p&gt;\n\n&lt;p&gt;Note that when I raged against the LM Studio UI marking it as experimental, I was talking about the GGUF support, because llama.cpp doesn&amp;#39;t mark their support as experimental, and it&amp;#39;s been there for quite a while, so it&amp;#39;s unclear why LM Studio did that. MLX FA is much newer though.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhlj69",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6y62z5/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754344785,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754344785,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y5fv6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Diligent-Direction95",
                      "can_mod_post": false,
                      "created_utc": 1754344587,
                      "send_replies": true,
                      "parent_id": "t1_n6xtldw",
                      "score": 1,
                      "author_fullname": "t2_f0rtcb0u",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Unsure if its an implementation detail of MoE / MLX or what, but Q8KV is totally broken for A3B instruct 2507 at ~30k context. Tried it this morning and just totally broken response, turned off works perfectly. \n\nit was my first try with KV in lm studio so ill need to try out a few more models",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y5fv6",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unsure if its an implementation detail of MoE / MLX or what, but Q8KV is totally broken for A3B instruct 2507 at ~30k context. Tried it this morning and just totally broken response, turned off works perfectly. &lt;/p&gt;\n\n&lt;p&gt;it was my first try with KV in lm studio so ill need to try out a few more models&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlj69",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6y5fv6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754344587,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xtldw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pristine-Woodpecker",
            "can_mod_post": false,
            "created_utc": 1754340959,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 2,
            "author_fullname": "t2_5b972ieo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;many commenters in this community seem to strongly suggest against it.\n\nAh yeah, just like https://www.reddit.com/r/LocalLLaMA/comments/1mh5wve/comment/n6tx66h/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button.\n\nShow the benchmarks showing the degradation or shut up. When the llama.cpp implementation landed, the developers did exactly this: https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347\n\nUsing a Q8/Q8 KV cache is so firmly within margin of error compared to not quantizing...that it scores better than unquantized cache for some network quants (of course it's not really better - that's the point of being in margin of error).\n\nThere's still people arguing against FlashAttention. LM Studio marks it as an experimental feature in their llama.cpp backend (?!). I don't even...",
            "edited": 1754345000,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xtldw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;many commenters in this community seem to strongly suggest against it.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Ah yeah, just like &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mh5wve/comment/n6tx66h/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mh5wve/comment/n6tx66h/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Show the benchmarks showing the degradation or shut up. When the llama.cpp implementation landed, the developers did exactly this: &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347\"&gt;https://github.com/ggml-org/llama.cpp/pull/7412#issuecomment-2120427347&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Using a Q8/Q8 KV cache is so firmly within margin of error compared to not quantizing...that it scores better than unquantized cache for some network quants (of course it&amp;#39;s not really better - that&amp;#39;s the point of being in margin of error).&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s still people arguing against FlashAttention. LM Studio marks it as an experimental feature in their llama.cpp backend (?!). I don&amp;#39;t even...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xtldw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754340959,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xainf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Expensive-Apricot-25",
            "can_mod_post": false,
            "created_utc": 1754335453,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 2,
            "author_fullname": "t2_idqkwio0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "look, the larger model you can load into vram, the better.\n\nin practice tho, if you can only load a 14b reasoning model at 5k context, it's useless and you are going to downgrade to a 8b with usable context. Using kv quant, u can run the 14b at 10k context, which is far more usable, and would be better than the 8b in theory. \n\nthis exact scenario happened to me. one thing I will say is that i notice the models making (unusual) typos. whereas before, even tiny LLMs, never made typos. \n\nbut imo, typos are minor errors. like it will forget an underscore in a variable name while coding, and instead leave a space, (but the code is correct otherwise), or it will cite something wrong, like in the context it is 9:15pm, but then it says it is 7:15pm (but when I re ran it got it right, and was hard to reproduce)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xainf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;look, the larger model you can load into vram, the better.&lt;/p&gt;\n\n&lt;p&gt;in practice tho, if you can only load a 14b reasoning model at 5k context, it&amp;#39;s useless and you are going to downgrade to a 8b with usable context. Using kv quant, u can run the 14b at 10k context, which is far more usable, and would be better than the 8b in theory. &lt;/p&gt;\n\n&lt;p&gt;this exact scenario happened to me. one thing I will say is that i notice the models making (unusual) typos. whereas before, even tiny LLMs, never made typos. &lt;/p&gt;\n\n&lt;p&gt;but imo, typos are minor errors. like it will forget an underscore in a variable name while coding, and instead leave a space, (but the code is correct otherwise), or it will cite something wrong, like in the context it is 9:15pm, but then it says it is 7:15pm (but when I re ran it got it right, and was hard to reproduce)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xainf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754335453,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xh6p6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "hksbindra",
            "can_mod_post": false,
            "created_utc": 1754337422,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 2,
            "author_fullname": "t2_5dvq05mr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "KV cache quantization &gt;= model quantization as much as the VRAM allows.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xh6p6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;KV cache quantization &amp;gt;= model quantization as much as the VRAM allows.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xh6p6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754337422,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xp2su",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fizzy1242",
            "can_mod_post": false,
            "created_utc": 1754339671,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 2,
            "author_fullname": "t2_16zcsx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Q8 is fine for smaller contexts from my experience (below 16k)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xp2su",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Q8 is fine for smaller contexts from my experience (below 16k)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xp2su/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754339671,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xvfvd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MutantEggroll",
            "can_mod_post": false,
            "created_utc": 1754341504,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 2,
            "author_fullname": "t2_6ncfftb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've been very happy with Q8\\_0 quant for both K &amp; V cache, specifically for agentic coding.\n\nIn my experience, I've seen Qwen3-Coder-30B-A3B:Q6\\_K\\_XL (with the Unsloth template patch) with 72K context one-shot a Flappy Bird clone in Python using Roo Code's Orchestrator mode. Devstral-Small-2507:Q6\\_K\\_XL with 80K context *almost* one-shot the same thing. And in more real-world scenarios, where I'm giving the model much more narrow-scope tasks, I very rarely have to give any correction/clarification to get the result I'm looking for. \n\nIt's possible I'm in a \"sweet spot\" for this though - in my usual workflows, I hit 50K+ context pretty quickly due to file reads, MCP calls, etc., and with 32GB VRAM and Q8 KV quantization, I can use \\~30B models at Q5/Q6 and still have *just* enough room for the necessary context. I can definitely see how even 24GB VRAM would mean dropping to very low model quants, or dropping to smaller models altogether, and I would not be surprised to hear of perplexity issues coming from KV quantization with smaller models/quants.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xvfvd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been very happy with Q8_0 quant for both K &amp;amp; V cache, specifically for agentic coding.&lt;/p&gt;\n\n&lt;p&gt;In my experience, I&amp;#39;ve seen Qwen3-Coder-30B-A3B:Q6_K_XL (with the Unsloth template patch) with 72K context one-shot a Flappy Bird clone in Python using Roo Code&amp;#39;s Orchestrator mode. Devstral-Small-2507:Q6_K_XL with 80K context &lt;em&gt;almost&lt;/em&gt; one-shot the same thing. And in more real-world scenarios, where I&amp;#39;m giving the model much more narrow-scope tasks, I very rarely have to give any correction/clarification to get the result I&amp;#39;m looking for. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s possible I&amp;#39;m in a &amp;quot;sweet spot&amp;quot; for this though - in my usual workflows, I hit 50K+ context pretty quickly due to file reads, MCP calls, etc., and with 32GB VRAM and Q8 KV quantization, I can use ~30B models at Q5/Q6 and still have &lt;em&gt;just&lt;/em&gt; enough room for the necessary context. I can definitely see how even 24GB VRAM would mean dropping to very low model quants, or dropping to smaller models altogether, and I would not be surprised to hear of perplexity issues coming from KV quantization with smaller models/quants.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xvfvd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754341504,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ynmwa",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TacticalRock",
            "can_mod_post": false,
            "created_utc": 1754350606,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 3,
            "author_fullname": "t2_1f2ibv45",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Unfortunately it's pretty model/arch dependent. Some are fine at 8-bit, some become a little incoherent (namely Qwen 2.5). If you want a better algo, exllama I think has a slightly better caching implementation than llama.cpp.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ynmwa",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately it&amp;#39;s pretty model/arch dependent. Some are fine at 8-bit, some become a little incoherent (namely Qwen 2.5). If you want a better algo, exllama I think has a slightly better caching implementation than llama.cpp.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6ynmwa/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754350606,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "1c60b73a-72f2-11ee-bdcc-8e2a7b94d6a0",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ys54k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Cool-Hornet4434",
            "can_mod_post": false,
            "created_utc": 1754352139,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 2,
            "author_fullname": "t2_t53ymebt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I use Q4 KV cache almost exclusively, but with Gemma 3 adding SWA, that meant I could afford to drop to Q8.  I didn't notice any difference at all.  I then reduced the context a bit and dropped to FP8, no real obvious difference... I dropped a bit more context and ran at FP16...  No real obvious change.\n\nMy theory is that Quantizing KV cache only really matters if you're doing something that requires a high level or precision over a long context length.  For something like Coding?  That might make or break it.... For average chit-chat or role play? You'll never notice a difference.  \n\nI remember trying out SWA for the first time too, and got Gemma 3 up to her full 128K context length.  I inserted a strange phrase around 2K context, and every so often I would ask random questions to see if Gemma could recall the key phrase.  She had zero trouble recalling the phrase all the way to 120K context.   \n\nOf course once I tested her on the phrase I would delete that question/answer from her context and continue so Gemma wasn't using the times I checked as a more recent checkpoint.  \n\nI went from 2K, to 8K, to 12K, to 20K, to 60K, to 80K, to 100K, to 120K context and each time Gemma had zero trouble recalling the phrase or details about the phrase.  It was something like \"14 Desperate Cockatiels whispered the digits of Pi backwards while stomping on grapes.\"    and I could ask \"how many birds was it?\"  \"What kind of bird?\"   \"what was it they were whispering?\"   \"What were they stomping on?\" ....  \n\nAll of that was with Q4 KV Cache.  \n\nNow again, if there's something that depends on a large portion of your context be precisely recalled for the rest of your context to make sense (like coding) then maybe it's worth it to run at FP16 or FP8.... but otherwise, as far as context goes?  It's free real estate. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ys54k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "textgen web UI"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use Q4 KV cache almost exclusively, but with Gemma 3 adding SWA, that meant I could afford to drop to Q8.  I didn&amp;#39;t notice any difference at all.  I then reduced the context a bit and dropped to FP8, no real obvious difference... I dropped a bit more context and ran at FP16...  No real obvious change.&lt;/p&gt;\n\n&lt;p&gt;My theory is that Quantizing KV cache only really matters if you&amp;#39;re doing something that requires a high level or precision over a long context length.  For something like Coding?  That might make or break it.... For average chit-chat or role play? You&amp;#39;ll never notice a difference.  &lt;/p&gt;\n\n&lt;p&gt;I remember trying out SWA for the first time too, and got Gemma 3 up to her full 128K context length.  I inserted a strange phrase around 2K context, and every so often I would ask random questions to see if Gemma could recall the key phrase.  She had zero trouble recalling the phrase all the way to 120K context.   &lt;/p&gt;\n\n&lt;p&gt;Of course once I tested her on the phrase I would delete that question/answer from her context and continue so Gemma wasn&amp;#39;t using the times I checked as a more recent checkpoint.  &lt;/p&gt;\n\n&lt;p&gt;I went from 2K, to 8K, to 12K, to 20K, to 60K, to 80K, to 100K, to 120K context and each time Gemma had zero trouble recalling the phrase or details about the phrase.  It was something like &amp;quot;14 Desperate Cockatiels whispered the digits of Pi backwards while stomping on grapes.&amp;quot;    and I could ask &amp;quot;how many birds was it?&amp;quot;  &amp;quot;What kind of bird?&amp;quot;   &amp;quot;what was it they were whispering?&amp;quot;   &amp;quot;What were they stomping on?&amp;quot; ....  &lt;/p&gt;\n\n&lt;p&gt;All of that was with Q4 KV Cache.  &lt;/p&gt;\n\n&lt;p&gt;Now again, if there&amp;#39;s something that depends on a large portion of your context be precisely recalled for the rest of your context to make sense (like coding) then maybe it&amp;#39;s worth it to run at FP16 or FP8.... but otherwise, as far as context goes?  It&amp;#39;s free real estate. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6ys54k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754352139,
            "author_flair_text": "textgen web UI",
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6x7m4x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DorphinPack",
            "can_mod_post": false,
            "created_utc": 1754334618,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 4,
            "author_fullname": "t2_zebuyjw9s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The only clear verdict is Ollama needs to make it configurable at runtime so people dont get burned and decry the whole concept.\n\nu/GL-AI nailed it and we need to be broadly making sure people move past thinking about good or bad toward thinking about use cases.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6x7m4x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The only clear verdict is Ollama needs to make it configurable at runtime so people dont get burned and decry the whole concept.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"/u/GL-AI\"&gt;u/GL-AI&lt;/a&gt; nailed it and we need to be broadly making sure people move past thinking about good or bad toward thinking about use cases.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6x7m4x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754334618,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6y8cw5",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "RedKnightRG",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6y76jv",
                                "score": 1,
                                "author_fullname": "t2_tlq31",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah I saw that but it was one Set of tests a lifetime ago in model terms.   Sure would be nice to see something more recent and looking specifically at agentic coding flows rather than just perplexity!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6y8cw5",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah I saw that but it was one Set of tests a lifetime ago in model terms.   Sure would be nice to see something more recent and looking specifically at agentic coding flows rather than just perplexity!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhlj69",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6y8cw5/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754345517,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754345517,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6y76jv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pristine-Woodpecker",
                      "can_mod_post": false,
                      "created_utc": 1754345138,
                      "send_replies": true,
                      "parent_id": "t1_n6xa0wu",
                      "score": 1,
                      "author_fullname": "t2_5b972ieo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;You know, have there been any blogs or repos put up with any benchmarking data at all with various KV cache quantizations?\n\nThe llama.cpp developers did just that when they made the feature, I linked it above.\n\nConclusions: K much more sensitive than V. Q8 essentially lossless/free. Prefer V quant over using a smaller model.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6y76jv",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;You know, have there been any blogs or repos put up with any benchmarking data at all with various KV cache quantizations?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The llama.cpp developers did just that when they made the feature, I linked it above.&lt;/p&gt;\n\n&lt;p&gt;Conclusions: K much more sensitive than V. Q8 essentially lossless/free. Prefer V quant over using a smaller model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhlj69",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6y76jv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754345138,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xa0wu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RedKnightRG",
            "can_mod_post": false,
            "created_utc": 1754335311,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 1,
            "author_fullname": "t2_tlq31",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You know, have there been any blogs or repos put up with any benchmarking data at all with various KV cache quantizations?\n\nModels keep coming so fast that it would be hard to keep up to date but even a snapshot in time would be useful.  Something like 'here is a github repo with a sample code base of \\~100 files and here are 10 refactor/dev tasks and here are the total tokens, the total turns/time, and the % of test cases that the final result could pass without human intervention midway for 10 models with 4 of the most popular quants and fp16, q8, and q4 KV quantization'...   Even just restricting the test matrix to hold KV and quants fixed and equal and only looking at the most common quants would still lead to a massive amount of tests to run but boy would that data be useful to examine if anyone has done it!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xa0wu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You know, have there been any blogs or repos put up with any benchmarking data at all with various KV cache quantizations?&lt;/p&gt;\n\n&lt;p&gt;Models keep coming so fast that it would be hard to keep up to date but even a snapshot in time would be useful.  Something like &amp;#39;here is a github repo with a sample code base of ~100 files and here are 10 refactor/dev tasks and here are the total tokens, the total turns/time, and the % of test cases that the final result could pass without human intervention midway for 10 models with 4 of the most popular quants and fp16, q8, and q4 KV quantization&amp;#39;...   Even just restricting the test matrix to hold KV and quants fixed and equal and only looking at the most common quants would still lead to a massive amount of tests to run but boy would that data be useful to examine if anyone has done it!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6xa0wu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754335311,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y0f78",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754343008,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have only started using it recently, but thusfar am pretty happy with q8_0 quantization of both K and V cache.  It has allowed me to increase Gemma3-27B context quite a bit while still fitting inference in my 32GB MI60.\n\nI've been meaning to try it with Tulu3-405B and pure-CPU inference, but haven't yet.  Right now the Q4_K_M barely fits in my 256GB main memory with 8K context, but that's with unquantized context.  If I can increase that to 16K, it will make the model more useful for RAG applications.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y0f78",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have only started using it recently, but thusfar am pretty happy with q8_0 quantization of both K and V cache.  It has allowed me to increase Gemma3-27B context quite a bit while still fitting inference in my 32GB MI60.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been meaning to try it with Tulu3-405B and pure-CPU inference, but haven&amp;#39;t yet.  Right now the Q4_K_M barely fits in my 256GB main memory with 8K context, but that&amp;#39;s with unquantized context.  If I can increase that to 16K, it will make the model more useful for RAG applications.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6y0f78/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754343008,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yqtef",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mayo551",
            "can_mod_post": false,
            "created_utc": 1754351678,
            "send_replies": true,
            "parent_id": "t3_1mhlj69",
            "score": 2,
            "author_fullname": "t2_vsz5kd9o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "FP16 Context. Always.\n\nQ8 can be okay at times, but still noticeable. Anything below Q8 is garbage.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yqtef",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;FP16 Context. Always.&lt;/p&gt;\n\n&lt;p&gt;Q8 can be okay at times, but still noticeable. Anything below Q8 is garbage.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhlj69/whats_the_verdict_on_using_quantized_kv_cache/n6yqtef/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754351678,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhlj69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]