[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm running into a performance issue with a self-hosted agent and could use some help. I've successfully set up an agent system, but the inference is extremely slow because it's only using the CPU.\n\n**My Setup:**\n\n* **Model:**Â Qwen3-Coder-480B-A35B-Instruct-GGUFÂ (Q8\\_0 quant from unsloth)\n* **Hardware:**Â RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM\n* **Backend:**Â LatestÂ llama.cppÂ compiled from source, using theÂ llama-serverÂ binary.\n* **Agent:**Â A simple Python script usingÂ requestsÂ to call theÂ /completionÂ endpoint.\n\n**The Problem:**\n\nI'm launching the server with this command:\n\n    ./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host 0.0.0.0 --port 8080\n\nThe server loads the model successfully, andÂ nvidia-smiÂ confirms that the GPU memory is used (**83% VRAM used**). However, when my agent sends a prompt and the model starts generating a response, theÂ **GPU Utilization stays at 0-1%**, while a single CPU core is being used.\n\n**What I've already confirmed:**\n\n1. The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).\n2. The Python agent script works and correctly communicates with the server.\n3. The issue is purely that the actual token generation computation is not happening on the GPU.\n\n**My Question:**\n\nIs there a specific command-line argument for the newÂ llama-serverÂ (likeÂ --main-gpuÂ in the oldÂ mainÂ binary) that I'm missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions ofÂ llama.cpp?\n\nAny advice would be greatly appreciated. ThanksI'm running into a performance issue with a self-hosted agent and could use some help. I've successfully set up an agent system, but the inference is extremely slow because it's only using the [CPU.My](http://CPU.My) Setup:Model:Â Qwen3-Coder-480B-A35B-Instruct-GGUFÂ (Q8\\_0 quant from unsloth)  \n  \nHardware:Â RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM  \n  \nBackend:Â LatestÂ llama.cppÂ compiled from source, using theÂ llama-serverÂ binary.  \n  \nAgent:Â A simple Python script usingÂ requestsÂ to call theÂ /completionÂ endpoint.The Problem:I'm launching the server with this command:Generated code./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host [0.0.0.0](http://0.0.0.0) \\--port 8080  \nUse codeÂ with caution.The server loads the model successfully, andÂ nvidia-smiÂ confirms that the GPU memory is used (83% VRAM used). However, when my agent sends a prompt and the model starts generating a response, theÂ GPU Utilization stays at 0-1%, while a single CPU core is being used.What I've already confirmed:The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).  \n  \nThe Python agent script works and correctly communicates with the server.  \n  \nThe issue is purely that the actual token generation computation is not happening on the [GPU.My](http://GPU.My) Question:Is there a specific command-line argument for the newÂ llama-serverÂ (likeÂ --main-gpuÂ in the oldÂ mainÂ binary) that I'm missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions ofÂ llama.cpp?Any advice would be greatly appreciated. Thanks",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Need help debugging: llama-server uses GPU Memory but 0% GPU Util for inference (CPU only)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mf581n",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2kxlmun5",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754075077,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running into a performance issue with a self-hosted agent and could use some help. I&amp;#39;ve successfully set up an agent system, but the inference is extremely slow because it&amp;#39;s only using the CPU.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt;Â Qwen3-Coder-480B-A35B-Instruct-GGUFÂ (Q8_0 quant from unsloth)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;Â RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt;Â LatestÂ llama.cppÂ compiled from source, using theÂ llama-serverÂ binary.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Agent:&lt;/strong&gt;Â A simple Python script usingÂ requestsÂ to call theÂ /completionÂ endpoint.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m launching the server with this command:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host 0.0.0.0 --port 8080\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The server loads the model successfully, andÂ nvidia-smiÂ confirms that the GPU memory is used (&lt;strong&gt;83% VRAM used&lt;/strong&gt;). However, when my agent sends a prompt and the model starts generating a response, theÂ &lt;strong&gt;GPU Utilization stays at 0-1%&lt;/strong&gt;, while a single CPU core is being used.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve already confirmed:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).&lt;/li&gt;\n&lt;li&gt;The Python agent script works and correctly communicates with the server.&lt;/li&gt;\n&lt;li&gt;The issue is purely that the actual token generation computation is not happening on the GPU.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;My Question:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Is there a specific command-line argument for the newÂ llama-serverÂ (likeÂ --main-gpuÂ in the oldÂ mainÂ binary) that I&amp;#39;m missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions ofÂ llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;Any advice would be greatly appreciated. ThanksI&amp;#39;m running into a performance issue with a self-hosted agent and could use some help. I&amp;#39;ve successfully set up an agent system, but the inference is extremely slow because it&amp;#39;s only using the &lt;a href=\"http://CPU.My\"&gt;CPU.My&lt;/a&gt; Setup:Model:Â Qwen3-Coder-480B-A35B-Instruct-GGUFÂ (Q8_0 quant from unsloth)  &lt;/p&gt;\n\n&lt;p&gt;Hardware:Â RunPod with RTX 5090 (32GB VRAM), 32 vCPU, 125GB RAM  &lt;/p&gt;\n\n&lt;p&gt;Backend:Â LatestÂ llama.cppÂ compiled from source, using theÂ llama-serverÂ binary.  &lt;/p&gt;\n\n&lt;p&gt;Agent:Â A simple Python script usingÂ requestsÂ to call theÂ /completionÂ endpoint.The Problem:I&amp;#39;m launching the server with this command:Generated code./llama-server --model /path/to/model.gguf --n-gpu-layers 3 -c 8192 --host &lt;a href=\"http://0.0.0.0\"&gt;0.0.0.0&lt;/a&gt; --port 8080&lt;br/&gt;\nUse codeÂ with caution.The server loads the model successfully, andÂ nvidia-smiÂ confirms that the GPU memory is used (83% VRAM used). However, when my agent sends a prompt and the model starts generating a response, theÂ GPU Utilization stays at 0-1%, while a single CPU core is being used.What I&amp;#39;ve already confirmed:The model is loaded correctly, and layers are offloaded (offloaded 3/63 layers to GPU).  &lt;/p&gt;\n\n&lt;p&gt;The Python agent script works and correctly communicates with the server.  &lt;/p&gt;\n\n&lt;p&gt;The issue is purely that the actual token generation computation is not happening on the &lt;a href=\"http://GPU.My\"&gt;GPU.My&lt;/a&gt; Question:Is there a specific command-line argument for the newÂ llama-serverÂ (likeÂ --main-gpuÂ in the oldÂ mainÂ binary) that I&amp;#39;m missing to force inference to run on the GPU? Or is this a known issue/bug with recent versions ofÂ llama.cpp?Any advice would be greatly appreciated. Thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mf581n",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Rezvord",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/",
            "subreddit_subscribers": 508541,
            "created_utc": 1754075077,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6fe1ti",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "RevolutionaryLime758",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6er2gp",
                                "score": 1,
                                "author_fullname": "t2_lvhso4a4q",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "# ðŸ˜±",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6fe1ti",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;h1&gt;ðŸ˜±&lt;/h1&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mf581n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/n6fe1ti/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754085530,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754085530,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6er2gp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Rezvord",
                      "can_mod_post": false,
                      "created_utc": 1754078492,
                      "send_replies": true,
                      "parent_id": "t1_n6emtna",
                      "score": 0,
                      "author_fullname": "t2_2kxlmun5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "So how should prompt looks like to terminal?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6er2gp",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So how should prompt looks like to terminal?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf581n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/n6er2gp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754078492,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6emtna",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754077221,
            "send_replies": true,
            "parent_id": "t3_1mf581n",
            "score": 5,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You offloaded 3 layers of a 63 layer model.  If it ran equally fast on CPU and GPU then you'd expect 3/63 = 5% utilization.  Not sure what the memory bandwidth of your runpod could be but I think ~500GBps is the max possible for CPUs while the 5090 is 1700GBps so you'd expect like 3/1700 / (3/1700 + 60/500) = 1.5%.\n\nIf you want to do better, instead of `--n-gpu-layers 3` run with `--n-gpu-layers 99 --override-tensors exps=CPU` that will offload the un-routed tensors to the GPU which represent about 1/3 of the active parameters.  That gives .33/1700 / (.33/1700 + .66/500) ~= 13% which is not great but better.  (I actually tested this myself with a 4090, which only has 1000GBps memory and I indeed get the predicted ~20% utilization.)  You can also throw a few full layers on there too, e.g. `--override-tensors \\.[0-1]\\.=CUDA0,exps=CPU` but don't expect much gains from these.\n\nP.S. Another optimization that will be important for you with your ~500GB model: llama.cpp defaults to using the GPU to process the prompt (for lengths &gt;=32).  That means it needs to stream the 500GB of model to the GPU to process a single batch of tokens (given by `--ubatch-size`).  By default, ubatch=512 which means PP becomes limited to `64GB/s (PCIe) / 500GB (model) * 512 tok (ubatch) = 65tok/2` (in theory, practice is probably worse).  You will want to bump `--ubatch-size` to like 2048 or 4096 which should boost PP by like 3.5x or 6x.\n\nThis is of particular importance for people with a &lt;=4090 since PCIe 4 means you get about half that performacne.  You can disable GPU prompt processing with `--no-op-offload`, and while CPU might be faster than ubatch=512 (depending on CPU) it'll usually lose on larger batches.  The threshold of 32 is compiled in, AFAICT",
            "edited": 1754077914,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6emtna",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You offloaded 3 layers of a 63 layer model.  If it ran equally fast on CPU and GPU then you&amp;#39;d expect 3/63 = 5% utilization.  Not sure what the memory bandwidth of your runpod could be but I think ~500GBps is the max possible for CPUs while the 5090 is 1700GBps so you&amp;#39;d expect like 3/1700 / (3/1700 + 60/500) = 1.5%.&lt;/p&gt;\n\n&lt;p&gt;If you want to do better, instead of &lt;code&gt;--n-gpu-layers 3&lt;/code&gt; run with &lt;code&gt;--n-gpu-layers 99 --override-tensors exps=CPU&lt;/code&gt; that will offload the un-routed tensors to the GPU which represent about 1/3 of the active parameters.  That gives .33/1700 / (.33/1700 + .66/500) ~= 13% which is not great but better.  (I actually tested this myself with a 4090, which only has 1000GBps memory and I indeed get the predicted ~20% utilization.)  You can also throw a few full layers on there too, e.g. &lt;code&gt;--override-tensors \\.[0-1]\\.=CUDA0,exps=CPU&lt;/code&gt; but don&amp;#39;t expect much gains from these.&lt;/p&gt;\n\n&lt;p&gt;P.S. Another optimization that will be important for you with your ~500GB model: llama.cpp defaults to using the GPU to process the prompt (for lengths &amp;gt;=32).  That means it needs to stream the 500GB of model to the GPU to process a single batch of tokens (given by &lt;code&gt;--ubatch-size&lt;/code&gt;).  By default, ubatch=512 which means PP becomes limited to &lt;code&gt;64GB/s (PCIe) / 500GB (model) * 512 tok (ubatch) = 65tok/2&lt;/code&gt; (in theory, practice is probably worse).  You will want to bump &lt;code&gt;--ubatch-size&lt;/code&gt; to like 2048 or 4096 which should boost PP by like 3.5x or 6x.&lt;/p&gt;\n\n&lt;p&gt;This is of particular importance for people with a &amp;lt;=4090 since PCIe 4 means you get about half that performacne.  You can disable GPU prompt processing with &lt;code&gt;--no-op-offload&lt;/code&gt;, and while CPU might be faster than ubatch=512 (depending on CPU) it&amp;#39;ll usually lose on larger batches.  The threshold of 32 is compiled in, AFAICT&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/n6emtna/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754077221,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf581n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6f37fd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Rezvord",
                      "can_mod_post": false,
                      "created_utc": 1754082106,
                      "send_replies": true,
                      "parent_id": "t1_n6etqb1",
                      "score": -1,
                      "author_fullname": "t2_2kxlmun5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Okay so how to fix it? What I should do?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6f37fd",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Okay so how to fix it? What I should do?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf581n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/n6f37fd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754082106,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6etqb1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "segmond",
            "can_mod_post": false,
            "created_utc": 1754079288,
            "send_replies": true,
            "parent_id": "t3_1mf581n",
            "score": 4,
            "author_fullname": "t2_ah13x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The GPU processes the 3 layer very fast you barely notice, it's being used.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6etqb1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The GPU processes the 3 layer very fast you barely notice, it&amp;#39;s being used.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/n6etqb1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754079288,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mf581n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6eqqiu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Rezvord",
                      "can_mod_post": false,
                      "created_utc": 1754078393,
                      "send_replies": true,
                      "parent_id": "t1_n6ej6gt",
                      "score": 0,
                      "author_fullname": "t2_2kxlmun5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "If i use prompt:   \n\n\n/workspace/llama.cpp/build/bin/llama-run --n-gpu-layers 99 --override-tensors '{\"\\*exps\\*\": \"cpu\"}' -c 8192 --ubatch-size 2048 /workspace/Q8\\_0/Q8\\_0/Qwen3-Coder-480B-A35B-Instruct-Q8\\_0-00001-of-00011.gguf\n\n\n\nError: Failed to parse arguments.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6eqqiu",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If i use prompt:   &lt;/p&gt;\n\n&lt;p&gt;/workspace/llama.cpp/build/bin/llama-run --n-gpu-layers 99 --override-tensors &amp;#39;{&amp;quot;*exps*&amp;quot;: &amp;quot;cpu&amp;quot;}&amp;#39; -c 8192 --ubatch-size 2048 /workspace/Q8_0/Q8_0/Qwen3-Coder-480B-A35B-Instruct-Q8_0-00001-of-00011.gguf&lt;/p&gt;\n\n&lt;p&gt;Error: Failed to parse arguments.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf581n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/n6eqqiu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754078393,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ej6gt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dry_Veterinarian9227",
            "can_mod_post": false,
            "created_utc": 1754076128,
            "send_replies": true,
            "parent_id": "t3_1mf581n",
            "score": -2,
            "author_fullname": "t2_gjk8u1usg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Have you tried using Docker? I have a similar setup with ollama, qwen 2.5, and it works nicely on Docker. Maybe the server does not see the GPU, try ./llama-server --list-devices command. Please make sure the CUDA backend is used, you can check it in the ollama server start logs.  You can fine-tune which tensors go where, for example --override-tensors=\"\\*attn.\\*=GPU,\\*ffn\\_.\\*\\_exps.\\*=CPU\". I hope it helps you.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ej6gt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Have you tried using Docker? I have a similar setup with ollama, qwen 2.5, and it works nicely on Docker. Maybe the server does not see the GPU, try ./llama-server --list-devices command. Please make sure the CUDA backend is used, you can check it in the ollama server start logs.  You can fine-tune which tensors go where, for example --override-tensors=&amp;quot;*attn.*=GPU,*ffn_.*_exps.*=CPU&amp;quot;. I hope it helps you.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf581n/need_help_debugging_llamaserver_uses_gpu_memory/n6ej6gt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754076128,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf581n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -2
          }
        }
      ],
      "before": null
    }
  }
]