[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "If set incorrectly, the `max_token` parameter may cause a response to be cut off. If set too high, the response may be too verbose. Thinking models use most tokens in the thinking stage, non-thinking models do not.\n\nSome models suggest an adequate output length (i.e. `Qwen3-Coder-480B-A35B-Instruct` [suggests](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct) 65,536 tokens). But not all do.\n\nHow should I think about setting this value? Should I even think about it at all? Should this be done by the publisher of the model?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How to think about the value of max_token when using different models for inference?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m84j9w",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_6z17cidd9",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753363964,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If set incorrectly, the &lt;code&gt;max_token&lt;/code&gt; parameter may cause a response to be cut off. If set too high, the response may be too verbose. Thinking models use most tokens in the thinking stage, non-thinking models do not.&lt;/p&gt;\n\n&lt;p&gt;Some models suggest an adequate output length (i.e. &lt;code&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/code&gt; &lt;a href=\"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct\"&gt;suggests&lt;/a&gt; 65,536 tokens). But not all do.&lt;/p&gt;\n\n&lt;p&gt;How should I think about setting this value? Should I even think about it at all? Should this be done by the publisher of the model?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?auto=webp&amp;s=313bb0869a50cdf98069a47cd062047c974d9797",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d107a6b6b4389cb37d48d7ce4ff4d5aa35e4d93a",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=70a0bfd3fdb60bf07218589a46c055ba6044e2f8",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ad6b787991925588cd294c0ea3a744e9386e4bff",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1547f625cbccf70a7763a9c35af1919246072a2e",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2250994bcaf9a21420cff56896f998fee7edfc4f",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4fac2905be106e725dfbc4a288758fa9e2ff29d",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "SU4EkoBE9zB_i4T28BH-B8NRspWSu8pgjF1RIMOo6CQ"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m84j9w",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "nonredditaccount",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m84j9w/how_to_think_about_the_value_of_max_token_when/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m84j9w/how_to_think_about_the_value_of_max_token_when/",
            "subreddit_subscribers": 504023,
            "created_utc": 1753363964,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4x9tq7",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DinoAmino",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4wgkzl",
                                "score": 2,
                                "author_fullname": "t2_j1v7f",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Just don't set it. Leave it be and the max tokens available will be context_ limit - (context + prompts.)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4x9tq7",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just don&amp;#39;t set it. Leave it be and the max tokens available will be context_ limit - (context + prompts.)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m84j9w",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m84j9w/how_to_think_about_the_value_of_max_token_when/n4x9tq7/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753373736,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753373736,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4yrcyr",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ShengrenR",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4wgkzl",
                                "score": 3,
                                "author_fullname": "t2_ji4n4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "max tokens doesn't affect the way that the model behaves - it's not like looking at that parameter and thinking \"I have all this room\" - it doesn't have any knowledge of that, it's purely about how the framework around it handles things.\n\nIf you're running things locally tokens = context window = VRAM/memory use - so you're effectively limited by your hardware, as well as how well the model has been trained to use context windows of varying sizes.\n\nWhen a model goes to 'respond' it's taking all of the input context (instructions, previous responses, anything else added) and stuffing that in the front end - this is now taking up part of that \\~256k (on paper) max where the model will behave.. as it generates more tokens it's adding to that context (remember, each new, individual token, is a new pass) - so when you set 65k 'max new tokens' you're basically just telling the inference engine to cut the thing off if it ever hits 65k output tokens in a single go (that's a LOT of output for most models, I doubt you see it often, especially in non-'reasoning' models).  The final output should be no different than had you set max to 1 and kept feeding that previous context back in as a request.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4yrcyr",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;max tokens doesn&amp;#39;t affect the way that the model behaves - it&amp;#39;s not like looking at that parameter and thinking &amp;quot;I have all this room&amp;quot; - it doesn&amp;#39;t have any knowledge of that, it&amp;#39;s purely about how the framework around it handles things.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re running things locally tokens = context window = VRAM/memory use - so you&amp;#39;re effectively limited by your hardware, as well as how well the model has been trained to use context windows of varying sizes.&lt;/p&gt;\n\n&lt;p&gt;When a model goes to &amp;#39;respond&amp;#39; it&amp;#39;s taking all of the input context (instructions, previous responses, anything else added) and stuffing that in the front end - this is now taking up part of that ~256k (on paper) max where the model will behave.. as it generates more tokens it&amp;#39;s adding to that context (remember, each new, individual token, is a new pass) - so when you set 65k &amp;#39;max new tokens&amp;#39; you&amp;#39;re basically just telling the inference engine to cut the thing off if it ever hits 65k output tokens in a single go (that&amp;#39;s a LOT of output for most models, I doubt you see it often, especially in non-&amp;#39;reasoning&amp;#39; models).  The final output should be no different than had you set max to 1 and kept feeding that previous context back in as a request.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m84j9w",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m84j9w/how_to_think_about_the_value_of_max_token_when/n4yrcyr/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753388559,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753388559,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4wgkzl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "nonredditaccount",
                      "can_mod_post": false,
                      "created_utc": 1753365554,
                      "send_replies": true,
                      "parent_id": "t1_n4wf3zm",
                      "score": 1,
                      "author_fullname": "t2_6z17cidd9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Understood. Is \"let them output whatever they want\" the same as me setting an extremely high `max_tokens`? In other words, if I set `max_tokens` to exceed what the model expects, is that sufficient or will the model try to \"use\" all the tokens I've given it?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4wgkzl",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Understood. Is &amp;quot;let them output whatever they want&amp;quot; the same as me setting an extremely high &lt;code&gt;max_tokens&lt;/code&gt;? In other words, if I set &lt;code&gt;max_tokens&lt;/code&gt; to exceed what the model expects, is that sufficient or will the model try to &amp;quot;use&amp;quot; all the tokens I&amp;#39;ve given it?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m84j9w",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m84j9w/how_to_think_about_the_value_of_max_token_when/n4wgkzl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753365554,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4wf3zm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "mtmttuan",
            "can_mod_post": false,
            "created_utc": 1753365108,
            "send_replies": true,
            "parent_id": "t3_1m84j9w",
            "score": 3,
            "author_fullname": "t2_6mjqz0at",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I just let models output whatever they want. They aren't that lengthy anyway.   \nWell obviously you would have some tasks that you want the model to output shorter for budget or long waiting time, but generally I think a better solution is simply prompt it to be shorter. Max token is good only when you need to absolutely limit them",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4wf3zm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just let models output whatever they want. They aren&amp;#39;t that lengthy anyway.&lt;br/&gt;\nWell obviously you would have some tasks that you want the model to output shorter for budget or long waiting time, but generally I think a better solution is simply prompt it to be shorter. Max token is good only when you need to absolutely limit them&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m84j9w/how_to_think_about_the_value_of_max_token_when/n4wf3zm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753365108,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m84j9w",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]