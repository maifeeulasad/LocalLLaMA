[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey! I’d like to set up my machine to work on my codebase while I’m AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.\n\nI’ve already tested a few tools — kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.\n\nAre there any more or less standardized setups for this kind of workflow?\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What's the current go-to setup for a fully-local coding agent that continuously improves code?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfpn4a",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.57,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_nt892",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754138620,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey! I’d like to set up my machine to work on my codebase while I’m AFK. Ideally, it would randomly pick from a list of pre-defined tasks (e.g. optimize performance, simplify code, find bugs, add tests, implement TODOs), work on it for as long as needed, then open a merge request. After that, it should revert the changes and move on to the next task or project, continuing until I turn it off.&lt;/p&gt;\n\n&lt;p&gt;I’ve already tested a few tools — kwaak, Harbor, All Hands, AutoGPT, and maybe more. But honestly, with so many options out there, I feel a bit lost.&lt;/p&gt;\n\n&lt;p&gt;Are there any more or less standardized setups for this kind of workflow?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mfpn4a",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "sasik520",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/",
            "subreddit_subscribers": 509054,
            "created_utc": 1754138620,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ir087",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1754139680,
            "send_replies": true,
            "parent_id": "t3_1mfpn4a",
            "score": 5,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\\&gt; Are there any more or less standardized setups for this kind of workflow?  \n  \nno",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ir087",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; Are there any more or less standardized setups for this kind of workflow?  &lt;/p&gt;\n\n&lt;p&gt;no&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/n6ir087/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754139680,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mfpn4a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ir5wn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kmouratidis",
            "can_mod_post": false,
            "created_utc": 1754139741,
            "send_replies": true,
            "parent_id": "t3_1mfpn4a",
            "score": 2,
            "author_fullname": "t2_k6u7rfxb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I used to be a fan of All-Hands/OpenHands, but then it basically stopped working for the models I was using (Qwen, Mistral, Devstral) because it's string replace doesn't handle spacing well and almost always that tool call fails. Note, I've only tried the UI so far, haven't messed around with the CLI yet.\n\nBut I tried RooCode (which is an editor plugin) yesterday for the first time, and it exceeded my expectations. Maybe you can configure some extras like MCP servers or simple scripts to do the rest of what you need? Not that it needs an MCP server to run a few git commands. Not sure how any of this would work in a headless environment though.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ir5wn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I used to be a fan of All-Hands/OpenHands, but then it basically stopped working for the models I was using (Qwen, Mistral, Devstral) because it&amp;#39;s string replace doesn&amp;#39;t handle spacing well and almost always that tool call fails. Note, I&amp;#39;ve only tried the UI so far, haven&amp;#39;t messed around with the CLI yet.&lt;/p&gt;\n\n&lt;p&gt;But I tried RooCode (which is an editor plugin) yesterday for the first time, and it exceeded my expectations. Maybe you can configure some extras like MCP servers or simple scripts to do the rest of what you need? Not that it needs an MCP server to run a few git commands. Not sure how any of this would work in a headless environment though.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/n6ir5wn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754139741,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfpn4a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iz4xp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "sasik520",
                      "can_mod_post": false,
                      "created_utc": 1754142691,
                      "send_replies": true,
                      "parent_id": "t1_n6it8a1",
                      "score": 1,
                      "author_fullname": "t2_nt892",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I wouldn't be that pessimistic. Manual tests were promising.\n\nWhen you say you would create an agent, what would you use to create it?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iz4xp",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wouldn&amp;#39;t be that pessimistic. Manual tests were promising.&lt;/p&gt;\n\n&lt;p&gt;When you say you would create an agent, what would you use to create it?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfpn4a",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/n6iz4xp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754142691,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6it8a1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "IdoPIdo",
            "can_mod_post": false,
            "created_utc": 1754140535,
            "send_replies": true,
            "parent_id": "t3_1mfpn4a",
            "score": 1,
            "author_fullname": "t2_27z5afy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There aren't any standardized setups but if that's what you want I'd create an agent that can invoke gemini-cli (which is free) to do the actual coding.   \nI would personally write detailed markdown specifications for each task, explaining exactly what you want to improve. Then have an agent go over the markdown file and create a task for gemini-cli so it can do it for you.  \nIf you want a model to jsut go over your codebase and continuously make it better you're going to get shit code. What is better? I might prefer Pydantic while you think it's unnecessary. Without further clarification how will the model know which option you truely want?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6it8a1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There aren&amp;#39;t any standardized setups but if that&amp;#39;s what you want I&amp;#39;d create an agent that can invoke gemini-cli (which is free) to do the actual coding.&lt;br/&gt;\nI would personally write detailed markdown specifications for each task, explaining exactly what you want to improve. Then have an agent go over the markdown file and create a task for gemini-cli so it can do it for you.&lt;br/&gt;\nIf you want a model to jsut go over your codebase and continuously make it better you&amp;#39;re going to get shit code. What is better? I might prefer Pydantic while you think it&amp;#39;s unnecessary. Without further clarification how will the model know which option you truely want?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfpn4a/whats_the_current_goto_setup_for_a_fullylocal/n6it8a1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754140535,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfpn4a",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]