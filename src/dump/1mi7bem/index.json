[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "No more need for super-complex regular expression in the -ot option! Just do `--cpu-moe` or `--n-cpu-moe #` and reduce the number until the model no longer fits on the GPU.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "New llama.cpp options make MoE offloading trivial: `--n-cpu-moe`",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 70,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mi7bem",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "ups": 24,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_5b972ieo",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 24,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=a6683566ea3a2ab3f10144b97e4072ab0639db68",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754395457,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "github.com",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;No more need for super-complex regular expression in the -ot option! Just do &lt;code&gt;--cpu-moe&lt;/code&gt; or &lt;code&gt;--n-cpu-moe #&lt;/code&gt; and reduce the number until the model no longer fits on the GPU.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://github.com/ggml-org/llama.cpp/pull/15077",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?auto=webp&amp;s=6d449514411bd16575e7077a57c169d46f49d2e5",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ad9e8961b664b6710eb58c6fa604f119639c53e1",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=734713090c897be69c4245b0a525571c45526c1e",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=03bee70399e29da7291f5a8bd625456274a76ec6",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc3c355c6e5a0c2867654d9ea9c2e7c8ed9c618b",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a4e77bcd6622924abcac1fa3acd0c374064d0fda",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc470b6cb695b2d267be6deae5c3dd1b77f91432",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "z9bB4lcxUZhZHvTMdXPfCmtA3BVsCM9FB8umPl48qlU"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1mi7bem",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Pristine-Woodpecker",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/",
            "stickied": false,
            "url": "https://github.com/ggml-org/llama.cpp/pull/15077",
            "subreddit_subscribers": 510540,
            "created_utc": 1754395457,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n71fctg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Muted-Celebration-47",
            "can_mod_post": false,
            "created_utc": 1754395681,
            "send_replies": true,
            "parent_id": "t3_1mi7bem",
            "score": 4,
            "author_fullname": "t2_q2qi86l3f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yeah, I found this way is easier than find the best -ot by yourself. This --n-cpu-moe option is perfect fit with GLM4.5-Air gguf case.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n71fctg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, I found this way is easier than find the best -ot by yourself. This --n-cpu-moe option is perfect fit with GLM4.5-Air gguf case.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/n71fctg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754395681,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi7bem",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n71htev",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1754396603,
            "send_replies": true,
            "parent_id": "t3_1mi7bem",
            "score": 3,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My name was mentioned ;) so I tested it today in the morning with GLM\n\n`llama-server -ts 18/17/18 -ngl 99 -m ~/models/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf --n-cpu-moe 2 --jinja --host` [`0.0.0.0`](http://0.0.0.0)\n\n  \nI am getting over 45 t/s on 3x3090",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n71htev",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My name was mentioned ;) so I tested it today in the morning with GLM&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama-server -ts 18/17/18 -ngl 99 -m ~/models/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf --n-cpu-moe 2 --jinja --host&lt;/code&gt; &lt;a href=\"http://0.0.0.0\"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I am getting over 45 t/s on 3x3090&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/n71htev/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754396603,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mi7bem",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n71hl3x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "thenomadexplorerlife",
            "can_mod_post": false,
            "created_utc": 1754396518,
            "send_replies": true,
            "parent_id": "t3_1mi7bem",
            "score": 1,
            "author_fullname": "t2_ftxyqwjr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This seems a good enhancement! Just curious and may be a bit off-topic, is there a way to do something similar using two machines? For example, I have a Mac mini 64GB RAM and another linux laptop with 32GB RAM. It would be nice if I can run some layers in Mac GPU and remaining layers in linux laptop. This will allow me to run larger models by combining the RAM of two machines to load the model. New models are becoming bigger and buying a new machine with more RAM is out of budget for me.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n71hl3x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This seems a good enhancement! Just curious and may be a bit off-topic, is there a way to do something similar using two machines? For example, I have a Mac mini 64GB RAM and another linux laptop with 32GB RAM. It would be nice if I can run some layers in Mac GPU and remaining layers in linux laptop. This will allow me to run larger models by combining the RAM of two machines to load the model. New models are becoming bigger and buying a new machine with more RAM is out of budget for me.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/n71hl3x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754396518,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi7bem",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n71ipfe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Secure_Reflection409",
            "can_mod_post": false,
            "created_utc": 1754396930,
            "send_replies": true,
            "parent_id": "t3_1mi7bem",
            "score": 1,
            "author_fullname": "t2_by77ogdhr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Excellenté!\n\n\nReally impressed with LCP's web interface, too.\n\n\nIf it had a context estimator like LMS it would prolly be perfect.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n71ipfe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Excellenté!&lt;/p&gt;\n\n&lt;p&gt;Really impressed with LCP&amp;#39;s web interface, too.&lt;/p&gt;\n\n&lt;p&gt;If it had a context estimator like LMS it would prolly be perfect.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi7bem/new_llamacpp_options_make_moe_offloading_trivial/n71ipfe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754396930,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi7bem",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]