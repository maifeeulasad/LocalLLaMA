[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone,\n\nI'm really interested in understanding pretraining of LLMs (not just fine-tuning). But it's been extremely difficult to find clear, practical resources or workflows for actually learning this from scratch. Most tutorials either skip over the hard parts, focus only on fine-tuning very small LLMs that can't be used in most cases. On top of that, even trying things on your own is extremely expensive, especially for someone just trying to learn.\n\nSo my questions are\n\nHow can someone with limited compute or resources learn the concepts and process of LLM pretraining, or proper post-training llms\n\nIs there any small-scale setup or framework like TinyLLaMA or nanoGPT that I can use locally to understand the architecture and training loop deeply\n\nAndrej Karpathy helped me a lot to have a rough understanding on these. What else?\n\nAre there any solid open-source learning paths, repos, blogs, or courses that explain this step by step\n\nAny way to experiment without burning cash on GPUs? I'm not looking to train GPT-3 myself. I just want to get practical and theoretical clarity on how pretraining works end to end. Also open to reading research papers if you think they help\n\nThanks in advance",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How can I actually learn and try LLM pretraining? (or post training a large LLM )",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mk8oll",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 13,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rdvat0vg1",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 13,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754592964,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m really interested in understanding pretraining of LLMs (not just fine-tuning). But it&amp;#39;s been extremely difficult to find clear, practical resources or workflows for actually learning this from scratch. Most tutorials either skip over the hard parts, focus only on fine-tuning very small LLMs that can&amp;#39;t be used in most cases. On top of that, even trying things on your own is extremely expensive, especially for someone just trying to learn.&lt;/p&gt;\n\n&lt;p&gt;So my questions are&lt;/p&gt;\n\n&lt;p&gt;How can someone with limited compute or resources learn the concepts and process of LLM pretraining, or proper post-training llms&lt;/p&gt;\n\n&lt;p&gt;Is there any small-scale setup or framework like TinyLLaMA or nanoGPT that I can use locally to understand the architecture and training loop deeply&lt;/p&gt;\n\n&lt;p&gt;Andrej Karpathy helped me a lot to have a rough understanding on these. What else?&lt;/p&gt;\n\n&lt;p&gt;Are there any solid open-source learning paths, repos, blogs, or courses that explain this step by step&lt;/p&gt;\n\n&lt;p&gt;Any way to experiment without burning cash on GPUs? I&amp;#39;m not looking to train GPT-3 myself. I just want to get practical and theoretical clarity on how pretraining works end to end. Also open to reading research papers if you think they help&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mk8oll",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Distinct-Drive1307",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/",
            "subreddit_subscribers": 513813,
            "created_utc": 1754592964,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7i1bvu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Affectionate-Cap-600",
                      "can_mod_post": false,
                      "created_utc": 1754605756,
                      "send_replies": true,
                      "parent_id": "t1_n7gx9dt",
                      "score": 2,
                      "author_fullname": "t2_5oltmr5b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;you could probably do a 100 to 400M model using a combination of tricks here using possibly just a free Colab or Kaggle T4 training run.\n\n\nseriously?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7i1bvu",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;you could probably do a 100 to 400M model using a combination of tricks here using possibly just a free Colab or Kaggle T4 training run.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;seriously?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mk8oll",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/n7i1bvu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754605756,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7gx9dt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1754593697,
            "send_replies": true,
            "parent_id": "t3_1mk8oll",
            "score": 7,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Tinystories, DataDecide, Textbooks are All You Need and LIMA are probably your best bets.\n\nThe common theme in all of them is that you can train surprisingly large models on limited compute with very carefully planned data pipelines.\n\nTinystories has recipes for very small models (\\~100m category), and before you say that's not what you're aiming to do, it's important to start with the basics and improve from there.\n\nDataDecide has a lot of good recipes to decide on high quality pre-training data, and Textbooks are All You Need details strategies to produce high quality synthetic data that's more efficient per token trained.\n\nI think if you combine all of these plus some tricks from the Keller Jordan nanoGPT fork / speedrun, you could probably train a decently sized model on a limited budget. I haven't verified this, but I'm guessing with some modifications to suit the hardware you could probably do a 100 to 400M model using a combination of tricks here using possibly just a free Colab or Kaggle T4 training run.\n\nIf you wanted to you could also then add on some tricks from ScaleSmart and grow it part way through training to hit a larger total size. Maybe you could do an initial 300m run and scale to 600m and get a kind of usable mode?\n\nYou could then instruct-tune it with LIMA strategies and it'd...Probably be able to chat with you, I think.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7gx9dt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tinystories, DataDecide, Textbooks are All You Need and LIMA are probably your best bets.&lt;/p&gt;\n\n&lt;p&gt;The common theme in all of them is that you can train surprisingly large models on limited compute with very carefully planned data pipelines.&lt;/p&gt;\n\n&lt;p&gt;Tinystories has recipes for very small models (~100m category), and before you say that&amp;#39;s not what you&amp;#39;re aiming to do, it&amp;#39;s important to start with the basics and improve from there.&lt;/p&gt;\n\n&lt;p&gt;DataDecide has a lot of good recipes to decide on high quality pre-training data, and Textbooks are All You Need details strategies to produce high quality synthetic data that&amp;#39;s more efficient per token trained.&lt;/p&gt;\n\n&lt;p&gt;I think if you combine all of these plus some tricks from the Keller Jordan nanoGPT fork / speedrun, you could probably train a decently sized model on a limited budget. I haven&amp;#39;t verified this, but I&amp;#39;m guessing with some modifications to suit the hardware you could probably do a 100 to 400M model using a combination of tricks here using possibly just a free Colab or Kaggle T4 training run.&lt;/p&gt;\n\n&lt;p&gt;If you wanted to you could also then add on some tricks from ScaleSmart and grow it part way through training to hit a larger total size. Maybe you could do an initial 300m run and scale to 600m and get a kind of usable mode?&lt;/p&gt;\n\n&lt;p&gt;You could then instruct-tune it with LIMA strategies and it&amp;#39;d...Probably be able to chat with you, I think.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/n7gx9dt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754593697,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk8oll",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7h2hhz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "V4ldeLund",
            "can_mod_post": false,
            "created_utc": 1754595231,
            "send_replies": true,
            "parent_id": "t3_1mk8oll",
            "score": 3,
            "author_fullname": "t2_yajejgr3g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://huggingface.co/spaces/nanotron/ultrascale-playbook\n\n\nThis might be helpful ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7h2hhz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/spaces/nanotron/ultrascale-playbook\"&gt;https://huggingface.co/spaces/nanotron/ultrascale-playbook&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;This might be helpful &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/n7h2hhz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754595231,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk8oll",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7gverc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pleasant_Tree_1727",
            "can_mod_post": false,
            "created_utc": 1754593158,
            "send_replies": true,
            "parent_id": "t3_1mk8oll",
            "score": 2,
            "author_fullname": "t2_ofdndf1vl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "up",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7gverc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;up&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/n7gverc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754593158,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk8oll",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7h02i9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Imjustmisunderstood",
            "can_mod_post": false,
            "created_utc": 1754594524,
            "send_replies": true,
            "parent_id": "t3_1mk8oll",
            "score": 1,
            "author_fullname": "t2_s7g9g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "[Build an LLM from scratch with Andrej Karpathy, renowned LLM researcher.](https://youtu.be/kCc8FmEb1nY?si=iLS0X7yJGQODlEOT)\n\n[Towards Data Science is an excellent blog with visuals that explains everything machine learning, data science, and the math related from the ground up, including cutting edge research!](https://towardsdatascience.com/)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7h02i9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://youtu.be/kCc8FmEb1nY?si=iLS0X7yJGQODlEOT\"&gt;Build an LLM from scratch with Andrej Karpathy, renowned LLM researcher.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://towardsdatascience.com/\"&gt;Towards Data Science is an excellent blog with visuals that explains everything machine learning, data science, and the math related from the ground up, including cutting edge research!&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk8oll/how_can_i_actually_learn_and_try_llm_pretraining/n7h02i9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754594524,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk8oll",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]