[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "So I've been lurking here watching everyone complain about needing multiple 4090s for anything decent, while I'm over here with my humble RTX 4060 (8GB VRAM) somehow running Llama 405B at a respectable 2.3 tokens per second.\n\nBefore you ask - no, I didn't sell my kidney for more hardware. No, I'm not using cloud. Yes, it's actually local.\n\nThe secret? I discovered that if you:\n\n1. Use the right quantisation (Q2\\_K)\n2. Offload exactly 12 layers to GPU\n3. Set your context to 2048\n4. Use mmap with the right flags\n5. Sacrifice a rubber duck to the CUDA gods\n\nYou can actually get decent performance out of budget hardware.\n\nI know what you're thinking - \"2.3 tokens/sec is trash.\" But consider this: my girlfriend's MacBook Air can't even run Llama 7B without turning into a space heater, and here I am having full conversations with 405B while watching Netflix on my second monitor.\n\nIs it perfect? No. Would I recommend this to my grandmother? Absolutely not. But am I weirdly proud of making a $300 GPU punch way above its weight class? You bet.\n\nThe best part? When people ask \"can you run X model locally?\" I can finally say yes instead of crying into my single-digit VRAM.\n\nSince people are asking, I'll document the exact setup and share it this weekend. Just need to make sure I'm not accidentally summoning any demons with my config files.\n\n RIP my DMs. Yes, I will share the setup. No, it doesn't work with Stable Diffusion. Yes, my GPU is probably going to die early, but that's a problem for future me.\n\n# ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "My RTX 4060 is running Llama 405B at 2.3 tokens/sec. Don't ask me how.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjtt3g",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.48,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_qv5y69oq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754552525,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I&amp;#39;ve been lurking here watching everyone complain about needing multiple 4090s for anything decent, while I&amp;#39;m over here with my humble RTX 4060 (8GB VRAM) somehow running Llama 405B at a respectable 2.3 tokens per second.&lt;/p&gt;\n\n&lt;p&gt;Before you ask - no, I didn&amp;#39;t sell my kidney for more hardware. No, I&amp;#39;m not using cloud. Yes, it&amp;#39;s actually local.&lt;/p&gt;\n\n&lt;p&gt;The secret? I discovered that if you:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Use the right quantisation (Q2_K)&lt;/li&gt;\n&lt;li&gt;Offload exactly 12 layers to GPU&lt;/li&gt;\n&lt;li&gt;Set your context to 2048&lt;/li&gt;\n&lt;li&gt;Use mmap with the right flags&lt;/li&gt;\n&lt;li&gt;Sacrifice a rubber duck to the CUDA gods&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;You can actually get decent performance out of budget hardware.&lt;/p&gt;\n\n&lt;p&gt;I know what you&amp;#39;re thinking - &amp;quot;2.3 tokens/sec is trash.&amp;quot; But consider this: my girlfriend&amp;#39;s MacBook Air can&amp;#39;t even run Llama 7B without turning into a space heater, and here I am having full conversations with 405B while watching Netflix on my second monitor.&lt;/p&gt;\n\n&lt;p&gt;Is it perfect? No. Would I recommend this to my grandmother? Absolutely not. But am I weirdly proud of making a $300 GPU punch way above its weight class? You bet.&lt;/p&gt;\n\n&lt;p&gt;The best part? When people ask &amp;quot;can you run X model locally?&amp;quot; I can finally say yes instead of crying into my single-digit VRAM.&lt;/p&gt;\n\n&lt;p&gt;Since people are asking, I&amp;#39;ll document the exact setup and share it this weekend. Just need to make sure I&amp;#39;m not accidentally summoning any demons with my config files.&lt;/p&gt;\n\n&lt;p&gt;RIP my DMs. Yes, I will share the setup. No, it doesn&amp;#39;t work with Stable Diffusion. Yes, my GPU is probably going to die early, but that&amp;#39;s a problem for future me.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mjtt3g",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Nipurn_1234",
            "discussion_type": null,
            "num_comments": 36,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754552525,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dx0pe",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "i_need_good_name",
                      "can_mod_post": false,
                      "created_utc": 1754558104,
                      "send_replies": true,
                      "parent_id": "t1_n7dpshc",
                      "score": 17,
                      "author_fullname": "t2_ihhyi741",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "He forgot to mention his Threadripper + 300GB Ram",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dx0pe",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;He forgot to mention his Threadripper + 300GB Ram&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dx0pe/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754558104,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 17
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7dpshc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754553897,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 22,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You forgot the part where you tell us about the mobo, cpu and ram.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dpshc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You forgot the part where you tell us about the mobo, cpu and ram.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dpshc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754553897,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 22
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7edhcp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DanRey90",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7e6ewl",
                                "score": 2,
                                "author_fullname": "t2_iaygrkrh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "No one would call Llama Maverick “Llama 405B”, it doesn’t even have 405B parameters. Then again, the post looks AI-generated, so maybe it hallucinated it, who knows.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7edhcp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No one would call Llama Maverick “Llama 405B”, it doesn’t even have 405B parameters. Then again, the post looks AI-generated, so maybe it hallucinated it, who knows.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjtt3g",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7edhcp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754566321,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754566321,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7e6ewl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "notdba",
                      "can_mod_post": false,
                      "created_utc": 1754563127,
                      "send_replies": true,
                      "parent_id": "t1_n7dqilb",
                      "score": 3,
                      "author_fullname": "t2_4aai0pnm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It is most likely the 400B-A17B Llama4 Maverick, that only uses 3B parameters from the routed expert per forward pass. In comparison, gpt-oss-120b uses 3.6B, while gpt-oss-20b uses 2.4B.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7e6ewl",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is most likely the 400B-A17B Llama4 Maverick, that only uses 3B parameters from the routed expert per forward pass. In comparison, gpt-oss-120b uses 3.6B, while gpt-oss-20b uses 2.4B.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7e6ewl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754563127,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7e2ckt",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DanRey90",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7dvoxp",
                                "score": 2,
                                "author_fullname": "t2_iaygrkrh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It’s not ONLY dependent of memory bandwidth, but it’s an upper bound. What I meant by my comment is that there’s no physical way to get that 2.3t/s speed without AT LEAST 230GB/s RAM. I don’t have any experience with CPU inference so I can’t help you there. Look for ktransformers, they have some benchmarks running massive models using an Intel Xeon with tons of RAM plus a nVidia GPU. They recommend Xeon instead of Threadripper because it has AMX instructions, which help a bit.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7e2ckt",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s not ONLY dependent of memory bandwidth, but it’s an upper bound. What I meant by my comment is that there’s no physical way to get that 2.3t/s speed without AT LEAST 230GB/s RAM. I don’t have any experience with CPU inference so I can’t help you there. Look for ktransformers, they have some benchmarks running massive models using an Intel Xeon with tons of RAM plus a nVidia GPU. They recommend Xeon instead of Threadripper because it has AMX instructions, which help a bit.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjtt3g",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7e2ckt/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754561066,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754561066,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dvoxp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "munkiemagik",
                      "can_mod_post": false,
                      "created_utc": 1754557324,
                      "send_replies": true,
                      "parent_id": "t1_n7dqilb",
                      "score": 1,
                      "author_fullname": "t2_if95iuzc",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hi Im just interested in the proposed math. So you are saying that LLMs inference performance is ONLY bound by memory bandwidth and even CPU speed is more than sufficient for the actual compute part of things, ie that's why it is so important to have high memory bandwidth GPUs/systems?\n\nIm interested in this specifically as I recently built an older gen threadripper system (for other reasons) but wanted plenty of PCIE lanes for multi-gpu to run some LLMs on it and eventually get to fine tuning and training. The best I could get out of a large model (Qwen 235B A22B Q3KS) was around 9 t/s. This is with layers offloaded to both GPU (1x4090) and CPU 8x16GB DDR4 3200) I will be selling the 4090 at some point to put towards multi-GPU setup for running LLMs\n\nI find 9t/s too slow to be useable as a daily driver personally and am wondering what are the options to consider to enable improvements in t/s. I got blind-sided by the stated octa-channel memory on the threadripper pro 3945WX, I learnt after I built it up that in fact with its 2 CCDs mem bandwidth is not as great as you would have expected for 8 channel ram. So the only way (without getting a whole new platform) is to pick up a higher CCD Threadripper Pro 5965WX and above (when/if prices drop) to reasonable levels.\n\nfrom other user benchmarking I believe the 5965WX should give me almost twice the mem bandwidth compared to my 3945WX (I am testing around 75-80GB/s on my ddr4 3200, The posts i've seen for the 5965WX show around 150GB/s) so am I likely to see my current 9t/s increase proportionally with the increased memory bandwidth of the 5965WX? Threadripper 7965WX has decent uplift in octa-channel memory bandwidth but I don't see that coming down to my 'hobby' price point for a few more years. \n\nI bought the 3945WX as I got a really good deal on the CPU and motherboard, less than what most WRX80 motherboards sell for by themselves. I know threadripper pro isnt the ideal choice of CPU for everyone especially considering its price but I also want the high single thread clocks.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dvoxp",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi Im just interested in the proposed math. So you are saying that LLMs inference performance is ONLY bound by memory bandwidth and even CPU speed is more than sufficient for the actual compute part of things, ie that&amp;#39;s why it is so important to have high memory bandwidth GPUs/systems?&lt;/p&gt;\n\n&lt;p&gt;Im interested in this specifically as I recently built an older gen threadripper system (for other reasons) but wanted plenty of PCIE lanes for multi-gpu to run some LLMs on it and eventually get to fine tuning and training. The best I could get out of a large model (Qwen 235B A22B Q3KS) was around 9 t/s. This is with layers offloaded to both GPU (1x4090) and CPU 8x16GB DDR4 3200) I will be selling the 4090 at some point to put towards multi-GPU setup for running LLMs&lt;/p&gt;\n\n&lt;p&gt;I find 9t/s too slow to be useable as a daily driver personally and am wondering what are the options to consider to enable improvements in t/s. I got blind-sided by the stated octa-channel memory on the threadripper pro 3945WX, I learnt after I built it up that in fact with its 2 CCDs mem bandwidth is not as great as you would have expected for 8 channel ram. So the only way (without getting a whole new platform) is to pick up a higher CCD Threadripper Pro 5965WX and above (when/if prices drop) to reasonable levels.&lt;/p&gt;\n\n&lt;p&gt;from other user benchmarking I believe the 5965WX should give me almost twice the mem bandwidth compared to my 3945WX (I am testing around 75-80GB/s on my ddr4 3200, The posts i&amp;#39;ve seen for the 5965WX show around 150GB/s) so am I likely to see my current 9t/s increase proportionally with the increased memory bandwidth of the 5965WX? Threadripper 7965WX has decent uplift in octa-channel memory bandwidth but I don&amp;#39;t see that coming down to my &amp;#39;hobby&amp;#39; price point for a few more years. &lt;/p&gt;\n\n&lt;p&gt;I bought the 3945WX as I got a really good deal on the CPU and motherboard, less than what most WRX80 motherboards sell for by themselves. I know threadripper pro isnt the ideal choice of CPU for everyone especially considering its price but I also want the high single thread clocks.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dvoxp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754557324,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7dqilb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DanRey90",
            "can_mod_post": false,
            "created_utc": 1754554318,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 14,
            "author_fullname": "t2_iaygrkrh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That math isn’t mathing. About 100GB of the model lives in RAM/SSD with that setup. That means that to get 2.3 tok/s, you need a minimum of 230GB/s RAM. That’s not posible on consumer PCs. Which means you’re either lying, or omitting crucial information. If you have a workstation-grade CPU and RAM (Threadripper or something), the fact that your GPU costs $300 means jack shit. Your GPU isn’t punching above its weight class, its sorry ass is being lifted by the rest of your setup.",
            "edited": 1754556541,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dqilb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That math isn’t mathing. About 100GB of the model lives in RAM/SSD with that setup. That means that to get 2.3 tok/s, you need a minimum of 230GB/s RAM. That’s not posible on consumer PCs. Which means you’re either lying, or omitting crucial information. If you have a workstation-grade CPU and RAM (Threadripper or something), the fact that your GPU costs $300 means jack shit. Your GPU isn’t punching above its weight class, its sorry ass is being lifted by the rest of your setup.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dqilb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754554318,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 14
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7dxgan",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Immortal_Tuttle",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7dt5m9",
                                "score": 1,
                                "author_fullname": "t2_10n9s1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Wait. Are you telling me that my 4 NVMe PC that I'm just building can run LLMs? It's 12TB total with 4070ti 12GB",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7dxgan",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wait. Are you telling me that my 4 NVMe PC that I&amp;#39;m just building can run LLMs? It&amp;#39;s 12TB total with 4070ti 12GB&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjtt3g",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dxgan/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754558352,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754558352,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7eey0b",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Wise-Comb8596",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7e3pqc",
                                                    "score": 1,
                                                    "author_fullname": "t2_1sqe3qr1mf",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Because most people aren't assuming 100% uptime allowing this service to perpetually tie up all his computer's resources",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7eey0b",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because most people aren&amp;#39;t assuming 100% uptime allowing this service to perpetually tie up all his computer&amp;#39;s resources&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mjtt3g",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7eey0b/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754566930,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754566930,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7e3pqc",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Rich_Artist_8327",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7dy2ho",
                                          "score": 3,
                                          "author_fullname": "t2_1jk2ep8a52",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Isnt the model once written in SSD and then just read from it? Why it needs to swap?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7e3pqc",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isnt the model once written in SSD and then just read from it? Why it needs to swap?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mjtt3g",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7e3pqc/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754561772,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754561772,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7dy2ho",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "e79683074",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7dt5m9",
                                "score": 0,
                                "author_fullname": "t2_xj04i",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "No, but swapping (which is write intensive) will. Of course, once it's swapped, it's mostly reads, but it will do it again and again every time the model is launched.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7dy2ho",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, but swapping (which is write intensive) will. Of course, once it&amp;#39;s swapped, it&amp;#39;s mostly reads, but it will do it again and again every time the model is launched.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjtt3g",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dy2ho/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754558709,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754558709,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dt5m9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Rich_Artist_8327",
                      "can_mod_post": false,
                      "created_utc": 1754555853,
                      "send_replies": true,
                      "parent_id": "t1_n7doi14",
                      "score": 10,
                      "author_fullname": "t2_1jk2ep8a52",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Sorry but reads from SSD wont wear the SSD.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dt5m9",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry but reads from SSD wont wear the SSD.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dt5m9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754555853,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 10
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7dvyij",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Financial_Ring_4874",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7dpm2l",
                                "score": 1,
                                "author_fullname": "t2_a1sb6xpr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Definitely agree with you. Seems like a bit of a pointless exercise.\nI've never bothered with such a butchered quant. It's just made more sense to me to run something with fewer parameters @ higher quant.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7dvyij",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Definitely agree with you. Seems like a bit of a pointless exercise.\nI&amp;#39;ve never bothered with such a butchered quant. It&amp;#39;s just made more sense to me to run something with fewer parameters @ higher quant.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjtt3g",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dvyij/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754557479,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754557479,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dpm2l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "DanRey90",
                      "can_mod_post": false,
                      "created_utc": 1754553796,
                      "send_replies": true,
                      "parent_id": "t1_n7doi14",
                      "score": 5,
                      "author_fullname": "t2_iaygrkrh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "mmap won’t affect the life of the SSD, it just makes reads, it’s not like swap. Otherwise, I agree, he’s basically just running a lobotomized (Q2_K) large model in RAM at unuseable speeds with tiny context just to brag that he could. Which is fine and all, but isn’t exactly groundbreaking or reasonable.\n\nIt would make more sense to pick a different model. Even staying in the Llama family, Llama 3.3 70B is almost comparable to 405B, he could run it at a more reasonable quant with more speed. Or give Llama 4 Scout a chance (I don’t know if it’s as bad as people said), MoE are better suited for GPU-poor setups. Or maybe realize we’re on 2025 and run GLM Air at a decent quant, or Qwen 235B at a more aggresive quant.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dpm2l",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;mmap won’t affect the life of the SSD, it just makes reads, it’s not like swap. Otherwise, I agree, he’s basically just running a lobotomized (Q2_K) large model in RAM at unuseable speeds with tiny context just to brag that he could. Which is fine and all, but isn’t exactly groundbreaking or reasonable.&lt;/p&gt;\n\n&lt;p&gt;It would make more sense to pick a different model. Even staying in the Llama family, Llama 3.3 70B is almost comparable to 405B, he could run it at a more reasonable quant with more speed. Or give Llama 4 Scout a chance (I don’t know if it’s as bad as people said), MoE are better suited for GPU-poor setups. Or maybe realize we’re on 2025 and run GLM Air at a decent quant, or Qwen 235B at a more aggresive quant.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dpm2l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754553796,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dpn31",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "it0",
                      "can_mod_post": false,
                      "created_utc": 1754553812,
                      "send_replies": true,
                      "parent_id": "t1_n7doi14",
                      "score": -1,
                      "author_fullname": "t2_6szcn",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "No , you need that to watch Netflix.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dpn31",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No , you need that to watch Netflix.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dpn31/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754553812,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7doi14",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "thepriceisright__",
            "can_mod_post": false,
            "created_utc": 1754553164,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 26,
            "author_fullname": "t2_x22mr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "ok so you either have a server class mobo with a shitton of memory or you’re running it from ssds that are going to live fast and die young, but that 4060 ain’t doing shit.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7doi14",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ok so you either have a server class mobo with a shitton of memory or you’re running it from ssds that are going to live fast and die young, but that 4060 ain’t doing shit.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7doi14/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754553164,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 26
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7doind",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "thepriceisright__",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7dnqvr",
                                "score": 6,
                                "author_fullname": "t2_x22mr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "lol exactly",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7doind",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;lol exactly&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": true,
                                "can_gild": false,
                                "link_id": "t3_1mjtt3g",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7doind/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754553174,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754553174,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7duj8p",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "dnhanhtai0147",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7dnqvr",
                                "score": 1,
                                "author_fullname": "t2_rtp5dsu",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Not even a single percent lower? 😂",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7duj8p",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not even a single percent lower? 😂&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjtt3g",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7duj8p/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754556653,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754556653,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dnqvr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Doc_TB",
                      "can_mod_post": false,
                      "created_utc": 1754552725,
                      "send_replies": true,
                      "parent_id": "t1_n7dnl7v",
                      "score": 28,
                      "author_fullname": "t2_1199f1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "2.3 tokens/sec",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dnqvr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;2.3 tokens/sec&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dnqvr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754552725,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 28
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7dnl7v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "MaxKruse96",
            "can_mod_post": false,
            "created_utc": 1754552634,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 6,
            "author_fullname": "t2_pfi81",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "ok and how fast is it without the gpu?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dnl7v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ok and how fast is it without the gpu?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dnl7v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754552634,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dx75p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Unhappy_Geologist637",
            "can_mod_post": false,
            "created_utc": 1754558208,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 3,
            "author_fullname": "t2_1sguufwh9u",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; I know what you're thinking - \"2.3 tokens/sec is trash.\" \n\nNope, my first thought was \"Q2_K is trash\".\n\nDid you try running a 70B at higher quants and found it less good than this 405B at Q2?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dx75p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I know what you&amp;#39;re thinking - &amp;quot;2.3 tokens/sec is trash.&amp;quot; &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Nope, my first thought was &amp;quot;Q2_K is trash&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;Did you try running a 70B at higher quants and found it less good than this 405B at Q2?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dx75p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754558208,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7drdds",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DeltaSqueezer",
            "can_mod_post": false,
            "created_utc": 1754554816,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 2,
            "author_fullname": "t2_8jqx3m14",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "While it is possible to run some big models slowly, it might be more practical to run a smaller model quickly e.g. Qwen3-4B-2507",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7drdds",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While it is possible to run some big models slowly, it might be more practical to run a smaller model quickly e.g. Qwen3-4B-2507&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7drdds/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754554816,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dxa38",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "My_Unbiased_Opinion",
                      "can_mod_post": false,
                      "created_utc": 1754558254,
                      "send_replies": true,
                      "parent_id": "t1_n7drnmw",
                      "score": 2,
                      "author_fullname": "t2_esiyl0yb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Very likely using ik_llama.cpp",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dxa38",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Very likely using ik_llama.cpp&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dxa38/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754558254,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7drnmw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Reimelt",
            "can_mod_post": false,
            "created_utc": 1754554980,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 2,
            "author_fullname": "t2_fr40n",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nice. Then try Qwen 235B, too. Should perform better. Did you use llama.cpp?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7drnmw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice. Then try Qwen 235B, too. Should perform better. Did you use llama.cpp?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7drnmw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754554980,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7drv1l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kevin_1994",
            "can_mod_post": false,
            "created_utc": 1754555100,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 2,
            "author_fullname": "t2_o015g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I wish people would write without putting it through an LLM",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7drv1l",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wish people would write without putting it through an LLM&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7drv1l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754555100,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dscol",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Jadeshell",
            "can_mod_post": false,
            "created_utc": 1754555383,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 1,
            "author_fullname": "t2_11m5fj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What about a 750TI 2GB, is the data reduction using Q2 sufficient a 7 or 13 B model would run okay enough?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dscol",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What about a 750TI 2GB, is the data reduction using Q2 sufficient a 7 or 13 B model would run okay enough?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dscol/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754555383,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dzjeb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "101m4n",
            "can_mod_post": false,
            "created_utc": 1754559548,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 1,
            "author_fullname": "t2_p7nc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What's your CPU?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dzjeb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your CPU?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dzjeb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754559548,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dpbgr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ready_to_fuck_yeahh",
            "can_mod_post": false,
            "created_utc": 1754553628,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": -1,
            "author_fullname": "t2_j4lnwvga2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "HOW",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dpbgr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;HOW&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dpbgr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754553628,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dpkr3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "killerstreak976",
            "can_mod_post": false,
            "created_utc": 1754553775,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 0,
            "author_fullname": "t2_3qe4rnnq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I can't wait to read more about this when you share it lol.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dpkr3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t wait to read more about this when you share it lol.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dpkr3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754553775,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7e0jql",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "EndlessZone123",
            "can_mod_post": false,
            "created_utc": 1754560103,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": 0,
            "author_fullname": "t2_10pxq3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Your CPU motherboard and ram probably costs more than her macbook and your gpu combined. What even is the comparison here? Q2 is pretty bad even on a large model and it proves less than nothing at 2k context. That GPU aint doing shit when you only offloaded 10% of the layers, meaning the GPU would increase speed by a max of 10% if it could process all its layers instantly. If you are using GGUF with offloading I dont even know what cuda is doing here. Most people want more vram because vram is the only thing running larger models at usuable quants and context size with also usuable 10-20+ t/s. 2.3t/s is respectable for cpu, but it does not mean it's really usuable.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7e0jql",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your CPU motherboard and ram probably costs more than her macbook and your gpu combined. What even is the comparison here? Q2 is pretty bad even on a large model and it proves less than nothing at 2k context. That GPU aint doing shit when you only offloaded 10% of the layers, meaning the GPU would increase speed by a max of 10% if it could process all its layers instantly. If you are using GGUF with offloading I dont even know what cuda is doing here. Most people want more vram because vram is the only thing running larger models at usuable quants and context size with also usuable 10-20+ t/s. 2.3t/s is respectable for cpu, but it does not mean it&amp;#39;s really usuable.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7e0jql/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754560103,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7dqqxt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "SokkaHaikuBot",
                      "can_mod_post": false,
                      "created_utc": 1754554452,
                      "send_replies": true,
                      "parent_id": "t1_n7dqq1l",
                      "score": -1,
                      "author_fullname": "t2_fp4tafeb2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "^[Sokka-Haiku](https://www.reddit.com/r/SokkaHaikuBot/comments/15kyv9r/what_is_a_sokka_haiku/) ^by ^bilalazhar72:\n\n*So you are running*\n\n*A bad model by today's*\n\n*Standards just to flex huh*\n\n---\n^Remember ^that ^one ^time ^Sokka ^accidentally ^used ^an ^extra ^syllable ^in ^that ^Haiku ^Battle ^in ^Ba ^Sing ^Se? ^That ^was ^a ^Sokka ^Haiku ^and ^you ^just ^made ^one.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7dqqxt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;sup&gt;&lt;a href=\"https://www.reddit.com/r/SokkaHaikuBot/comments/15kyv9r/what_is_a_sokka_haiku/\"&gt;Sokka-Haiku&lt;/a&gt;&lt;/sup&gt; &lt;sup&gt;by&lt;/sup&gt; &lt;sup&gt;bilalazhar72:&lt;/sup&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;So you are running&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;A bad model by today&amp;#39;s&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Standards just to flex huh&lt;/em&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;&lt;sup&gt;Remember&lt;/sup&gt; &lt;sup&gt;that&lt;/sup&gt; &lt;sup&gt;one&lt;/sup&gt; &lt;sup&gt;time&lt;/sup&gt; &lt;sup&gt;Sokka&lt;/sup&gt; &lt;sup&gt;accidentally&lt;/sup&gt; &lt;sup&gt;used&lt;/sup&gt; &lt;sup&gt;an&lt;/sup&gt; &lt;sup&gt;extra&lt;/sup&gt; &lt;sup&gt;syllable&lt;/sup&gt; &lt;sup&gt;in&lt;/sup&gt; &lt;sup&gt;that&lt;/sup&gt; &lt;sup&gt;Haiku&lt;/sup&gt; &lt;sup&gt;Battle&lt;/sup&gt; &lt;sup&gt;in&lt;/sup&gt; &lt;sup&gt;Ba&lt;/sup&gt; &lt;sup&gt;Sing&lt;/sup&gt; &lt;sup&gt;Se?&lt;/sup&gt; &lt;sup&gt;That&lt;/sup&gt; &lt;sup&gt;was&lt;/sup&gt; &lt;sup&gt;a&lt;/sup&gt; &lt;sup&gt;Sokka&lt;/sup&gt; &lt;sup&gt;Haiku&lt;/sup&gt; &lt;sup&gt;and&lt;/sup&gt; &lt;sup&gt;you&lt;/sup&gt; &lt;sup&gt;just&lt;/sup&gt; &lt;sup&gt;made&lt;/sup&gt; &lt;sup&gt;one.&lt;/sup&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjtt3g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dqqxt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754554452,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7dqq1l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bilalazhar72",
            "can_mod_post": false,
            "created_utc": 1754554438,
            "send_replies": true,
            "parent_id": "t3_1mjtt3g",
            "score": -1,
            "author_fullname": "t2_98pip3bs",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "so you are running a bad model by today's standards just to flex huh",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dqq1l",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;so you are running a bad model by today&amp;#39;s standards just to flex huh&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjtt3g/my_rtx_4060_is_running_llama_405b_at_23_tokenssec/n7dqq1l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754554438,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjtt3g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        }
      ],
      "before": null
    }
  }
]