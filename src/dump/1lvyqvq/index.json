[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’ve been experimenting with local LLMs, and while I’ve had success running some models, I’m overwhelmed by the sheer number of options. I’d love some advice on how to narrow things down:\n\n* **What should I look for** in a model (e.g., size, architecture, benchmarks)?\n* **Where’s the best place to find** reliable models (HF, GGUF repos, etc.)?\n* **How can I estimate performance** on my 20GB VRAM GPU without downloading dozens of models?\n\nI’d prefer not to waste time and storage testing models blindly, so any tips on evaluating them beforehand would be hugely appreciated!\n\n*(Bonus: If you have personal favorites for my setup, I’m open to suggestions—but I’m mostly interested in learning how to decide.)*\n\n*EDIT:*  \n*My primary use cases:*\n\n1. *Brainstorming (big part of my job—needs creative, coherent output).*\n2. *Summarizing long texts (documents, articles, etc.).*\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Help im lost",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lvyqvq",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_13xhzq",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1752106852,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752106603,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve been experimenting with local LLMs, and while I’ve had success running some models, I’m overwhelmed by the sheer number of options. I’d love some advice on how to narrow things down:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;What should I look for&lt;/strong&gt; in a model (e.g., size, architecture, benchmarks)?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Where’s the best place to find&lt;/strong&gt; reliable models (HF, GGUF repos, etc.)?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;How can I estimate performance&lt;/strong&gt; on my 20GB VRAM GPU without downloading dozens of models?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’d prefer not to waste time and storage testing models blindly, so any tips on evaluating them beforehand would be hugely appreciated!&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;(Bonus: If you have personal favorites for my setup, I’m open to suggestions—but I’m mostly interested in learning how to decide.)&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;My primary use cases:&lt;/em&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;em&gt;Brainstorming (big part of my job—needs creative, coherent output).&lt;/em&gt;&lt;/li&gt;\n&lt;li&gt;&lt;em&gt;Summarizing long texts (documents, articles, etc.).&lt;/em&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lvyqvq",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "DaBe99",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/",
            "subreddit_subscribers": 497022,
            "created_utc": 1752106603,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2a5xy8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DaBe99",
                      "can_mod_post": false,
                      "created_utc": 1752109074,
                      "send_replies": true,
                      "parent_id": "t1_n2a3kat",
                      "score": 2,
                      "author_fullname": "t2_13xhzq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the suggestion. I totall agree that sticking with established models like Gemma3 is the pragmatic approach, especially for reliability.\n\nThat said, part of me still obsesses over whether I'm leaving performance on the table by not min-maxing (I know it's Irrational... but i can't help it lol).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2a5xy8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the suggestion. I totall agree that sticking with established models like Gemma3 is the pragmatic approach, especially for reliability.&lt;/p&gt;\n\n&lt;p&gt;That said, part of me still obsesses over whether I&amp;#39;m leaving performance on the table by not min-maxing (I know it&amp;#39;s Irrational... but i can&amp;#39;t help it lol).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lvyqvq",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a5xy8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752109074,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2a3kat",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "pkdc0001",
            "can_mod_post": false,
            "created_utc": 1752108258,
            "send_replies": true,
            "parent_id": "t3_1lvyqvq",
            "score": 3,
            "author_fullname": "t2_yh1y8mfje",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you don't want to play with unknowns stay with the brand names, for what you mention Gemma 3 12B should be extremely capable, easy to run. If you download LM Studio, you can talk to it with chat interface or https request easily \n\nhttps://deepmind.google/models/gemma/gemma-3/",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2a3kat",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you don&amp;#39;t want to play with unknowns stay with the brand names, for what you mention Gemma 3 12B should be extremely capable, easy to run. If you download LM Studio, you can talk to it with chat interface or https request easily &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://deepmind.google/models/gemma/gemma-3/\"&gt;https://deepmind.google/models/gemma/gemma-3/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a3kat/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752108258,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvyqvq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2auh21",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DarthFluttershy_",
            "can_mod_post": false,
            "created_utc": 1752117945,
            "send_replies": true,
            "parent_id": "t3_1lvyqvq",
            "score": 2,
            "author_fullname": "t2_d52if",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "1. For \"learning how to decide,\" maybe just put $5 into an openrouter account and use their chat or a frontend with their API to just try some models out. They have a good selection of the local models hosted there.\n\n2. LM Studio is the user-friendliest local LLM platform, IMO, great for getting started. Others give you a bit more ability to optimize, though. LM Studio will pull from HF for you as an integrated feature, but HF is the best place regardless.  \n  \n3. 20 GB of RAM will run any reasonable quant of 12B easily, but you can probably get decent speeds on 27B models or so with some offloading. As a general rule, the size of the model is the amount of VRAM you'll need to fully offload it before any context, so expect to need 1.2-1.5x the model size for a full offload.  \n  \n4. What speed do you consider acceptable? If you don't mind slower tokens, you can do partial VRAM offloads for smarter models. I have 12GB of VRAM and can get about 17-20 tokens per second on a 30B-A3B model (because it's a mixture of experts and doesn't use all the memory at once), but only 3 tokens/s on Gemma3 27B, wereas with a full offload of Gemma 3 12B I get 30 tokesn/s. You might squeeze the 27B in completely though. Aim for Q4 or Q5 quants generally.\n\n5. How long of texts do you want to summarize? Sumamrizing long texts might be tricky locally, since it's gonna eat memory... so you might need a smaller LLM to expand context if RAG isn't doing it for you (I have very little experience with local RAG)\n\n6. I'd suggest look at Mistral 24B, Gemma 3 27B (or 12B if you want a it really fast), and Qwen3 30B A3B, which gives you a lot of bang for you VRAM, but I've had mixed results in following instructions.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2auh21",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;For &amp;quot;learning how to decide,&amp;quot; maybe just put $5 into an openrouter account and use their chat or a frontend with their API to just try some models out. They have a good selection of the local models hosted there.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;LM Studio is the user-friendliest local LLM platform, IMO, great for getting started. Others give you a bit more ability to optimize, though. LM Studio will pull from HF for you as an integrated feature, but HF is the best place regardless.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;20 GB of RAM will run any reasonable quant of 12B easily, but you can probably get decent speeds on 27B models or so with some offloading. As a general rule, the size of the model is the amount of VRAM you&amp;#39;ll need to fully offload it before any context, so expect to need 1.2-1.5x the model size for a full offload.  &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;What speed do you consider acceptable? If you don&amp;#39;t mind slower tokens, you can do partial VRAM offloads for smarter models. I have 12GB of VRAM and can get about 17-20 tokens per second on a 30B-A3B model (because it&amp;#39;s a mixture of experts and doesn&amp;#39;t use all the memory at once), but only 3 tokens/s on Gemma3 27B, wereas with a full offload of Gemma 3 12B I get 30 tokesn/s. You might squeeze the 27B in completely though. Aim for Q4 or Q5 quants generally.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;How long of texts do you want to summarize? Sumamrizing long texts might be tricky locally, since it&amp;#39;s gonna eat memory... so you might need a smaller LLM to expand context if RAG isn&amp;#39;t doing it for you (I have very little experience with local RAG)&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I&amp;#39;d suggest look at Mistral 24B, Gemma 3 27B (or 12B if you want a it really fast), and Qwen3 30B A3B, which gives you a lot of bang for you VRAM, but I&amp;#39;ve had mixed results in following instructions.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2auh21/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752117945,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvyqvq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2c2zmk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Corporate_Drone31",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2a6fiu",
                                "score": 1,
                                "author_fullname": "t2_32o8hu91",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "FYI, Hugging Face now has a built-in GGUF metadata reader. You can access it from the model card page from the right-hand bar - just click the \"button\" corresponding to the quant level and it'll show up.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2c2zmk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;FYI, Hugging Face now has a built-in GGUF metadata reader. You can access it from the model card page from the right-hand bar - just click the &amp;quot;button&amp;quot; corresponding to the quant level and it&amp;#39;ll show up.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lvyqvq",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2c2zmk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752140659,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752140659,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2a6fiu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DaBe99",
                      "can_mod_post": false,
                      "created_utc": 1752109242,
                      "send_replies": true,
                      "parent_id": "t1_n2a40z8",
                      "score": 1,
                      "author_fullname": "t2_13xhzq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the tip! I'll see if I can find the tool you're referring to.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2a6fiu",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the tip! I&amp;#39;ll see if I can find the tool you&amp;#39;re referring to.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lvyqvq",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a6fiu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752109242,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2a40z8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RhubarbSimilar1683",
            "can_mod_post": false,
            "created_utc": 1752108417,
            "send_replies": true,
            "parent_id": "t3_1lvyqvq",
            "score": 1,
            "author_fullname": "t2_1k4sjdwzk2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "HF is ideal as a source but they might not always work for you. As for estimating performance there is a tool that can read gguf metadata or extract metadata from gguf models but I don't remember its name anymore. It's a command line tool on github",
            "edited": 1752108775,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2a40z8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;HF is ideal as a source but they might not always work for you. As for estimating performance there is a tool that can read gguf metadata or extract metadata from gguf models but I don&amp;#39;t remember its name anymore. It&amp;#39;s a command line tool on github&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a40z8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752108417,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvyqvq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2ai1va",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "techmago",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2a8vud",
                                "score": 1,
                                "author_fullname": "t2_azy5rpp",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Ah, notes:\n\ni did saw a expressive difference between mistral q4 from mistral q8.\n\nIf oyu are using ollama... mistral mem usage is fucked up.   \nI'm running ollama from a exerimental branch because of that\n\n[https://github.com/ollama/ollama/pull/11090](https://github.com/ollama/ollama/pull/11090)  \nthe new memory allocation system from this guy help a lot\n\n  \nAlso, if oyu are using ollama, use q8 insted the default (fp16) for the context.  \nMake a huge diference in context memory size.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2ai1va",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah, notes:&lt;/p&gt;\n\n&lt;p&gt;i did saw a expressive difference between mistral q4 from mistral q8.&lt;/p&gt;\n\n&lt;p&gt;If oyu are using ollama... mistral mem usage is fucked up.&lt;br/&gt;\nI&amp;#39;m running ollama from a exerimental branch because of that&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ollama/ollama/pull/11090\"&gt;https://github.com/ollama/ollama/pull/11090&lt;/a&gt;&lt;br/&gt;\nthe new memory allocation system from this guy help a lot&lt;/p&gt;\n\n&lt;p&gt;Also, if oyu are using ollama, use q8 insted the default (fp16) for the context.&lt;br/&gt;\nMake a huge diference in context memory size.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lvyqvq",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2ai1va/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752113295,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752113295,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2a8vud",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DaBe99",
                      "can_mod_post": false,
                      "created_utc": 1752110089,
                      "send_replies": true,
                      "parent_id": "t1_n2a7l89",
                      "score": 1,
                      "author_fullname": "t2_13xhzq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah, Gemini Pro is honestly in its own tier. I use it daily and just love it. Sadly, work rules mean I’m stuck with local setups for privacy, so I’m looking for the next best thing.  \nThis sounds like a solid setup. Really appreciate the tip... gonna try this asap,",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2a8vud",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, Gemini Pro is honestly in its own tier. I use it daily and just love it. Sadly, work rules mean I’m stuck with local setups for privacy, so I’m looking for the next best thing.&lt;br/&gt;\nThis sounds like a solid setup. Really appreciate the tip... gonna try this asap,&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lvyqvq",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a8vud/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752110089,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2a7l89",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "techmago",
            "can_mod_post": false,
            "created_utc": 1752109643,
            "send_replies": true,
            "parent_id": "t3_1lvyqvq",
            "score": 1,
            "author_fullname": "t2_azy5rpp",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "20 GB ram you can use mistral:24 and quen3:32b\n\nuse openwebui as frontend and use the 2 models at the same time.  \nGreat for brainstorms.\n\nPlus, for sumary, mistral is the best model i tested so far (the best is gemini-pro)\n\n(yes, i was too lazy to make a for)\n\nhttps://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;format=png&amp;auto=webp&amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2a7l89",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;20 GB ram you can use mistral:24 and quen3:32b&lt;/p&gt;\n\n&lt;p&gt;use openwebui as frontend and use the 2 models at the same time.&lt;br/&gt;\nGreat for brainstorms.&lt;/p&gt;\n\n&lt;p&gt;Plus, for sumary, mistral is the best model i tested so far (the best is gemini-pro)&lt;/p&gt;\n\n&lt;p&gt;(yes, i was too lazy to make a for)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe\"&gt;https://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a7l89/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752109643,
            "media_metadata": {
              "ggfdb5ti5ybf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 52,
                    "x": 108,
                    "u": "https://preview.redd.it/ggfdb5ti5ybf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46b3f03849691d469b74bc8ce41e6ff0b9ae4666"
                  },
                  {
                    "y": 105,
                    "x": 216,
                    "u": "https://preview.redd.it/ggfdb5ti5ybf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=925c0f303e2b593f68f6af4b0af283fc0accefbb"
                  },
                  {
                    "y": 156,
                    "x": 320,
                    "u": "https://preview.redd.it/ggfdb5ti5ybf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65dc2d00b89afafbf55c5a0763d3a9f5e5025c2f"
                  },
                  {
                    "y": 313,
                    "x": 640,
                    "u": "https://preview.redd.it/ggfdb5ti5ybf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbe646fd69f9818c129fc52d953661f49c6ddbf8"
                  },
                  {
                    "y": 470,
                    "x": 960,
                    "u": "https://preview.redd.it/ggfdb5ti5ybf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d53f2504fb6ba8775d7938d0d78b138037f55615"
                  },
                  {
                    "y": 529,
                    "x": 1080,
                    "u": "https://preview.redd.it/ggfdb5ti5ybf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf1219cb71bf2f25e4f0c5188e0e93e00483c054"
                  }
                ],
                "s": {
                  "y": 1139,
                  "x": 2323,
                  "u": "https://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;format=png&amp;auto=webp&amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe"
                },
                "id": "ggfdb5ti5ybf1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lvyqvq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]