[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Just tested **GPT-OSS-120B (MXFP4)** locally using **LM Studio v0.3.22 (Beta build 2)** on my machine with an **RTX 5090 (32 GB VRAM)** \\+ **Ryzen 9 9950X3D** \\+ **96 GB RAM**.\n\nEverything is mostly default. I only enabled **Flash Attention** manually and adjusted GPU offload to 30/36 layers + Guardrails **OFF +** Limit Model Offload to dedicated GPU Memory **OFF**.\n\n**Result:**  \n→ \\~10.48 tokens/sec  \n→ \\~2.27s to first token\n\nModel loads and runs stable. Clearly heavier than the 20B, but impressive that it runs at \\~10.48 tokens/sec.\n\nhttps://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf\n\n[Flash Attention + GPU offload to 30\\/36 layers](https://preview.redd.it/91wp98m0dnhf1.png?width=552&amp;format=png&amp;auto=webp&amp;s=78cc538e4cb36ccedee82c183652d81206a6f5cb)\n\n[Guardrails OFF + Limit Model Offload to dedicated GPU Memory OFF](https://preview.redd.it/y3xf186sdnhf1.png?width=1623&amp;format=png&amp;auto=webp&amp;s=0f4272d01c2ca4c7e119a3c5fe478600cef64fe8)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "10.48 tok/sec - GPT-OSS-120B on RTX 5090 32 VRAM + 96 RAM  in LM Studio (default settings + FlashAttention + Guardrails: OFF)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 136,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "stsclnt8enhf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 105,
                    "x": 108,
                    "u": "https://preview.redd.it/stsclnt8enhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7f6dd51aed8a152d315502e78ab53301323bc0ef"
                  },
                  {
                    "y": 210,
                    "x": 216,
                    "u": "https://preview.redd.it/stsclnt8enhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=966fb1e1cb64c5c8dc88bb836652890ff34439f1"
                  },
                  {
                    "y": 311,
                    "x": 320,
                    "u": "https://preview.redd.it/stsclnt8enhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ed091eb6b358e43c7bfbbdf889ac20fcbbaa586"
                  },
                  {
                    "y": 622,
                    "x": 640,
                    "u": "https://preview.redd.it/stsclnt8enhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8b6313cac456abd31c3c3eeac04993778cf38845"
                  },
                  {
                    "y": 934,
                    "x": 960,
                    "u": "https://preview.redd.it/stsclnt8enhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=05c6c85282fdff6dd80e4115c0aeb4c13e29626b"
                  },
                  {
                    "y": 1051,
                    "x": 1080,
                    "u": "https://preview.redd.it/stsclnt8enhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=be697e8225707ae5566af1690409e8b59a45b05d"
                  }
                ],
                "s": {
                  "y": 1460,
                  "x": 1500,
                  "u": "https://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf"
                },
                "id": "stsclnt8enhf1"
              },
              "y3xf186sdnhf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 65,
                    "x": 108,
                    "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d3bc2cda8bea6aea361aa243691322afd48392f"
                  },
                  {
                    "y": 131,
                    "x": 216,
                    "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=db770f6e1665b480e50365ab5319e3a65bded0a8"
                  },
                  {
                    "y": 194,
                    "x": 320,
                    "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a7f81884d0bd86e5b3a0d03c859a2cb7bfe32c03"
                  },
                  {
                    "y": 389,
                    "x": 640,
                    "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be0a3efe7e807481098e8c44cc3d5f321bbf6b38"
                  },
                  {
                    "y": 583,
                    "x": 960,
                    "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=481baa48b62b97aa879127d4bb27e64833b79195"
                  },
                  {
                    "y": 656,
                    "x": 1080,
                    "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=03a74398d00d8e46a2f994e787637aa0edc5b0b1"
                  }
                ],
                "s": {
                  "y": 987,
                  "x": 1623,
                  "u": "https://preview.redd.it/y3xf186sdnhf1.png?width=1623&amp;format=png&amp;auto=webp&amp;s=0f4272d01c2ca4c7e119a3c5fe478600cef64fe8"
                },
                "id": "y3xf186sdnhf1"
              },
              "91wp98m0dnhf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 145,
                    "x": 108,
                    "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d271699d538d330e79b5ff955d66ee657768202"
                  },
                  {
                    "y": 290,
                    "x": 216,
                    "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=df4aab64d650bd96e35c6899ca160953e98d499f"
                  },
                  {
                    "y": 430,
                    "x": 320,
                    "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9e71d3ba1393d614de2384d0b7cdedf4be3315fd"
                  }
                ],
                "s": {
                  "y": 743,
                  "x": 552,
                  "u": "https://preview.redd.it/91wp98m0dnhf1.png?width=552&amp;format=png&amp;auto=webp&amp;s=78cc538e4cb36ccedee82c183652d81206a6f5cb"
                },
                "id": "91wp98m0dnhf1"
              }
            },
            "name": "t3_1mk9c1u",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_h755hoap",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/eT9GlQ-I-zAVfkF4YhcL6w037-GGjldTgsuaXGBx2Po.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754594454,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just tested &lt;strong&gt;GPT-OSS-120B (MXFP4)&lt;/strong&gt; locally using &lt;strong&gt;LM Studio v0.3.22 (Beta build 2)&lt;/strong&gt; on my machine with an &lt;strong&gt;RTX 5090 (32 GB VRAM)&lt;/strong&gt; + &lt;strong&gt;Ryzen 9 9950X3D&lt;/strong&gt; + &lt;strong&gt;96 GB RAM&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Everything is mostly default. I only enabled &lt;strong&gt;Flash Attention&lt;/strong&gt; manually and adjusted GPU offload to 30/36 layers + Guardrails &lt;strong&gt;OFF +&lt;/strong&gt; Limit Model Offload to dedicated GPU Memory &lt;strong&gt;OFF&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt;&lt;br/&gt;\n→ ~10.48 tokens/sec&lt;br/&gt;\n→ ~2.27s to first token&lt;/p&gt;\n\n&lt;p&gt;Model loads and runs stable. Clearly heavier than the 20B, but impressive that it runs at ~10.48 tokens/sec.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf\"&gt;https://preview.redd.it/stsclnt8enhf1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c1819cd7e6971d4f826572e95ef666013d723cf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/91wp98m0dnhf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78cc538e4cb36ccedee82c183652d81206a6f5cb\"&gt;Flash Attention + GPU offload to 30/36 layers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/y3xf186sdnhf1.png?width=1623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f4272d01c2ca4c7e119a3c5fe478600cef64fe8\"&gt;Guardrails OFF + Limit Model Offload to dedicated GPU Memory OFF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1mk9c1u",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Spiritual_Tie_5574",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/",
            "subreddit_subscribers": 513417,
            "created_utc": 1754594454,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7hc290",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "naxan6",
                      "can_mod_post": false,
                      "created_utc": 1754598033,
                      "send_replies": true,
                      "parent_id": "t1_n7h96sh",
                      "score": 1,
                      "author_fullname": "t2_2iq66pt3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "https://preview.redd.it/eaoe576ionhf1.png?width=1353&amp;format=png&amp;auto=webp&amp;s=8d99e18a2e1c236f529eb9dbd15d02558ec6440d\n\nI can second that: With the same (short) prompt I get 17.9 t/s\n\nSpecs: 16 GB RTX 5060 Ti + 128 GB DDR5 5600 / Ryzen 9 9900X  \nllama-b6111-bin-win-cuda-12.4-x64:  \n.\\\\llama-server.exe -c 60000  --chat-template-kwargs \"{\\\\\"reasoning\\_effort\\\\\": \\\\\"low\\\\\"}\" -fa -ctk f16 -ctv f16 -m \"c:/....../gpt-oss-120b-GGUF/gpt-oss-120b-BF16.gguf\" -ub 512 --temp 1.0 --top-p 1.0 --top-k 0 --min-p 0 --repeat-penalty 1.0 --no-mmap -sm none -ngl 99 --n-cpu-moe 44",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7hc290",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/eaoe576ionhf1.png?width=1353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d99e18a2e1c236f529eb9dbd15d02558ec6440d\"&gt;https://preview.redd.it/eaoe576ionhf1.png?width=1353&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d99e18a2e1c236f529eb9dbd15d02558ec6440d&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I can second that: With the same (short) prompt I get 17.9 t/s&lt;/p&gt;\n\n&lt;p&gt;Specs: 16 GB RTX 5060 Ti + 128 GB DDR5 5600 / Ryzen 9 9900X&lt;br/&gt;\nllama-b6111-bin-win-cuda-12.4-x64:&lt;br/&gt;\n.\\llama-server.exe -c 60000  --chat-template-kwargs &amp;quot;{\\&amp;quot;reasoning_effort\\&amp;quot;: \\&amp;quot;low\\&amp;quot;}&amp;quot; -fa -ctk f16 -ctv f16 -m &amp;quot;c:/....../gpt-oss-120b-GGUF/gpt-oss-120b-BF16.gguf&amp;quot; -ub 512 --temp 1.0 --top-p 1.0 --top-k 0 --min-p 0 --repeat-penalty 1.0 --no-mmap -sm none -ngl 99 --n-cpu-moe 44&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mk9c1u",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/n7hc290/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754598033,
                      "media_metadata": {
                        "eaoe576ionhf1": {
                          "status": "valid",
                          "e": "Image",
                          "m": "image/png",
                          "p": [
                            {
                              "y": 52,
                              "x": 108,
                              "u": "https://preview.redd.it/eaoe576ionhf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=5cb2e7eea5cbba66af0e3c53a990e195379227e2"
                            },
                            {
                              "y": 104,
                              "x": 216,
                              "u": "https://preview.redd.it/eaoe576ionhf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9835220210522e29becf1089512666ad17831371"
                            },
                            {
                              "y": 155,
                              "x": 320,
                              "u": "https://preview.redd.it/eaoe576ionhf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=964edb6268a9f6c2899100a20c3a90b69849adeb"
                            },
                            {
                              "y": 310,
                              "x": 640,
                              "u": "https://preview.redd.it/eaoe576ionhf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6104e096cf7c50a625c7fb7efa2732f860368aa2"
                            },
                            {
                              "y": 466,
                              "x": 960,
                              "u": "https://preview.redd.it/eaoe576ionhf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7b25ea20182df2f467d32b745eaa64ba0a097441"
                            },
                            {
                              "y": 524,
                              "x": 1080,
                              "u": "https://preview.redd.it/eaoe576ionhf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1aff4b181931623d307ddfd99f5fa0a8bac385cf"
                            }
                          ],
                          "s": {
                            "y": 657,
                            "x": 1353,
                            "u": "https://preview.redd.it/eaoe576ionhf1.png?width=1353&amp;format=png&amp;auto=webp&amp;s=8d99e18a2e1c236f529eb9dbd15d02558ec6440d"
                          },
                          "id": "eaoe576ionhf1"
                        }
                      },
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7h96sh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1754597210,
            "send_replies": true,
            "parent_id": "t3_1mk9c1u",
            "score": 5,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try llama.cpp with`-ot \".ffn_(up|down)_exps.=CPU\"`  \nThis offloads up and down projection MoE layers instead of full MoE layers.  \nYou should get 30 t/s! [https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune#improving-generation-speed](https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune#improving-generation-speed)  \nI have a budget workstation that costs less than your GPU alone and I get 20 t/s with Unsloth their 120b. That is 20 t/s for the first 1K tokens, it slows down to 13 t/s at 30K context.  \nMy specs: 16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000",
            "edited": 1754597444,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7h96sh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try llama.cpp with&lt;code&gt;-ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot;&lt;/code&gt;&lt;br/&gt;\nThis offloads up and down projection MoE layers instead of full MoE layers.&lt;br/&gt;\nYou should get 30 t/s! &lt;a href=\"https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune#improving-generation-speed\"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune#improving-generation-speed&lt;/a&gt;&lt;br/&gt;\nI have a budget workstation that costs less than your GPU alone and I get 20 t/s with Unsloth their 120b. That is 20 t/s for the first 1K tokens, it slows down to 13 t/s at 30K context.&lt;br/&gt;\nMy specs: 16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/n7h96sh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754597210,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk9c1u",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ho7ae",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cosmobaud",
            "can_mod_post": false,
            "created_utc": 1754601581,
            "send_replies": true,
            "parent_id": "t3_1mk9c1u",
            "score": 1,
            "author_fullname": "t2_60fho",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Huh I would have thought it would be faster.  Here it is on a minipc with RTX4000\n\nOS: Ubuntu 24.04.2 LTS x86_64\nHost: MotherBoard Series 1.0\nKernel: 6.14.0-27-generic\nUptime: 5 days, 22 hours, 7 mins\nPackages: 1752 (dpkg), 10 (snap)\nShell: bash 5.2.21\nResolution: 2560x1440\nCPU: AMD Ryzen 9 7945HX (32) @ 5.462GHz\nGPU: NVIDIA RTX 4000 SFF Ada Generation\nGPU: AMD ATI 04:00.0 Raphael\nMemory: 54.6GiB / 94.2GiB\n\n$ ollama run gpt-oss:120b --verbose \"How many r's in a strawberry?\"\nThinking...\nThe user asks: \"How many r's in a strawberry?\" Likely a simple question: Count the letter 'r' in the word\n\"strawberry\". The word \"strawberry\" spelled s t r a w b e r r y. Contains: r at position 3, r at position 8, r at\nposition 9? Actually let's write: s(1) t(2) r(3) a(4) w(5) b(6) e(7) r(8) r(9) y(10). So there are three r's. So\nanswer: 3.\n\nCould also interpret \"How many r's in a strawberry?\" Might be a trick: The phrase \"a strawberry\" includes\n\"strawberry\" preceded by \"a \". The phrase \"a strawberry\" has letters: a space s t r a w b e r r y. So there are\nthree r's still. So answer is three.\n\nThus respond: There are three r's. Possibly add a little fun.\n...done thinking.\n\nThere are **three** r’s in the word “strawberry” (s t **r** a w b e **r** **r** y).\n\ntotal duration:       3m24.968655526s\nload duration:        79.660753ms\nprompt eval count:    75 token(s)\nprompt eval duration: 814.271741ms\nprompt eval rate:     92.11 tokens/s\neval count:           266 token(s)\neval duration:        33.145313857s\neval rate:            8.03 tokens/s\n$",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ho7ae",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Huh I would have thought it would be faster.  Here it is on a minipc with RTX4000&lt;/p&gt;\n\n&lt;p&gt;OS: Ubuntu 24.04.2 LTS x86_64\nHost: MotherBoard Series 1.0\nKernel: 6.14.0-27-generic\nUptime: 5 days, 22 hours, 7 mins\nPackages: 1752 (dpkg), 10 (snap)\nShell: bash 5.2.21\nResolution: 2560x1440\nCPU: AMD Ryzen 9 7945HX (32) @ 5.462GHz\nGPU: NVIDIA RTX 4000 SFF Ada Generation\nGPU: AMD ATI 04:00.0 Raphael\nMemory: 54.6GiB / 94.2GiB&lt;/p&gt;\n\n&lt;p&gt;$ ollama run gpt-oss:120b --verbose &amp;quot;How many r&amp;#39;s in a strawberry?&amp;quot;\nThinking...\nThe user asks: &amp;quot;How many r&amp;#39;s in a strawberry?&amp;quot; Likely a simple question: Count the letter &amp;#39;r&amp;#39; in the word\n&amp;quot;strawberry&amp;quot;. The word &amp;quot;strawberry&amp;quot; spelled s t r a w b e r r y. Contains: r at position 3, r at position 8, r at\nposition 9? Actually let&amp;#39;s write: s(1) t(2) r(3) a(4) w(5) b(6) e(7) r(8) r(9) y(10). So there are three r&amp;#39;s. So\nanswer: 3.&lt;/p&gt;\n\n&lt;p&gt;Could also interpret &amp;quot;How many r&amp;#39;s in a strawberry?&amp;quot; Might be a trick: The phrase &amp;quot;a strawberry&amp;quot; includes\n&amp;quot;strawberry&amp;quot; preceded by &amp;quot;a &amp;quot;. The phrase &amp;quot;a strawberry&amp;quot; has letters: a space s t r a w b e r r y. So there are\nthree r&amp;#39;s still. So answer is three.&lt;/p&gt;\n\n&lt;p&gt;Thus respond: There are three r&amp;#39;s. Possibly add a little fun.\n...done thinking.&lt;/p&gt;\n\n&lt;p&gt;There are &lt;strong&gt;three&lt;/strong&gt; r’s in the word “strawberry” (s t &lt;strong&gt;r&lt;/strong&gt; a w b e &lt;strong&gt;r&lt;/strong&gt; &lt;strong&gt;r&lt;/strong&gt; y).&lt;/p&gt;\n\n&lt;p&gt;total duration:       3m24.968655526s\nload duration:        79.660753ms\nprompt eval count:    75 token(s)\nprompt eval duration: 814.271741ms\nprompt eval rate:     92.11 tokens/s\neval count:           266 token(s)\neval duration:        33.145313857s\neval rate:            8.03 tokens/s\n$&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk9c1u/1048_toksec_gptoss120b_on_rtx_5090_32_vram_96_ram/n7ho7ae/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754601581,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk9c1u",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]