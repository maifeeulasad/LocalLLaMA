[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm new to local LLM and all I have right now is that a GTX 1060 6g from 2017, when I get an upgrade in the 4000 series, I would like to know what are your suggested models that hallucinate the least? ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What models have the least likelihood of hallucinations?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhcfe4",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.38,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1urq336nt3",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754311498,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m new to local LLM and all I have right now is that a GTX 1060 6g from 2017, when I get an upgrade in the 4000 series, I would like to know what are your suggested models that hallucinate the least? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhcfe4",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "vulgar1171",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754311498,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6v1nmq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Juan_Valadez",
            "can_mod_post": false,
            "created_utc": 1754311602,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 12,
            "author_fullname": "t2_1qzdx33fm4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Wikipedia",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v1nmq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wikipedia&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6v1nmq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754311602,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6vdz0q",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "GhostInThePudding",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6v30fq",
                                "score": 1,
                                "author_fullname": "t2_15sgncv9z5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Open WebUI I think is what most people use for messing around with that stuff. I find it works pretty well for me.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6vdz0q",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Open WebUI I think is what most people use for messing around with that stuff. I find it works pretty well for me.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhcfe4",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6vdz0q/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754315766,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754315766,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6vfg4g",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ArchdukeofHyperbole",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6v30fq",
                                "score": 1,
                                "author_fullname": "t2_1p41v97q5d",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I made a semantic search engine of Wikipedia. It takes about 30GB of ram. If you want to do similar without having to create the embeddings of wiki yourself, there's datasets on higgingface. Just gotta make a program that splits parquet files, putting the vectors into a quantized faiss, paragraphs into a compressed file, and json for metadata. Gotta make sure you have access to the same embedding model that was used in the dataset. It's pretty fast on searches too, but slows down a little (still useable imo) if trying to do something like 1,000,000 search results.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6vfg4g",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I made a semantic search engine of Wikipedia. It takes about 30GB of ram. If you want to do similar without having to create the embeddings of wiki yourself, there&amp;#39;s datasets on higgingface. Just gotta make a program that splits parquet files, putting the vectors into a quantized faiss, paragraphs into a compressed file, and json for metadata. Gotta make sure you have access to the same embedding model that was used in the dataset. It&amp;#39;s pretty fast on searches too, but slows down a little (still useable imo) if trying to do something like 1,000,000 search results.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhcfe4",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6vfg4g/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754316235,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754316235,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6v30fq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "vulgar1171",
                      "can_mod_post": false,
                      "created_utc": 1754312085,
                      "send_replies": true,
                      "parent_id": "t1_n6v2hje",
                      "score": 1,
                      "author_fullname": "t2_1urq336nt3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is there an offline way to use something that RAG does? Or at least if you don't have Internet or if it's out, is there a way to feed the llm offline way such as downloading information on articles and storing them on a USB drive and then taking the drive to the LLM and feeding it it that way?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6v30fq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is there an offline way to use something that RAG does? Or at least if you don&amp;#39;t have Internet or if it&amp;#39;s out, is there a way to feed the llm offline way such as downloading information on articles and storing them on a USB drive and then taking the drive to the LLM and feeding it it that way?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhcfe4",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6v30fq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754312085,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6v2hje",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "GhostInThePudding",
            "can_mod_post": false,
            "created_utc": 1754311898,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 4,
            "author_fullname": "t2_15sgncv9z5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Whether a model hallucinates or not depends more on how you use it than the model itself.\n\nI'm sure there are some particularly bad ones out there. But with any model you can reasonably run on a single consumer GPU, hallucinations will be frequent for basically any general question. But they can be used with extremely few hallucinations for things like summarzing data, using RAG and so on.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v2hje",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Whether a model hallucinates or not depends more on how you use it than the model itself.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure there are some particularly bad ones out there. But with any model you can reasonably run on a single consumer GPU, hallucinations will be frequent for basically any general question. But they can be used with extremely few hallucinations for things like summarzing data, using RAG and so on.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6v2hje/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754311898,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6v6iaa",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "WyattTheSkid",
            "can_mod_post": false,
            "created_utc": 1754313309,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 2,
            "author_fullname": "t2_1flwpwd3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The Llama 3 line is a solid and strong candidate for locally hosted chatgpt replacement. If you want a pretty good general purpose assistant Llama 3.1 8b will serve you just fine but if you’re serious about this and want something that feels more like the big closed source flagship llms then you want a model with 70b parameters or larger. As far as hallucinations go, they all do it once in a while but I find that reasoning models (the ones that produce &lt;think&gt; &lt;/think&gt; tags) are far less prone to hallucinations than their non reasoning counterpart",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v6iaa",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The Llama 3 line is a solid and strong candidate for locally hosted chatgpt replacement. If you want a pretty good general purpose assistant Llama 3.1 8b will serve you just fine but if you’re serious about this and want something that feels more like the big closed source flagship llms then you want a model with 70b parameters or larger. As far as hallucinations go, they all do it once in a while but I find that reasoning models (the ones that produce &amp;lt;think&amp;gt; &amp;lt;/think&amp;gt; tags) are far less prone to hallucinations than their non reasoning counterpart&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6v6iaa/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754313309,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ve7rj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DarkArtsMastery",
            "can_mod_post": false,
            "created_utc": 1754315843,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 2,
            "author_fullname": "t2_1a0fe1cy0a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "[https://github.com/vectara/hallucination-leaderboard](https://github.com/vectara/hallucination-leaderboard)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ve7rj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/vectara/hallucination-leaderboard\"&gt;https://github.com/vectara/hallucination-leaderboard&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6ve7rj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754315843,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6vjz53",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "moarmagic",
            "can_mod_post": false,
            "created_utc": 1754317649,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 2,
            "author_fullname": "t2_164x4a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think you need to understand that the issue with hallucinations is less that the model just likes to lie, and more that models don't have great mechanisms to understand the boundaries of their knowledge. So when you ask questions that are close to something they know, they'll give you an answer that /sounds/ right, because it's a statistically likely sounding thing. \n\nSo yeah, if you need hyper specific information that has a lot of variables - like say, medical knowledge- the chances of the specific information you need not being in the training data (or that information becoming less accurate through quants etc) increases the risk of hallucinations. If on the other hand, you just want a model to help you format your thoughts, plan your day- well, that doesn't need specific information.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6vjz53",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think you need to understand that the issue with hallucinations is less that the model just likes to lie, and more that models don&amp;#39;t have great mechanisms to understand the boundaries of their knowledge. So when you ask questions that are close to something they know, they&amp;#39;ll give you an answer that /sounds/ right, because it&amp;#39;s a statistically likely sounding thing. &lt;/p&gt;\n\n&lt;p&gt;So yeah, if you need hyper specific information that has a lot of variables - like say, medical knowledge- the chances of the specific information you need not being in the training data (or that information becoming less accurate through quants etc) increases the risk of hallucinations. If on the other hand, you just want a model to help you format your thoughts, plan your day- well, that doesn&amp;#39;t need specific information.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6vjz53/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754317649,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6v51ys",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "XiRw",
            "can_mod_post": false,
            "created_utc": 1754312807,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 1,
            "author_fullname": "t2_l4qac",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think if your context window count is low, any model will remember less and try to fill in the gaps with nonsense. But you need more power for a higher number .",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v51ys",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think if your context window count is low, any model will remember less and try to fill in the gaps with nonsense. But you need more power for a higher number .&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6v51ys/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754312807,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6va23m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jonasaba",
            "can_mod_post": false,
            "created_utc": 1754314498,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 1,
            "author_fullname": "t2_1umovk3s4e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Series does not matter as much as the amount of VRAM you can get.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6va23m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Series does not matter as much as the amount of VRAM you can get.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6va23m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754314498,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y6mnc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DinoAmino",
            "can_mod_post": false,
            "created_utc": 1754344960,
            "send_replies": true,
            "parent_id": "t3_1mhcfe4",
            "score": 1,
            "author_fullname": "t2_j1v7f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The TruthfulQA and SimpleQA benchmarks are industry standard and help determine how much an LLM hallucinates.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y6mnc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The TruthfulQA and SimpleQA benchmarks are industry standard and help determine how much an LLM hallucinates.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcfe4/what_models_have_the_least_likelihood_of/n6y6mnc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754344960,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcfe4",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]