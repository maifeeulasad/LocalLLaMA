[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "So I’ve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I’m curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\nI’m still trying to fully grasp how these models compare to transformers, so I’d love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What do new architectures offer and what are their limits?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1m76df6",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_y1vyie97k",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753269030,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So I’ve been diving into alternative architectures to transformers recently, and I came across a few interesting ones. liquid foundation models (lfm), Mamba (ssm based) and RWKV. I’m curious about what these new architectures offer and what their limitations are. From what I understand, they all seem to be better at handling long sequences, SSMs and LFMs are more resource efficient and LFMs seem to struggle with wide area applications (?)\nI’m still trying to fully grasp how these models compare to transformers, so I’d love to hear more about the strengths and weaknesses of these newer architectures. Any insights would be appreciated!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m76df6",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ba2sYd",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/",
            "subreddit_subscribers": 503257,
            "created_utc": 1753269030,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4p751v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Maykey",
            "can_mod_post": false,
            "created_utc": 1753272952,
            "send_replies": true,
            "parent_id": "t3_1m76df6",
            "score": 1,
            "author_fullname": "t2_17tuu7pv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Don't know anything about LFM(And after playing with liquid model on lambda chat, don't particularly want to know), but Mamba and RWKV need O(N) time time inference N tokens, which is very good for long prompts.\n\nProblems, is there since there is no access to previous tokens. All history is stored into fixed-size state so it's easy for mamba to lose history. It's fun  to feed pixels-based info instead, for example many moons ago I tried to see what will happen if you try to rotate an image using very small  mamba2 + conv2d, it'll turn into [this](https://imgur.com/a/TkkG8fE)  which was quite funny\n\nAlso mamba is very sensitive to precision.  Transformers can easily be quantized and still will be okayish. Mamba will lose its shit.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4p751v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t know anything about LFM(And after playing with liquid model on lambda chat, don&amp;#39;t particularly want to know), but Mamba and RWKV need O(N) time time inference N tokens, which is very good for long prompts.&lt;/p&gt;\n\n&lt;p&gt;Problems, is there since there is no access to previous tokens. All history is stored into fixed-size state so it&amp;#39;s easy for mamba to lose history. It&amp;#39;s fun  to feed pixels-based info instead, for example many moons ago I tried to see what will happen if you try to rotate an image using very small  mamba2 + conv2d, it&amp;#39;ll turn into &lt;a href=\"https://imgur.com/a/TkkG8fE\"&gt;this&lt;/a&gt;  which was quite funny&lt;/p&gt;\n\n&lt;p&gt;Also mamba is very sensitive to precision.  Transformers can easily be quantized and still will be okayish. Mamba will lose its shit.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m76df6/what_do_new_architectures_offer_and_what_are/n4p751v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753272952,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m76df6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]