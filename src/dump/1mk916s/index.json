[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "# A personal Benchmark\n\nFor a while now I have been testing LLMs with a benchmark I informally call \"Twisted Math\". The basic idea is that we take a very common mathematics problem, e.g., Tower of Hanoi, the birthday paradox, etc., and subtly change the problem constraints so that the original reasoning does not hold any more.\n\n# Do LLMs blindly regurgitate its training or do they \"think\"?\n\nSince the conditioning tokens naturally lead to the common (and in this case wrong) solution, the question is, do LLMs have some semblance of internal reasoning or just strong fuzzy retrieval? To date I have not seen LLMs beat this test. I would love to have your thoughts on this!\n\nP.S.: The \"catch\" for this problem is that we never mentioned that a large disk cannot be placed on a small one, thus making the answer 2\\*N -1 instead of 2\\^N - 1. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Twisted math test for LLMs",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 52,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mk916s",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "ups": 6,
            "domain": "i.redd.it",
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1kb53dh",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/oC1QlDExaSTBxJBJ2pxSQr_SUxSYlSe8m-KhYgGb78E.jpg",
            "author_cakeday": true,
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754593763,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;h1&gt;A personal Benchmark&lt;/h1&gt;\n\n&lt;p&gt;For a while now I have been testing LLMs with a benchmark I informally call &amp;quot;Twisted Math&amp;quot;. The basic idea is that we take a very common mathematics problem, e.g., Tower of Hanoi, the birthday paradox, etc., and subtly change the problem constraints so that the original reasoning does not hold any more.&lt;/p&gt;\n\n&lt;h1&gt;Do LLMs blindly regurgitate its training or do they &amp;quot;think&amp;quot;?&lt;/h1&gt;\n\n&lt;p&gt;Since the conditioning tokens naturally lead to the common (and in this case wrong) solution, the question is, do LLMs have some semblance of internal reasoning or just strong fuzzy retrieval? To date I have not seen LLMs beat this test. I would love to have your thoughts on this!&lt;/p&gt;\n\n&lt;p&gt;P.S.: The &amp;quot;catch&amp;quot; for this problem is that we never mentioned that a large disk cannot be placed on a small one, thus making the answer 2*N -1 instead of 2^N - 1. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/ihugrx1wanhf1.jpeg",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?auto=webp&amp;s=6f1de1c53a7797c3521db8143d91a4d3e44d61e4",
                    "width": 1583,
                    "height": 595
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a37990c1cde7c492692a1858b33ea3571b01f1e",
                      "width": 108,
                      "height": 40
                    },
                    {
                      "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=718fa77cb37811f3c3fce9303ebe9ad0fe0220fa",
                      "width": 216,
                      "height": 81
                    },
                    {
                      "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=896c87335aac90ae2cbb365644bfbdcf0b43b317",
                      "width": 320,
                      "height": 120
                    },
                    {
                      "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8a09dc56505ba25d79c8bf3c47fec2eec04dca45",
                      "width": 640,
                      "height": 240
                    },
                    {
                      "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=e98e1ae27773aae146eff7fb06e5311f270f2e3e",
                      "width": 960,
                      "height": 360
                    },
                    {
                      "url": "https://preview.redd.it/ihugrx1wanhf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5b3f692ae0bccee0a3bd206b88f591911a9b91d9",
                      "width": 1080,
                      "height": 405
                    }
                  ],
                  "variants": {},
                  "id": "50q1mawbq5QnfkZOocv1dqNmA6uUnHO3Ft2EUAN-94E"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mk916s",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "espressoVi",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/",
            "stickied": false,
            "url": "https://i.redd.it/ihugrx1wanhf1.jpeg",
            "subreddit_subscribers": 513813,
            "created_utc": 1754593763,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "body": "My perspective is slightly different, or maybe I am misunderstanding your point. The banana plate test looks like it is testing for entity chain tracking ability (something like https://aclanthology.org/2024.emnlp-main.731/), but the key idea here is to test whether LLMs reason about problems by making use of the provided description or simply try to match to the closest training sample in some sense.\n\nEdit: Wrong paper link.",
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "body": "LOL! Did I just fall for the same trap as an LLM? AM I AN LLM?",
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "body": "Nonsense! How many r's in strrrrrrrrrrrrrrawberrrrrrrrrry?",
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n7hbfgt",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "espressoVi",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n7havtx",
                                                              "score": 1,
                                                              "author_fullname": "t2_1kb53dh",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "author_cakeday": true,
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n7hbfgt",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nonsense! How many r&amp;#39;s in strrrrrrrrrrrrrrawberrrrrrrrrry?&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mk916s",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7hbfgt/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754597854,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754597854,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7havtx",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Chromix_",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7h9jqc",
                                                    "score": 1,
                                                    "author_fullname": "t2_k7w2h",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Yes, and you're used in a LLM-as-a-judge scenario here, where the judge is usually a higher-capability model than the judged model ðŸ˜‰",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7havtx",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, and you&amp;#39;re used in a LLM-as-a-judge scenario here, where the judge is usually a higher-capability model than the judged model ðŸ˜‰&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mk916s",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7havtx/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754597699,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754597699,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7h9jqc",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "espressoVi",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7h3pmh",
                                          "score": 2,
                                          "author_fullname": "t2_1kb53dh",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "author_cakeday": true,
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7h9jqc",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LOL! Did I just fall for the same trap as an LLM? AM I AN LLM?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mk916s",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7h9jqc/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754597315,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754597315,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7h3pmh",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Chromix_",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7h1uca",
                                "score": 1,
                                "author_fullname": "t2_k7w2h",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It'd be entity tracking if the banana was on the plate. Here the banana is under the plate, a case that's apparently not well trained for. Thus, LLMs (used to) assume that the banana is carried around with the plate, or stuck to it. It gets even funnier when the plate is placed in a microwave at the end, as LLMs then jump to their safety training, warning about exploding bananas in the microwave.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7h3pmh",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;d be entity tracking if the banana was on the plate. Here the banana is under the plate, a case that&amp;#39;s apparently not well trained for. Thus, LLMs (used to) assume that the banana is carried around with the plate, or stuck to it. It gets even funnier when the plate is placed in a microwave at the end, as LLMs then jump to their safety training, warning about exploding bananas in the microwave.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mk916s",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7h3pmh/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754595598,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754595598,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7h1uca",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "espressoVi",
                      "can_mod_post": false,
                      "created_utc": 1754595044,
                      "send_replies": true,
                      "parent_id": "t1_n7h0gt1",
                      "score": 1,
                      "author_fullname": "t2_1kb53dh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "author_cakeday": true,
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7h1uca",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My perspective is slightly different, or maybe I am misunderstanding your point. The banana plate test looks like it is testing for entity chain tracking ability (something like &lt;a href=\"https://aclanthology.org/2024.emnlp-main.731/\"&gt;https://aclanthology.org/2024.emnlp-main.731/&lt;/a&gt;), but the key idea here is to test whether LLMs reason about problems by making use of the provided description or simply try to match to the closest training sample in some sense.&lt;/p&gt;\n\n&lt;p&gt;Edit: Wrong paper link.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mk916s",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7h1uca/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754595044,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7h0gt1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Chromix_",
            "can_mod_post": false,
            "created_utc": 1754594641,
            "send_replies": true,
            "parent_id": "t3_1mk916s",
            "score": 2,
            "author_fullname": "t2_k7w2h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Looks like a useful continuation of the old [banana plate test](https://www.reddit.com/r/LocalLLaMA/comments/1c67c62/mixtral_8x22b_does_not_know_where_the_banana_is/). You can find a lot more of those in the [misguided attention benchmark](https://cpldcpu.github.io/MisguidedAttention/).\n\nTests like these might uncover unbalanced training data, where basically only the intended, positive case is trained, but rarely anything with different mechanics.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7h0gt1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Looks like a useful continuation of the old &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1c67c62/mixtral_8x22b_does_not_know_where_the_banana_is/\"&gt;banana plate test&lt;/a&gt;. You can find a lot more of those in the &lt;a href=\"https://cpldcpu.github.io/MisguidedAttention/\"&gt;misguided attention benchmark&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;Tests like these might uncover unbalanced training data, where basically only the intended, positive case is trained, but rarely anything with different mechanics.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7h0gt1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754594641,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk916s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "body": "Maybe it is a slow roll-out?",
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7h83ve",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Green-Ad-3964",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7h0bsg",
                                "score": 0,
                                "author_fullname": "t2_sfb08i7a",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "slow roll-out for a seemingly meh model :(",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7h83ve",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;slow roll-out for a seemingly meh model :(&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mk916s",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7h83ve/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754596890,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754596890,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7h0bsg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "espressoVi",
                      "can_mod_post": false,
                      "created_utc": 1754594600,
                      "send_replies": true,
                      "parent_id": "t1_n7gygt8",
                      "score": 2,
                      "author_fullname": "t2_1kb53dh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "author_cakeday": true,
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7h0bsg",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe it is a slow roll-out?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mk916s",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7h0bsg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754594600,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7gygt8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Green-Ad-3964",
            "can_mod_post": false,
            "created_utc": 1754594052,
            "send_replies": true,
            "parent_id": "t3_1mk916s",
            "score": 1,
            "author_fullname": "t2_sfb08i7a",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hmm...why I still don't have gpt-5?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7gygt8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hmm...why I still don&amp;#39;t have gpt-5?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7gygt8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754594052,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk916s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7hcw35",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ColorlessCrowfeet",
            "can_mod_post": false,
            "created_utc": 1754598271,
            "send_replies": true,
            "parent_id": "t3_1mk916s",
            "score": 2,
            "author_fullname": "t2_9g89e30o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;Do LLMs blindly regurgitate its training or do they \"think\"?\n\nThey can reason but can also be misled by pattern matches. Sort of like people.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7hcw35",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Do LLMs blindly regurgitate its training or do they &amp;quot;think&amp;quot;?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;They can reason but can also be misled by pattern matches. Sort of like people.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk916s/twisted_math_test_for_llms/n7hcw35/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754598271,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk916s",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]