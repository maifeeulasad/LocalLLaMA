[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have recently released my experimental library *Actors.* Actors is a hackable library for doing Multi-Turn Multi-Agent RL with LLMs for the **GPU poor** and **middle class**.\n\nCheck it out here: [https://github.com/RD211/actors](https://github.com/RD211/actors)\n\nKey features:  \n\\- **Multi-Trainable-Agents**: You can do things like adversarial, collaborative or simulation-like environments.  \n\\- **Multi-Environments**: Lets you make very complex environments and makes it easy to combine them together.\n\n**VRAM Efficiency**, obviously if we want to train several models at the same time we need to be careful with VRAM, thus Actors does the following:  \n\\- Smart offloading of optimizer states and model parameters when not needed (does not impact training time significantly).  \n\\- Streamed weight updates to vLLM that do not make a spike in memory usage.  \n\\- A small triton kernel for reference Log-probs calculations.  \n\\- in-memory LoRA  updates to vLLM.\n\nThe library also supports LoRA/QLoRA training, Multi-GPU and soon Multi-Node. On one GPU it seems to be just a bit worse in VRAM than Unsloth.  \n  \n**Algorithms,** we currently have **GSPO** and **GRPO** both with **Liger-Kernel** implementations but you can probably get DAPO and some others by just adjusting some of the settings.\n\n\nFeedback and issues are welcome!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "RL Library for Multi-Trainable-Agents",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mcqrwh",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2o0i8kuj",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753834758,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753831115,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have recently released my experimental library &lt;em&gt;Actors.&lt;/em&gt; Actors is a hackable library for doing Multi-Turn Multi-Agent RL with LLMs for the &lt;strong&gt;GPU poor&lt;/strong&gt; and &lt;strong&gt;middle class&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;Check it out here: &lt;a href=\"https://github.com/RD211/actors\"&gt;https://github.com/RD211/actors&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Key features:&lt;br/&gt;\n- &lt;strong&gt;Multi-Trainable-Agents&lt;/strong&gt;: You can do things like adversarial, collaborative or simulation-like environments.&lt;br/&gt;\n- &lt;strong&gt;Multi-Environments&lt;/strong&gt;: Lets you make very complex environments and makes it easy to combine them together.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;VRAM Efficiency&lt;/strong&gt;, obviously if we want to train several models at the same time we need to be careful with VRAM, thus Actors does the following:&lt;br/&gt;\n- Smart offloading of optimizer states and model parameters when not needed (does not impact training time significantly).&lt;br/&gt;\n- Streamed weight updates to vLLM that do not make a spike in memory usage.&lt;br/&gt;\n- A small triton kernel for reference Log-probs calculations.&lt;br/&gt;\n- in-memory LoRA  updates to vLLM.&lt;/p&gt;\n\n&lt;p&gt;The library also supports LoRA/QLoRA training, Multi-GPU and soon Multi-Node. On one GPU it seems to be just a bit worse in VRAM than Unsloth.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Algorithms,&lt;/strong&gt; we currently have &lt;strong&gt;GSPO&lt;/strong&gt; and &lt;strong&gt;GRPO&lt;/strong&gt; both with &lt;strong&gt;Liger-Kernel&lt;/strong&gt; implementations but you can probably get DAPO and some others by just adjusting some of the settings.&lt;/p&gt;\n\n&lt;p&gt;Feedback and issues are welcome!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?auto=webp&amp;s=d1c178e9209b8e83156e14a1ff72b92a0721fd8f",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1b2e184f5160e98cc31a4243590278640475fdef",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=63a02b458617c0d0ceff2a45cdc6709316610006",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5e418c06b19810a48fcbc0e741cc9ba5d2cd1413",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cc18776cd35ebb7e21739094063171f691aa1f7b",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e0d7d62db37e6673039732a16af989a3492b949b",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2006937f67001eb0977dff97aad1540a191354ea",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "YpM_Gv8sDN9Tw5otowad96JD2Rpmx0NuvrQqzchsW1w"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mcqrwh",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "rd211x",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcqrwh/rl_library_for_multitrainableagents/",
            "subreddit_subscribers": 506711,
            "created_utc": 1753831115,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]