[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I see that LLM is afar superior technology than Vector database and LLM is trained on Natural Language Processing. So is it not always better to send the query to LLM first which can understand the user intent better than anything?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Is LLM first RAG better than traditional RAG?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lwx77q",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_adnzl8f8x",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752206450,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I see that LLM is afar superior technology than Vector database and LLM is trained on Natural Language Processing. So is it not always better to send the query to LLM first which can understand the user intent better than anything?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lwx77q",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "Maleficent_Mess6445",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/",
            "subreddit_subscribers": 497353,
            "created_utc": 1752206450,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2i9x1c",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Maleficent_Mess6445",
                      "can_mod_post": false,
                      "created_utc": 1752216841,
                      "send_replies": true,
                      "parent_id": "t1_n2hqa2b",
                      "score": 1,
                      "author_fullname": "t2_adnzl8f8x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It just means that user query goes to LLM first instead of vector database",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2i9x1c",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It just means that user query goes to LLM first instead of vector database&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lwx77q",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2i9x1c/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752216841,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2hqa2b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BobbyL2k",
            "can_mod_post": false,
            "created_utc": 1752207267,
            "send_replies": true,
            "parent_id": "t3_1lwx77q",
            "score": 2,
            "author_fullname": "t2_ghoyg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I assume by “LLM first RAG”, you mean agentic setup where the LLM behaves like an agent and call search tools.\n\nThere’s no true “traditional RAG”. The retrieval in RAG could be a super basic keyword search over documents, to LLM powered query expansion into vector embedded documents with comprehensive results merge and reranking.\n\nIt entirely depends on how you architect it. I don’t like agentic setups when I want the system to answer only based on documents. So force feeding the documents into the context is better for my use case.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2hqa2b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I assume by “LLM first RAG”, you mean agentic setup where the LLM behaves like an agent and call search tools.&lt;/p&gt;\n\n&lt;p&gt;There’s no true “traditional RAG”. The retrieval in RAG could be a super basic keyword search over documents, to LLM powered query expansion into vector embedded documents with comprehensive results merge and reranking.&lt;/p&gt;\n\n&lt;p&gt;It entirely depends on how you architect it. I don’t like agentic setups when I want the system to answer only based on documents. So force feeding the documents into the context is better for my use case.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2hqa2b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752207267,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lwx77q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2i9ujs",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Maleficent_Mess6445",
                      "can_mod_post": false,
                      "created_utc": 1752216805,
                      "send_replies": true,
                      "parent_id": "t1_n2hvrhg",
                      "score": 0,
                      "author_fullname": "t2_adnzl8f8x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "No. Where the user query goes to LLM first instead of Vector database",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2i9ujs",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No. Where the user query goes to LLM first instead of Vector database&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lwx77q",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2i9ujs/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752216805,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2hvrhg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ok-Pipe-5151",
            "can_mod_post": false,
            "created_utc": 1752209701,
            "send_replies": true,
            "parent_id": "t3_1lwx77q",
            "score": 2,
            "author_fullname": "t2_uxbdufm8b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What is even \"LLM first\" RAG? Using a reranker LLM to rerank the results?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2hvrhg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What is even &amp;quot;LLM first&amp;quot; RAG? Using a reranker LLM to rerank the results?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2hvrhg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752209701,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lwx77q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2iard9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Maleficent_Mess6445",
                      "can_mod_post": false,
                      "created_utc": 1752217295,
                      "send_replies": true,
                      "parent_id": "t1_n2i3wnl",
                      "score": 1,
                      "author_fullname": "t2_adnzl8f8x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I see. You're right, LLM is compute intensive when hosted locally but if privacy is not a major concern, this issue is resolved with API. Hallucination is a problem but I don't think  vector db is a solution to it. Vector DB is far inferior technology wrt LLM and is not trained on NLP.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2iard9",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I see. You&amp;#39;re right, LLM is compute intensive when hosted locally but if privacy is not a major concern, this issue is resolved with API. Hallucination is a problem but I don&amp;#39;t think  vector db is a solution to it. Vector DB is far inferior technology wrt LLM and is not trained on NLP.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lwx77q",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2iard9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752217295,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2i3wnl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1752213675,
            "send_replies": true,
            "parent_id": "t3_1lwx77q",
            "score": 2,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The critical difference is that you can populate your RAG database with true things, relevant to some subject.  This helps ground inference in truth, and (mostly) avoids hallucinations.\n\n\"Thinking\" inference has the advantage of generating only augmenting information which is relevant to the user's prompt, but it relies on the model's intrinsic world knowledge to do so, and a hallucination early on can throw everything else off.  It is also much, much more compute-intensive than RAG.\n\nThere's a place for each, especially when you cannot compile a high-quality RAG database for all of the subjects on which you want to infer.\n\nThere's also a place for combining the two approaches, via HyDE -- https://medium.com/prompt-engineering/hyde-revolutionising-search-with-hypothetical-document-embeddings-3474df795af8",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2i3wnl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The critical difference is that you can populate your RAG database with true things, relevant to some subject.  This helps ground inference in truth, and (mostly) avoids hallucinations.&lt;/p&gt;\n\n&lt;p&gt;&amp;quot;Thinking&amp;quot; inference has the advantage of generating only augmenting information which is relevant to the user&amp;#39;s prompt, but it relies on the model&amp;#39;s intrinsic world knowledge to do so, and a hallucination early on can throw everything else off.  It is also much, much more compute-intensive than RAG.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a place for each, especially when you cannot compile a high-quality RAG database for all of the subjects on which you want to infer.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also a place for combining the two approaches, via HyDE -- &lt;a href=\"https://medium.com/prompt-engineering/hyde-revolutionising-search-with-hypothetical-document-embeddings-3474df795af8\"&gt;https://medium.com/prompt-engineering/hyde-revolutionising-search-with-hypothetical-document-embeddings-3474df795af8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2i3wnl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752213675,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1lwx77q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2hy30l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "I_Short_TSLA",
            "can_mod_post": false,
            "created_utc": 1752210793,
            "send_replies": true,
            "parent_id": "t3_1lwx77q",
            "score": 1,
            "author_fullname": "t2_2zs0bfio",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It depends on how much data you are dealing with. Retrieval is a tool in case you are dealing with tons and tons of data, let’s say 100x the model context size or more.\n\nYou only use retrieval for grounding if you have no other choice.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2hy30l",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends on how much data you are dealing with. Retrieval is a tool in case you are dealing with tons and tons of data, let’s say 100x the model context size or more.&lt;/p&gt;\n\n&lt;p&gt;You only use retrieval for grounding if you have no other choice.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2hy30l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752210793,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lwx77q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2ic8pe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ok_Needleworker_5247",
            "can_mod_post": false,
            "created_utc": 1752218102,
            "send_replies": true,
            "parent_id": "t3_1lwx77q",
            "score": 2,
            "author_fullname": "t2_1gmprv51a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The choice between sending a query to an LLM or a RAG setup first isn't always clear-cut. Google's \"Data Gemma,\" as detailed in [this article](https://pub.towardsai.net/demystifying-googles-data-gemma-f07a470c2a39), offers a novel approach to address hallucinations by integrating with a structured knowledge graph. It uses a combination of question expansion and a reliable NL API, providing a structured path to minimize errors. This setup can be particularly effective for complex queries where precision and reliability are crucial. Worth checking out if you're exploring advanced RAG methods.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2ic8pe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The choice between sending a query to an LLM or a RAG setup first isn&amp;#39;t always clear-cut. Google&amp;#39;s &amp;quot;Data Gemma,&amp;quot; as detailed in &lt;a href=\"https://pub.towardsai.net/demystifying-googles-data-gemma-f07a470c2a39\"&gt;this article&lt;/a&gt;, offers a novel approach to address hallucinations by integrating with a structured knowledge graph. It uses a combination of question expansion and a reliable NL API, providing a structured path to minimize errors. This setup can be particularly effective for complex queries where precision and reliability are crucial. Worth checking out if you&amp;#39;re exploring advanced RAG methods.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lwx77q/is_llm_first_rag_better_than_traditional_rag/n2ic8pe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752218102,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lwx77q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]