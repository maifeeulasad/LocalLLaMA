[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I don't know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md\n\nSo I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?\n\nEDIT: I'm trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let's say it's a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model's output, but I'm trying to gauge what would happen if I simulate this scenario 100 times for a bias towards one sex.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Ollama + Open WebUI -- is there a way for the same query to run through the same model multiple times (could be 3 times, could be 100 times), then gather all the answers together to summarise/count?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m7pi3t",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_4hrx8",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753316394,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753315438,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if it matters, but I followed this to install (because Nvidia drivers on Linux is a pain!): &lt;a href=\"https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md\"&gt;https://github.com/NeuralFalconYT/Ollama-Open-WebUI-Windows-Installation/blob/main/README.md&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;So I would like to type in a query into a model with some preset system prompt. I would like that model to run over this query multiple times. Then after all of them are done, I would like for the responses to be gathered for a summary. Would such task be possible?&lt;/p&gt;\n\n&lt;p&gt;EDIT: I&amp;#39;m trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let&amp;#39;s say it&amp;#39;s a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model&amp;#39;s output, but I&amp;#39;m trying to gauge what would happen if I simulate this scenario 100 times for a bias towards one sex.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?auto=webp&amp;s=194fc1da74b1f56e6bca7cecb75e5a68c11008c1",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e1a05a3ead9734d6cb7b7045fdd787ff15a290e5",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=867ae8b82b8f457ac666d89cfaf3611953cc358e",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cbf750440a0a81a0c33ea061fa002223db7b35d7",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e492becae66517bde05cbff2d3abe83139c4065f",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a08a1c535e0e82ec2dc485d89bdfe54012f28a75",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a17c83b48123663530d34879b1da1dc4ccf3d160",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "KAPkJfhQQ8pBbbpz0387aAfxvFPP7H5QShgqfAGc9Ek"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m7pi3t",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "jinnyjuice",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/",
            "subreddit_subscribers": 503759,
            "created_utc": 1753315438,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4tkvq7",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "HistorianPotential48",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4tamx4",
                                "score": 1,
                                "author_fullname": "t2_4dzthia7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I don't know if Open WebUI has built-in function like that, but for your test case, it's easier to call ollama API by your own code, then you can design your own flow in detail. [B-score](https://b-score.github.io/) from a while ago suggests multi-turn conversations can in some way help with model bias, so controlling by code you'll have easier time to test between single-turn and multi-turn.\n\nModel bias is also a hot topic, people talked about [how LLM usually picks 27 from 1\\~50](https://www.reddit.com/r/OpenAI/comments/1jj39nn/i_saw_some_people_talking_that_they_got_the/). I'd imagine you'll find [lots of papers](https://arxiv.org/pdf/2309.00770) on this, so perhaps it's great to check pioneers' researches. You might also need to consider about models that were trained from different data might have different bias too.\n\nAnother problem I'd consider is that as LLM is predicting next token from existing tokens, perhaps queries with same meaning but different wordings/orderings might affect test result.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4tkvq7",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know if Open WebUI has built-in function like that, but for your test case, it&amp;#39;s easier to call ollama API by your own code, then you can design your own flow in detail. &lt;a href=\"https://b-score.github.io/\"&gt;B-score&lt;/a&gt; from a while ago suggests multi-turn conversations can in some way help with model bias, so controlling by code you&amp;#39;ll have easier time to test between single-turn and multi-turn.&lt;/p&gt;\n\n&lt;p&gt;Model bias is also a hot topic, people talked about &lt;a href=\"https://www.reddit.com/r/OpenAI/comments/1jj39nn/i_saw_some_people_talking_that_they_got_the/\"&gt;how LLM usually picks 27 from 1~50&lt;/a&gt;. I&amp;#39;d imagine you&amp;#39;ll find &lt;a href=\"https://arxiv.org/pdf/2309.00770\"&gt;lots of papers&lt;/a&gt; on this, so perhaps it&amp;#39;s great to check pioneers&amp;#39; researches. You might also need to consider about models that were trained from different data might have different bias too.&lt;/p&gt;\n\n&lt;p&gt;Another problem I&amp;#39;d consider is that as LLM is predicting next token from existing tokens, perhaps queries with same meaning but different wordings/orderings might affect test result.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m7pi3t",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/n4tkvq7/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753320006,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753320006,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n4trbzf",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Fit-Produce420",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4tamx4",
                                "score": 1,
                                "author_fullname": "t2_tewf9bdwg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That just isn't how models work. \n\n\nIt isn't running 100 experiments, it's pretending to predict the answer as if it had. \n\n\nYou can't call that output an \"experiment,\" the experiment you ARE doing is \"What's happens when I ask this LLM to do something?\" \"What's happens if I ask it 100 times?\"\n\n\n\n\nI get the impression that you believe there will be \"truth\" if you do the experiment say, 100,000 times. \n\n\nJust ask it \"Is there a God?\" One million times, average the results, and we'll know if God exists or not? ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4trbzf",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That just isn&amp;#39;t how models work. &lt;/p&gt;\n\n&lt;p&gt;It isn&amp;#39;t running 100 experiments, it&amp;#39;s pretending to predict the answer as if it had. &lt;/p&gt;\n\n&lt;p&gt;You can&amp;#39;t call that output an &amp;quot;experiment,&amp;quot; the experiment you ARE doing is &amp;quot;What&amp;#39;s happens when I ask this LLM to do something?&amp;quot; &amp;quot;What&amp;#39;s happens if I ask it 100 times?&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;I get the impression that you believe there will be &amp;quot;truth&amp;quot; if you do the experiment say, 100,000 times. &lt;/p&gt;\n\n&lt;p&gt;Just ask it &amp;quot;Is there a God?&amp;quot; One million times, average the results, and we&amp;#39;ll know if God exists or not? &lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m7pi3t",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/n4trbzf/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753322260,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753322260,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4tamx4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "jinnyjuice",
                      "can_mod_post": false,
                      "created_utc": 1753316441,
                      "send_replies": true,
                      "parent_id": "t1_n4t9k9y",
                      "score": 0,
                      "author_fullname": "t2_4hrx8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Sorry it wasn't clear, but I'm trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let's say it's a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model's output, but I'm trying to gauge what would happen if I simulate this scenario 100 times towards a bias towards one sex.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4tamx4",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry it wasn&amp;#39;t clear, but I&amp;#39;m trying to benchmark variation biases for research. The prompt could be any scenario, but if I were to make an example, let&amp;#39;s say it&amp;#39;s a scenario where I meet with a random stranger. The stranger should have 50/50 chance of being a gentleman/lady as the model&amp;#39;s output, but I&amp;#39;m trying to gauge what would happen if I simulate this scenario 100 times towards a bias towards one sex.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7pi3t",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/n4tamx4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753316441,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4t9k9y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Fit-Produce420",
            "can_mod_post": false,
            "created_utc": 1753316065,
            "send_replies": true,
            "parent_id": "t3_1m7pi3t",
            "score": 2,
            "author_fullname": "t2_tewf9bdwg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You don't need to do that, you're just describing how the model already chooses.\n\n\nThe only thing you'd be \"testing\" is the models temperature/randomness which you can simply edit yourself. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t9k9y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You don&amp;#39;t need to do that, you&amp;#39;re just describing how the model already chooses.&lt;/p&gt;\n\n&lt;p&gt;The only thing you&amp;#39;d be &amp;quot;testing&amp;quot; is the models temperature/randomness which you can simply edit yourself. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/n4t9k9y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753316065,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7pi3t",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4trbpz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "The_Soul_Collect0r",
            "can_mod_post": false,
            "created_utc": 1753322258,
            "send_replies": true,
            "parent_id": "t3_1m7pi3t",
            "score": 1,
            "author_fullname": "t2_13x9rnhkbw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "OP,\n\nspin up llama.cpp, setup parallelism to max the model can handle, execute query and save the query cache to disk then reload it back per slot. Ensure, if possible, that the required response is one token y/n, 1 or 0, limit response length to 1 - discard all non valid responses. Hammer it with varying seed requests using standard v1 openai non streaming endpoint on all available slots.\n\nI had the same idea, used as a judge MCP tool, for models to try gauge the correctness of their own result/response. With slots loaded and reusing context cache, the cache has to be constructed once, the response is only one token, depending on the model and hw ofc, but you could get absurd number of results in no time.  \nP.S.  \nUse new high throughput mode on llama.cpp",
            "edited": 1753322444,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4trbpz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;OP,&lt;/p&gt;\n\n&lt;p&gt;spin up llama.cpp, setup parallelism to max the model can handle, execute query and save the query cache to disk then reload it back per slot. Ensure, if possible, that the required response is one token y/n, 1 or 0, limit response length to 1 - discard all non valid responses. Hammer it with varying seed requests using standard v1 openai non streaming endpoint on all available slots.&lt;/p&gt;\n\n&lt;p&gt;I had the same idea, used as a judge MCP tool, for models to try gauge the correctness of their own result/response. With slots loaded and reusing context cache, the cache has to be constructed once, the response is only one token, depending on the model and hw ofc, but you could get absurd number of results in no time.&lt;br/&gt;\nP.S.&lt;br/&gt;\nUse new high throughput mode on llama.cpp&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7pi3t/ollama_open_webui_is_there_a_way_for_the_same/n4trbpz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753322258,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7pi3t",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]