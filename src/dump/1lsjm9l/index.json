[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Inspired by the awesome work presented by Kathleen Kenealy on ViT benchmarks in PyTorch DDP and Jax TPUs by Google DeepMind, I developed this intensive article on the solid foundations to transformers, Vision Transformers, and Distributed Learning,  and to say I learnt a lot would be an understatement. After a few revisions (extending and including Jax sharded parallelism), I will transform it into a book. The article starts off with the interesting reference to Dr Mihai Nica‚Äôs interesting ‚ÄúA random variable is not random, and it‚Äôs not a variable\", kicking off the article‚Äôs explorations of human language transformation to machine readable computationally crunchable tokens and embeddings, using rich animations to then redirect us to building Llama2 from the core, basing it as the ‚Äòequilibrium in the model space map‚Äô, a phrase meaning a solid understanding of Llama2 architecture could easily be mapped to any SOTA LLM variant with few iterations. I spin a fast inference as I document Modal‚Äôs awesome magic gpu pipelining without ssh. I then show the major transformations from Llama2 to ViT, coauthored by the infamous Lucas Beyer &amp; co. I then narrow to the four variants of ViTs benchmarked by DeepMind where I explore the architectures by further referencing the paper ‚ÄúScaling ViTs‚Äù. The final section then explores parallelism, starting from Open-MPI in C, building programs in peer-to-peer and collective communications before then finally building data parallelism in DDP and exploring helix editor, tmux, ssh tunneling on RunPod to run distributed training. I then ultimately explore Fully Sharded Data Parallel and the transformations to the training pipeline!\n\nThe Article:https://drive.google.com/file/d/1CPwbWaJ\\_NiBZJ6NbHDlPBFYe9hf36Y0q/view?usp=sharing\n\nI built this article, standing on the shoulders of giants, people who never stopped building and enjoying open-source, and I appreciate the much you share on X, r/LocalLLaMA, and GPU MODE, led by Mark Saroufim &amp; co on YouTube! Your expertise has motivated me to learn a whole lot more by being curious!\n\nIf you feel I can thrive well in your collaborative team, working towards impactful research, I am currently open to work starting this Fall, open to relocation, open to internships with return offers available. Currently based in Massachusetts. Please do reach out, and please share with your networks, I really do appreciate!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "From The Foundations of Transformers to Scaling Vision Transformers",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lsjm9l",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_yy5sq4dor",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1751754061,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1751748220,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Inspired by the awesome work presented by Kathleen Kenealy on ViT benchmarks in PyTorch DDP and Jax TPUs by Google DeepMind, I developed this intensive article on the solid foundations to transformers, Vision Transformers, and Distributed Learning,  and to say I learnt a lot would be an understatement. After a few revisions (extending and including Jax sharded parallelism), I will transform it into a book. The article starts off with the interesting reference to Dr Mihai Nica‚Äôs interesting ‚ÄúA random variable is not random, and it‚Äôs not a variable&amp;quot;, kicking off the article‚Äôs explorations of human language transformation to machine readable computationally crunchable tokens and embeddings, using rich animations to then redirect us to building Llama2 from the core, basing it as the ‚Äòequilibrium in the model space map‚Äô, a phrase meaning a solid understanding of Llama2 architecture could easily be mapped to any SOTA LLM variant with few iterations. I spin a fast inference as I document Modal‚Äôs awesome magic gpu pipelining without ssh. I then show the major transformations from Llama2 to ViT, coauthored by the infamous Lucas Beyer &amp;amp; co. I then narrow to the four variants of ViTs benchmarked by DeepMind where I explore the architectures by further referencing the paper ‚ÄúScaling ViTs‚Äù. The final section then explores parallelism, starting from Open-MPI in C, building programs in peer-to-peer and collective communications before then finally building data parallelism in DDP and exploring helix editor, tmux, ssh tunneling on RunPod to run distributed training. I then ultimately explore Fully Sharded Data Parallel and the transformations to the training pipeline!&lt;/p&gt;\n\n&lt;p&gt;The Article:&lt;a href=\"https://drive.google.com/file/d/1CPwbWaJ%5C_NiBZJ6NbHDlPBFYe9hf36Y0q/view?usp=sharing\"&gt;https://drive.google.com/file/d/1CPwbWaJ\\_NiBZJ6NbHDlPBFYe9hf36Y0q/view?usp=sharing&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I built this article, standing on the shoulders of giants, people who never stopped building and enjoying open-source, and I appreciate the much you share on X, &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt;, and GPU MODE, led by Mark Saroufim &amp;amp; co on YouTube! Your expertise has motivated me to learn a whole lot more by being curious!&lt;/p&gt;\n\n&lt;p&gt;If you feel I can thrive well in your collaborative team, working towards impactful research, I am currently open to work starting this Fall, open to relocation, open to internships with return offers available. Currently based in Massachusetts. Please do reach out, and please share with your networks, I really do appreciate!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1lsjm9l",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Des_goes_Brrr",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lsjm9l/from_the_foundations_of_transformers_to_scaling/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lsjm9l/from_the_foundations_of_transformers_to_scaling/",
            "subreddit_subscribers": 495396,
            "created_utc": 1751748220,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n1j9m5i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Accomplished_Mode170",
            "can_mod_post": false,
            "created_utc": 1751750832,
            "send_replies": true,
            "parent_id": "t3_1lsjm9l",
            "score": 1,
            "author_fullname": "t2_4hfmiefj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "BLUF Bro, I basically read through the history of attention mechanisms to learn you found an [awesome way to selectively sample sparsity](https://arxiv.org/pdf/2411.00284) ü§£üìäüåà\n\nLove the concept of dancing across latent spaces though; [sparsity is cool](https://www.tilderesearch.com/blog/sparse-attn)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1j9m5i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;BLUF Bro, I basically read through the history of attention mechanisms to learn you found an &lt;a href=\"https://arxiv.org/pdf/2411.00284\"&gt;awesome way to selectively sample sparsity&lt;/a&gt; ü§£üìäüåà&lt;/p&gt;\n\n&lt;p&gt;Love the concept of dancing across latent spaces though; &lt;a href=\"https://www.tilderesearch.com/blog/sparse-attn\"&gt;sparsity is cool&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lsjm9l/from_the_foundations_of_transformers_to_scaling/n1j9m5i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751750832,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lsjm9l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]