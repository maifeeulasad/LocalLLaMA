[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hello guys.\n\nI’ve been holding off on doing this for a while. \n\nI work in IT and I’ve been in computer science for many years, but I am a complete novice on LLMs. I want to be able to run the best and baddest models that I see everyone talking about here and I was hoping for some advice that might be useful to other people who find this thread also.\n\nSo, I’m looking to spend about $8 to $10K, and I’m torn between buying from a reputable company (I’ve been burned by a few though…) or perhaps having Microcenter or a similar place build one to my specifications. It seems though that the prices from companies like digital storm rise very quickly and even $10,000 doesn’t necessarily get you a high-end rig.\n\nAny advice would be very much appreciated and hopefully once I have one, I can contribute to this forum. \n\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Looking to buy/build a killer LLM/AI/ML/Deep Learning workstation",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1meknnb",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2hgc6342",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754015060,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys.&lt;/p&gt;\n\n&lt;p&gt;I’ve been holding off on doing this for a while. &lt;/p&gt;\n\n&lt;p&gt;I work in IT and I’ve been in computer science for many years, but I am a complete novice on LLMs. I want to be able to run the best and baddest models that I see everyone talking about here and I was hoping for some advice that might be useful to other people who find this thread also.&lt;/p&gt;\n\n&lt;p&gt;So, I’m looking to spend about $8 to $10K, and I’m torn between buying from a reputable company (I’ve been burned by a few though…) or perhaps having Microcenter or a similar place build one to my specifications. It seems though that the prices from companies like digital storm rise very quickly and even $10,000 doesn’t necessarily get you a high-end rig.&lt;/p&gt;\n\n&lt;p&gt;Any advice would be very much appreciated and hopefully once I have one, I can contribute to this forum. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1meknnb",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Particular_Cancel947",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1meknnb/looking_to_buybuild_a_killer_llmaimldeep_learning/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meknnb/looking_to_buybuild_a_killer_llmaimldeep_learning/",
            "subreddit_subscribers": 508191,
            "created_utc": 1754015060,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bd58e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754036721,
            "send_replies": true,
            "parent_id": "t3_1meknnb",
            "score": 2,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; I want to be able to run the best and baddest models\n\n&gt; I’m looking to spend about $8 to $10K\n\nsorry, it's impossible. For the \"baddest\" models you need hundreds of thousands, with merely 10k you'll be able to run only basic models.\n\nA rough estimation: you could run local models with the same amount of \"B\"s as \"GB\"s memory in a GPU, so with 10k you could get one RTX PRO 6000 96GB VRAM and will be able to run up to 96B models (72B realistically), or up to 192B in a low quality. While this is better than that tiny 30B model everyone is talking about this is still much worse than proprietary thousands-of-Billions models and even than free DeepSeek 671B.",
            "edited": 1754037160,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bd58e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I want to be able to run the best and baddest models&lt;/p&gt;\n\n&lt;p&gt;I’m looking to spend about $8 to $10K&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;sorry, it&amp;#39;s impossible. For the &amp;quot;baddest&amp;quot; models you need hundreds of thousands, with merely 10k you&amp;#39;ll be able to run only basic models.&lt;/p&gt;\n\n&lt;p&gt;A rough estimation: you could run local models with the same amount of &amp;quot;B&amp;quot;s as &amp;quot;GB&amp;quot;s memory in a GPU, so with 10k you could get one RTX PRO 6000 96GB VRAM and will be able to run up to 96B models (72B realistically), or up to 192B in a low quality. While this is better than that tiny 30B model everyone is talking about this is still much worse than proprietary thousands-of-Billions models and even than free DeepSeek 671B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meknnb/looking_to_buybuild_a_killer_llmaimldeep_learning/n6bd58e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754036721,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meknnb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6a9g0s",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Particular_Cancel947",
                      "can_mod_post": false,
                      "created_utc": 1754017189,
                      "send_replies": true,
                      "parent_id": "t1_n6a7qlg",
                      "score": 1,
                      "author_fullname": "t2_2hgc6342",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hey, thanks so much for the good information! I love Macs. They’re so sleek and well designed, but I do have a lot of stuff that runs on windows unfortunately",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6a9g0s",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey, thanks so much for the good information! I love Macs. They’re so sleek and well designed, but I do have a lot of stuff that runs on windows unfortunately&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meknnb",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meknnb/looking_to_buybuild_a_killer_llmaimldeep_learning/n6a9g0s/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754017189,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6a7qlg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cfogrady",
            "can_mod_post": false,
            "created_utc": 1754016525,
            "send_replies": true,
            "parent_id": "t3_1meknnb",
            "score": 2,
            "author_fullname": "t2_y0abrfm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Mac Studio M3 Ultra with 512GB unified memory is probably the best for running anything. With MoE on the rise, now might be a good time to go big rather than fast. \n\nAlternatively an RTX Pro 6000 will get you plenty of speed, but be more limited to medium size models.\n\nFor clean and simple setups, I think those are the two I would choose between, but I would bet some people could advise some great multi GPU setups with crazy power demands if you want something in the middle.\n\nI'm also a noob, and going smaller on my own hardware, so I'd listen to more experienced commenters.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6a7qlg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mac Studio M3 Ultra with 512GB unified memory is probably the best for running anything. With MoE on the rise, now might be a good time to go big rather than fast. &lt;/p&gt;\n\n&lt;p&gt;Alternatively an RTX Pro 6000 will get you plenty of speed, but be more limited to medium size models.&lt;/p&gt;\n\n&lt;p&gt;For clean and simple setups, I think those are the two I would choose between, but I would bet some people could advise some great multi GPU setups with crazy power demands if you want something in the middle.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m also a noob, and going smaller on my own hardware, so I&amp;#39;d listen to more experienced commenters.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meknnb/looking_to_buybuild_a_killer_llmaimldeep_learning/n6a7qlg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754016525,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meknnb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bgmm3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Herr_Drosselmeyer",
            "can_mod_post": false,
            "created_utc": 1754038704,
            "send_replies": true,
            "parent_id": "t3_1meknnb",
            "score": 1,
            "author_fullname": "t2_1zr9gwsn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Best models at 10k isn't going to happen, at least not if you want decent speeds. Large Language Models are called that for a reason, they require huge amounts of RAM and compute.\n\nIf you want a premade, there's [https://www.hp.com/us-en/workstations/z8-fury.html](https://www.hp.com/us-en/workstations/z8-fury.html) . The page isn't up to date but that can be configured with up to 4 RTX 6000 Pro workstation GPUs, at least that's my understanding. That would give you 384 GB of VRAM and plenty of system RAM. Ballparking such a system at 70k, should be able to run all but the very largest models at usable speeds.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bgmm3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Best models at 10k isn&amp;#39;t going to happen, at least not if you want decent speeds. Large Language Models are called that for a reason, they require huge amounts of RAM and compute.&lt;/p&gt;\n\n&lt;p&gt;If you want a premade, there&amp;#39;s &lt;a href=\"https://www.hp.com/us-en/workstations/z8-fury.html\"&gt;https://www.hp.com/us-en/workstations/z8-fury.html&lt;/a&gt; . The page isn&amp;#39;t up to date but that can be configured with up to 4 RTX 6000 Pro workstation GPUs, at least that&amp;#39;s my understanding. That would give you 384 GB of VRAM and plenty of system RAM. Ballparking such a system at 70k, should be able to run all but the very largest models at usable speeds.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meknnb/looking_to_buybuild_a_killer_llmaimldeep_learning/n6bgmm3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754038704,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meknnb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bql4r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kmouratidis",
            "can_mod_post": false,
            "created_utc": 1754044186,
            "send_replies": true,
            "parent_id": "t3_1meknnb",
            "score": 1,
            "author_fullname": "t2_k6u7rfxb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Buy 4 used servers and shove 8 used MI50s each. Configure each node in the cluster as pipeline parallel, and each octet of GPUs as tensor parallel. Congrats, you can run full weights DeepSeek entirely on your 1TB VRAM. Throughput will be really great, latency... not so much.\n\nBuild cost, inference speed, model size: pick 2. Or add \"privacy\" and pick 3 by having APIs as an option.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bql4r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Buy 4 used servers and shove 8 used MI50s each. Configure each node in the cluster as pipeline parallel, and each octet of GPUs as tensor parallel. Congrats, you can run full weights DeepSeek entirely on your 1TB VRAM. Throughput will be really great, latency... not so much.&lt;/p&gt;\n\n&lt;p&gt;Build cost, inference speed, model size: pick 2. Or add &amp;quot;privacy&amp;quot; and pick 3 by having APIs as an option.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meknnb/looking_to_buybuild_a_killer_llmaimldeep_learning/n6bql4r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754044186,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meknnb",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]