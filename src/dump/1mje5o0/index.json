[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Disclaimer: I can only confidently say that this meets the Works On My Machine™ threshold, YMMV.\n\nThe wizards at Unsloth seem to have fixed the tool-calling issues that have been plaguing Qwen3-Coder-30B-A3B, see HF discussion [here](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10). Note that the .ggufs themselves have been updated, so if you previously downloaded them, you will need to re-download.\n\nI've tried this on my machine with excellent results - not a single tool call failure due to bad formatting after several hours of pure vibe coding in Roo Code. Posting my config in case it can be a useful template for others:\n\n**Hardware**  \nOS: Windows 11 24H2 (Build 26100.4770)  \nGPU: RTX 5090  \nCPU: i9-13900K  \nSystem RAM: 64GB DDR5-5600\n\n**LLM Provider**  \nLM Studio 0.3.22 (Build 1)  \nEngine: CUDA 12 llama.cpp v1.44.0\n\n**OpenAI API Endpoint**  \nOpen WebUI v0.6.18  \nRunning in Docker on a separate Debian VM\n\n**Model Config**  \nunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5\\_K\\_XL (Q6\\_K\\_XL also worked)  \nContext: 81920  \nFlash Attention: Enabled  \nKV Cache Quantization: **None** (I think this is important!)  \nPrompt: Latest from Unsloth (see [here](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/template))  \nTemperature: 0.7  \nTop-K Sampling: 20  \nRepeat Penalty: 1.05  \nMin P Sampling: 0.05  \nTop P Sampling: 0.8  \nAll other settings left at default\n\n**IDE**  \nVisual Studio Code 1.102.3  \nRoo Code v3.25.7  \n~~Using all default settings, no custom instructions~~  \nEDIT: Forgot that I enabled one Experimental feature: Background Editing. My theory is that by preventing editor windows from opening (which I believe get included in context), there is less \"irrelevant\" context for the model to get confused by.\n\nEDIT2: After further testing, I have seen occurrences of tool call failures due to bad formatting, mostly omitting required arguments. However, it has always self-resolved after a retry or two, and the occurrence rate is much lower and less \"sticky\" than previously. So still a major improvement, but not quite 100% resolved.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "PSA: Qwen3-Coder-30B-A3B tool calling fixed by Unsloth wizards",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "News"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mje5o0",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 52,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_6ncfftb",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 52,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754537668,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754508492,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I can only confidently say that this meets the Works On My Machine™ threshold, YMMV.&lt;/p&gt;\n\n&lt;p&gt;The wizards at Unsloth seem to have fixed the tool-calling issues that have been plaguing Qwen3-Coder-30B-A3B, see HF discussion &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10\"&gt;here&lt;/a&gt;. Note that the .ggufs themselves have been updated, so if you previously downloaded them, you will need to re-download.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried this on my machine with excellent results - not a single tool call failure due to bad formatting after several hours of pure vibe coding in Roo Code. Posting my config in case it can be a useful template for others:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br/&gt;\nOS: Windows 11 24H2 (Build 26100.4770)&lt;br/&gt;\nGPU: RTX 5090&lt;br/&gt;\nCPU: i9-13900K&lt;br/&gt;\nSystem RAM: 64GB DDR5-5600&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LLM Provider&lt;/strong&gt;&lt;br/&gt;\nLM Studio 0.3.22 (Build 1)&lt;br/&gt;\nEngine: CUDA 12 llama.cpp v1.44.0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OpenAI API Endpoint&lt;/strong&gt;&lt;br/&gt;\nOpen WebUI v0.6.18&lt;br/&gt;\nRunning in Docker on a separate Debian VM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model Config&lt;/strong&gt;&lt;br/&gt;\nunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL (Q6_K_XL also worked)&lt;br/&gt;\nContext: 81920&lt;br/&gt;\nFlash Attention: Enabled&lt;br/&gt;\nKV Cache Quantization: &lt;strong&gt;None&lt;/strong&gt; (I think this is important!)&lt;br/&gt;\nPrompt: Latest from Unsloth (see &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/template\"&gt;here&lt;/a&gt;)&lt;br/&gt;\nTemperature: 0.7&lt;br/&gt;\nTop-K Sampling: 20&lt;br/&gt;\nRepeat Penalty: 1.05&lt;br/&gt;\nMin P Sampling: 0.05&lt;br/&gt;\nTop P Sampling: 0.8&lt;br/&gt;\nAll other settings left at default&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;IDE&lt;/strong&gt;&lt;br/&gt;\nVisual Studio Code 1.102.3&lt;br/&gt;\nRoo Code v3.25.7&lt;br/&gt;\n&lt;del&gt;Using all default settings, no custom instructions&lt;/del&gt;&lt;br/&gt;\nEDIT: Forgot that I enabled one Experimental feature: Background Editing. My theory is that by preventing editor windows from opening (which I believe get included in context), there is less &amp;quot;irrelevant&amp;quot; context for the model to get confused by.&lt;/p&gt;\n\n&lt;p&gt;EDIT2: After further testing, I have seen occurrences of tool call failures due to bad formatting, mostly omitting required arguments. However, it has always self-resolved after a retry or two, and the occurrence rate is much lower and less &amp;quot;sticky&amp;quot; than previously. So still a major improvement, but not quite 100% resolved.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?auto=webp&amp;s=63a653cdb5e6be20957a0b02e80a91b2ee631399",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0eb6c11e7056136830a5db513d40d379d31b6add",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e23ca01cf50f75e0c9732e0ca0ea1eb21385f01b",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ad5a0aca0e3a9ccbd0c36ac271bd8bd766cda75",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3337cd00ae59cba7172fadebc6b1b88f3c899f31",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aaf83a350081412f8fcc647175d26e7ab0c3e828",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98ee440353031df341472d04c49d581ca89d9e05",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#cc3600",
            "id": "1mje5o0",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "MutantEggroll",
            "discussion_type": null,
            "num_comments": 11,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/",
            "subreddit_subscribers": 512875,
            "created_utc": 1754508492,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ct24q",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MutantEggroll",
                      "can_mod_post": false,
                      "created_utc": 1754537480,
                      "send_replies": true,
                      "parent_id": "t1_n7b95vm",
                      "score": 1,
                      "author_fullname": "t2_6ncfftb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ah that's a bummer to hear - I'd been thinking about making the jump to \"pure\" llama.cpp, but perhaps I'll hold off until things stabilize with Qwen3-Coder. Despite these tool-calling issues, I've been extremely impressed.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ct24q",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah that&amp;#39;s a bummer to hear - I&amp;#39;d been thinking about making the jump to &amp;quot;pure&amp;quot; llama.cpp, but perhaps I&amp;#39;ll hold off until things stabilize with Qwen3-Coder. Despite these tool-calling issues, I&amp;#39;ve been extremely impressed.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mje5o0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7ct24q/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754537480,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7cz9wm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Complex-Emergency-60",
                      "can_mod_post": false,
                      "created_utc": 1754540091,
                      "send_replies": true,
                      "parent_id": "t1_n7b95vm",
                      "score": 1,
                      "author_fullname": "t2_1i6lvuup5v",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Actually able to get it working with llama.cpp by adjusting this flag in Continue within VScode. - [https://i.imgur.com/FgC1CjM.png](https://i.imgur.com/FgC1CjM.png)\n\nGetting about 85 tokens a second?\n\nHere are my settings with a single 4090 and 128gb...\n\nllama bat file ---------&gt;\n\nwt -w 0 nt -d \"C:\\\\Users\\\\xxxxx\\\\Desktop\\\\llama\\_cpp\\_build\\\\llama.cpp\\\\build\\\\bin\\\\Release\" powershell -NoExit -Command \".\\\\llama-server.exe -m 'Qwen3-Coder-30B-A3B-Instruct-UD-Q4\\_K\\_XL.gguf' -c 32768 --gpu-layers 52 --threads 12 --parallel 1 --main-gpu 0 -fa --port 8000 --jinja\"\n\n\n\n\n\n\nContinue Config File-------&gt;\n\nname: Local Assistant\n\nversion: 1.0.0\n\nschema: v1\n\n\n\nmodels:\n\n  \\- name: Local LLaMA CPP\n\nprovider: openai\n\napiBase: \"http://xxxxxxxxx:8000\"\n\nmodel: \"Qwen3-Coder-30B-A3B-Instruct-UD-Q4\\_K\\_XL\"\n\ncapabilities:\n\n\\- tool\\_use\n\nroles:\n\n\\- chat\n\n\\- edit    # For editing code\n\n\\- apply   # For applying changes\n\ndefaultCompletionOptions:\n\ntemperature: 0.7\n\nmaxTokens: 4096\n\nstop: \\[\\]  # Ensure no premature stopping\n\n\n\ncontext:\n\n  \\- provider: code\n\n  \\- provider: folder\n\n  \\- provider: codebase",
                      "edited": 1754540814,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7cz9wm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Actually able to get it working with llama.cpp by adjusting this flag in Continue within VScode. - &lt;a href=\"https://i.imgur.com/FgC1CjM.png\"&gt;https://i.imgur.com/FgC1CjM.png&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Getting about 85 tokens a second?&lt;/p&gt;\n\n&lt;p&gt;Here are my settings with a single 4090 and 128gb...&lt;/p&gt;\n\n&lt;p&gt;llama bat file ---------&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;wt -w 0 nt -d &amp;quot;C:\\Users\\xxxxx\\Desktop\\llama_cpp_build\\llama.cpp\\build\\bin\\Release&amp;quot; powershell -NoExit -Command &amp;quot;.\\llama-server.exe -m &amp;#39;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf&amp;#39; -c 32768 --gpu-layers 52 --threads 12 --parallel 1 --main-gpu 0 -fa --port 8000 --jinja&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Continue Config File-------&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;name: Local Assistant&lt;/p&gt;\n\n&lt;p&gt;version: 1.0.0&lt;/p&gt;\n\n&lt;p&gt;schema: v1&lt;/p&gt;\n\n&lt;p&gt;models:&lt;/p&gt;\n\n&lt;p&gt;- name: Local LLaMA CPP&lt;/p&gt;\n\n&lt;p&gt;provider: openai&lt;/p&gt;\n\n&lt;p&gt;apiBase: &amp;quot;http://xxxxxxxxx:8000&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;model: &amp;quot;Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;capabilities:&lt;/p&gt;\n\n&lt;p&gt;- tool_use&lt;/p&gt;\n\n&lt;p&gt;roles:&lt;/p&gt;\n\n&lt;p&gt;- chat&lt;/p&gt;\n\n&lt;p&gt;- edit    # For editing code&lt;/p&gt;\n\n&lt;p&gt;- apply   # For applying changes&lt;/p&gt;\n\n&lt;p&gt;defaultCompletionOptions:&lt;/p&gt;\n\n&lt;p&gt;temperature: 0.7&lt;/p&gt;\n\n&lt;p&gt;maxTokens: 4096&lt;/p&gt;\n\n&lt;p&gt;stop: []  # Ensure no premature stopping&lt;/p&gt;\n\n&lt;p&gt;context:&lt;/p&gt;\n\n&lt;p&gt;- provider: code&lt;/p&gt;\n\n&lt;p&gt;- provider: folder&lt;/p&gt;\n\n&lt;p&gt;- provider: codebase&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mje5o0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7cz9wm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754540091,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7b95vm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "JMowery",
            "can_mod_post": false,
            "created_utc": 1754518057,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 8,
            "author_fullname": "t2_21tz0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I was participating in that discussion. Worth pointing out that it appears to have been fixed in relation to LM Studio.\n\nWith llama.cpp (which is what I primarily use), it's just as bad, if not even worse. But this seems to have way more to do with issues in how llama.cpp is doing things than anything the Unsloth teams have done (I do appreciate the work of Unsloth to try to get this going), so just keep that in mind.\n\nAlso worth pointing out that with LM Studio I get 1.75x - 2x slower performance compared to Llama.cpp, despite enabling flash attention, KV cache, etc. No idea why that is, but I definitely feel it when running the model. It also slows down dramatically as more context is added.\n\nHopefully a miracle update from llama.cpp can make it work well.",
            "edited": 1754518254,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7b95vm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was participating in that discussion. Worth pointing out that it appears to have been fixed in relation to LM Studio.&lt;/p&gt;\n\n&lt;p&gt;With llama.cpp (which is what I primarily use), it&amp;#39;s just as bad, if not even worse. But this seems to have way more to do with issues in how llama.cpp is doing things than anything the Unsloth teams have done (I do appreciate the work of Unsloth to try to get this going), so just keep that in mind.&lt;/p&gt;\n\n&lt;p&gt;Also worth pointing out that with LM Studio I get 1.75x - 2x slower performance compared to Llama.cpp, despite enabling flash attention, KV cache, etc. No idea why that is, but I definitely feel it when running the model. It also slows down dramatically as more context is added.&lt;/p&gt;\n\n&lt;p&gt;Hopefully a miracle update from llama.cpp can make it work well.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7b95vm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754518057,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ai558",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Several_Income_9912",
            "can_mod_post": false,
            "created_utc": 1754510173,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 3,
            "author_fullname": "t2_brycdlrf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "tried with\n\n    $env:LLAMA_SET_ROWS = \"1\"\n    G:\\workspace\\llama.cpp\\build\\bin\\Release\\llama-server.exe `\n    -hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL `\n    --ctx-size 64000 `\n    -ngl 99 `\n    --threads -1 `\n    --n-predict 16000 `\n    --jinja `\n    --flash-attn `\n    --top-k 20 `\n    --top-p 0.8 `\n    --temp 0.7 `\n    --min-p 0.05 `\n    --presence-penalty 1.05 `\n    --no-context-shift `\n    --n-cpu-moe 16\n    \n\nand still got a bunch of   \nError\n\nKilo Code tried to use list\\_files without value for required parameter 'path'. Retrying...  \nvery early",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ai558",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;tried with&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$env:LLAMA_SET_ROWS = &amp;quot;1&amp;quot;\nG:\\workspace\\llama.cpp\\build\\bin\\Release\\llama-server.exe `\n-hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL `\n--ctx-size 64000 `\n-ngl 99 `\n--threads -1 `\n--n-predict 16000 `\n--jinja `\n--flash-attn `\n--top-k 20 `\n--top-p 0.8 `\n--temp 0.7 `\n--min-p 0.05 `\n--presence-penalty 1.05 `\n--no-context-shift `\n--n-cpu-moe 16\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and still got a bunch of&lt;br/&gt;\nError&lt;/p&gt;\n\n&lt;p&gt;Kilo Code tried to use list_files without value for required parameter &amp;#39;path&amp;#39;. Retrying...&lt;br/&gt;\nvery early&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7ai558/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754510173,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7eje6k",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "chisleu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7crfzv",
                                "score": 1,
                                "author_fullname": "t2_cbxyn",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "lmstudio. I have a mac book pro so I can just run this locally in RAM and it uses the GPU for compute.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7eje6k",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;lmstudio. I have a mac book pro so I can just run this locally in RAM and it uses the GPU for compute.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mje5o0",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7eje6k/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754568695,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754568695,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7crfzv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MutantEggroll",
                      "can_mod_post": false,
                      "created_utc": 1754536829,
                      "send_replies": true,
                      "parent_id": "t1_n7c82ik",
                      "score": 1,
                      "author_fullname": "t2_6ncfftb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ah, I wish I could fit an 8 bit into VRAM. I have a suspicion that this model is rather susceptible to quantization. I had *really* big problems with tool calling when I initially had KV cache quantized - could barely get through 10 tool calls before it lost its brains and forgot required arguments every time.\n\nWhat provider are you using?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7crfzv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah, I wish I could fit an 8 bit into VRAM. I have a suspicion that this model is rather susceptible to quantization. I had &lt;em&gt;really&lt;/em&gt; big problems with tool calling when I initially had KV cache quantized - could barely get through 10 tool calls before it lost its brains and forgot required arguments every time.&lt;/p&gt;\n\n&lt;p&gt;What provider are you using?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mje5o0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7crfzv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754536829,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7c82ik",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "chisleu",
            "can_mod_post": false,
            "created_utc": 1754529730,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 3,
            "author_fullname": "t2_cbxyn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://preview.redd.it/e95s7n6k1ihf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=90036ecf0354a9154aa328b9ae56b3275e7548e6\n\nI'm using the 8 bit and haven't had a single issue with a tool call failing with Qwen3 coder 30b a3b",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7c82ik",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/e95s7n6k1ihf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90036ecf0354a9154aa328b9ae56b3275e7548e6\"&gt;https://preview.redd.it/e95s7n6k1ihf1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90036ecf0354a9154aa328b9ae56b3275e7548e6&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m using the 8 bit and haven&amp;#39;t had a single issue with a tool call failing with Qwen3 coder 30b a3b&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7c82ik/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754529730,
            "media_metadata": {
              "e95s7n6k1ihf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 88,
                    "x": 108,
                    "u": "https://preview.redd.it/e95s7n6k1ihf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2aabd65884317a1c617b172da4fcdcea6ecd068e"
                  },
                  {
                    "y": 176,
                    "x": 216,
                    "u": "https://preview.redd.it/e95s7n6k1ihf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=339a440514fd884fe4d5eeca17afdfac5bfc0828"
                  },
                  {
                    "y": 261,
                    "x": 320,
                    "u": "https://preview.redd.it/e95s7n6k1ihf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8baf3695f3600738440ee415ce2f933bd59637d7"
                  },
                  {
                    "y": 522,
                    "x": 640,
                    "u": "https://preview.redd.it/e95s7n6k1ihf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7f8a2c01e8394e0193c552ae70d6dc2dd79de407"
                  },
                  {
                    "y": 783,
                    "x": 960,
                    "u": "https://preview.redd.it/e95s7n6k1ihf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5906b0d8ca55210a62033c1c676b7d098ddde83e"
                  }
                ],
                "s": {
                  "y": 816,
                  "x": 1000,
                  "u": "https://preview.redd.it/e95s7n6k1ihf1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=90036ecf0354a9154aa328b9ae56b3275e7548e6"
                },
                "id": "e95s7n6k1ihf1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bynrr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MutantEggroll",
                      "can_mod_post": false,
                      "created_utc": 1754526491,
                      "send_replies": true,
                      "parent_id": "t1_n7bcizr",
                      "score": 1,
                      "author_fullname": "t2_6ncfftb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "According to the discussion, only the Unsloth quants have the fix baked in, and since the issue affects llama.cpp, I would guess that the Ollama quants would suffer from this.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bynrr",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;According to the discussion, only the Unsloth quants have the fix baked in, and since the issue affects llama.cpp, I would guess that the Ollama quants would suffer from this.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mje5o0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7bynrr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754526491,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7bcizr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jackdareel",
            "can_mod_post": false,
            "created_utc": 1754519122,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 2,
            "author_fullname": "t2_1puly589vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are the quants offered by Ollama affected?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bcizr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are the quants offered by Ollama affected?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7bcizr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754519122,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dcrch",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "redeemer_pl",
            "can_mod_post": false,
            "created_utc": 1754546653,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 1,
            "author_fullname": "t2_cgwh4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's not a real fix, but workaround forcing the model to use different tool call format (that llama.cpp handles) that is originally should use (xml instead of json formatted tool calls).\n\nThe proper fix (for llama.cpp-based workflows) is to update llama.cpp's internal tool call parsing to handle the new &lt;xml&gt; format, instead of forcing the model to use a different one.\n\nhttps://github.com/ggml-org/llama.cpp/issues/15012",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dcrch",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not a real fix, but workaround forcing the model to use different tool call format (that llama.cpp handles) that is originally should use (xml instead of json formatted tool calls).&lt;/p&gt;\n\n&lt;p&gt;The proper fix (for llama.cpp-based workflows) is to update llama.cpp&amp;#39;s internal tool call parsing to handle the new &amp;lt;xml&amp;gt; format, instead of forcing the model to use a different one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/issues/15012\"&gt;https://github.com/ggml-org/llama.cpp/issues/15012&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7dcrch/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754546653,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7dy5e3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Nicks2408",
            "can_mod_post": false,
            "created_utc": 1754558754,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 1,
            "author_fullname": "t2_8izjwnj2k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Does anyone know whether it is fixed in the mlx dwq version?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7dy5e3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does anyone know whether it is fixed in the mlx dwq version?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7dy5e3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754558754,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]