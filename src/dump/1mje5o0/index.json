[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Disclaimer: I can only confidently say that this meets the Works On My Machine™ threshold, YMMV.\n\nThe wizards at Unsloth seem to have fixed the tool-calling issues that have been plaguing Qwen3-Coder-30B-A3B, see HF discussion [here](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10). Note that the .ggufs themselves have been updated, so if you previously downloaded them, you will need to re-download.\n\nI've tried this on my machine with excellent results - not a single tool call failure due to bad formatting after several hours of pure vibe coding in Roo Code. Posting my config in case it can be a useful template for others:\n\n**Hardware**  \nOS: Windows 11 24H2 (Build 26100.4770)  \nGPU: RTX 5090  \nCPU: i9-13900K  \nSystem RAM: 64GB DDR5-5600\n\n**LLM Provider**  \nLM Studio 0.3.22 (Build 1)  \nEngine: CUDA 12 llama.cpp v1.44.0\n\n**OpenAI API Endpoint**  \nOpen WebUI v0.6.18  \nRunning in Docker on a separate Debian VM\n\n**Model Config**  \nunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5\\_K\\_XL (Q6\\_K\\_XL also worked)  \nContext: 81920  \nFlash Attention: Enabled  \nKV Cache Quantization: **None** (I think this is important!)  \nPrompt: Latest from Unsloth (see [here](https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/template))  \nTemperature: 0.7  \nTop-K Sampling: 20  \nRepeat Penalty: 1.05  \nMin P Sampling: 0.05  \nTop P Sampling: 0.8  \nAll other settings left at default\n\n**IDE**  \nVisual Studio Code 1.102.3  \nRoo Code v3.25.7  \n~~Using all default settings, no custom instructions~~  \nEDIT: Forgot that I enabled one Experimental feature: Background Editing. My theory is that by preventing editor windows from opening (which I believe get included in context), there is less \"irrelevant\" context for the model to get confused by.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "PSA: Qwen3-Coder-30B-A3B tool calling fixed by Unsloth wizards",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "News"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mje5o0",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 28,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_6ncfftb",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 28,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754510512,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754508492,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Disclaimer: I can only confidently say that this meets the Works On My Machine™ threshold, YMMV.&lt;/p&gt;\n\n&lt;p&gt;The wizards at Unsloth seem to have fixed the tool-calling issues that have been plaguing Qwen3-Coder-30B-A3B, see HF discussion &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10\"&gt;here&lt;/a&gt;. Note that the .ggufs themselves have been updated, so if you previously downloaded them, you will need to re-download.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve tried this on my machine with excellent results - not a single tool call failure due to bad formatting after several hours of pure vibe coding in Roo Code. Posting my config in case it can be a useful template for others:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br/&gt;\nOS: Windows 11 24H2 (Build 26100.4770)&lt;br/&gt;\nGPU: RTX 5090&lt;br/&gt;\nCPU: i9-13900K&lt;br/&gt;\nSystem RAM: 64GB DDR5-5600&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;LLM Provider&lt;/strong&gt;&lt;br/&gt;\nLM Studio 0.3.22 (Build 1)&lt;br/&gt;\nEngine: CUDA 12 llama.cpp v1.44.0&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OpenAI API Endpoint&lt;/strong&gt;&lt;br/&gt;\nOpen WebUI v0.6.18&lt;br/&gt;\nRunning in Docker on a separate Debian VM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model Config&lt;/strong&gt;&lt;br/&gt;\nunsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL (Q6_K_XL also worked)&lt;br/&gt;\nContext: 81920&lt;br/&gt;\nFlash Attention: Enabled&lt;br/&gt;\nKV Cache Quantization: &lt;strong&gt;None&lt;/strong&gt; (I think this is important!)&lt;br/&gt;\nPrompt: Latest from Unsloth (see &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/template\"&gt;here&lt;/a&gt;)&lt;br/&gt;\nTemperature: 0.7&lt;br/&gt;\nTop-K Sampling: 20&lt;br/&gt;\nRepeat Penalty: 1.05&lt;br/&gt;\nMin P Sampling: 0.05&lt;br/&gt;\nTop P Sampling: 0.8&lt;br/&gt;\nAll other settings left at default&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;IDE&lt;/strong&gt;&lt;br/&gt;\nVisual Studio Code 1.102.3&lt;br/&gt;\nRoo Code v3.25.7&lt;br/&gt;\n&lt;del&gt;Using all default settings, no custom instructions&lt;/del&gt;&lt;br/&gt;\nEDIT: Forgot that I enabled one Experimental feature: Background Editing. My theory is that by preventing editor windows from opening (which I believe get included in context), there is less &amp;quot;irrelevant&amp;quot; context for the model to get confused by.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?auto=webp&amp;s=63a653cdb5e6be20957a0b02e80a91b2ee631399",
                    "width": 1200,
                    "height": 648
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0eb6c11e7056136830a5db513d40d379d31b6add",
                      "width": 108,
                      "height": 58
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e23ca01cf50f75e0c9732e0ca0ea1eb21385f01b",
                      "width": 216,
                      "height": 116
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1ad5a0aca0e3a9ccbd0c36ac271bd8bd766cda75",
                      "width": 320,
                      "height": 172
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3337cd00ae59cba7172fadebc6b1b88f3c899f31",
                      "width": 640,
                      "height": 345
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aaf83a350081412f8fcc647175d26e7ab0c3e828",
                      "width": 960,
                      "height": 518
                    },
                    {
                      "url": "https://external-preview.redd.it/qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98ee440353031df341472d04c49d581ca89d9e05",
                      "width": 1080,
                      "height": 583
                    }
                  ],
                  "variants": {},
                  "id": "qFy6nLE5oHZQRyn0F8iZ9nDzvmM3NhjUiEdJcbf9cDI"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#cc3600",
            "id": "1mje5o0",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "MutantEggroll",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/",
            "subreddit_subscribers": 512426,
            "created_utc": 1754508492,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ai558",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Several_Income_9912",
            "can_mod_post": false,
            "created_utc": 1754510173,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 3,
            "author_fullname": "t2_brycdlrf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "tried with\n\n    $env:LLAMA_SET_ROWS = \"1\"\n    G:\\workspace\\llama.cpp\\build\\bin\\Release\\llama-server.exe `\n    -hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL `\n    --ctx-size 64000 `\n    -ngl 99 `\n    --threads -1 `\n    --n-predict 16000 `\n    --jinja `\n    --flash-attn `\n    --top-k 20 `\n    --top-p 0.8 `\n    --temp 0.7 `\n    --min-p 0.05 `\n    --presence-penalty 1.05 `\n    --no-context-shift `\n    --n-cpu-moe 16\n    \n\nand still got a bunch of   \nError\n\nKilo Code tried to use list\\_files without value for required parameter 'path'. Retrying...  \nvery early",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ai558",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;tried with&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$env:LLAMA_SET_ROWS = &amp;quot;1&amp;quot;\nG:\\workspace\\llama.cpp\\build\\bin\\Release\\llama-server.exe `\n-hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q5_K_XL `\n--ctx-size 64000 `\n-ngl 99 `\n--threads -1 `\n--n-predict 16000 `\n--jinja `\n--flash-attn `\n--top-k 20 `\n--top-p 0.8 `\n--temp 0.7 `\n--min-p 0.05 `\n--presence-penalty 1.05 `\n--no-context-shift `\n--n-cpu-moe 16\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;and still got a bunch of&lt;br/&gt;\nError&lt;/p&gt;\n\n&lt;p&gt;Kilo Code tried to use list_files without value for required parameter &amp;#39;path&amp;#39;. Retrying...&lt;br/&gt;\nvery early&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7ai558/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754510173,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7b95vm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "JMowery",
            "can_mod_post": false,
            "created_utc": 1754518057,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 3,
            "author_fullname": "t2_21tz0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I was participating in that discussion. Worth pointing out that it appears to have been fixed in relation to LM Studio.\n\nWith llama.cpp (which is what I primarily use), it's just as bad, if not even worse. But this seems to have way more to do with issues in how llama.cpp is doing things than anything the Unsloth teams have done (I do appreciate the work of Unsloth to try to get this going), so just keep that in mind.\n\nAlso worth pointing out that with LM Studio I get 1.75x - 2x slower performance compared to Llama.cpp, despite enabling flash attention, KV cache, etc. No idea why that is, but I definitely feel it when running the model. It also slows down dramatically as more context is added.\n\nHopefully a miracle update from llama.cpp can make it work well.",
            "edited": 1754518254,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7b95vm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was participating in that discussion. Worth pointing out that it appears to have been fixed in relation to LM Studio.&lt;/p&gt;\n\n&lt;p&gt;With llama.cpp (which is what I primarily use), it&amp;#39;s just as bad, if not even worse. But this seems to have way more to do with issues in how llama.cpp is doing things than anything the Unsloth teams have done (I do appreciate the work of Unsloth to try to get this going), so just keep that in mind.&lt;/p&gt;\n\n&lt;p&gt;Also worth pointing out that with LM Studio I get 1.75x - 2x slower performance compared to Llama.cpp, despite enabling flash attention, KV cache, etc. No idea why that is, but I definitely feel it when running the model. It also slows down dramatically as more context is added.&lt;/p&gt;\n\n&lt;p&gt;Hopefully a miracle update from llama.cpp can make it work well.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7b95vm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754518057,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bynrr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MutantEggroll",
                      "can_mod_post": false,
                      "created_utc": 1754526491,
                      "send_replies": true,
                      "parent_id": "t1_n7bcizr",
                      "score": 1,
                      "author_fullname": "t2_6ncfftb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "According to the discussion, only the Unsloth quants have the fix baked in, and since the issue affects llama.cpp, I would guess that the Ollama quants would suffer from this.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bynrr",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;According to the discussion, only the Unsloth quants have the fix baked in, and since the issue affects llama.cpp, I would guess that the Ollama quants would suffer from this.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mje5o0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7bynrr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754526491,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7bcizr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jackdareel",
            "can_mod_post": false,
            "created_utc": 1754519122,
            "send_replies": true,
            "parent_id": "t3_1mje5o0",
            "score": 2,
            "author_fullname": "t2_1puly589vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are the quants offered by Ollama affected?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bcizr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are the quants offered by Ollama affected?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mje5o0/psa_qwen3coder30ba3b_tool_calling_fixed_by/n7bcizr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754519122,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mje5o0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]