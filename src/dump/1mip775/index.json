[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hello,\n\nI'm currently considering an upgrade from my RTX 3060 Ti to the RX 7900 XT, mainly for improved performance running LLMs. My primary interest is in models like the Qwen3-30B-A3B and the newly released OpenAI GPT-OSS 20B. Since the RX 7900 XTX is almost sold out everywhere, the RX 7900 XT seems like my next best option.\n\nFor context, my current RTX 3060 Ti achieves around 20 tok/s with Qwen3-30B-A3B at Q3\\_K\\_L, and approximately 16-19 tok/s with GPT-OSS with basic short prompts in **LM Studio** with CUDA 12 llama.cpp runtime engine. Additionally, I have 32GB DDR5 RAM, which I'm planning to upgrade to 64GB or more to run these models more efficiently, and Ryzen a 5 7600X.\n\nHas anyone tested these specific models with the RX 7900 XT? If so, could you share the token generation speeds you're achieving, and whether you're using ROCm with Vulkan? Any advice or recommendations would be greatly appreciated as this information would help me finalize my decision on whether to purchase the RX 7900 XT now or wait for the availability of the RX 7900 XTX.\n\nThanks.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "RX 7900 XT for LLMs?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mip775",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ckmk0mz",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754437302,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently considering an upgrade from my RTX 3060 Ti to the RX 7900 XT, mainly for improved performance running LLMs. My primary interest is in models like the Qwen3-30B-A3B and the newly released OpenAI GPT-OSS 20B. Since the RX 7900 XTX is almost sold out everywhere, the RX 7900 XT seems like my next best option.&lt;/p&gt;\n\n&lt;p&gt;For context, my current RTX 3060 Ti achieves around 20 tok/s with Qwen3-30B-A3B at Q3_K_L, and approximately 16-19 tok/s with GPT-OSS with basic short prompts in &lt;strong&gt;LM Studio&lt;/strong&gt; with CUDA 12 llama.cpp runtime engine. Additionally, I have 32GB DDR5 RAM, which I&amp;#39;m planning to upgrade to 64GB or more to run these models more efficiently, and Ryzen a 5 7600X.&lt;/p&gt;\n\n&lt;p&gt;Has anyone tested these specific models with the RX 7900 XT? If so, could you share the token generation speeds you&amp;#39;re achieving, and whether you&amp;#39;re using ROCm with Vulkan? Any advice or recommendations would be greatly appreciated as this information would help me finalize my decision on whether to purchase the RX 7900 XT now or wait for the availability of the RX 7900 XTX.&lt;/p&gt;\n\n&lt;p&gt;Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mip775",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Reaper_9382",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mip775/rx_7900_xt_for_llms/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mip775/rx_7900_xt_for_llms/",
            "subreddit_subscribers": 511363,
            "created_utc": 1754437302,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n753trf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Reaper_9382",
                      "can_mod_post": false,
                      "created_utc": 1754438233,
                      "send_replies": true,
                      "parent_id": "t1_n7535l1",
                      "score": 2,
                      "author_fullname": "t2_ckmk0mz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's impressive! Thanks for sharing your benchmark.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n753trf",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s impressive! Thanks for sharing your benchmark.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mip775",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mip775/rx_7900_xt_for_llms/n753trf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754438233,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7535l1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "vvimpcrvsh",
            "can_mod_post": false,
            "created_utc": 1754438007,
            "send_replies": true,
            "parent_id": "t3_1mip775",
            "score": 2,
            "author_fullname": "t2_ea6q7od",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here are those two models benchmarked on my 7900 XT:\n\n```\n$ llama-bench -m gpt-oss-20b-mxfp4.gguf -fa 1\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XT, gfx1100 (0x1100), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | ROCm       |  99 |  1 |           pp512 |       1337.99 ± 4.44 |\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | ROCm       |  99 |  1 |           tg128 |        120.25 ± 0.02 |\n\nbuild: 9515c613 (6097)\n\n$ llama-bench -m qwen3\\:30ba3b-instruct-2507-iq4_XS.gguf -fa 1\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XT, gfx1100 (0x1100), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen3moe 30B.A3B IQ4_XS - 4.25 bpw |  15.32 GiB |    30.53 B | ROCm       |  99 |  1 |           pp512 |      1255.71 ± 13.54 |\n| qwen3moe 30B.A3B IQ4_XS - 4.25 bpw |  15.32 GiB |    30.53 B | ROCm       |  99 |  1 |           tg128 |         89.67 ± 0.04 |\n\nbuild: 9515c613 (6097)\n```",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7535l1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here are those two models benchmarked on my 7900 XT:&lt;/p&gt;\n\n&lt;p&gt;```\n$ llama-bench -m gpt-oss-20b-mxfp4.gguf -fa 1\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XT, gfx1100 (0x1100), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | ROCm       |  99 |  1 |           pp512 |       1337.99 ± 4.44 |\n| gpt-oss ?B MXFP4 MoE           |  11.27 GiB |    20.91 B | ROCm       |  99 |  1 |           tg128 |        120.25 ± 0.02 |&lt;/p&gt;\n\n&lt;p&gt;build: 9515c613 (6097)&lt;/p&gt;\n\n&lt;p&gt;$ llama-bench -m qwen3:30ba3b-instruct-2507-iq4_XS.gguf -fa 1\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 ROCm devices:\n  Device 0: AMD Radeon RX 7900 XT, gfx1100 (0x1100), VMM: no, Wave Size: 32\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\n| qwen3moe 30B.A3B IQ4_XS - 4.25 bpw |  15.32 GiB |    30.53 B | ROCm       |  99 |  1 |           pp512 |      1255.71 ± 13.54 |\n| qwen3moe 30B.A3B IQ4_XS - 4.25 bpw |  15.32 GiB |    30.53 B | ROCm       |  99 |  1 |           tg128 |         89.67 ± 0.04 |&lt;/p&gt;\n\n&lt;p&gt;build: 9515c613 (6097)\n```&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mip775/rx_7900_xt_for_llms/n7535l1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754438007,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mip775",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n757wy1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DeProgrammer99",
            "can_mod_post": false,
            "created_utc": 1754439622,
            "send_replies": true,
            "parent_id": "t3_1mip775",
            "score": 1,
            "author_fullname": "t2_w4j8t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here are benchmarks on my RX 7900 XTX and RTX 4060 Ti for Qwen3-14B-UD-Q5\\_K\\_XL, same CPU and settings and all:\n\nRX 7900 XTX: pp 556.5 t/s, tg 46.9 t/s\n\nRTX 4060 Ti: pp 661.5 t/s, tg 22.3 t/s\n\nThat's with the same 4528 token prompt and 10,291 tokens generated (though I had the context window set to 8192).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n757wy1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here are benchmarks on my RX 7900 XTX and RTX 4060 Ti for Qwen3-14B-UD-Q5_K_XL, same CPU and settings and all:&lt;/p&gt;\n\n&lt;p&gt;RX 7900 XTX: pp 556.5 t/s, tg 46.9 t/s&lt;/p&gt;\n\n&lt;p&gt;RTX 4060 Ti: pp 661.5 t/s, tg 22.3 t/s&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s with the same 4528 token prompt and 10,291 tokens generated (though I had the context window set to 8192).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mip775/rx_7900_xt_for_llms/n757wy1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754439622,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mip775",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n75ajym",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ForsookComparison",
            "can_mod_post": false,
            "created_utc": 1754440533,
            "send_replies": true,
            "parent_id": "t3_1mip775",
            "score": 1,
            "author_fullname": "t2_on5es7pe3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; Qwen3-30B-A3B and the newly released OpenAI GPT-OSS 20B\n\nThe 7900xt gets you 20GB at 800GB/s and respectable prompt-processing. Should be a good time.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75ajym",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Qwen3-30B-A3B and the newly released OpenAI GPT-OSS 20B&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The 7900xt gets you 20GB at 800GB/s and respectable prompt-processing. Should be a good time.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mip775/rx_7900_xt_for_llms/n75ajym/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754440533,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mip775",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]