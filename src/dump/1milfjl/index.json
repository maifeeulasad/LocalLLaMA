[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi everyone!\n\nI was super excited for this brand new model from OpenAI and I wanted to run it on my following specs:\n\nOS: Windows 10 64bit\n\nSoftware: LM Studio 0.3.24 b4\n\nOS RAM: 16 GB\n\nGPU VRAM: 8 GB (this is AMD GPU RX Vega 56)\n\nInference engine: Vulkan / CPU.\n\nNormally I can run Qwen 30B A3B MoE models just fine, so I was quite surprised to find out that I can't really run this much smaller 20B model the same way on Vulkan inference engine!\n\nI was starting to lose hope, but then I decided to try the last resort - switching from glorious Vulkan inference engine to just CPU inference. That means saying goodbye to offloading some layers of the model to GPU for inference boost, but surprisingly switching to CPU only actually solved the problem!\n\nSo if you're like me, struggling to make this work with your GPU, please go to your \"Mission Control\" settings (Ctrl / Cmd + Shift + R), click the Runtime tab (see #1 on the attached screenshot). Make sure to download the latest versions of the runtimes (hit that Refresh button and then the green Download button for each inference engine that needs an update). Next, switch from Vulkan (or whatever GPU enabled engine you were using before) to CPU inference (see #2 on the attached screenshot). Next time you load the model, it should load properly, as long as you have enough OS RAM. Since this model requires a lot of memory, it's best to run it with at least 16 GB of RAM, otherwise you're risking that some part of the model will be loaded into the swap file on your hard drive which will make the inference most likely slower.\n\nWith that said, I'd really like to thank to both llama.cpp developers and LM Studio developers for adding support for this new model very early, but I'd also like to ask for further improvements of the support for this model, so that we could also use the Vulkan inference for offloading into the GPU.\n\nI know some people said that CPU inference on MoE models is faster, but being able to use that extra memory on my GPU on Vulkan inference engine would make all the difference for me. If for nothing else, at least I would be able to use larger context window.\n\nThanks everyone and good luck, have fun!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "OpenAI's GPT-OSS 20B in LM Studio is a bit tricky, but I finally made it work, here's how I did it...",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 83,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1milfjl",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.73,
            "author_flair_background_color": null,
            "ups": 7,
            "domain": "i.redd.it",
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_qz1qjc86",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/nlCPFbUvFibO3-lFvBJ_G03bkAHmafU8kw3Ri7f2TtI.jpg",
            "author_cakeday": true,
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754428065,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt;\n\n&lt;p&gt;I was super excited for this brand new model from OpenAI and I wanted to run it on my following specs:&lt;/p&gt;\n\n&lt;p&gt;OS: Windows 10 64bit&lt;/p&gt;\n\n&lt;p&gt;Software: LM Studio 0.3.24 b4&lt;/p&gt;\n\n&lt;p&gt;OS RAM: 16 GB&lt;/p&gt;\n\n&lt;p&gt;GPU VRAM: 8 GB (this is AMD GPU RX Vega 56)&lt;/p&gt;\n\n&lt;p&gt;Inference engine: Vulkan / CPU.&lt;/p&gt;\n\n&lt;p&gt;Normally I can run Qwen 30B A3B MoE models just fine, so I was quite surprised to find out that I can&amp;#39;t really run this much smaller 20B model the same way on Vulkan inference engine!&lt;/p&gt;\n\n&lt;p&gt;I was starting to lose hope, but then I decided to try the last resort - switching from glorious Vulkan inference engine to just CPU inference. That means saying goodbye to offloading some layers of the model to GPU for inference boost, but surprisingly switching to CPU only actually solved the problem!&lt;/p&gt;\n\n&lt;p&gt;So if you&amp;#39;re like me, struggling to make this work with your GPU, please go to your &amp;quot;Mission Control&amp;quot; settings (Ctrl / Cmd + Shift + R), click the Runtime tab (see #1 on the attached screenshot). Make sure to download the latest versions of the runtimes (hit that Refresh button and then the green Download button for each inference engine that needs an update). Next, switch from Vulkan (or whatever GPU enabled engine you were using before) to CPU inference (see #2 on the attached screenshot). Next time you load the model, it should load properly, as long as you have enough OS RAM. Since this model requires a lot of memory, it&amp;#39;s best to run it with at least 16 GB of RAM, otherwise you&amp;#39;re risking that some part of the model will be loaded into the swap file on your hard drive which will make the inference most likely slower.&lt;/p&gt;\n\n&lt;p&gt;With that said, I&amp;#39;d really like to thank to both llama.cpp developers and LM Studio developers for adding support for this new model very early, but I&amp;#39;d also like to ask for further improvements of the support for this model, so that we could also use the Vulkan inference for offloading into the GPU.&lt;/p&gt;\n\n&lt;p&gt;I know some people said that CPU inference on MoE models is faster, but being able to use that extra memory on my GPU on Vulkan inference engine would make all the difference for me. If for nothing else, at least I would be able to use larger context window.&lt;/p&gt;\n\n&lt;p&gt;Thanks everyone and good luck, have fun!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/gysfnhl7l9hf1.png",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/gysfnhl7l9hf1.png?auto=webp&amp;s=ea078de3ffb58e5e6f92651fad9dbae87b091f68",
                    "width": 1332,
                    "height": 791
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/gysfnhl7l9hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0c8b87b7c2e75094fd0bbec8080a4bd0a49af6fb",
                      "width": 108,
                      "height": 64
                    },
                    {
                      "url": "https://preview.redd.it/gysfnhl7l9hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=61b42ec9b40cc45d271f93e2cedc4e2782df26ce",
                      "width": 216,
                      "height": 128
                    },
                    {
                      "url": "https://preview.redd.it/gysfnhl7l9hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d054707531f8f89f3d308f1d3925a278eb3fedac",
                      "width": 320,
                      "height": 190
                    },
                    {
                      "url": "https://preview.redd.it/gysfnhl7l9hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc4f4237497d84c2478f5e704e3e5d9c995a5aae",
                      "width": 640,
                      "height": 380
                    },
                    {
                      "url": "https://preview.redd.it/gysfnhl7l9hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=78388852f99c897c7c47cca8718a39c38f0df04a",
                      "width": 960,
                      "height": 570
                    },
                    {
                      "url": "https://preview.redd.it/gysfnhl7l9hf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=82c52cb6771a023d45ba3f5f8e347d7ace751185",
                      "width": 1080,
                      "height": 641
                    }
                  ],
                  "variants": {},
                  "id": "HeoI1wjq_lY0awE4-qKX8GkDO4fDbEbRfCuNkMGyMn8"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1milfjl",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Cool-Chemical-5629",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1milfjl/openais_gptoss_20b_in_lm_studio_is_a_bit_tricky/",
            "stickied": false,
            "url": "https://i.redd.it/gysfnhl7l9hf1.png",
            "subreddit_subscribers": 511364,
            "created_utc": 1754428065,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n74awnn",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "dionysio211",
                      "can_mod_post": false,
                      "created_utc": 1754428707,
                      "send_replies": true,
                      "parent_id": "t1_n749zi9",
                      "score": 1,
                      "author_fullname": "t2_k6vnr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I haven't been able to run it on Vulkan with anything other than the default context limit and I have 46GB of VRAM on that system. It runs super great without modifying that though, but there are parsing issues with the new harmony system that need to be resolved.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74awnn",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t been able to run it on Vulkan with anything other than the default context limit and I have 46GB of VRAM on that system. It runs super great without modifying that though, but there are parsing issues with the new harmony system that need to be resolved.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1milfjl",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1milfjl/openais_gptoss_20b_in_lm_studio_is_a_bit_tricky/n74awnn/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754428707,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n749zi9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "custodiam99",
            "can_mod_post": false,
            "created_utc": 1754428412,
            "send_replies": true,
            "parent_id": "t3_1milfjl",
            "score": 1,
            "author_fullname": "t2_nqnhgqqf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It runs on CUDA, but ROCm is not supported yet. Vulkan works, but it can't use the system memory too.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n749zi9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It runs on CUDA, but ROCm is not supported yet. Vulkan works, but it can&amp;#39;t use the system memory too.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1milfjl/openais_gptoss_20b_in_lm_studio_is_a_bit_tricky/n749zi9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754428412,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1milfjl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n74dq1a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mfwl",
            "can_mod_post": false,
            "created_utc": 1754429612,
            "send_replies": true,
            "parent_id": "t3_1milfjl",
            "score": 1,
            "author_fullname": "t2_s8990",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Running fine on linux on my Ryzen AI 350 in GPU/Vulkan mode.  64GB RAM btw.  LM Studio has to be updated, earlier today the download link was wrong.  You should be on 0.3.21 build 4.\n\nI'm getting 22-23 TPS for the first chat, has trailed off to 15 TPS w/ 21s TTFT after 6k context.\n\nLoading the model with more than 16k context crashes (model cannot load).  I can easily run Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf (unsloth) at 100k, and TTFT stays around .8 second at 7k context.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74dq1a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Running fine on linux on my Ryzen AI 350 in GPU/Vulkan mode.  64GB RAM btw.  LM Studio has to be updated, earlier today the download link was wrong.  You should be on 0.3.21 build 4.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting 22-23 TPS for the first chat, has trailed off to 15 TPS w/ 21s TTFT after 6k context.&lt;/p&gt;\n\n&lt;p&gt;Loading the model with more than 16k context crashes (model cannot load).  I can easily run Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-Q6_K.gguf (unsloth) at 100k, and TTFT stays around .8 second at 7k context.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1milfjl/openais_gptoss_20b_in_lm_studio_is_a_bit_tricky/n74dq1a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754429612,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1milfjl",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]