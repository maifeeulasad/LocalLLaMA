[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’m working on a science project at a University of Applied Sciences. We plan to purchase a server with an NVIDIA H200 GPU. This system will host LLM services for students.\n\nFor development purposes, we’d like to have a second system where speed isn’t critical, but it should still be capable of running the same models we plan to use in production (probably up to 70B parameters). We don’t have the budget to simply replicate the production system — ideally, the dev system should be under €10k.\n\nMy research led me to the NVIDIA DGX Spark and similar solutions from other vendors, but none of the resellers I contacted had any idea when these systems will be available. (Paper launch?)\n\nI also found the GMKtec EVO-X2, which seems to be the AMD equivalent of the Spark. It’s cheap and available, but I don’t have any experience with ROCm, and developing on an AMD machine for a CUDA-based production system seems like an odd choice. On the other hand, we don’t plan to develop at the CUDA level, but rather focus on pipelines and orchestration.\n\nA third option would be to build a system with a few older cards like K40s or something similar.\n\nWhat would you advise?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Affordable dev system (spark alternative?)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lo35gq",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.63,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_9mg5e1cr",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1751275859,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m working on a science project at a University of Applied Sciences. We plan to purchase a server with an NVIDIA H200 GPU. This system will host LLM services for students.&lt;/p&gt;\n\n&lt;p&gt;For development purposes, we’d like to have a second system where speed isn’t critical, but it should still be capable of running the same models we plan to use in production (probably up to 70B parameters). We don’t have the budget to simply replicate the production system — ideally, the dev system should be under €10k.&lt;/p&gt;\n\n&lt;p&gt;My research led me to the NVIDIA DGX Spark and similar solutions from other vendors, but none of the resellers I contacted had any idea when these systems will be available. (Paper launch?)&lt;/p&gt;\n\n&lt;p&gt;I also found the GMKtec EVO-X2, which seems to be the AMD equivalent of the Spark. It’s cheap and available, but I don’t have any experience with ROCm, and developing on an AMD machine for a CUDA-based production system seems like an odd choice. On the other hand, we don’t plan to develop at the CUDA level, but rather focus on pipelines and orchestration.&lt;/p&gt;\n\n&lt;p&gt;A third option would be to build a system with a few older cards like K40s or something similar.&lt;/p&gt;\n\n&lt;p&gt;What would you advise?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lo35gq",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "_camera_up",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/",
            "subreddit_subscribers": 492928,
            "created_utc": 1751275859,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n0k39kx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1751281657,
            "send_replies": true,
            "parent_id": "t3_1lo35gq",
            "score": 1,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why not get some laptops with the RTX 5090? Those come with 24GB of VRAM. Not exactly 70B territory (unless you're fine with Q2/iQ2 quants), but that's probably the easiest way to have an integrated solution with as close CUDA-features support as the H200.\n\nAlternatively, build a desktop with a desktop 5090. Will probably cost the same as the laptop and have better performance and more VRAM (32GB vs 24GB). The only question is availability to buy as a whole system with warranty and support for the university, which will greatly depend on where you live.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0k39kx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why not get some laptops with the RTX 5090? Those come with 24GB of VRAM. Not exactly 70B territory (unless you&amp;#39;re fine with Q2/iQ2 quants), but that&amp;#39;s probably the easiest way to have an integrated solution with as close CUDA-features support as the H200.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, build a desktop with a desktop 5090. Will probably cost the same as the laptop and have better performance and more VRAM (32GB vs 24GB). The only question is availability to buy as a whole system with warranty and support for the university, which will greatly depend on where you live.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0k39kx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751281657,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lo35gq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n0k96ex",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Ok_Hope_4007",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n0k3gr2",
                                "score": 1,
                                "author_fullname": "t2_pjzish3n",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I would disagree. It just depends on what your development focus is.\nThe only major difference is the inference engine for your llm. You can ground your llm service stack on an openai compatible inference endpoint which could be llama.cpp on mac and llama.cpp/vllm/slang etc on your linux h200 server or even a third party subscription...\n\nBut i assume that the actual 'development' is the pipeline/services that define what you use the LLm for and this stack is most likely built on top of some kind of framework and custom code combination which i see not being any different on mac than linux.\n\nI suggested this as an alternative because one could develop your service stack AND host a variety of LLMs on a single machine. Once you are happy you would swap out the api_url from a slow mac to a fast h200. \n\nBut you are right if the majority of your focus is on how to setup/configure  a runtime environment for the llm.",
                                "edited": 1751284668,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n0k96ex",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would disagree. It just depends on what your development focus is.\nThe only major difference is the inference engine for your llm. You can ground your llm service stack on an openai compatible inference endpoint which could be llama.cpp on mac and llama.cpp/vllm/slang etc on your linux h200 server or even a third party subscription...&lt;/p&gt;\n\n&lt;p&gt;But i assume that the actual &amp;#39;development&amp;#39; is the pipeline/services that define what you use the LLm for and this stack is most likely built on top of some kind of framework and custom code combination which i see not being any different on mac than linux.&lt;/p&gt;\n\n&lt;p&gt;I suggested this as an alternative because one could develop your service stack AND host a variety of LLMs on a single machine. Once you are happy you would swap out the api_url from a slow mac to a fast h200. &lt;/p&gt;\n\n&lt;p&gt;But you are right if the majority of your focus is on how to setup/configure  a runtime environment for the llm.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lo35gq",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0k96ex/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751284363,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751284363,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n0k3gr2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FullstackSensei",
                      "can_mod_post": false,
                      "created_utc": 1751281754,
                      "send_replies": true,
                      "parent_id": "t1_n0junq6",
                      "score": 1,
                      "author_fullname": "t2_17n3nqtj56",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "OP is literally saying they want a development system for an H200 production system. Buying a mac means literally everything is different.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n0k3gr2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;OP is literally saying they want a development system for an H200 production system. Buying a mac means literally everything is different.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lo35gq",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0k3gr2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751281754,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n0junq6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ok_Hope_4007",
            "can_mod_post": false,
            "created_utc": 1751277027,
            "send_replies": true,
            "parent_id": "t3_1lo35gq",
            "score": 0,
            "author_fullname": "t2_pjzish3n",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Have you considered a Mac Studio M4/M3 ? If you are not relying on fiddeling with CUDA and need to run LLMs for Prototyping/Development then these will fit perfectly iny opinion. The 96/128GB variant will probably be sufficient and most likely within your budget. \nOf course prompt processing is relatively slow but that might not be an issue on a development machine. \nI like to link to the llama.cpp [Benchmark](https://github.com/ggml-org/llama.cpp/discussions/4167) \nIt will give you at least a hint on a baseline of llm performance for different macs.\n\nEDIT\n\n[This post](https://www.reddit.com/r/LocalLLaMA/s/sclrqRR4F3) lists performance for larger llms and an m4 max chip.",
            "edited": 1751277292,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0junq6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Have you considered a Mac Studio M4/M3 ? If you are not relying on fiddeling with CUDA and need to run LLMs for Prototyping/Development then these will fit perfectly iny opinion. The 96/128GB variant will probably be sufficient and most likely within your budget. \nOf course prompt processing is relatively slow but that might not be an issue on a development machine. \nI like to link to the llama.cpp &lt;a href=\"https://github.com/ggml-org/llama.cpp/discussions/4167\"&gt;Benchmark&lt;/a&gt; \nIt will give you at least a hint on a baseline of llm performance for different macs.&lt;/p&gt;\n\n&lt;p&gt;EDIT&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/sclrqRR4F3\"&gt;This post&lt;/a&gt; lists performance for larger llms and an m4 max chip.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lo35gq/affordable_dev_system_spark_alternative/n0junq6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751277027,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lo35gq",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]