[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "hey! I've been using ollama models locally across my devices for a few months now. Particularly on my M2 Mac mini - although it's the base model with only 8GB of RAM. I've been using ollama since they provide an easy-to-use web interface to see the models, quickly download them, and run them, but also many other apps/clients for LLMs support it.\n\nHowever, recently I've seen stuff like MLX-LM and llama-cpp (?) that are supposedly quicker than Ollama. Not too sure on the details, but I think I get a grasp, just that the models are architecturally different? \n\nAnyways, I'd appreciate some help to get the most out of my low-end hardware? as I mentioned above I have that Mac, but also this laptop with 16GB of RAM and some crappy CPU (&amp; integrated GPU).\n\n[My laptop specs after running Neofetch on Nobara linux.](https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;format=png&amp;auto=webp&amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111)\n\n  \n\n\nI've looked around HuggingFace before, but found the UI very confusing lol. \n\nAppreciate any help!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Mediocre local LLM user -- tips?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 32,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "kfs4he9t5vff1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 25,
                    "x": 108,
                    "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=26bc242a070779a5a8a624284d0cc720b0397d13"
                  },
                  {
                    "y": 50,
                    "x": 216,
                    "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f73808e9ed0bd733c2c1656c5053a189a08b824c"
                  },
                  {
                    "y": 74,
                    "x": 320,
                    "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c3849fd1e8d5c94f4cd0c9a7f6f63d361fe151f4"
                  }
                ],
                "s": {
                  "y": 118,
                  "x": 507,
                  "u": "https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;format=png&amp;auto=webp&amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111"
                },
                "id": "kfs4he9t5vff1"
              }
            },
            "name": "t3_1mckrn1",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1lavzg2ok9",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/Lj_WeVpZphKOlxbkqLqEF6lQTf5fzZRORkn5Z1CRnwY.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753816824,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;hey! I&amp;#39;ve been using ollama models locally across my devices for a few months now. Particularly on my M2 Mac mini - although it&amp;#39;s the base model with only 8GB of RAM. I&amp;#39;ve been using ollama since they provide an easy-to-use web interface to see the models, quickly download them, and run them, but also many other apps/clients for LLMs support it.&lt;/p&gt;\n\n&lt;p&gt;However, recently I&amp;#39;ve seen stuff like MLX-LM and llama-cpp (?) that are supposedly quicker than Ollama. Not too sure on the details, but I think I get a grasp, just that the models are architecturally different? &lt;/p&gt;\n\n&lt;p&gt;Anyways, I&amp;#39;d appreciate some help to get the most out of my low-end hardware? as I mentioned above I have that Mac, but also this laptop with 16GB of RAM and some crappy CPU (&amp;amp; integrated GPU).&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/kfs4he9t5vff1.png?width=507&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b38f98521e717e55e16ec4d0eb2258d7e196111\"&gt;My laptop specs after running Neofetch on Nobara linux.&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve looked around HuggingFace before, but found the UI very confusing lol. &lt;/p&gt;\n\n&lt;p&gt;Appreciate any help!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mckrn1",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Junior-Ad-2186",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/",
            "subreddit_subscribers": 506973,
            "created_utc": 1753816824,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5w66zi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1753834922,
            "send_replies": true,
            "parent_id": "t3_1mckrn1",
            "score": 2,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try LM Studio. It's the easiest way to run local models. It automatically detects your hardware and advice you what hugging face models are best for it. You just need to read. It's very simple. We're all learning AI. Some more advanced, some beginners, but that doesn't matter, the important thing is that every day you improve your knowledge. AI models and technology are becoming too simple that in one year all these difficult tools will be integrated, and you will only use them. On a not so distant future, we  will be all users, anyway. üôèüëçüí•üëå",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5w66zi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try LM Studio. It&amp;#39;s the easiest way to run local models. It automatically detects your hardware and advice you what hugging face models are best for it. You just need to read. It&amp;#39;s very simple. We&amp;#39;re all learning AI. Some more advanced, some beginners, but that doesn&amp;#39;t matter, the important thing is that every day you improve your knowledge. AI models and technology are becoming too simple that in one year all these difficult tools will be integrated, and you will only use them. On a not so distant future, we  will be all users, anyway. üôèüëçüí•üëå&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/n5w66zi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753834922,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mckrn1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5umu1x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1753818031,
            "send_replies": true,
            "parent_id": "t3_1mckrn1",
            "score": 1,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ollama is based on llama.cpp so they support the same models except for vision adapters which have a different format. Llama.cpp is much more optimized because it's on the bleeding edge while ollama's version lags behind, among other reasons. Another project based on llama.cpp but is kept much more up to date is KoboldCPP. It's also easy to run with any GGUF you download, while with ollama you're limited by the models in their repository (unless you add the model manually in some way that is not exactly easy).\n\nFor downloading models in huggingface, make sure to click in \"quantizations\" on the side bar on the right, to find GGUFs of the model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5umu1x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama is based on llama.cpp so they support the same models except for vision adapters which have a different format. Llama.cpp is much more optimized because it&amp;#39;s on the bleeding edge while ollama&amp;#39;s version lags behind, among other reasons. Another project based on llama.cpp but is kept much more up to date is KoboldCPP. It&amp;#39;s also easy to run with any GGUF you download, while with ollama you&amp;#39;re limited by the models in their repository (unless you add the model manually in some way that is not exactly easy).&lt;/p&gt;\n\n&lt;p&gt;For downloading models in huggingface, make sure to click in &amp;quot;quantizations&amp;quot; on the side bar on the right, to find GGUFs of the model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/n5umu1x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753818031,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mckrn1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ukpmr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "chisleu",
            "can_mod_post": false,
            "created_utc": 1753817417,
            "send_replies": true,
            "parent_id": "t3_1mckrn1",
            "score": 1,
            "author_fullname": "t2_cbxyn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "LM Studio is The Way. It will get you up and running on the mac no problem. You are extremely limited to what models you can run though.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ukpmr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LM Studio is The Way. It will get you up and running on the mac no problem. You are extremely limited to what models you can run though.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mckrn1/mediocre_local_llm_user_tips/n5ukpmr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753817417,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mckrn1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]