[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Couldn't find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.\n\nI ran a quick benchmark on the RTX 5060 Ti 16GB, and I'm quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I've also included the RTX 4090 results for a broader comparison.\nI'm also planning to run some fine-tuning benchmarks later.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Benchmarking Qwen3 8B Inference: M1 vs RTX 5060 Ti 16 vs RTX 4090",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 57,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfnq2r",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.82,
            "author_flair_background_color": null,
            "ups": 55,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_a85vzco5",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 55,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/YkVLBy_Uabot6hZJtvt-2lgE4cyPGdL7zFQtJSHp-Xk.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754132261,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Couldn&amp;#39;t find a direct comparison between the M1 Macbook pro and the new RTX 5060 Ti for local LLM inference. So, I decided to run a 16 small benchmark myself, and I think the results will be useful for others in the same boat.&lt;/p&gt;\n\n&lt;p&gt;I ran a quick benchmark on the RTX 5060 Ti 16GB, and I&amp;#39;m quite impressed with the results, especially coming from my M1 Macbook pro with 16GB ram. \nI used the Qwen3 8B model with Ollama to test the performance, and I&amp;#39;ve also included the RTX 4090 results for a broader comparison.\nI&amp;#39;m also planning to run some fine-tuning benchmarks later.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/erib4a6t7lgf1.jpeg",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?auto=webp&amp;s=5c7238d3074b0c94ed28a56fd05d130ca36e4062",
                    "width": 1280,
                    "height": 525
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8aa6f94f1fd0b07761158c32a4f411fea4ff01e",
                      "width": 108,
                      "height": 44
                    },
                    {
                      "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4fd7073605b2305ef0bdfa02c28ed14504e5cadf",
                      "width": 216,
                      "height": 88
                    },
                    {
                      "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f51b098c5db731c8b7427cd4c5e5918907eaf170",
                      "width": 320,
                      "height": 131
                    },
                    {
                      "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=78f7c7660b84535a6ababa69d8821a1d6acfd96f",
                      "width": 640,
                      "height": 262
                    },
                    {
                      "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=4f3fce6514c1c2d99e8304633aabf8e744412a87",
                      "width": 960,
                      "height": 393
                    },
                    {
                      "url": "https://preview.redd.it/erib4a6t7lgf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4087fbfd9cd44fbf60685b1fe8cf9814b669d42c",
                      "width": 1080,
                      "height": 442
                    }
                  ],
                  "variants": {},
                  "id": "tc9bMPtFnkPdsr6iuVPGSgsfnHwUyG7fExgbegYm3H4"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mfnq2r",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "kargafe",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/",
            "stickied": false,
            "url": "https://i.redd.it/erib4a6t7lgf1.jpeg",
            "subreddit_subscribers": 509054,
            "created_utc": 1754132261,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6j74ve",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "tmvr",
            "can_mod_post": false,
            "created_utc": 1754145413,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 21,
            "author_fullname": "t2_11qlhv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This makes perfect sense for token generation:\n\nM1 = 68GB/s  \n5060Ti = 448GB/s  \n4090 = 1008GB/s\n\nSo if 11.4 = 100% then 448/68 = 6.6x and 11.4 x 6.6 = 75 which is roughly what you get with the 5060Ti. The 4090 has 2.25x on the 5060Ti so ideal scaling would be 164 tok/s on the 4090, but with small models like the Q3 8B here the efficiency is not full with the 4090.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6j74ve",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This makes perfect sense for token generation:&lt;/p&gt;\n\n&lt;p&gt;M1 = 68GB/s&lt;br/&gt;\n5060Ti = 448GB/s&lt;br/&gt;\n4090 = 1008GB/s&lt;/p&gt;\n\n&lt;p&gt;So if 11.4 = 100% then 448/68 = 6.6x and 11.4 x 6.6 = 75 which is roughly what you get with the 5060Ti. The 4090 has 2.25x on the 5060Ti so ideal scaling would be 164 tok/s on the 4090, but with small models like the Q3 8B here the efficiency is not full with the 4090.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6j74ve/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754145413,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 21
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6jnc8f",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OmarBessa",
                      "can_mod_post": false,
                      "created_utc": 1754150565,
                      "send_replies": true,
                      "parent_id": "t1_n6ivllg",
                      "score": 1,
                      "author_fullname": "t2_guxix",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "nice website",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6jnc8f",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;nice website&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfnq2r",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6jnc8f/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754150565,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6lo8gt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Shoddy-Tutor9563",
                      "can_mod_post": false,
                      "created_utc": 1754174583,
                      "send_replies": true,
                      "parent_id": "t1_n6ivllg",
                      "score": 1,
                      "author_fullname": "t2_56opwsoz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Idk how trustworthy this site is.\nI'm seeing very odd reports there, like:\n- models without names\n- 5090 being slower than 4090 on the same models",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6lo8gt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Idk how trustworthy this site is.\nI&amp;#39;m seeing very odd reports there, like:\n- models without names\n- 5090 being slower than 4090 on the same models&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfnq2r",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6lo8gt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754174583,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ivllg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "FullOf_Bad_Ideas",
            "can_mod_post": false,
            "created_utc": 1754141417,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 6,
            "author_fullname": "t2_9s7pmakgx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can use LocalScore AI benchmark to compare results on those.\n\n5060 Ti - https://www.localscore.ai/accelerator/860\n\nM1 Macbook Pro (10 cores, I think that's it - https://www.localscore.ai/accelerator/929\n\nIt's a nice website that I think should be promoted (it doesn't seem to be a commercial project) more here, since it fits into common question of \"what LLMs can I run on this computer\" nicely.",
            "edited": 1754150635,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ivllg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can use LocalScore AI benchmark to compare results on those.&lt;/p&gt;\n\n&lt;p&gt;5060 Ti - &lt;a href=\"https://www.localscore.ai/accelerator/860\"&gt;https://www.localscore.ai/accelerator/860&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;M1 Macbook Pro (10 cores, I think that&amp;#39;s it - &lt;a href=\"https://www.localscore.ai/accelerator/929\"&gt;https://www.localscore.ai/accelerator/929&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s a nice website that I think should be promoted (it doesn&amp;#39;t seem to be a commercial project) more here, since it fits into common question of &amp;quot;what LLMs can I run on this computer&amp;quot; nicely.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6ivllg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754141417,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ik38a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "robertotomas",
            "can_mod_post": false,
            "created_utc": 1754136895,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 10,
            "author_fullname": "t2_65zz9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The M1 came out 2 years before the 4090 (and three years before the 4090 was available to nearly anyone). Also, for inference, you should be comparing the highest throughput version (ie the ultra). The base M1 was meant for general compute tasks like email, browsing, and coding.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ik38a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The M1 came out 2 years before the 4090 (and three years before the 4090 was available to nearly anyone). Also, for inference, you should be comparing the highest throughput version (ie the ultra). The base M1 was meant for general compute tasks like email, browsing, and coding.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6ik38a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754136895,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6im40x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "croninsiglos",
                      "can_mod_post": false,
                      "created_utc": 1754137727,
                      "send_replies": true,
                      "parent_id": "t1_n6id5vx",
                      "score": 16,
                      "author_fullname": "t2_v7qje",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I don't have the OP's exact prompt, but M4 Max will do about 63 tokens/second with the default ollama 4 bit quant of qwen3:8b\n\n  \n...but of course, why run qwen3:8b when I can run the 8 bit dynamic quant of Qwen3 30B A3B 2507 from unsloth with vastly better benchmarks at the same speed.",
                      "edited": 1754138371,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6im40x",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t have the OP&amp;#39;s exact prompt, but M4 Max will do about 63 tokens/second with the default ollama 4 bit quant of qwen3:8b&lt;/p&gt;\n\n&lt;p&gt;...but of course, why run qwen3:8b when I can run the 8 bit dynamic quant of Qwen3 30B A3B 2507 from unsloth with vastly better benchmarks at the same speed.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfnq2r",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6im40x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754137727,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 16
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6j2yd4",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "ShinyAnkleBalls",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6j261g",
                                "score": 6,
                                "author_fullname": "t2_2m3au2xb",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I didn't recall the numbers, but my 3090 was significantly outperforming my colleague's M4 max MBP when we were comparing qwen2 coder 32B performance a while back.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6j2yd4",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I didn&amp;#39;t recall the numbers, but my 3090 was significantly outperforming my colleague&amp;#39;s M4 max MBP when we were comparing qwen2 coder 32B performance a while back.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfnq2r",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6j2yd4/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754144025,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754144025,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6j261g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "CheatCodesOfLife",
                      "can_mod_post": false,
                      "created_utc": 1754143757,
                      "send_replies": true,
                      "parent_id": "t1_n6id5vx",
                      "score": 6,
                      "author_fullname": "t2_32el727b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not sure how the release date is relevant, but here's the RTX3090 (from released September 2020):\nQwen3-8b-Instruct q4_k\n\n    prompt eval time =     160.63 ms /   465 tokens (    0.35 ms per token,  2894.87 tokens per second)\n           eval time =   16806.22 ms /  2039 tokens (    8.24 ms per token,   121.32 tokens per second)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6j261g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not sure how the release date is relevant, but here&amp;#39;s the RTX3090 (from released September 2020):\nQwen3-8b-Instruct q4_k&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;prompt eval time =     160.63 ms /   465 tokens (    0.35 ms per token,  2894.87 tokens per second)\n       eval time =   16806.22 ms /  2039 tokens (    8.24 ms per token,   121.32 tokens per second)\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfnq2r",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6j261g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754143757,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6iw4q3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Maleficent_Age1577",
                      "can_mod_post": false,
                      "created_utc": 1754141611,
                      "send_replies": true,
                      "parent_id": "t1_n6id5vx",
                      "score": 1,
                      "author_fullname": "t2_gxl5vlowd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "If you have M4 Max be free to give out the stats.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6iw4q3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you have M4 Max be free to give out the stats.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfnq2r",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6iw4q3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754141611,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6id5vx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "christianweyer",
            "can_mod_post": false,
            "created_utc": 1754133771,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 8,
            "author_fullname": "t2_1knv07p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "AFAIK, the M1 chip was released in 2020. The 4090 in 2022. The 5060 Ti in 2025.  \nWould be nice to see e.g. an M4 Max machine in this comparison. But maybe you don't have one ;-)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6id5vx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;AFAIK, the M1 chip was released in 2020. The 4090 in 2022. The 5060 Ti in 2025.&lt;br/&gt;\nWould be nice to see e.g. an M4 Max machine in this comparison. But maybe you don&amp;#39;t have one ;-)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6id5vx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754133771,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6jgqza",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Tyme4Trouble",
            "can_mod_post": false,
            "created_utc": 1754148471,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 2,
            "author_fullname": "t2_973amyap",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Gotta commend the OP for graphing the data out. I find it’s a lot easier to make connections when I can see the difference. For future benchmarks here are some suggestions. \n\n- **Tok/s** can measure either throughput at batch x or latency at batch 1\n- **TPOT** Time per output token is what most people think of as LLM performance. It measures the latency of each token after the first in milliseconds. You can convert this to tok/s by dividing 1000 by TPOT in ms. Just remember to label it batch 1 or per user.\n- **TTFT** Is the time required to process the prompt and generate the first token.\n- **Batch size**. How many requests are being processed in parallel. \n\nOh and don’t forget to share which inference engine and model quant. Ollama and Qwen3 8B Q4_K_M for example. \n\nThese metrics are key to understanding system performance or comparing different engine parameters or quantization levels in a way that folks can test and reproduce.  \n\nKeep up the good work. Charts like these are invaluable for validating other’s experiences.\n\nEdit: formatting",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jgqza",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Gotta commend the OP for graphing the data out. I find it’s a lot easier to make connections when I can see the difference. For future benchmarks here are some suggestions. &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Tok/s&lt;/strong&gt; can measure either throughput at batch x or latency at batch 1&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;TPOT&lt;/strong&gt; Time per output token is what most people think of as LLM performance. It measures the latency of each token after the first in milliseconds. You can convert this to tok/s by dividing 1000 by TPOT in ms. Just remember to label it batch 1 or per user.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;TTFT&lt;/strong&gt; Is the time required to process the prompt and generate the first token.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch size&lt;/strong&gt;. How many requests are being processed in parallel. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Oh and don’t forget to share which inference engine and model quant. Ollama and Qwen3 8B Q4_K_M for example. &lt;/p&gt;\n\n&lt;p&gt;These metrics are key to understanding system performance or comparing different engine parameters or quantization levels in a way that folks can test and reproduce.  &lt;/p&gt;\n\n&lt;p&gt;Keep up the good work. Charts like these are invaluable for validating other’s experiences.&lt;/p&gt;\n\n&lt;p&gt;Edit: formatting&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6jgqza/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754148471,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6iaxis",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "croninsiglos",
            "can_mod_post": false,
            "created_utc": 1754132653,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 4,
            "author_fullname": "t2_v7qje",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you comparing a mobile chip to desktop GPUs or are those the mobile versions?\n\nYou might also want to factor in electricity costs and determine cost per one million tokens.",
            "edited": 1754132843,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6iaxis",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you comparing a mobile chip to desktop GPUs or are those the mobile versions?&lt;/p&gt;\n\n&lt;p&gt;You might also want to factor in electricity costs and determine cost per one million tokens.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6iaxis/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754132653,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6jcvtc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jakegh",
            "can_mod_post": false,
            "created_utc": 1754147256,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 1,
            "author_fullname": "t2_6vt1n",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I got ~130tokens/sec on Qwen3-Coder-30B-A3B with a 5090, if that helps.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jcvtc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I got ~130tokens/sec on Qwen3-Coder-30B-A3B with a 5090, if that helps.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6jcvtc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754147256,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6kd4il",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CryptoCryst828282",
            "can_mod_post": false,
            "created_utc": 1754158744,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 2,
            "author_fullname": "t2_b8e4vw6kg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Mac is good for people who want to play, if you plan to use it agentic the time to first token will kill you, i would take 4 5060ti over even a 10k mac.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kd4il",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mac is good for people who want to play, if you plan to use it agentic the time to first token will kill you, i would take 4 5060ti over even a 10k mac.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6kd4il/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754158744,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6l56oo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TheClusters",
            "can_mod_post": false,
            "created_utc": 1754168084,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": -1,
            "author_fullname": "t2_9fsawe4wb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "And what is the point of comparing the basic M1 from 2020 with a modern generation GPU? \n\nThe image does not contain information about the quantization level and type of framework/runtime (llama.cpp, mlx, or maybe even pytorch via mps???). A completely useless graph.\n\nI can also provide the same useless information: the Qwen3 8B model on my M1 gives 73 t/s and time to first token = 0.65s, which is faster than your rtx 5060ti. Now guess what type of M1 I have, what runtime I used, and what the model's quantization level was...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6l56oo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And what is the point of comparing the basic M1 from 2020 with a modern generation GPU? &lt;/p&gt;\n\n&lt;p&gt;The image does not contain information about the quantization level and type of framework/runtime (llama.cpp, mlx, or maybe even pytorch via mps???). A completely useless graph.&lt;/p&gt;\n\n&lt;p&gt;I can also provide the same useless information: the Qwen3 8B model on my M1 gives 73 t/s and time to first token = 0.65s, which is faster than your rtx 5060ti. Now guess what type of M1 I have, what runtime I used, and what the model&amp;#39;s quantization level was...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6l56oo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754168084,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ikzgs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kargafe",
            "can_mod_post": false,
            "created_utc": 1754137265,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 0,
            "author_fullname": "t2_a85vzco5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's a MacBook Pro with an M1 chip.\n\nMy main curiosity is to compare my setup. My focus is to understand if this would be a significant upgrade or a small gain for me. I know this isn't a real benchmark, but it gives some insight.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ikzgs",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a MacBook Pro with an M1 chip.&lt;/p&gt;\n\n&lt;p&gt;My main curiosity is to compare my setup. My focus is to understand if this would be a significant upgrade or a small gain for me. I know this isn&amp;#39;t a real benchmark, but it gives some insight.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6ikzgs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754137265,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6kdbei",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CryptoCryst828282",
                      "can_mod_post": false,
                      "created_utc": 1754158806,
                      "send_replies": true,
                      "parent_id": "t1_n6jp653",
                      "score": 1,
                      "author_fullname": "t2_b8e4vw6kg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Any way you slice it, they are fun for a chat bot or something, but you dont want to do agentic on any of the macs.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6kdbei",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Any way you slice it, they are fun for a chat bot or something, but you dont want to do agentic on any of the macs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfnq2r",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6kdbei/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754158806,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6jp653",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Smooth-Ad5257",
            "can_mod_post": false,
            "created_utc": 1754151154,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 0,
            "author_fullname": "t2_n31sv1e6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Let me compare a 5 year old, pre-pre-pre- gen apple with the rest, yea!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jp653",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Let me compare a 5 year old, pre-pre-pre- gen apple with the rest, yea!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6jp653/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754151154,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ipota",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MacaronAppropriate80",
            "can_mod_post": false,
            "created_utc": 1754139171,
            "send_replies": true,
            "parent_id": "t3_1mfnq2r",
            "score": 0,
            "author_fullname": "t2_9i6oydpgx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "are you sure ollama support mlx?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ipota",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;are you sure ollama support mlx?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfnq2r/benchmarking_qwen3_8b_inference_m1_vs_rtx_5060_ti/n6ipota/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754139171,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfnq2r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]