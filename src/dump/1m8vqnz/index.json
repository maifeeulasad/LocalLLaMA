[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi,  \nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?  \nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?  \nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.  \nOf course it depends of the model.\n\nThen there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.  \nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?  \nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  \n  \nI guess 4x is for sure too slow.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Tensor parallel - pcie bandwidth requirement",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8vqnz",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1jk2ep8a52",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753440020,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753439838,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\nCan anyone say is PCI 4.0 16X going to be bottleneck with tensor parallel inference, lets say with 4090 or 7900 XTX cards 2 or 4?&lt;br/&gt;\nIs there anywhere data how much inference is using PCIE bandwidth, can it be measured during inference?&lt;br/&gt;\nI have currently 2 7900 XTX in 8x pcie 4.0 and both cards uses max 200W during inference. My guess is they would maybe use more and the 8x lane might be bottleneck.&lt;br/&gt;\nOf course it depends of the model.&lt;/p&gt;\n\n&lt;p&gt;Then there is PCIE 5.0 cards, where the connection is 64GB/S instead 32GB/s.&lt;br/&gt;\nIs that safe or will that also be bottleneck with 2 - 4 5090 cards? Who knows?&lt;br/&gt;\nHas anyone tested inference in tensor parallel, first with 8X lanes and then 16x lanes? Big difference? I am now talking mainly vLLM and others which can do tensor parallel, not Ollama etc.  &lt;/p&gt;\n\n&lt;p&gt;I guess 4x is for sure too slow.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m8vqnz",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Rich_Artist_8327",
            "discussion_type": null,
            "num_comments": 15,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753439838,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n53r8nx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "evil0sheep",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n52dbx4",
                                "score": 2,
                                "author_fullname": "t2_4ty73",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It depends entirely on your batch size and how good your KV cache hit rate is. For a single user chatbot with no speculative decoding and proper kv cache management you only need to move across handful of embedding vectors between GPUs per transformer block per token during token generation. If you start batching to serve multiple users or for training or to do speculative decoding then you should multiply that by batch_size, and if your kv cache hit rate goes to zero (for example prompt processing or rag processing) then you should multiply by the sequence length too. For training where the batches are very wide and none of the tokens are in the KV cache you need to multiply by both and so your inter-gpu bandwidth starts to get big in a hurry. What’s your application? How many users are you planning on serving?\n\n\nEdit: a good exercise for you here might be to read attention is all you need and megatron-lm and the speculative decoding paper, and then for your chosen model try to calculate how much memory bandwidth and flops and inter-gpu bandwidth is required for a given tok/s and batch_size as you read.",
                                "edited": 1753458435,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n53r8nx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends entirely on your batch size and how good your KV cache hit rate is. For a single user chatbot with no speculative decoding and proper kv cache management you only need to move across handful of embedding vectors between GPUs per transformer block per token during token generation. If you start batching to serve multiple users or for training or to do speculative decoding then you should multiply that by batch_size, and if your kv cache hit rate goes to zero (for example prompt processing or rag processing) then you should multiply by the sequence length too. For training where the batches are very wide and none of the tokens are in the KV cache you need to multiply by both and so your inter-gpu bandwidth starts to get big in a hurry. What’s your application? How many users are you planning on serving?&lt;/p&gt;\n\n&lt;p&gt;Edit: a good exercise for you here might be to read attention is all you need and megatron-lm and the speculative decoding paper, and then for your chosen model try to calculate how much memory bandwidth and flops and inter-gpu bandwidth is required for a given tok/s and batch_size as you read.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vqnz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n53r8nx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753457965,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753457965,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n52dbx4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Rich_Artist_8327",
                      "can_mod_post": false,
                      "created_utc": 1753441545,
                      "send_replies": true,
                      "parent_id": "t1_n52afa8",
                      "score": 2,
                      "author_fullname": "t2_1jk2ep8a52",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "are you absolutely sure that it does not matter in tensor parallel? I know it does not matter like with llama.cpp or Ollama etc.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52dbx4",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;are you absolutely sure that it does not matter in tensor parallel? I know it does not matter like with llama.cpp or Ollama etc.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vqnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52dbx4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753441545,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n53jfkp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "panchovix",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n52b2v4",
                                "score": 2,
                                "author_fullname": "t2_j1kqr",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You want faster PCIe speeds as with distributed training you have to move the data across the GPU continuously.\n\nOn your case you can't have better PCIe interconnect speed as it is X8 max, just make sure you use PCIe 5.0.\n\nNow if the P2P patched is updated to work with RTX 50 series then it would get a benefit.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n53jfkp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 405B"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You want faster PCIe speeds as with distributed training you have to move the data across the GPU continuously.&lt;/p&gt;\n\n&lt;p&gt;On your case you can&amp;#39;t have better PCIe interconnect speed as it is X8 max, just make sure you use PCIe 5.0.&lt;/p&gt;\n\n&lt;p&gt;Now if the P2P patched is updated to work with RTX 50 series then it would get a benefit.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vqnz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n53jfkp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753455802,
                                "author_flair_text": "Llama 405B",
                                "treatment_tags": [],
                                "created_utc": 1753455802,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n52b2v4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "cybran3",
                      "can_mod_post": false,
                      "created_utc": 1753440507,
                      "send_replies": true,
                      "parent_id": "t1_n52afa8",
                      "score": 1,
                      "author_fullname": "t2_41gmkw5z",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "How does it affect training? I have 2 RTX 5060 Ti 16 GB GPUs. I’ll be training some custom transformers (not LLMs) and I will use distributed training. I’m wondering how would it affect the speed? Since my GPUs specifications say they use PCIe 5.0 8x and my mobo supports this for 2 GPUs (Gigabyte B850 AI TOP).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52b2v4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How does it affect training? I have 2 RTX 5060 Ti 16 GB GPUs. I’ll be training some custom transformers (not LLMs) and I will use distributed training. I’m wondering how would it affect the speed? Since my GPUs specifications say they use PCIe 5.0 8x and my mobo supports this for 2 GPUs (Gigabyte B850 AI TOP).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vqnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52b2v4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753440507,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52afa8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "koushd",
            "can_mod_post": false,
            "created_utc": 1753440198,
            "send_replies": true,
            "parent_id": "t3_1m8vqnz",
            "score": 4,
            "author_fullname": "t2_4yut6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Unless you’re training it doesn’t matter. x8 is more than enough.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52afa8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unless you’re training it doesn’t matter. x8 is more than enough.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52afa8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753440198,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vqnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52g0xb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "imchkkim",
            "can_mod_post": false,
            "created_utc": 1753442738,
            "send_replies": true,
            "parent_id": "t3_1m8vqnz",
            "score": 1,
            "author_fullname": "t2_12bbk9b8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "No big difference at inference. About 30% speed difference when loading models into VRAM",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52g0xb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No big difference at inference. About 30% speed difference when loading models into VRAM&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52g0xb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753442738,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vqnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n53zj40",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "evil0sheep",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n53z71s",
                                          "score": 1,
                                          "author_fullname": "t2_4ty73",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Interconnect bandwidth requirement is a linear function of the product of the embedding dimension, the sequence length, the batch size, and the kv cache miss rate. For single user token generation with no speculative decoding on your home gpu (llama-14b) that’s on the order of C x 2000 x 4000 x  1 x (1/4000)=2000 x C. For training the same model with a batch size of 1024 that’s 2000 x 4000 x 1024 x 1 =8,192,000,000 x C, so about 4 million times higher. For the latter you need very high bandwidth direct gpu interconnects. For the former 4 lanes of pcie is more than enough. Depending where you land in between those two use cases will determine what physical resource will bound your performance for a given hardware topology.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n53zj40",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interconnect bandwidth requirement is a linear function of the product of the embedding dimension, the sequence length, the batch size, and the kv cache miss rate. For single user token generation with no speculative decoding on your home gpu (llama-14b) that’s on the order of C x 2000 x 4000 x  1 x (1/4000)=2000 x C. For training the same model with a batch size of 1024 that’s 2000 x 4000 x 1024 x 1 =8,192,000,000 x C, so about 4 million times higher. For the latter you need very high bandwidth direct gpu interconnects. For the former 4 lanes of pcie is more than enough. Depending where you land in between those two use cases will determine what physical resource will bound your performance for a given hardware topology.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m8vqnz",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n53zj40/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753460276,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753460276,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n53z71s",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "evil0sheep",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n52swuk",
                                "score": 2,
                                "author_fullname": "t2_4ty73",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Interconnect bandwidth requirement is a linear function of the product of the embedding dimension, the sequence length, the batch size, and the kv cache miss rate. For single user token generation with no speculative decoding on your home gpu (llama-14b) that’s on the order of C*2000*4000*1*(1/4000)=2000*C. For training the same model with a batch size of 1024 that’s 2000*4000*1024*1 =8,192,000,000 *C, so about 4 million times higher. For the latter you need very high bandwidth direct gpu interconnects. For the former 4 lanes of pcie is more than enough. Depending where you land in between those two use cases will determine what physical resource will bound your performance for a given hardware topology.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n53z71s",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interconnect bandwidth requirement is a linear function of the product of the embedding dimension, the sequence length, the batch size, and the kv cache miss rate. For single user token generation with no speculative decoding on your home gpu (llama-14b) that’s on the order of C&lt;em&gt;2000&lt;/em&gt;4000&lt;em&gt;1&lt;/em&gt;(1/4000)=2000&lt;em&gt;C. For training the same model with a batch size of 1024 that’s 2000&lt;/em&gt;4000&lt;em&gt;1024&lt;/em&gt;1 =8,192,000,000 *C, so about 4 million times higher. For the latter you need very high bandwidth direct gpu interconnects. For the former 4 lanes of pcie is more than enough. Depending where you land in between those two use cases will determine what physical resource will bound your performance for a given hardware topology.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vqnz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n53z71s/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753460182,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753460182,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n52zgfq",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Rich_Artist_8327",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n52yq0l",
                                          "score": 1,
                                          "author_fullname": "t2_1jk2ep8a52",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks, you have nvidia, I just ordered nvidia but currently with rocm and amd",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n52zgfq",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, you have nvidia, I just ordered nvidia but currently with rocm and amd&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m8vqnz",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52zgfq/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753449864,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753449864,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n52yq0l",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Nepherpitu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n52swuk",
                                "score": 1,
                                "author_fullname": "t2_plp1w",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah, probably you will notice difference for parallel processing with VLLM, sglang or exllama, didn't checked this scenario.\n\nHere is simple command:\n\n```\n      docker run --name vllm-qwen3-32b --rm --ipc=host --gpus=all\n      -e \"CUDA_VISIBLE_DEVICES=1,2\"\n      -e \"CUDA_DEVICE_ORDER=PCI_BUS_ID\"\n      -e \"VLLM_ATTENTION_BACKEND=FLASH_ATTN\"\n      -v \"\\\\wsl$\\Ubuntu\\home\\unat\\vllm\\huggingface:/root/.cache/huggingface\"\n      -v \"\\\\wsl$\\Ubuntu\\home\\unat\\vllm\\vllm-qwen-32b:/root/.cache/vllm\"\n      -p ${PORT}:30000\n      vllm-nightly:2025-07-02-v1\n      --model /root/.cache/huggingface/Qwen3-32B-AWQ\n      --tensor-parallel-size 2\n      --port 30000\n      --host 0.0.0.0\n      --served-model-name qwen3-32b-vllm\n      --enable-auto-tool-choice\n      --tool-call-parser hermes\n      --reasoning-parser qwen3\n      --rope-scaling {\\\"rope_type\\\":\\\"yarn\\\",\\\"factor\\\":2.0,\\\"original_max_position_embeddings\\\":32768}\n      --max-model-len 65536\n      --max-seq-len-to-capture 65536\n      --max-num-seqs 2\n      --gpu-memory-utilization 0.9\n      --trust-remote-code\n```\n\nAnd in this case there are almost no difference between X2, X4 or X8.\n\nI tested it.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n52yq0l",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, probably you will notice difference for parallel processing with VLLM, sglang or exllama, didn&amp;#39;t checked this scenario.&lt;/p&gt;\n\n&lt;p&gt;Here is simple command:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;\n      docker run --name vllm-qwen3-32b --rm --ipc=host --gpus=all\n      -e &amp;quot;CUDA_VISIBLE_DEVICES=1,2&amp;quot;\n      -e &amp;quot;CUDA_DEVICE_ORDER=PCI_BUS_ID&amp;quot;\n      -e &amp;quot;VLLM_ATTENTION_BACKEND=FLASH_ATTN&amp;quot;\n      -v &amp;quot;\\\\wsl$\\Ubuntu\\home\\unat\\vllm\\huggingface:/root/.cache/huggingface&amp;quot;\n      -v &amp;quot;\\\\wsl$\\Ubuntu\\home\\unat\\vllm\\vllm-qwen-32b:/root/.cache/vllm&amp;quot;\n      -p ${PORT}:30000\n      vllm-nightly:2025-07-02-v1\n      --model /root/.cache/huggingface/Qwen3-32B-AWQ\n      --tensor-parallel-size 2\n      --port 30000\n      --host 0.0.0.0\n      --served-model-name qwen3-32b-vllm\n      --enable-auto-tool-choice\n      --tool-call-parser hermes\n      --reasoning-parser qwen3\n      --rope-scaling {\\&amp;quot;rope_type\\&amp;quot;:\\&amp;quot;yarn\\&amp;quot;,\\&amp;quot;factor\\&amp;quot;:2.0,\\&amp;quot;original_max_position_embeddings\\&amp;quot;:32768}\n      --max-model-len 65536\n      --max-seq-len-to-capture 65536\n      --max-num-seqs 2\n      --gpu-memory-utilization 0.9\n      --trust-remote-code\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;And in this case there are almost no difference between X2, X4 or X8.&lt;/p&gt;\n\n&lt;p&gt;I tested it.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8vqnz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52yq0l/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753449628,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753449628,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n52swuk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Rich_Artist_8327",
                      "can_mod_post": false,
                      "created_utc": 1753447687,
                      "send_replies": true,
                      "parent_id": "t1_n52hupi",
                      "score": 1,
                      "author_fullname": "t2_1jk2ep8a52",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Then why datacenter GPUs have much faster interconnect and why everyone says how good 3090 nvlink is. So it looks like it does not matter at all. But I believe it matters, depends of the model and load.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52swuk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Then why datacenter GPUs have much faster interconnect and why everyone says how good 3090 nvlink is. So it looks like it does not matter at all. But I believe it matters, depends of the model and load.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vqnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52swuk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753447687,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52hupi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Nepherpitu",
            "can_mod_post": false,
            "created_utc": 1753443527,
            "send_replies": true,
            "parent_id": "t3_1m8vqnz",
            "score": 1,
            "author_fullname": "t2_plp1w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "VLLM wasn't bottlenecked by anything higher than PCIE 4.0 X4 for single user and dual GPU setup. Was INSIGNIFICANTLY bottlenected by PCIE 4.0 X2. I mean, I got 3-5% uplift going from X2 to X4 and zero uplift from X4 to X8.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52hupi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;VLLM wasn&amp;#39;t bottlenecked by anything higher than PCIE 4.0 X4 for single user and dual GPU setup. Was INSIGNIFICANTLY bottlenected by PCIE 4.0 X2. I mean, I got 3-5% uplift going from X2 to X4 and zero uplift from X4 to X8.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n52hupi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753443527,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vqnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n57uz0v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Aaaaaaaaaeeeee",
            "can_mod_post": false,
            "created_utc": 1753507214,
            "send_replies": true,
            "parent_id": "t3_1m8vqnz",
            "score": 1,
            "author_fullname": "t2_el5pibmej",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\n\nhttps://m.bilibili.com/video/BV1vs421377R?share_source=copy_web&amp;vd_source=a0db244549aaef49ac546d9c806aa33c&amp;share_times=1\n\n\nThe video shows 8 modded 2080tis with 22gb vram per gpu, running llama3 70B (~140GB on disc) at 19 t/s with a single stream. \n\n\n\nTo achieve the same goal get at least PCI Express 3.0 x16 interface\n\n\nThis kind of information is very valuable when building a budget rig of multiple mid-range GPUs, those 2080tis went from 616 GB/s to 2660 GB/s tg. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n57uz0v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://m.bilibili.com/video/BV1vs421377R?share_source=copy_web&amp;amp;vd_source=a0db244549aaef49ac546d9c806aa33c&amp;amp;share_times=1\"&gt;https://m.bilibili.com/video/BV1vs421377R?share_source=copy_web&amp;amp;vd_source=a0db244549aaef49ac546d9c806aa33c&amp;amp;share_times=1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The video shows 8 modded 2080tis with 22gb vram per gpu, running llama3 70B (~140GB on disc) at 19 t/s with a single stream. &lt;/p&gt;\n\n&lt;p&gt;To achieve the same goal get at least PCI Express 3.0 x16 interface&lt;/p&gt;\n\n&lt;p&gt;This kind of information is very valuable when building a budget rig of multiple mid-range GPUs, those 2080tis went from 616 GB/s to 2660 GB/s tg. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n57uz0v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753507214,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vqnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n591tvo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Rich_Artist_8327",
                      "can_mod_post": false,
                      "created_utc": 1753530879,
                      "send_replies": true,
                      "parent_id": "t1_n582u6d",
                      "score": 1,
                      "author_fullname": "t2_1jk2ep8a52",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "ofcourse I know those, I have build many servers.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n591tvo",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ofcourse I know those, I have build many servers.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8vqnz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n591tvo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753530879,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n582u6d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FieldProgrammable",
            "can_mod_post": false,
            "created_utc": 1753511421,
            "send_replies": true,
            "parent_id": "t3_1m8vqnz",
            "score": 1,
            "author_fullname": "t2_moet0t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A couple of things that can muddy the waters even when you narrow the question down to tensor parallel inference. First is that many users reporting results neglect to distinguish the difference between CPU lanes and chipset lanes as this creates a significant increase in latency and contention with other parts of the system.\n\nSecond is that any pro series cards will have access to PCIE P2P transfers, which helps bypass system memory when transferring data between cards. Even when you are using cards you know don't have P2P support, you would also need to know their system RAM configuration, a server CPU has potentially much more system RAM bandwidth available than a consumer CPU.\n\nAs soon as data needs to leave a GPU during intra token inference then suddenly all the other variables in system configuration (both hardware and software) come into play making comparisons between different setups very difficult. \n\nIn the end, all of this comes down to costs and what you as a user are willing to pay for a certain level of performance. It is possible to say that increasing PCIE bandwidth in a multi GPU setup will increase inference speed, it is possible to prove that tensor parallel inference requires more bandwidth than pipelined inference but whether a given increase in inference speed is significant is subjective.\n\nThere are more metrics than just generation speed that other people consider and may care about more or less than you when they give advice. Things like prompt processing speed, time to first token and max time to last token are affected by system bottlenecks in different ways to token generation.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n582u6d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A couple of things that can muddy the waters even when you narrow the question down to tensor parallel inference. First is that many users reporting results neglect to distinguish the difference between CPU lanes and chipset lanes as this creates a significant increase in latency and contention with other parts of the system.&lt;/p&gt;\n\n&lt;p&gt;Second is that any pro series cards will have access to PCIE P2P transfers, which helps bypass system memory when transferring data between cards. Even when you are using cards you know don&amp;#39;t have P2P support, you would also need to know their system RAM configuration, a server CPU has potentially much more system RAM bandwidth available than a consumer CPU.&lt;/p&gt;\n\n&lt;p&gt;As soon as data needs to leave a GPU during intra token inference then suddenly all the other variables in system configuration (both hardware and software) come into play making comparisons between different setups very difficult. &lt;/p&gt;\n\n&lt;p&gt;In the end, all of this comes down to costs and what you as a user are willing to pay for a certain level of performance. It is possible to say that increasing PCIE bandwidth in a multi GPU setup will increase inference speed, it is possible to prove that tensor parallel inference requires more bandwidth than pipelined inference but whether a given increase in inference speed is significant is subjective.&lt;/p&gt;\n\n&lt;p&gt;There are more metrics than just generation speed that other people consider and may care about more or less than you when they give advice. Things like prompt processing speed, time to first token and max time to last token are affected by system bottlenecks in different ways to token generation.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8vqnz/tensor_parallel_pcie_bandwidth_requirement/n582u6d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753511421,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8vqnz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]