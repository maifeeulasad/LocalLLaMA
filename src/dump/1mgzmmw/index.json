[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.\n\nThe idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.\n\nI intuit that it might be necessary to multiply each model's logits by a scalar to normalize them before appending, but it's not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.\n\nThen the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.\n\nMy initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in https://arxiv.org/abs/2505.24832, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model's knowledge beyond the limits of RAG (which works great, but only to the limits of context).\n\nThinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.\n\nThat's as far as I've gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn't work?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Idea for combining multiple models' inference via normalizing logit lists -- would it work, has someone already done it, and how could it be made better?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mgzmmw",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": "#bbbdbf",
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "is_original_content": false,
            "author_fullname": "t2_cpegz",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754268584,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I was pondering the problem of extending the useful lifespans of older models with knowledge cut-offs in the distant past, when something occurred to me with applications beyond just that.&lt;/p&gt;\n\n&lt;p&gt;The idea is that you would infer on a context with different models (ideally in parallel), but stop short of the minmax step, and append their logit lists together into one long list.&lt;/p&gt;\n\n&lt;p&gt;I intuit that it might be necessary to multiply each model&amp;#39;s logits by a scalar to normalize them before appending, but it&amp;#39;s not clear to me if you could get away with a constant scalar for each model, or if it would need to be dynamic.  Maybe something as simple as choosing a scalar which makes the sums of the top N logits equal between the logit lists?  Not sure.&lt;/p&gt;\n\n&lt;p&gt;Then the minmax step would convert all of the logits in the combined list to a probability distribution, and the next token chosen from that, as normal.&lt;/p&gt;\n\n&lt;p&gt;My initial thought was that older models could be thus augmented with newer models with new knowledge, which might be low-parameter and trained to the limit described in &lt;a href=\"https://arxiv.org/abs/2505.24832\"&gt;https://arxiv.org/abs/2505.24832&lt;/a&gt;, to maximize memorized knowledge per parameter.  That seems like a (relatively) cheap way to update the amalgam model&amp;#39;s knowledge beyond the limits of RAG (which works great, but only to the limits of context).&lt;/p&gt;\n\n&lt;p&gt;Thinking about it further, though, it seems like it might also be a way to improve overall inference competence, by including models in the amalgam with comparable parameter counts but different skillsets.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s as far as I&amp;#39;ve gotten.  Does it sound like something someone else has already done?  Does anyone see reasons it wouldn&amp;#39;t work?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mgzmmw",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ttkciar",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "light",
            "permalink": "/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/",
            "subreddit_subscribers": 509624,
            "created_utc": 1754268584,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6si1nv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754269016,
            "send_replies": true,
            "parent_id": "t3_1mgzmmw",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thinking about it further, choosing a normalizing scalar to make the sums of top N logits equal is logically equivalent to just picking a model at random and inferring the next token with just that model.  Not particularly useful.\n\nWhat's really needed is a normalization scalar which suppresses low-confidence logits and amplifies high-confidence logits, but that might just be a constant scalar per model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6si1nv",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thinking about it further, choosing a normalizing scalar to make the sums of top N logits equal is logically equivalent to just picking a model at random and inferring the next token with just that model.  Not particularly useful.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s really needed is a normalization scalar which suppresses low-confidence logits and amplifies high-confidence logits, but that might just be a constant scalar per model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgzmmw/idea_for_combining_multiple_models_inference_via/n6si1nv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754269016,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mgzmmw",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]