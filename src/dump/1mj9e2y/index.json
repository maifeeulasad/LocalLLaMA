[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Not sure what I was expecting, but my new 512gb Mac Studio doesn't seem to be the workhorse I hoped for - I guess I expected a faster performance. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "underwhelmed by 512gb M3 ultra Mac Studio",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mj9e2y",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.66,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_sk7nmjrs",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754497782,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Not sure what I was expecting, but my new 512gb Mac Studio doesn&amp;#39;t seem to be the workhorse I hoped for - I guess I expected a faster performance. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mj9e2y",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ChevChance",
            "discussion_type": null,
            "num_comments": 46,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/",
            "subreddit_subscribers": 512426,
            "created_utc": 1754497782,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n79jj28",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "-dysangel-",
                      "can_mod_post": false,
                      "created_utc": 1754500434,
                      "send_replies": true,
                      "parent_id": "t1_n79apct",
                      "score": 7,
                      "author_fullname": "t2_12ggykute6",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Prefill is slow. The generation speed of R1 is fine.\n\n  \nNote to OP - make sure to use an inference server that has caching enabled, like llama.cpp with --cache-reuse on. This makes the Ultra perfect for chatting to smarter models, and makes agentic use much more feasible",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79jj28",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Prefill is slow. The generation speed of R1 is fine.&lt;/p&gt;\n\n&lt;p&gt;Note to OP - make sure to use an inference server that has caching enabled, like llama.cpp with --cache-reuse on. This makes the Ultra perfect for chatting to smarter models, and makes agentic use much more feasible&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79jj28/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754500434,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7ajkjy",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "devshore",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7aj6hp",
                                          "score": 1,
                                          "author_fullname": "t2_ufbr1m7p",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "On a mac, yea because its super slow",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7ajkjy",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;On a mac, yea because its super slow&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj9e2y",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7ajkjy/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754510577,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754510577,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7aj6hp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "nomorebuttsplz",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7ahnrj",
                                "score": 1,
                                "author_fullname": "t2_syq52",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Somewhere efficiency would need to be improved.\n\nEven if you used the Mac m3u for 24 hours a day and 365 days a year, and averaged 12 t/s throughput (reasonable for short context and SOTA models rivaling Claude), you would only get about $2000 worth of tokens at six dollars per million tokens over the course of 1 year.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7aj6hp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Somewhere efficiency would need to be improved.&lt;/p&gt;\n\n&lt;p&gt;Even if you used the Mac m3u for 24 hours a day and 365 days a year, and averaged 12 t/s throughput (reasonable for short context and SOTA models rivaling Claude), you would only get about $2000 worth of tokens at six dollars per million tokens over the course of 1 year.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7aj6hp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754510465,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754510465,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ahnrj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "devshore",
                      "can_mod_post": false,
                      "created_utc": 1754510038,
                      "send_replies": true,
                      "parent_id": "t1_n79apct",
                      "score": 1,
                      "author_fullname": "t2_ufbr1m7p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thats because anthropoc is taking massive losses. Once the true pricing comes in, it will be cheaper to buy a 30k rig in monthly payments than to pay for an LLM",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ahnrj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thats because anthropoc is taking massive losses. Once the true pricing comes in, it will be cheaper to buy a 30k rig in monthly payments than to pay for an LLM&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7ahnrj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754510038,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n79apct",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "nomorebuttsplz",
            "can_mod_post": false,
            "created_utc": 1754497982,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 12,
            "author_fullname": "t2_syq52",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "which part is too slow? Prefill or text gen?\n\nIn general, local llms make sense as a security move, not a tokens/dollar move.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79apct",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;which part is too slow? Prefill or text gen?&lt;/p&gt;\n\n&lt;p&gt;In general, local llms make sense as a security move, not a tokens/dollar move.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79apct/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754497982,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n79f4b2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "101m4n",
            "can_mod_post": false,
            "created_utc": 1754499225,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 12,
            "author_fullname": "t2_p7nc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; I guess I expected a faster performance.\n\nThere is a small but vocal minority on here who will say \"just get a mac\" whenever someone asks for hardware recommendations. This is, in most practical cases, bad advice. Token generation is fairly bandwidth bound, but prefill isn't and compute/bandwidth costs increase as the context grows.\n\nYou've got tons of capacity and a fair chunk of bandwidth, which is great as you can load very large models, but the macs just don't have the compute to go with it. There is a reason the professionals (in this segment at least) are buying big linux workstations with rtx6000 pros in them.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79f4b2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I guess I expected a faster performance.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;There is a small but vocal minority on here who will say &amp;quot;just get a mac&amp;quot; whenever someone asks for hardware recommendations. This is, in most practical cases, bad advice. Token generation is fairly bandwidth bound, but prefill isn&amp;#39;t and compute/bandwidth costs increase as the context grows.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ve got tons of capacity and a fair chunk of bandwidth, which is great as you can load very large models, but the macs just don&amp;#39;t have the compute to go with it. There is a reason the professionals (in this segment at least) are buying big linux workstations with rtx6000 pros in them.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79f4b2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754499225,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n79d7tr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "DinoAmino",
                      "can_mod_post": false,
                      "created_utc": 1754498700,
                      "send_replies": true,
                      "parent_id": "t1_n79aq01",
                      "score": 9,
                      "author_fullname": "t2_j1v7f",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Processing lots of context, probably. Sorry you didn't get the memo OP.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79d7tr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Processing lots of context, probably. Sorry you didn&amp;#39;t get the memo OP.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79d7tr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754498700,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 9
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n79aq01",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Red_Redditor_Reddit",
            "can_mod_post": false,
            "created_utc": 1754497987,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 7,
            "author_fullname": "t2_8eelmfjg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What's it doin?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79aq01",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s it doin?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79aq01/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754497987,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7a6pay",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "nomorebuttsplz",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n79fqiu",
                                          "score": 1,
                                          "author_fullname": "t2_syq52",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "The m3 Ultra can do about 800 tokens/second for prefill at 10k context with OSS. \n\nIt slows down to about 500 by 20k, and about 200 by about 45k. So I think it depends on what you're processing. That's three and a half minutes for 45k, but only about 12 seconds for 9k context.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7a6pay",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The m3 Ultra can do about 800 tokens/second for prefill at 10k context with OSS. &lt;/p&gt;\n\n&lt;p&gt;It slows down to about 500 by 20k, and about 200 by about 45k. So I think it depends on what you&amp;#39;re processing. That&amp;#39;s three and a half minutes for 45k, but only about 12 seconds for 9k context.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj9e2y",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7a6pay/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754506905,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754506905,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n79fqiu",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "SandboChang",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n79e8kf",
                                "score": 2,
                                "author_fullname": "t2_10icmj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It’s not bad, my M4 Max 128GB does this with the 120B one:\n50 TPS@0 context,\n10 TPS@12k context,\nWith 12k context (around 1000 lines of Python code), it takes 50 seconds to process. \n\nIt is acceptable for local use, but to be fair I think it is too slow to justify the cost if you got the machine for LLM alone, unless you desperately have to go local (but then I must say I am enjoying my 5090 with Qwen3 Coder 30B/gpt-oss 20B much more).",
                                "edited": 1754506132,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79fqiu",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s not bad, my M4 Max 128GB does this with the 120B one:\n50 TPS@0 context,\n10 TPS@12k context,\nWith 12k context (around 1000 lines of Python code), it takes 50 seconds to process. &lt;/p&gt;\n\n&lt;p&gt;It is acceptable for local use, but to be fair I think it is too slow to justify the cost if you got the machine for LLM alone, unless you desperately have to go local (but then I must say I am enjoying my 5090 with Qwen3 Coder 30B/gpt-oss 20B much more).&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79fqiu/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754499396,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754499396,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n79e8kf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "GreenTreeAndBlueSky",
                      "can_mod_post": false,
                      "created_utc": 1754498982,
                      "send_replies": true,
                      "parent_id": "t1_n79b1id",
                      "score": 2,
                      "author_fullname": "t2_1p50pl73j2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oos 120 would work great for them though, it has the space and since it's extremely sparse it should be fast enough",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79e8kf",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oos 120 would work great for them though, it has the space and since it&amp;#39;s extremely sparse it should be fast enough&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79e8kf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754498982,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7a6ce8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "brick-pop",
                      "can_mod_post": false,
                      "created_utc": 1754506804,
                      "send_replies": true,
                      "parent_id": "t1_n79b1id",
                      "score": 2,
                      "author_fullname": "t2_5u4jyklp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I think there's a very legitimate case for it. Nvidia GPU's will work faster for small-ish models, but the moment you need something bigger (32-100Gb VRAM), you're only left with Mac Studio or an array of high end professional GPU's\n\nHaven't tried it myself, but my guess is that 256/512gb Mac Studios may not scale well past a certain LLM size, where VRAM alone allows to load massive models but not necessarily compute the bigger boys faster\n\nIs there any benchmark/info on this topic?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7a6ce8",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think there&amp;#39;s a very legitimate case for it. Nvidia GPU&amp;#39;s will work faster for small-ish models, but the moment you need something bigger (32-100Gb VRAM), you&amp;#39;re only left with Mac Studio or an array of high end professional GPU&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;Haven&amp;#39;t tried it myself, but my guess is that 256/512gb Mac Studios may not scale well past a certain LLM size, where VRAM alone allows to load massive models but not necessarily compute the bigger boys faster&lt;/p&gt;\n\n&lt;p&gt;Is there any benchmark/info on this topic?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7a6ce8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754506804,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n79yz2a",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "eloquentemu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n79udmv",
                                          "score": 2,
                                          "author_fullname": "t2_lpdsy",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I'm guessing that was used though?  A quick Google still has a Epyc Rome workstation with 1TB as $13k and even eBay seems to be starting 1TB systems around $3-4k.  Some people just don't have the stomach for getting something from eBay/Craigslist or even installing a GPU (esp if this was a used rack server).  It's a great option, don't get me wrong, but it's beyond a lot of people.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n79yz2a",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m guessing that was used though?  A quick Google still has a Epyc Rome workstation with 1TB as $13k and even eBay seems to be starting 1TB systems around $3-4k.  Some people just don&amp;#39;t have the stomach for getting something from eBay/Craigslist or even installing a GPU (esp if this was a used rack server).  It&amp;#39;s a great option, don&amp;#39;t get me wrong, but it&amp;#39;s beyond a lot of people.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj9e2y",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79yz2a/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754504698,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754504698,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7bz1k6",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "mxforest",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n79udmv",
                                          "score": 1,
                                          "author_fullname": "t2_kenmq",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Did you try running Deepseek? I am legitimately considering a 1TB machine. Can you tell me how much pre-fill and token generation i can expect at Full precision?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7bz1k6",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Did you try running Deepseek? I am legitimately considering a 1TB machine. Can you tell me how much pre-fill and token generation i can expect at Full precision?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj9e2y",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7bz1k6/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754526622,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754526622,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n79udmv",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Willing_Landscape_61",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n79i5ur",
                                "score": 5,
                                "author_fullname": "t2_8lvrytgw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "\"Getting a prebuilt Epyc, Threadripper, Xeon with &gt;512GB RAM and a GPU is going to cost &gt;$15k.\" \n???\nI got a prebuilt Epyc Gen 2 with 1TB RAM for $2.5 k\nI sure could add $12.5k worth of GPUs but I sure don't have to.\nA single used 4090 is fine for a total just north of $4k.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79udmv",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Getting a prebuilt Epyc, Threadripper, Xeon with &amp;gt;512GB RAM and a GPU is going to cost &amp;gt;$15k.&amp;quot; \n???\nI got a prebuilt Epyc Gen 2 with 1TB RAM for $2.5 k\nI sure could add $12.5k worth of GPUs but I sure don&amp;#39;t have to.\nA single used 4090 is fine for a total just north of $4k.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79udmv/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754503394,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754503394,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": {
                                                                "kind": "Listing",
                                                                "data": {
                                                                  "after": null,
                                                                  "dist": null,
                                                                  "modhash": "",
                                                                  "geo_filter": "",
                                                                  "children": [
                                                                    {
                                                                      "kind": "t1",
                                                                      "data": {
                                                                        "subreddit_id": "t5_81eyvm",
                                                                        "approved_at_utc": null,
                                                                        "author_is_blocked": false,
                                                                        "comment_type": null,
                                                                        "awarders": [],
                                                                        "mod_reason_by": null,
                                                                        "banned_by": null,
                                                                        "author_flair_type": "text",
                                                                        "total_awards_received": 0,
                                                                        "subreddit": "LocalLLaMA",
                                                                        "author_flair_template_id": null,
                                                                        "distinguished": null,
                                                                        "likes": null,
                                                                        "replies": {
                                                                          "kind": "Listing",
                                                                          "data": {
                                                                            "after": null,
                                                                            "dist": null,
                                                                            "modhash": "",
                                                                            "geo_filter": "",
                                                                            "children": [
                                                                              {
                                                                                "kind": "t1",
                                                                                "data": {
                                                                                  "subreddit_id": "t5_81eyvm",
                                                                                  "approved_at_utc": null,
                                                                                  "author_is_blocked": false,
                                                                                  "comment_type": null,
                                                                                  "awarders": [],
                                                                                  "mod_reason_by": null,
                                                                                  "banned_by": null,
                                                                                  "author_flair_type": "text",
                                                                                  "total_awards_received": 0,
                                                                                  "subreddit": "LocalLLaMA",
                                                                                  "author_flair_template_id": null,
                                                                                  "distinguished": null,
                                                                                  "likes": null,
                                                                                  "replies": "",
                                                                                  "user_reports": [],
                                                                                  "saved": false,
                                                                                  "id": "n7bm42k",
                                                                                  "banned_at_utc": null,
                                                                                  "mod_reason_title": null,
                                                                                  "gilded": 0,
                                                                                  "archived": false,
                                                                                  "collapsed_reason_code": null,
                                                                                  "no_follow": true,
                                                                                  "author": "eloquentemu",
                                                                                  "can_mod_post": false,
                                                                                  "created_utc": 1754522298,
                                                                                  "send_replies": true,
                                                                                  "parent_id": "t1_n7aym5v",
                                                                                  "score": 1,
                                                                                  "author_fullname": "t2_lpdsy",
                                                                                  "approved_by": null,
                                                                                  "mod_note": null,
                                                                                  "all_awardings": [],
                                                                                  "body": "&gt; Ok, so an RTX Pro 6000 and a 5090\n\nThat would only fit the smallest quant and not a lot of context.  Why would you rather run the big model compressed into nothing than the smaller model?",
                                                                                  "edited": false,
                                                                                  "gildings": {},
                                                                                  "author_flair_css_class": null,
                                                                                  "name": "t1_n7bm42k",
                                                                                  "is_submitter": false,
                                                                                  "downs": 0,
                                                                                  "author_flair_richtext": [],
                                                                                  "author_patreon_flair": false,
                                                                                  "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Ok, so an RTX Pro 6000 and a 5090&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That would only fit the smallest quant and not a lot of context.  Why would you rather run the big model compressed into nothing than the smaller model?&lt;/p&gt;\n&lt;/div&gt;",
                                                                                  "removal_reason": null,
                                                                                  "collapsed_reason": null,
                                                                                  "link_id": "t3_1mj9e2y",
                                                                                  "associated_award": null,
                                                                                  "stickied": false,
                                                                                  "author_premium": false,
                                                                                  "can_gild": false,
                                                                                  "top_awarded_type": null,
                                                                                  "unrepliable_reason": null,
                                                                                  "author_flair_text_color": null,
                                                                                  "score_hidden": false,
                                                                                  "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7bm42k/",
                                                                                  "subreddit_type": "public",
                                                                                  "locked": false,
                                                                                  "report_reasons": null,
                                                                                  "created": 1754522298,
                                                                                  "author_flair_text": null,
                                                                                  "treatment_tags": [],
                                                                                  "collapsed": false,
                                                                                  "subreddit_name_prefixed": "r/LocalLLaMA",
                                                                                  "controversiality": 0,
                                                                                  "depth": 7,
                                                                                  "author_flair_background_color": null,
                                                                                  "collapsed_because_crowd_control": null,
                                                                                  "mod_reports": [],
                                                                                  "num_reports": null,
                                                                                  "ups": 1
                                                                                }
                                                                              }
                                                                            ],
                                                                            "before": null
                                                                          }
                                                                        },
                                                                        "user_reports": [],
                                                                        "saved": false,
                                                                        "id": "n7aym5v",
                                                                        "banned_at_utc": null,
                                                                        "mod_reason_title": null,
                                                                        "gilded": 0,
                                                                        "archived": false,
                                                                        "collapsed_reason_code": null,
                                                                        "no_follow": true,
                                                                        "author": "devshore",
                                                                        "can_mod_post": false,
                                                                        "send_replies": true,
                                                                        "parent_id": "t1_n7ap6zo",
                                                                        "score": 1,
                                                                        "author_fullname": "t2_ufbr1m7p",
                                                                        "approved_by": null,
                                                                        "mod_note": null,
                                                                        "all_awardings": [],
                                                                        "collapsed": false,
                                                                        "body": "Ok, so an RTX Pro 6000 and a 5090",
                                                                        "edited": false,
                                                                        "gildings": {},
                                                                        "author_flair_css_class": null,
                                                                        "name": "t1_n7aym5v",
                                                                        "is_submitter": false,
                                                                        "downs": 0,
                                                                        "author_flair_richtext": [],
                                                                        "author_patreon_flair": false,
                                                                        "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ok, so an RTX Pro 6000 and a 5090&lt;/p&gt;\n&lt;/div&gt;",
                                                                        "removal_reason": null,
                                                                        "collapsed_reason": null,
                                                                        "link_id": "t3_1mj9e2y",
                                                                        "associated_award": null,
                                                                        "stickied": false,
                                                                        "author_premium": false,
                                                                        "can_gild": false,
                                                                        "top_awarded_type": null,
                                                                        "unrepliable_reason": null,
                                                                        "author_flair_text_color": null,
                                                                        "score_hidden": false,
                                                                        "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7aym5v/",
                                                                        "subreddit_type": "public",
                                                                        "locked": false,
                                                                        "report_reasons": null,
                                                                        "created": 1754514857,
                                                                        "author_flair_text": null,
                                                                        "treatment_tags": [],
                                                                        "created_utc": 1754514857,
                                                                        "subreddit_name_prefixed": "r/LocalLLaMA",
                                                                        "controversiality": 0,
                                                                        "depth": 6,
                                                                        "author_flair_background_color": null,
                                                                        "collapsed_because_crowd_control": null,
                                                                        "mod_reports": [],
                                                                        "num_reports": null,
                                                                        "ups": 1
                                                                      }
                                                                    }
                                                                  ],
                                                                  "before": null
                                                                }
                                                              },
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n7ap6zo",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "eloquentemu",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n7aib5d",
                                                              "score": 2,
                                                              "author_fullname": "t2_lpdsy",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "The Q4_K_M is 202GB, which is what puts it in the 'inaccessible' class since most desktop PCs cap out at 128GB of RAM.  (You can go higher with DDR5 but it's expensive.)\n\nRealistically, if you offload all the non-expert tensors (i.e. `--cpu-moe` or `-ot exps=CPU`) you need 14GB VRAM for 16k of q8_0 quantized context.  Thus I would consider 16GB of VRAM to be a functional minimum (and I think it needs to be a single GPU, I don't think 2x8GB would work well, but IDK for sure).  You will still need ~190GB of RAM to fit the experts on CPU, though.  If you have more VRAM you can speed it up a little, but nothing less than like 48GB is going to make a huge difference since the CPU will be the bottleneck.\n\nOf course, that's Q4.  You could get like a IQ1_M (108GB) which, with some layers offloaded, would run okay on a more limited system (8-12GB VRAM, 128GB RAM) but at Q1 it's going to probably be unusually brain damaged and you'd be better off running Air.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n7ap6zo",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The Q4_K_M is 202GB, which is what puts it in the &amp;#39;inaccessible&amp;#39; class since most desktop PCs cap out at 128GB of RAM.  (You can go higher with DDR5 but it&amp;#39;s expensive.)&lt;/p&gt;\n\n&lt;p&gt;Realistically, if you offload all the non-expert tensors (i.e. &lt;code&gt;--cpu-moe&lt;/code&gt; or &lt;code&gt;-ot exps=CPU&lt;/code&gt;) you need 14GB VRAM for 16k of q8_0 quantized context.  Thus I would consider 16GB of VRAM to be a functional minimum (and I think it needs to be a single GPU, I don&amp;#39;t think 2x8GB would work well, but IDK for sure).  You will still need ~190GB of RAM to fit the experts on CPU, though.  If you have more VRAM you can speed it up a little, but nothing less than like 48GB is going to make a huge difference since the CPU will be the bottleneck.&lt;/p&gt;\n\n&lt;p&gt;Of course, that&amp;#39;s Q4.  You could get like a IQ1_M (108GB) which, with some layers offloaded, would run okay on a more limited system (8-12GB VRAM, 128GB RAM) but at Q1 it&amp;#39;s going to probably be unusually brain damaged and you&amp;#39;d be better off running Air.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mj9e2y",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7ap6zo/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754512152,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754512152,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 2
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7aib5d",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "devshore",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7a0tie",
                                                    "score": 1,
                                                    "author_fullname": "t2_ufbr1m7p",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I have googled how much VRAM is needed to run GLM 4.5 and just get answers for Air when I mean the full version. Can it be run locally with 20k ?",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7aib5d",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have googled how much VRAM is needed to run GLM 4.5 and just get answers for Air when I mean the full version. Can it be run locally with 20k ?&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mj9e2y",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7aib5d/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754510220,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754510220,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7a0tie",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "eloquentemu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n79md12",
                                          "score": 3,
                                          "author_fullname": "t2_lpdsy",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Well, if you mean with regards to my post: openrouter.ai or \"we have GLM-4.5 at home\" aka GLM-4.5-Air.\n\nIf you mean what's an optimal build?  As always depends on your budget and comfort level with computes.  For tolerable performance an old DDR4 server is the cheaper option, but DDR5 will basically double what that can do at about double the price :).  (DDR4 dual-socket might be more interesting if NUMA performance bugs ever get resolved.)  I personally went with Epyc 9004 / Genoa and 12x64GB DDR5 and been pretty happy with it.  You can land kind of in the middle with an engineering sample Sapphire Rapids (only $100!) and 8ch of DDR5.  But that goes to my point that it's not an _easy_ option.  If you are interested and want to chat feel free to DM me.\n\nWhile those won't get comparable _peak_ tok/s as the M3 Ultra, they should be less expensive and more expandable and with a discreet GPU you won't see nearly the same performance dropoff as you do with the M3.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7a0tie",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, if you mean with regards to my post: openrouter.ai or &amp;quot;we have GLM-4.5 at home&amp;quot; aka GLM-4.5-Air.&lt;/p&gt;\n\n&lt;p&gt;If you mean what&amp;#39;s an optimal build?  As always depends on your budget and comfort level with computes.  For tolerable performance an old DDR4 server is the cheaper option, but DDR5 will basically double what that can do at about double the price :).  (DDR4 dual-socket might be more interesting if NUMA performance bugs ever get resolved.)  I personally went with Epyc 9004 / Genoa and 12x64GB DDR5 and been pretty happy with it.  You can land kind of in the middle with an engineering sample Sapphire Rapids (only $100!) and 8ch of DDR5.  But that goes to my point that it&amp;#39;s not an &lt;em&gt;easy&lt;/em&gt; option.  If you are interested and want to chat feel free to DM me.&lt;/p&gt;\n\n&lt;p&gt;While those won&amp;#39;t get comparable &lt;em&gt;peak&lt;/em&gt; tok/s as the M3 Ultra, they should be less expensive and more expandable and with a discreet GPU you won&amp;#39;t see nearly the same performance dropoff as you do with the M3.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mj9e2y",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7a0tie/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754505228,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754505228,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n79md12",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Intelligent_Bet_3985",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n79i5ur",
                                "score": 1,
                                "author_fullname": "t2_zh7mlrg9t",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What would be a good choice to run models like GLM-4.5?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79md12",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What would be a good choice to run models like GLM-4.5?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79md12/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754501202,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754501202,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n79r2fs",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "getmevodka",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n79i5ur",
                                "score": 1,
                                "author_fullname": "t2_7uoa6r1b",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "the 256 gb m3 ultra is a good purchase regarding price performance and not wanting to tinker around all the time though imho.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79r2fs",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the 256 gb m3 ultra is a good purchase regarding price performance and not wanting to tinker around all the time though imho.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79r2fs/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754502476,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754502476,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n79i5ur",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1754500062,
                      "send_replies": true,
                      "parent_id": "t1_n79b1id",
                      "score": 2,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I disagree, I feel like I see the M3 Ultra being recommended and purchased with somewhat distressingly frequency :).\n\nI think a large part of the problem isn't so much the allure of 512GB _VRAM_ but more 512GB of any RAM... If someone says \"what can I buy that will run Deepseek?\" the only answer is really M3 Ultra or DIY.  Getting a prebuilt Epyc, Threadripper, Xeon with &gt;512GB  RAM and a GPU is going to cost &gt;$15k.\n\nStill, I definitely agree that maybe the real answer is that if you aren't comfortable with building or buying something SOTA then you shouldn't run SOTA models at home.  There is a wealth of very solid small / mid sized models (esp with GLM-4.5-Air now) so there isn't really a need anymore to chase massive models just for okay performance.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79i5ur",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I disagree, I feel like I see the M3 Ultra being recommended and purchased with somewhat distressingly frequency :).&lt;/p&gt;\n\n&lt;p&gt;I think a large part of the problem isn&amp;#39;t so much the allure of 512GB &lt;em&gt;VRAM&lt;/em&gt; but more 512GB of any RAM... If someone says &amp;quot;what can I buy that will run Deepseek?&amp;quot; the only answer is really M3 Ultra or DIY.  Getting a prebuilt Epyc, Threadripper, Xeon with &amp;gt;512GB  RAM and a GPU is going to cost &amp;gt;$15k.&lt;/p&gt;\n\n&lt;p&gt;Still, I definitely agree that maybe the real answer is that if you aren&amp;#39;t comfortable with building or buying something SOTA then you shouldn&amp;#39;t run SOTA models at home.  There is a wealth of very solid small / mid sized models (esp with GLM-4.5-Air now) so there isn&amp;#39;t really a need anymore to chase massive models just for okay performance.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79i5ur/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754500062,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n79gdxp",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "SandboChang",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n79egyw",
                                "score": 3,
                                "author_fullname": "t2_10icmj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If running LLM is your only reason to have it then I would say so. For the same money, maybe a couple 5090/a bunch of 3090, or just one Pro 6000 will be more interesting. \n\nAfter all it’s better to research more when spending this much money to see what you want the most, is it a balance of speed/quality, or is it to try larger models.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79gdxp",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If running LLM is your only reason to have it then I would say so. For the same money, maybe a couple 5090/a bunch of 3090, or just one Pro 6000 will be more interesting. &lt;/p&gt;\n\n&lt;p&gt;After all it’s better to research more when spending this much money to see what you want the most, is it a balance of speed/quality, or is it to try larger models.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79gdxp/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754499575,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754499575,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n79wk99",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "berni8k",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n79egyw",
                                "score": 1,
                                "author_fullname": "t2_hfyjp",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The Mac Studio is still a very power efficient way of running big LLM, but yeah it is never going to be fast. The Macs lack GPU processing for prompts, then lack memory bandwidth for fast inference of large models.\n\nRigs with multiple RTX 3090 cards are bang per buck kings when it comes to running medium sized models (70B to 200B), they are pretty power hungry but run at usable speeds. Once you get to models bigger than that even the 3090 doesn't have the memory bandwidth to generate fast. Only hugely expensive enterprise servers can run 400+B models fast,\n\nThat being said, you might not need as big of a model as you think. Even a 20B model can be impressively smart these days and it runs really fast even on a single GPU.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79wk99",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The Mac Studio is still a very power efficient way of running big LLM, but yeah it is never going to be fast. The Macs lack GPU processing for prompts, then lack memory bandwidth for fast inference of large models.&lt;/p&gt;\n\n&lt;p&gt;Rigs with multiple RTX 3090 cards are bang per buck kings when it comes to running medium sized models (70B to 200B), they are pretty power hungry but run at usable speeds. Once you get to models bigger than that even the 3090 doesn&amp;#39;t have the memory bandwidth to generate fast. Only hugely expensive enterprise servers can run 400+B models fast,&lt;/p&gt;\n\n&lt;p&gt;That being said, you might not need as big of a model as you think. Even a 20B model can be impressively smart these days and it runs really fast even on a single GPU.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79wk99/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754504013,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754504013,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n79egyw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ChevChance",
                      "can_mod_post": false,
                      "created_utc": 1754499047,
                      "send_replies": true,
                      "parent_id": "t1_n79b1id",
                      "score": 2,
                      "author_fullname": "t2_sk7nmjrs",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "good points. I'll return it to an apple store at the weekend.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79egyw",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;good points. I&amp;#39;ll return it to an apple store at the weekend.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79egyw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754499047,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n79s3ao",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Spirited_Example_341",
                      "can_mod_post": false,
                      "created_utc": 1754502755,
                      "send_replies": true,
                      "parent_id": "t1_n79b1id",
                      "score": 1,
                      "author_fullname": "t2_122x8ksifg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "this is why we need those TPS reports!!!!!!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79s3ao",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;this is why we need those TPS reports!!!!!!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79s3ao/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754502755,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n79b1id",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "SandboChang",
            "can_mod_post": false,
            "created_utc": 1754498080,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 13,
            "author_fullname": "t2_10icmj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think this has been discussed here over and over, and that you don't see many buying M3 Ultra just for LLM inference is a clear message, even though it looks like a steal on face value.\n\nWhile its huge RAM size is a big advantage that you can load a lot more models comparing to consumer GPUs, the slow PP, and drop in TPS over longer context can really make you doubt if this is the performance you are spending a lump sum on.\n\nInstead, for home inference I would get one or a couple (if budget allows) faster GPUs, and try to focus on those small but capable models like Qwen3 Coder 30B and now gpt-oss 20B.",
            "edited": 1754498702,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79b1id",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think this has been discussed here over and over, and that you don&amp;#39;t see many buying M3 Ultra just for LLM inference is a clear message, even though it looks like a steal on face value.&lt;/p&gt;\n\n&lt;p&gt;While its huge RAM size is a big advantage that you can load a lot more models comparing to consumer GPUs, the slow PP, and drop in TPS over longer context can really make you doubt if this is the performance you are spending a lump sum on.&lt;/p&gt;\n\n&lt;p&gt;Instead, for home inference I would get one or a couple (if budget allows) faster GPUs, and try to focus on those small but capable models like Qwen3 Coder 30B and now gpt-oss 20B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79b1id/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754498080,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 13
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n79f591",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "burner_sb",
            "can_mod_post": false,
            "created_utc": 1754499232,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 2,
            "author_fullname": "t2_1ez41r3ib1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You get a Mac because you want a Mac, and you get one with a lot of RAM because you want to run things with a lot of RAM or because you're curious about playing with big models. LLM inference is mostly something fun you can do with it, though increasingly the MoE models can be fast enough.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79f591",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You get a Mac because you want a Mac, and you get one with a lot of RAM because you want to run things with a lot of RAM or because you&amp;#39;re curious about playing with big models. LLM inference is mostly something fun you can do with it, though increasingly the MoE models can be fast enough.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79f591/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754499232,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n79pxsr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "pj-frey",
                      "can_mod_post": false,
                      "created_utc": 1754502170,
                      "send_replies": true,
                      "parent_id": "t1_n79lv3z",
                      "score": 1,
                      "author_fullname": "t2_1cqmvvyq8a",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "And the noice of the fans...",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79pxsr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And the noice of the fans...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79pxsr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754502170,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n79lv3z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "_hephaestus",
            "can_mod_post": false,
            "created_utc": 1754501068,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 2,
            "author_fullname": "t2_158x8q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What were you trying to do with it and what did you expect? I’m happy with mine, not the snappiest but I don’t want to run multiple 3090s and the performance seems fine for asynchronous things like paperless-ai and chatting.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79lv3z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What were you trying to do with it and what did you expect? I’m happy with mine, not the snappiest but I don’t want to run multiple 3090s and the performance seems fine for asynchronous things like paperless-ai and chatting.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79lv3z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754501068,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7av8e6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ChevChance",
                      "can_mod_post": false,
                      "created_utc": 1754513858,
                      "send_replies": true,
                      "parent_id": "t1_n7aeq4n",
                      "score": 1,
                      "author_fullname": "t2_sk7nmjrs",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Do you need tons of vram for fine tuning?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7av8e6",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you need tons of vram for fine tuning?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7av8e6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754513858,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7aeq4n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "chisleu",
            "can_mod_post": false,
            "created_utc": 1754509202,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 2,
            "author_fullname": "t2_cbxyn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You are doing it wrong. I made the same purchase. 512GB Studio 4TB SSD.\n\nIt's not an inference power house, but it can fine tune models using mlx.lm\n\nStart fine tuning models bro.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7aeq4n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You are doing it wrong. I made the same purchase. 512GB Studio 4TB SSD.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not an inference power house, but it can fine tune models using mlx.lm&lt;/p&gt;\n\n&lt;p&gt;Start fine tuning models bro.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7aeq4n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754509202,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n79fbth",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Maleficent_Age1577",
            "can_mod_post": false,
            "created_utc": 1754499283,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 3,
            "author_fullname": "t2_gxl5vlowd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you were expecting nvidia performance from mac with no gpu then you never kind of searched what you are buying.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79fbth",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you were expecting nvidia performance from mac with no gpu then you never kind of searched what you are buying.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79fbth/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754499283,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n79k6zc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "-dysangel-",
                      "can_mod_post": false,
                      "created_utc": 1754500616,
                      "send_replies": true,
                      "parent_id": "t1_n79gn04",
                      "score": 2,
                      "author_fullname": "t2_12ggykute6",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "it's the opposite. Good bandwidth and good output speed, but poor prompt processing. Once there are more efficient attention algorithms in mainstream use, it will start to get more useful IMO",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79k6zc",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;it&amp;#39;s the opposite. Good bandwidth and good output speed, but poor prompt processing. Once there are more efficient attention algorithms in mainstream use, it will start to get more useful IMO&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79k6zc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754500616,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n79gn04",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Traditional_Bet8239",
            "can_mod_post": false,
            "created_utc": 1754499643,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 1,
            "author_fullname": "t2_mw9ag8b4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How much ram are you utilizing? That could run a pretty hefty model although the bandwidth somewhat limits the output speed",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79gn04",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How much ram are you utilizing? That could run a pretty hefty model although the bandwidth somewhat limits the output speed&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79gn04/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754499643,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n79za1g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FORLLM",
            "can_mod_post": false,
            "created_utc": 1754504784,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 1,
            "author_fullname": "t2_1rav5zv0dv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "To anyone, how's mac support for other kinds of inference, like audio and video? Speed aside, is there actual support at all?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79za1g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To anyone, how&amp;#39;s mac support for other kinds of inference, like audio and video? Speed aside, is there actual support at all?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79za1g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754504784,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7bnjcn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "BanaBreadSingularity",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7bk2ou",
                                "score": 1,
                                "author_fullname": "t2_11d4n76hca",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Agree on the nuance you mention.\n\nI think for the fact that it's free content and he goes pretty in depth wrt to power, price as well as performance differential on \"vanilla\" models resp. 8b vs. 4b quantizations its fantastic content. \n\nStrong guidance for anyone looking to buy hardware in the field with solid baselines. \n\nThe speed differentials he displays for base loads are obviously gonna hold up (or worsen) under more intense load. \n\nI love him for these transparent and detailed baseline comparisons of Mac models, resp. GPUs.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7bnjcn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Agree on the nuance you mention.&lt;/p&gt;\n\n&lt;p&gt;I think for the fact that it&amp;#39;s free content and he goes pretty in depth wrt to power, price as well as performance differential on &amp;quot;vanilla&amp;quot; models resp. 8b vs. 4b quantizations its fantastic content. &lt;/p&gt;\n\n&lt;p&gt;Strong guidance for anyone looking to buy hardware in the field with solid baselines. &lt;/p&gt;\n\n&lt;p&gt;The speed differentials he displays for base loads are obviously gonna hold up (or worsen) under more intense load. &lt;/p&gt;\n\n&lt;p&gt;I love him for these transparent and detailed baseline comparisons of Mac models, resp. GPUs.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj9e2y",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7bnjcn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754522761,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754522761,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bk2ou",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MrMisterShin",
                      "can_mod_post": false,
                      "created_utc": 1754521641,
                      "send_replies": true,
                      "parent_id": "t1_n7a1j5k",
                      "score": 1,
                      "author_fullname": "t2_3dhighjq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "His comparisons are basic and aren’t great imo. \n\nHe doesn’t use enough context in his prompt, like if you were using agentic coding like Cline / Roo code etc (32k context minimum). \n\nWhen you start to use models beyond basic chat. Like using RAG, MCP, Web Search etc… He should be performing actions like these in his comparisons, because they fill up context and stress the hardware / bandwidth beyond basic short LLM chat prompt.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bk2ou",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;His comparisons are basic and aren’t great imo. &lt;/p&gt;\n\n&lt;p&gt;He doesn’t use enough context in his prompt, like if you were using agentic coding like Cline / Roo code etc (32k context minimum). &lt;/p&gt;\n\n&lt;p&gt;When you start to use models beyond basic chat. Like using RAG, MCP, Web Search etc… He should be performing actions like these in his comparisons, because they fill up context and stress the hardware / bandwidth beyond basic short LLM chat prompt.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7bk2ou/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754521641,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7apu86",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "zipzag",
                      "can_mod_post": false,
                      "created_utc": 1754512333,
                      "send_replies": true,
                      "parent_id": "t1_n7a1j5k",
                      "score": 0,
                      "author_fullname": "t2_59qc7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "He doesn't compare quality between local LLMs and the frontier models. I don't find tokens per second particularly useful.\n\nHe's a dev, yet he doesn't give an opinion of the local LLMs for coding.\n\nMy experience, which is common, is that for coding the difference between local and the big LLMs is more than what the test scores indicate. That gap will eventually close I think.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7apu86",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;He doesn&amp;#39;t compare quality between local LLMs and the frontier models. I don&amp;#39;t find tokens per second particularly useful.&lt;/p&gt;\n\n&lt;p&gt;He&amp;#39;s a dev, yet he doesn&amp;#39;t give an opinion of the local LLMs for coding.&lt;/p&gt;\n\n&lt;p&gt;My experience, which is common, is that for coding the difference between local and the big LLMs is more than what the test scores indicate. That gap will eventually close I think.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj9e2y",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7apu86/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754512333,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7a1j5k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BanaBreadSingularity",
            "can_mod_post": false,
            "created_utc": 1754505425,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 1,
            "author_fullname": "t2_11d4n76hca",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "FWIW, OP, this guy does in depth comparisons of Nvidia vs. M3/M2 Ultra (or other Studio chips) on his channel. \n\nNot affiliated, just a fan of his relevant, and high quality content: https://www.youtube.com/@AZisk",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7a1j5k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;FWIW, OP, this guy does in depth comparisons of Nvidia vs. M3/M2 Ultra (or other Studio chips) on his channel. &lt;/p&gt;\n\n&lt;p&gt;Not affiliated, just a fan of his relevant, and high quality content: &lt;a href=\"https://www.youtube.com/@AZisk\"&gt;https://www.youtube.com/@AZisk&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7a1j5k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754505425,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ajodr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1754510607,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 1,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How much did you pay for it? Which models and quants are you using? How much context and what pp and tg speed do you get?\nThx.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ajodr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How much did you pay for it? Which models and quants are you using? How much context and what pp and tg speed do you get?\nThx.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7ajodr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754510607,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7bsa1y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "philguyaz",
            "can_mod_post": false,
            "created_utc": 1754524324,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 1,
            "author_fullname": "t2_36dfv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I bought one and it's doing great as a development backbone for my AI SaaS product. It really excells with MoE models imo. It gives me 75% of the performance of a B200 server I have doing production over reasonable context.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bsa1y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I bought one and it&amp;#39;s doing great as a development backbone for my AI SaaS product. It really excells with MoE models imo. It gives me 75% of the performance of a B200 server I have doing production over reasonable context.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n7bsa1y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754524324,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n79kk0r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754500714,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 1,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I say every single time someone asks “should I buy a Mac for inference?” that the answer is a hard “no” unless you’re ok with it being super slow.\n\nIt has no real GPU. It has no PCIe with which to add a GPU. It cannot do prompt processing worth a f*ck.\n\nIn the context of LLMs the Mac is sadly a toy, not a tool. Sorry you learned this the expensive way.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79kk0r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I say every single time someone asks “should I buy a Mac for inference?” that the answer is a hard “no” unless you’re ok with it being super slow.&lt;/p&gt;\n\n&lt;p&gt;It has no real GPU. It has no PCIe with which to add a GPU. It cannot do prompt processing worth a f*ck.&lt;/p&gt;\n\n&lt;p&gt;In the context of LLMs the Mac is sadly a toy, not a tool. Sorry you learned this the expensive way.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79kk0r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754500714,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n79fk0o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tetherbot",
            "can_mod_post": false,
            "created_utc": 1754499346,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 1,
            "author_fullname": "t2_oa6ixox",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thanks for sharing this experience. As someone who has considered a Studio for this purpose, I think this is a useful real-world anecdote.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79fk0o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for sharing this experience. As someone who has considered a Studio for this purpose, I think this is a useful real-world anecdote.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79fk0o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754499346,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n79cqc7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "custodiam99",
            "can_mod_post": false,
            "created_utc": 1754498564,
            "send_replies": true,
            "parent_id": "t3_1mj9e2y",
            "score": 0,
            "author_fullname": "t2_nqnhgqqf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes, RAM versus VRAM versus bandwidth. SOTA small models changed the equation.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79cqc7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, RAM versus VRAM versus bandwidth. SOTA small models changed the equation.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj9e2y/underwhelmed_by_512gb_m3_ultra_mac_studio/n79cqc7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754498564,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj9e2y",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]