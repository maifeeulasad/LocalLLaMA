[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Why hasn't LoRA gained more popularity?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1maq0hg",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 56,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_12y48q",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 56,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753632201,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;In my impression, the focus is mostly on MCP, A2A, and RAG. While these are great for their respective use cases, you still have to send prompts to LLMs with 70 to 500 billion parameters, which is quite resource-intensive and expensive. The alternative is to settle for one of the smaller LLMs with around 8 billion parameters, but then the experience can feel too inconsistent. In search of a solution, I recently stumbled upon LoRA, which to my understanding, allows you to use a smaller LLM as a base and fine-tune it to become an expert in very specific topics. This results in a model that’s lighter and faster to run, with output that’s comparable (in a specific domain) to that of a 500-billion-parameter model. If that’s the case, why hasn’t there been more noticeable interest in fine-tuning with LoRA? I can imagine this could save a lot of money for businesses planning to build systems that rely on LLMs for constant inference.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1maq0hg",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "dabomb007",
            "discussion_type": null,
            "num_comments": 45,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/",
            "subreddit_subscribers": 505617,
            "created_utc": 1753632201,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5il876",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "moko990",
                      "can_mod_post": false,
                      "created_utc": 1753657533,
                      "send_replies": true,
                      "parent_id": "t1_n5gd5f0",
                      "score": 5,
                      "author_fullname": "t2_1kh1rmhlhh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;  I'm not sure of the reason, maybe merging LoRAs or models is too fiddly and the average user wouldn't get good results.\n\nActually there were lots of merges back in the days, look up miku merges that dominated the scene for quite a while here.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5il876",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I&amp;#39;m not sure of the reason, maybe merging LoRAs or models is too fiddly and the average user wouldn&amp;#39;t get good results.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Actually there were lots of merges back in the days, look up miku merges that dominated the scene for quite a while here.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5il876/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753657533,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5j2yox",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Shadow-Amulet-Ambush",
                      "can_mod_post": false,
                      "created_utc": 1753663942,
                      "send_replies": true,
                      "parent_id": "t1_n5gd5f0",
                      "score": 2,
                      "author_fullname": "t2_1loou9xu",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I think it’s just a vram thing. VRAM comes at a premium if you’re getting cards with any decent speed. 1 model plus 1 lora is more space than 1 model with a lora merged into it",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5j2yox",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think it’s just a vram thing. VRAM comes at a premium if you’re getting cards with any decent speed. 1 model plus 1 lora is more space than 1 model with a lora merged into it&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5j2yox/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753663942,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5gd5f0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1753632955,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 36,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's not about the use of LoRAs, it's about the use of fine tuning. Many fine tunes are made as LoRA but released as full model. I think it's because a merge that is quantized uses less VRAM and is easier to use than a LoRA. In a way, you can think of a LoRA as a \"diff\" between a model and its fine tune. The main advantage of a LoRA is being able to apply multiple of them to varying degrees, but in the LLM world that's usually made by merging the models directly, for some reason. I'm not sure of the reason, maybe merging LoRAs or models is too fiddly and the average user wouldn't get good results.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gd5f0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not about the use of LoRAs, it&amp;#39;s about the use of fine tuning. Many fine tunes are made as LoRA but released as full model. I think it&amp;#39;s because a merge that is quantized uses less VRAM and is easier to use than a LoRA. In a way, you can think of a LoRA as a &amp;quot;diff&amp;quot; between a model and its fine tune. The main advantage of a LoRA is being able to apply multiple of them to varying degrees, but in the LLM world that&amp;#39;s usually made by merging the models directly, for some reason. I&amp;#39;m not sure of the reason, maybe merging LoRAs or models is too fiddly and the average user wouldn&amp;#39;t get good results.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gd5f0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753632955,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 36
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5h27xx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "literum",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gwkpo",
                                "score": 7,
                                "author_fullname": "t2_6ac9lmhf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Which task? Try training a better 0.6b coding model than Sonnet 4. You can use all the data centers in the world if you want.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5h27xx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Which task? Try training a better 0.6b coding model than Sonnet 4. You can use all the data centers in the world if you want.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h27xx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753640232,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753640232,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 7
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gwkpo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Popular_Brief335",
                      "can_mod_post": false,
                      "created_utc": 1753638577,
                      "send_replies": true,
                      "parent_id": "t1_n5gcnjz",
                      "score": 0,
                      "author_fullname": "t2_1j9oxxzd6c",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Meh I can make 0.6B models smash medium models just fine ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gwkpo",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Meh I can make 0.6B models smash medium models just fine &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gwkpo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753638577,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5gcnjz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Jbbrack03",
            "can_mod_post": false,
            "created_utc": 1753632809,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 19,
            "author_fullname": "t2_zc0gl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "While having specialized knowledge is good, you'd be surprised how often it needs generalized knowledge to accompany it. It's often used by LLM's as context for a decision or direction. That's why large parameter LLM's consistently outscore small specialized LLM's in the exact field that the smaller LLM is specialized in.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gcnjz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;While having specialized knowledge is good, you&amp;#39;d be surprised how often it needs generalized knowledge to accompany it. It&amp;#39;s often used by LLM&amp;#39;s as context for a decision or direction. That&amp;#39;s why large parameter LLM&amp;#39;s consistently outscore small specialized LLM&amp;#39;s in the exact field that the smaller LLM is specialized in.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gcnjz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753632809,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 19
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5hu1vi",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "mpasila",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5hnlwd",
                                "score": 3,
                                "author_fullname": "t2_lhhagpdw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Free tier gives you T4 GPU which has 15gb of VRAM so not that much. Also you have to keep the session active or it will close it.. so it's kinda fighting you from training models. Better off just using Kaggle instead.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5hu1vi",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Free tier gives you T4 GPU which has 15gb of VRAM so not that much. Also you have to keep the session active or it will close it.. so it&amp;#39;s kinda fighting you from training models. Better off just using Kaggle instead.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5hu1vi/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753648688,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753648688,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5i1zih",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "stereoplegic",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5hnlwd",
                                "score": 1,
                                "author_fullname": "t2_4icne",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "You can with QLoRA by loading model weights quantized in NF4 (4-bit normalized float) and with gradient checkpointing, up to at least a 20b model according to Hugging Face's GPT-NeoX Colab demo at the time the QLoRA paper was published, but both of those will slow down the finetune. \n\nYou'll also run up against free Colab's storage limits (also 15gb IIRC?) and 30hr(?)/wk runtime limit.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5i1zih",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can with QLoRA by loading model weights quantized in NF4 (4-bit normalized float) and with gradient checkpointing, up to at least a 20b model according to Hugging Face&amp;#39;s GPT-NeoX Colab demo at the time the QLoRA paper was published, but both of those will slow down the finetune. &lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ll also run up against free Colab&amp;#39;s storage limits (also 15gb IIRC?) and 30hr(?)/wk runtime limit.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5i1zih/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753651138,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753651138,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5hnlwd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ExtremeAcceptable289",
                      "can_mod_post": false,
                      "created_utc": 1753646730,
                      "send_replies": true,
                      "parent_id": "t1_n5gcd55",
                      "score": 2,
                      "author_fullname": "t2_8hpbax1b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Couldnt you use google colab",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5hnlwd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Couldnt you use google colab&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5hnlwd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753646730,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5irpia",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HilLiedTroopsDied",
                      "can_mod_post": false,
                      "created_utc": 1753659790,
                      "send_replies": true,
                      "parent_id": "t1_n5gcd55",
                      "score": 2,
                      "author_fullname": "t2_1snfn3ui",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Getting a clean and sanitized/formatted dataset is the real time consuming part.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5irpia",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Getting a clean and sanitized/formatted dataset is the real time consuming part.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5irpia/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753659790,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5hxjch",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "__SlimeQ__",
                      "can_mod_post": false,
                      "created_utc": 1753649752,
                      "send_replies": true,
                      "parent_id": "t1_n5gcd55",
                      "score": 3,
                      "author_fullname": "t2_olbav",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "that's not true, I do loras in oobabooga on a 4060ti (16gb) and it's fine. if you can run inference you can run fine tuning. maybe a day of compute for a solid model.\n\nHonestly this whole line of thinking makes no sense. nearly every model on huggingface is a lora merge. the reason we started merging loras and redistributing the full weights is because lora had a memory leak that prevented it from running in production. the best way to deploy is/was to merge your lora into the base model, then convert to gguf.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5hxjch",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;that&amp;#39;s not true, I do loras in oobabooga on a 4060ti (16gb) and it&amp;#39;s fine. if you can run inference you can run fine tuning. maybe a day of compute for a solid model.&lt;/p&gt;\n\n&lt;p&gt;Honestly this whole line of thinking makes no sense. nearly every model on huggingface is a lora merge. the reason we started merging loras and redistributing the full weights is because lora had a memory leak that prevented it from running in production. the best way to deploy is/was to merge your lora into the base model, then convert to gguf.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5hxjch/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753649752,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5gcd55",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "simracerman",
            "can_mod_post": false,
            "created_utc": 1753632725,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 39,
            "author_fullname": "t2_vbzgnic",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MCP and RAG are quite accessible. LoRA is not. Training/fine tuning requires Nvidia powerful cards with sufficient amount of VRAM. That narrows it down to 3090/4090, maybe some 5000 series with 16GB. Then you have the data needed to fine tune a model. It takes time and resources that are inaccessible to many.\n\nThat said, someone owning a 3090 and above is likely satisfied with the output of good models like Qwen3, Gemma3 and LoRA is a not needed badly there anyways.\n\nI think LoRA would’ve been a bigger hit if we had it on more affordable hardware.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gcd55",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MCP and RAG are quite accessible. LoRA is not. Training/fine tuning requires Nvidia powerful cards with sufficient amount of VRAM. That narrows it down to 3090/4090, maybe some 5000 series with 16GB. Then you have the data needed to fine tune a model. It takes time and resources that are inaccessible to many.&lt;/p&gt;\n\n&lt;p&gt;That said, someone owning a 3090 and above is likely satisfied with the output of good models like Qwen3, Gemma3 and LoRA is a not needed badly there anyways.&lt;/p&gt;\n\n&lt;p&gt;I think LoRA would’ve been a bigger hit if we had it on more affordable hardware.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gcd55/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753632725,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 39
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5h3am2",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Mbando",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5h0x06",
                                          "score": 2,
                                          "author_fullname": "t2_kmzba",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Haha that’s totally right!! \n\nIt was a pilot to show tradeoffs for model size in very specific domains. We ended up building a production version for another service for deployment on SECRET systems for intel analysis.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5h3am2",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Haha that’s totally right!! &lt;/p&gt;\n\n&lt;p&gt;It was a pilot to show tradeoffs for model size in very specific domains. We ended up building a production version for another service for deployment on SECRET systems for intel analysis.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1maq0hg",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h3am2/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753640552,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753640552,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5h0x06",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "JollyJoker3",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gv5bz",
                                "score": 3,
                                "author_fullname": "t2_5sd4mer7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Since I had to ask an LLM\n\n&gt;This comment is describing a specialized AI system built for military doctrine retrieval. Let me break down what they accomplished:\n\n&gt;**The System Architecture:** They built a RAG (Retrieval-Augmented Generation) system specifically for US Army doctrine. RAG combines a knowledge base with a language model - when you ask a question, it first searches the knowledge base for relevant information, then feeds that context to the language model to generate an answer.\n\n&gt;**The Knowledge Base:** They curated a collection of US Army doctrinal publications and Field Manuals (FMs). These are the official documents that define military procedures, tactics, and protocols.\n\n&gt;**The Model:** Instead of using a general-purpose model, they fine-tuned (FT) a 7-billion parameter Mistral model specifically on this military content. This specialization made it much better at understanding and responding to military doctrine questions than GPT-4, even though GPT-4 is generally more capable.\n\n&gt;**The LIMA Reference:** LIMA (Less Is More for Alignment) is a principle showing that you can achieve strong performance with relatively small amounts of high-quality training data. They applied this in two ways:\n\n&gt;**Model training**: Used carefully selected, high-quality military content for fine-tuning rather than massive amounts of data\n\n&gt;**Retrieval**: Curated their knowledge base with only the most relevant, authoritative sources rather than including everything\n\n&gt;The key insight is that domain-specific fine-tuning with carefully selected data can outperform much larger general models when working within that specialized domain. Their military-focused system understood doctrine better than GPT-4 because it was purpose-built for that exact use case.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5h0x06",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Since I had to ask an LLM&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;This comment is describing a specialized AI system built for military doctrine retrieval. Let me break down what they accomplished:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The System Architecture:&lt;/strong&gt; They built a RAG (Retrieval-Augmented Generation) system specifically for US Army doctrine. RAG combines a knowledge base with a language model - when you ask a question, it first searches the knowledge base for relevant information, then feeds that context to the language model to generate an answer.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Knowledge Base:&lt;/strong&gt; They curated a collection of US Army doctrinal publications and Field Manuals (FMs). These are the official documents that define military procedures, tactics, and protocols.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Model:&lt;/strong&gt; Instead of using a general-purpose model, they fine-tuned (FT) a 7-billion parameter Mistral model specifically on this military content. This specialization made it much better at understanding and responding to military doctrine questions than GPT-4, even though GPT-4 is generally more capable.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The LIMA Reference:&lt;/strong&gt; LIMA (Less Is More for Alignment) is a principle showing that you can achieve strong performance with relatively small amounts of high-quality training data. They applied this in two ways:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model training&lt;/strong&gt;: Used carefully selected, high-quality military content for fine-tuning rather than massive amounts of data&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Retrieval&lt;/strong&gt;: Curated their knowledge base with only the most relevant, authoritative sources rather than including everything&lt;/p&gt;\n\n&lt;p&gt;The key insight is that domain-specific fine-tuning with carefully selected data can outperform much larger general models when working within that specialized domain. Their military-focused system understood doctrine better than GPT-4 because it was purpose-built for that exact use case.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h0x06/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753639844,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753639844,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5h54xg",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Mbando",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5h4zpl",
                                          "score": 2,
                                          "author_fullname": "t2_kmzba",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "And just to be clear, both test systems were RAG. Just swapped out the model.\n\nAlso on coding, we connected a stock 4o model to a very highly curated code based sample in RAG for a military coding language that is ITAR restricted. So commercial models have not been trained on it. But the combination of a very well curated vector database of code samples plus a really good foundational model works really well. The copilot writes about 70% of the simulation code and then one of our M&amp;S experts fixes the rest.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5h54xg",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And just to be clear, both test systems were RAG. Just swapped out the model.&lt;/p&gt;\n\n&lt;p&gt;Also on coding, we connected a stock 4o model to a very highly curated code based sample in RAG for a military coding language that is ITAR restricted. So commercial models have not been trained on it. But the combination of a very well curated vector database of code samples plus a really good foundational model works really well. The copilot writes about 70% of the simulation code and then one of our M&amp;amp;S experts fixes the rest.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1maq0hg",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h54xg/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753641100,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753641100,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5h4zpl",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "indicava",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gv5bz",
                                "score": 2,
                                "author_fullname": "t2_4dvff",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "RAG + FT sounds like a guaranteed winner.\n\nI mainly deal with fine tuning models around code and programming languages. From my experience (granted which is limited) RAG solutions for code are extremely complex and limited. So I adopted a very rigorous fine tuning pipeline and I’m getting more than decent results.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5h4zpl",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;RAG + FT sounds like a guaranteed winner.&lt;/p&gt;\n\n&lt;p&gt;I mainly deal with fine tuning models around code and programming languages. From my experience (granted which is limited) RAG solutions for code are extremely complex and limited. So I adopted a very rigorous fine tuning pipeline and I’m getting more than decent results.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": true,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h4zpl/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753641057,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753641057,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5is1h2",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "HilLiedTroopsDied",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gv5bz",
                                "score": 1,
                                "author_fullname": "t2_1snfn3ui",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Good old CamoGPT",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5is1h2",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Good old CamoGPT&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5is1h2/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753659905,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753659905,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gv5bz",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Mbando",
                      "can_mod_post": false,
                      "created_utc": 1753638168,
                      "send_replies": true,
                      "parent_id": "t1_n5gfbbh",
                      "score": 6,
                      "author_fullname": "t2_kmzba",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It’s been a while, but we had a really good Army doctrinal RAG stack (curated US Army doctrinal and FM pubs) using a 7b Mistral FT. It way outperformed GPT-4 on the same RAG stack. It was a good example of LIMA both in the model but also in retrieval.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gv5bz",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s been a while, but we had a really good Army doctrinal RAG stack (curated US Army doctrinal and FM pubs) using a 7b Mistral FT. It way outperformed GPT-4 on the same RAG stack. It was a good example of LIMA both in the model but also in retrieval.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gv5bz/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753638168,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n5haebj",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "liquid_bee_3",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n5h8erk",
                                                              "score": 1,
                                                              "author_fullname": "t2_18ijyp2nfl",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "im now wondering just how big is your data? if trained larger models (with experiments, sweeps, etc) in max a week with a LOT of tokens. most private domain data that needs CPT or CLM are not that big.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n5haebj",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;im now wondering just how big is your data? if trained larger models (with experiments, sweeps, etc) in max a week with a LOT of tokens. most private domain data that needs CPT or CLM are not that big.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1maq0hg",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5haebj/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753642682,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753642682,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5h8erk",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": false,
                                                    "author": "indicava",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5h7iwm",
                                                    "score": 4,
                                                    "author_fullname": "t2_4dvff",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "If everything works first time around, sure. \n\nBut what I described above (which I’m actually running now as we speak on vast.ai) is just an experiment. One of maybe 3-4 experiments I’ve done in the last week alone. Not to mention, that after getting the pipeline down perfect I finetune on much larger parameter models. \n\nThose $10-$100 add up pretty quick.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5h8erk",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If everything works first time around, sure. &lt;/p&gt;\n\n&lt;p&gt;But what I described above (which I’m actually running now as we speak on vast.ai) is just an experiment. One of maybe 3-4 experiments I’ve done in the last week alone. Not to mention, that after getting the pipeline down perfect I finetune on much larger parameter models. &lt;/p&gt;\n\n&lt;p&gt;Those $10-$100 add up pretty quick.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1maq0hg",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": true,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h8erk/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753642088,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753642088,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 4
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5h7iwm",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "liquid_bee_3",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5gox79",
                                          "score": 1,
                                          "author_fullname": "t2_18ijyp2nfl",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "H100 on runpod cost next to nothing. even with experimentation u can train a LOT of tokens for nor more than a few 10-100 dollars.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5h7iwm",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;H100 on runpod cost next to nothing. even with experimentation u can train a LOT of tokens for nor more than a few 10-100 dollars.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1maq0hg",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h7iwm/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753641822,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753641822,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gox79",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "indicava",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gmwrl",
                                "score": 4,
                                "author_fullname": "t2_4dvff",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I’ll give you time consuming, if you’ve got a well curated and annotated corpus that is definitely 80% of the time consumed setting this sort of thing up. \n\nBut a full parameter CLM fine tune on a tiny 1.5B parameter model with 16k context and a decent batch size is gonna need about 100GB-150GB VRAM. Not exactly hobbyist territory.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gox79",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’ll give you time consuming, if you’ve got a well curated and annotated corpus that is definitely 80% of the time consumed setting this sort of thing up. &lt;/p&gt;\n\n&lt;p&gt;But a full parameter CLM fine tune on a tiny 1.5B parameter model with 16k context and a decent batch size is gonna need about 100GB-150GB VRAM. Not exactly hobbyist territory.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": true,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gox79/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753636374,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753636374,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gmwrl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "liquid_bee_3",
                      "can_mod_post": false,
                      "created_utc": 1753635789,
                      "send_replies": true,
                      "parent_id": "t1_n5gfbbh",
                      "score": 2,
                      "author_fullname": "t2_18ijyp2nfl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "its not as expensive or time consuming as you think if data is in good shape.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gmwrl",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;its not as expensive or time consuming as you think if data is in good shape.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gmwrl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753635789,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5h0dfw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "JollyJoker3",
                      "can_mod_post": false,
                      "created_utc": 1753639681,
                      "send_replies": true,
                      "parent_id": "t1_n5gfbbh",
                      "score": 2,
                      "author_fullname": "t2_5sd4mer7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Any idea about specialized LoRAs for given programming languages? Coding agents tend to have problems with following best practices, being overly verbose, adding unnecessary stuff etc. I'd be very happy with adding knowledge specific to a given project or company as well, but I'm mostly wondering why we don't have Javascript LoRAs for Claude 4 Sonnet, for example.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5h0dfw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Any idea about specialized LoRAs for given programming languages? Coding agents tend to have problems with following best practices, being overly verbose, adding unnecessary stuff etc. I&amp;#39;d be very happy with adding knowledge specific to a given project or company as well, but I&amp;#39;m mostly wondering why we don&amp;#39;t have Javascript LoRAs for Claude 4 Sonnet, for example.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h0dfw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753639681,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ijz3w",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "superstarbootlegs",
                      "can_mod_post": false,
                      "created_utc": 1753657102,
                      "send_replies": true,
                      "parent_id": "t1_n5gfbbh",
                      "score": 1,
                      "author_fullname": "t2_190bhtiwx7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "this would make sense, I guess. So, if you wanted to write a new Shakespear plays maybe you'd use a Lora trained on the author styling, and MCP for the content.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ijz3w",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;this would make sense, I guess. So, if you wanted to write a new Shakespear plays maybe you&amp;#39;d use a Lora trained on the author styling, and MCP for the content.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5ijz3w/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753657102,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5iwcbe",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Delicious-Farmer-234",
                      "can_mod_post": false,
                      "created_utc": 1753661459,
                      "send_replies": true,
                      "parent_id": "t1_n5gfbbh",
                      "score": 1,
                      "author_fullname": "t2_6x6f3qy5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It is not impossible the model will definitely learn new knowledge using a Q&amp;A pair on a base model. I've done it many times in the past with close domain data. The issue for me has been trying to align the model after using RL. This is the part that has me stuck because there's so many methods. At the end the easiest solution was to use RAG",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5iwcbe",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is not impossible the model will definitely learn new knowledge using a Q&amp;amp;A pair on a base model. I&amp;#39;ve done it many times in the past with close domain data. The issue for me has been trying to align the model after using RL. This is the part that has me stuck because there&amp;#39;s so many methods. At the end the easiest solution was to use RAG&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5iwcbe/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753661459,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ilogr",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "BlipOnNobodysRadar",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5h84qm",
                                "score": 2,
                                "author_fullname": "t2_bwo44iuo",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "In diffusion models they can definitely add new knowledge.\n\nThere has been a ton of advancement in LoRA derivatives that are successfully used in diffusion tuning (even though their original papers were on LLMs). Some of them even claim to outperform full finetuning due to causing less disruption in the weights, such as [ABBA](https://github.com/CERT-Lab/abba).\n\nPeople also push the limits in LoRAs more on diffusion models with advanced optimizers, higher ranks, and all sorts of interesting techniques -- I wonder if the lack of success in LLMs is simply due to lack of motivated experimentation.",
                                "edited": 1753657954,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ilogr",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;In diffusion models they can definitely add new knowledge.&lt;/p&gt;\n\n&lt;p&gt;There has been a ton of advancement in LoRA derivatives that are successfully used in diffusion tuning (even though their original papers were on LLMs). Some of them even claim to outperform full finetuning due to causing less disruption in the weights, such as &lt;a href=\"https://github.com/CERT-Lab/abba\"&gt;ABBA&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;People also push the limits in LoRAs more on diffusion models with advanced optimizers, higher ranks, and all sorts of interesting techniques -- I wonder if the lack of success in LLMs is simply due to lack of motivated experimentation.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maq0hg",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5ilogr/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753657690,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753657690,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5h84qm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "G_S_7_wiz",
                      "can_mod_post": false,
                      "created_utc": 1753642004,
                      "send_replies": true,
                      "parent_id": "t1_n5gfbbh",
                      "score": 1,
                      "author_fullname": "t2_75royzx9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This comment should be the acceptable answer. You can't use LoRA to add custom knowledge to an LLM. We already tried this with  different set of parameters(rank, alpha, quantization, etc). In order to add custom knowledge you need to do continuous pre-training which requires a lot of data as well as compute.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5h84qm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This comment should be the acceptable answer. You can&amp;#39;t use LoRA to add custom knowledge to an LLM. We already tried this with  different set of parameters(rank, alpha, quantization, etc). In order to add custom knowledge you need to do continuous pre-training which requires a lot of data as well as compute.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h84qm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753642004,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5gfbbh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "indicava",
            "can_mod_post": false,
            "created_utc": 1753633586,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 22,
            "author_fullname": "t2_4dvff",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It’s very difficult (to nearly impossible) to “add knowledge” using a LoRA adapter. They’re great for fine tuning prose style, length, etc. \n\nFor adding domain specific knowledge you’re gonna need to do a full parameter fine tune with a full pipeline of CLM/SFT/RL. You’ll end up with a small specialized model which can perform on par or close to frontier models in a specific domain. \n\nThis is a very time consuming and somewhat expensive process. That’s why solutions like RAG or MCP (tool calling), both of which essentially “ground” the LLM’s context are much easier, more accessible and more popular (although not as robust as training a model).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gfbbh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It’s very difficult (to nearly impossible) to “add knowledge” using a LoRA adapter. They’re great for fine tuning prose style, length, etc. &lt;/p&gt;\n\n&lt;p&gt;For adding domain specific knowledge you’re gonna need to do a full parameter fine tune with a full pipeline of CLM/SFT/RL. You’ll end up with a small specialized model which can perform on par or close to frontier models in a specific domain. &lt;/p&gt;\n\n&lt;p&gt;This is a very time consuming and somewhat expensive process. That’s why solutions like RAG or MCP (tool calling), both of which essentially “ground” the LLM’s context are much easier, more accessible and more popular (although not as robust as training a model).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gfbbh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753633586,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 22
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5gsocz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "xadiant",
            "can_mod_post": false,
            "created_utc": 1753637460,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 4,
            "author_fullname": "t2_omgp6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is like asking why trains are not popular. You just aren't looking at the right place. Almost every single low budget fine-tune, including the previously legendary stuff are lora + base model merges.\n\nLora is so popular that you don't even realize it's insanely popular",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gsocz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is like asking why trains are not popular. You just aren&amp;#39;t looking at the right place. Almost every single low budget fine-tune, including the previously legendary stuff are lora + base model merges.&lt;/p&gt;\n\n&lt;p&gt;Lora is so popular that you don&amp;#39;t even realize it&amp;#39;s insanely popular&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gsocz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753637460,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5hpgn7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "tarruda",
                      "can_mod_post": false,
                      "created_utc": 1753647297,
                      "send_replies": true,
                      "parent_id": "t1_n5hgbmd",
                      "score": 1,
                      "author_fullname": "t2_dphk4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; The reason why people don’t finetune a small 8B model instead of using a larger one, is because this approach doesn’t work. You always end up with an overfited model that appears to perform great on a small test set, but in the real world has 0 generalization capabilities.\n\nDo you know one can create a dataset for LoRA fine tuning that doesn't overfit the model?\n\nI've been thinking of creating my own dataset for agentic coding, but haven't done so because I have no idea what kind of examples to use in the dataset. System prompt \"fine tuning\" always feels safer.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5hpgn7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The reason why people don’t finetune a small 8B model instead of using a larger one, is because this approach doesn’t work. You always end up with an overfited model that appears to perform great on a small test set, but in the real world has 0 generalization capabilities.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Do you know one can create a dataset for LoRA fine tuning that doesn&amp;#39;t overfit the model?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve been thinking of creating my own dataset for agentic coding, but haven&amp;#39;t done so because I have no idea what kind of examples to use in the dataset. System prompt &amp;quot;fine tuning&amp;quot; always feels safer.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5hpgn7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753647297,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5hgbmd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ikergarcia1996",
            "can_mod_post": false,
            "created_utc": 1753644486,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 5,
            "author_fullname": "t2_xpjbc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "LoRA is used everywhere. The finetuning service of OpenAI, or image/video companies are training LoRAs. Many of the post training stages of LLMs are done with LoRA instead of full finetuning… \n\nThe reason why people don’t finetune a small 8B model instead of using a larger one, is because this approach doesn’t work. You always end up with an overfited model that appears to perform great on a small test set, but in the real world has 0 generalization capabilities.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hgbmd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LoRA is used everywhere. The finetuning service of OpenAI, or image/video companies are training LoRAs. Many of the post training stages of LLMs are done with LoRA instead of full finetuning… &lt;/p&gt;\n\n&lt;p&gt;The reason why people don’t finetune a small 8B model instead of using a larger one, is because this approach doesn’t work. You always end up with an overfited model that appears to perform great on a small test set, but in the real world has 0 generalization capabilities.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5hgbmd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753644486,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5gna4r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "____vladrad",
            "can_mod_post": false,
            "created_utc": 1753635899,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 3,
            "author_fullname": "t2_u6i8a0ay",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Fine tuning is hard and a lot of LLMs are trained on mcp/rag to grab context. What I would use fine tuning like Lora for is something like a domain coder. Learn how the project is structured and how to navigate it like muscle memory. Or train it to use your mcps in a manner that you expect where it does not work out of the box. \n\nMy rule for fine tuning is don’t teach it knowledge instead teach it better to use rag in your environments. I hope that makes sense.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gna4r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fine tuning is hard and a lot of LLMs are trained on mcp/rag to grab context. What I would use fine tuning like Lora for is something like a domain coder. Learn how the project is structured and how to navigate it like muscle memory. Or train it to use your mcps in a manner that you expect where it does not work out of the box. &lt;/p&gt;\n\n&lt;p&gt;My rule for fine tuning is don’t teach it knowledge instead teach it better to use rag in your environments. I hope that makes sense.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5gna4r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753635899,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5go4ou",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753636146,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 4,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Huh?\n\nLoRA is incredibly popular. Most of the models on Huggingface are fine tunes of a base / instruct model for a specific use case.\n\nWith that said, actual training is not a trivial process. You have to understand the model, math, hyperparameters, data, etc. You have to avoid overwriting any existing representations and also introduce new ones gainfully. There's a lot of overhead going on there, and it's really not as simple as saying \"okay, I have this targetted dataset that does the thing I want\" (which is already hard); you also have to produce a general purpose dataset to keep existing skills, you also need to understand ML deployment (training is more complicated than inference for dependencies, etc).\n\nThere's a lot of things LoRA is great for, but it's not really an alternative to the systems that you're seeing built around LLMs (ie: LLM functions which I refuse to call MCP, and which really encompass half of RAG as well).\n\nLike, there's a sliding scale of things involved in the ecosystem. In order:\n\nPre-training, instruct tuning, RLHF / RLVR, inference deployment / ML ops, prompt engineering, external systems (MCP, A2A, LLM functions, Context Engineering, etc etc), RAG, and then all the regular tech stack (interface, etc).\n\nAs you go towards the right you get to easier (or, maybe more specialized and lower overhead is fairer to say) and more application focused issues. Any one of these is a huge area that you could have a full time person (or multiple full time people), working on these issues.\n\nThe other thing is that stuff further to the right tends to give you quicker results and better response from end-users for the amount of effort put in.\n\nLoRA kind of encompasses the three furthest left areas, meaning it's more complicated, more involved, and doesn't offer as immediate a user facing benefit as a lot of window dressing on the right.\n\nIf you'd like a good middle ground, based on your tone and opinion on the matter, you may actually prefer DSPy as a learnable system that's still fairly accessible (it only requires an LLM endpoint).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5go4ou",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Huh?&lt;/p&gt;\n\n&lt;p&gt;LoRA is incredibly popular. Most of the models on Huggingface are fine tunes of a base / instruct model for a specific use case.&lt;/p&gt;\n\n&lt;p&gt;With that said, actual training is not a trivial process. You have to understand the model, math, hyperparameters, data, etc. You have to avoid overwriting any existing representations and also introduce new ones gainfully. There&amp;#39;s a lot of overhead going on there, and it&amp;#39;s really not as simple as saying &amp;quot;okay, I have this targetted dataset that does the thing I want&amp;quot; (which is already hard); you also have to produce a general purpose dataset to keep existing skills, you also need to understand ML deployment (training is more complicated than inference for dependencies, etc).&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s a lot of things LoRA is great for, but it&amp;#39;s not really an alternative to the systems that you&amp;#39;re seeing built around LLMs (ie: LLM functions which I refuse to call MCP, and which really encompass half of RAG as well).&lt;/p&gt;\n\n&lt;p&gt;Like, there&amp;#39;s a sliding scale of things involved in the ecosystem. In order:&lt;/p&gt;\n\n&lt;p&gt;Pre-training, instruct tuning, RLHF / RLVR, inference deployment / ML ops, prompt engineering, external systems (MCP, A2A, LLM functions, Context Engineering, etc etc), RAG, and then all the regular tech stack (interface, etc).&lt;/p&gt;\n\n&lt;p&gt;As you go towards the right you get to easier (or, maybe more specialized and lower overhead is fairer to say) and more application focused issues. Any one of these is a huge area that you could have a full time person (or multiple full time people), working on these issues.&lt;/p&gt;\n\n&lt;p&gt;The other thing is that stuff further to the right tends to give you quicker results and better response from end-users for the amount of effort put in.&lt;/p&gt;\n\n&lt;p&gt;LoRA kind of encompasses the three furthest left areas, meaning it&amp;#39;s more complicated, more involved, and doesn&amp;#39;t offer as immediate a user facing benefit as a lot of window dressing on the right.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;d like a good middle ground, based on your tone and opinion on the matter, you may actually prefer DSPy as a learnable system that&amp;#39;s still fairly accessible (it only requires an LLM endpoint).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5go4ou/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753636146,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5h811o",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "liquid_bee_3",
                      "can_mod_post": false,
                      "created_utc": 1753641974,
                      "send_replies": true,
                      "parent_id": "t1_n5h3dsr",
                      "score": 1,
                      "author_fullname": "t2_18ijyp2nfl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "its def not way easier nor cheaper. api token prices add up.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5h811o",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;its def not way easier nor cheaper. api token prices add up.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maq0hg",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h811o/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753641974,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5h3dsr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753640579,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Probably because QA pairs are almost impossible to include all of the information you want to have added to an LLM.\nCatastrophic forgetting is also a concern.\nRag or prompt engineering is way easier/ faster to deploy.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5h3dsr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Probably because QA pairs are almost impossible to include all of the information you want to have added to an LLM.\nCatastrophic forgetting is also a concern.\nRag or prompt engineering is way easier/ faster to deploy.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5h3dsr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753640579,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hqeup",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Legumbrero",
            "can_mod_post": false,
            "created_utc": 1753647587,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_146dhkvfhd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Finetuning for downstream tasks tends to sacrifice general baseline performance and for LLM's it works ok for style and format but less well for knowledge (RAG tends to be the preferred approach for specialized knowledge).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hqeup",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Finetuning for downstream tasks tends to sacrifice general baseline performance and for LLM&amp;#39;s it works ok for style and format but less well for knowledge (RAG tends to be the preferred approach for specialized knowledge).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5hqeup/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753647587,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5i15n8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mark-haus",
            "can_mod_post": false,
            "created_utc": 1753650875,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_qpy5g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Because for most use cases where you’d reach for LORA; either RAG or agents can solve the problem with a fraction of the resources",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5i15n8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because for most use cases where you’d reach for LORA; either RAG or agents can solve the problem with a fraction of the resources&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5i15n8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753650875,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5i7rgs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "vincentz42",
            "can_mod_post": false,
            "created_utc": 1753652976,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_5tzp8vnx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "As some others have said, it is not really about LoRA, but fine-tuning vs prompt-engineering. And there are quite a few of the hurdles for fine-tuning LLMs IMHO:\n\n1. To fine-tune a domain specific LLM one must collect a dataset. But what would that data be, exactly? For example, fine-tuning LLMs on &lt;input, reasoning, expected output&gt; triplets would likely improve capability inthat specific area, but fine-tuning on domain-specific articles and/or code samples likely will not. Acquiring specific training data that would solve your particular problem is usually hard and often require expertise and human supervision.\n2. It is close to impossible to fine-tune an instruct LLM without losing its general capabilities, such as instruction following, agentic capability, reasoning, and world knowledge. And these general capabilities are quite important for most users.\n3. LoRA does not substantially lower the barrier of entry in LLM fine-tuning. It just saves you a certain amount of memory but offers no improvement to training speed. Fine-tuning anything larger than a 8B model would still requires multiple A100s + good distributed training strategies. Fine-tuning 70B+ LLMs would also require tensor parallelism on top of LoRA, which to my knowledge have near-zero support in popular opensource libraries.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5i7rgs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As some others have said, it is not really about LoRA, but fine-tuning vs prompt-engineering. And there are quite a few of the hurdles for fine-tuning LLMs IMHO:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;To fine-tune a domain specific LLM one must collect a dataset. But what would that data be, exactly? For example, fine-tuning LLMs on &amp;lt;input, reasoning, expected output&amp;gt; triplets would likely improve capability inthat specific area, but fine-tuning on domain-specific articles and/or code samples likely will not. Acquiring specific training data that would solve your particular problem is usually hard and often require expertise and human supervision.&lt;/li&gt;\n&lt;li&gt;It is close to impossible to fine-tune an instruct LLM without losing its general capabilities, such as instruction following, agentic capability, reasoning, and world knowledge. And these general capabilities are quite important for most users.&lt;/li&gt;\n&lt;li&gt;LoRA does not substantially lower the barrier of entry in LLM fine-tuning. It just saves you a certain amount of memory but offers no improvement to training speed. Fine-tuning anything larger than a 8B model would still requires multiple A100s + good distributed training strategies. Fine-tuning 70B+ LLMs would also require tensor parallelism on top of LoRA, which to my knowledge have near-zero support in popular opensource libraries.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5i7rgs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753652976,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5i8fvb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ObnoxiouslyVivid",
            "can_mod_post": false,
            "created_utc": 1753653192,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_n80ogax",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's more expensive to train an LLM than to use one.\n\nAlso, over time closed-source models just became good enough to run in specialized domains, so why bother?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5i8fvb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s more expensive to train an LLM than to use one.&lt;/p&gt;\n\n&lt;p&gt;Also, over time closed-source models just became good enough to run in specialized domains, so why bother?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5i8fvb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753653192,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ij5i3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "superstarbootlegs",
            "can_mod_post": false,
            "created_utc": 1753656815,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_190bhtiwx7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "good timing for me, I was literally wondering about Lora training approach last night. I come from Comfyui use where Lora's are the main way to enforce specific looks, styles, characters, into images and video creation. But you can train a Lora on 10 images and have a success, more often doesnt work out much better.\n\nI assumed the same would be happening in coding and document world, but I guess MCPs make more sense due to the size of the data you would have to feed the training model would often be huge. Plus the moment any new data got added or old data got changed, you'd have to train the Lora all over again.\n\nAt the end of the day its going to be by use-case. Image and videos, Loras are the way, data and documentation, probably not so much.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ij5i3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;good timing for me, I was literally wondering about Lora training approach last night. I come from Comfyui use where Lora&amp;#39;s are the main way to enforce specific looks, styles, characters, into images and video creation. But you can train a Lora on 10 images and have a success, more often doesnt work out much better.&lt;/p&gt;\n\n&lt;p&gt;I assumed the same would be happening in coding and document world, but I guess MCPs make more sense due to the size of the data you would have to feed the training model would often be huge. Plus the moment any new data got added or old data got changed, you&amp;#39;d have to train the Lora all over again.&lt;/p&gt;\n\n&lt;p&gt;At the end of the day its going to be by use-case. Image and videos, Loras are the way, data and documentation, probably not so much.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5ij5i3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753656815,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ivewu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Delicious-Farmer-234",
            "can_mod_post": false,
            "created_utc": 1753661116,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_6x6f3qy5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's how to align the model after a fine-tune and it's not easy so RAG is cheaper and more accurate. However I totally agree with you Qlora is definitely amazing and you can combine them.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ivewu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s how to align the model after a fine-tune and it&amp;#39;s not easy so RAG is cheaper and more accurate. However I totally agree with you Qlora is definitely amazing and you can combine them.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5ivewu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753661116,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5izf0x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "stylist-trend",
            "can_mod_post": false,
            "created_utc": 1753662613,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_cly8jcyo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I had never heard of LoRA before (at least outside the context of Meshtasticl, but this made me realize that expert model distillations (e.g. from a large qwen to a small UI-only mode) could be neat",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5izf0x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I had never heard of LoRA before (at least outside the context of Meshtasticl, but this made me realize that expert model distillations (e.g. from a large qwen to a small UI-only mode) could be neat&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5izf0x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753662613,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5izotz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CoruNethronX",
            "can_mod_post": false,
            "created_utc": 1753662716,
            "send_replies": true,
            "parent_id": "t3_1maq0hg",
            "score": 1,
            "author_fullname": "t2_1ovg3i5hnb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Recently made a PEFT/LORA finetune for Qwen3 600M to play as classification (yes/no) model per logprobs. Had to choose between more false positives or more false negatives, and got it to play as good as &lt;1:1000000 false negatives and around 3% false positives, that is quite enough for my task.\nVery impressed, that with the help of vibe-coding during dataset generator implementation and only around 2 hours of train time on laptop (for the total of around 4 hours) I got pretty working solution.\nLLMs help train LLMs for such a specific tasks in nearly no time even if you have only basic understanding of all the internals, the math statistics etc.\nIst's great.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5izotz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Recently made a PEFT/LORA finetune for Qwen3 600M to play as classification (yes/no) model per logprobs. Had to choose between more false positives or more false negatives, and got it to play as good as &amp;lt;1:1000000 false negatives and around 3% false positives, that is quite enough for my task.\nVery impressed, that with the help of vibe-coding during dataset generator implementation and only around 2 hours of train time on laptop (for the total of around 4 hours) I got pretty working solution.\nLLMs help train LLMs for such a specific tasks in nearly no time even if you have only basic understanding of all the internals, the math statistics etc.\nIst&amp;#39;s great.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maq0hg/why_hasnt_lora_gained_more_popularity/n5izotz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753662716,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maq0hg",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]