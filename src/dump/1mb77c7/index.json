[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey, apologies if this question has been posted before i haven’t been able to find any concrete info on it. \n\nIn my area i can get 8 3060 12GBs for the exact same price as two 3090s, I’m looking to run LLMs, Heavy ComfyUI workflows, training models, LoRas and just about any other AI development haha.\n\nI’ve never ran anything on a 2x+-gpu set up, is doubling the VRAM even worth the effort and time setting up? (big home labber, i can figure it out)\n\nand are 3060s even fast enough to use those 96GB of vram effectively?\nwhat’s the better bang for the buck? prices are the EXACT same.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "2x RTX 3090 24GB or 8x 3060 12GB",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mb77c7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.85,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 19,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_8x8948uy",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 19,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753678172,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey, apologies if this question has been posted before i haven’t been able to find any concrete info on it. &lt;/p&gt;\n\n&lt;p&gt;In my area i can get 8 3060 12GBs for the exact same price as two 3090s, I’m looking to run LLMs, Heavy ComfyUI workflows, training models, LoRas and just about any other AI development haha.&lt;/p&gt;\n\n&lt;p&gt;I’ve never ran anything on a 2x+-gpu set up, is doubling the VRAM even worth the effort and time setting up? (big home labber, i can figure it out)&lt;/p&gt;\n\n&lt;p&gt;and are 3060s even fast enough to use those 96GB of vram effectively?\nwhat’s the better bang for the buck? prices are the EXACT same.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mb77c7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "twotemp",
            "discussion_type": null,
            "num_comments": 18,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/",
            "subreddit_subscribers": 506191,
            "created_utc": 1753678172,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5k34gj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "mayo551",
            "can_mod_post": false,
            "created_utc": 1753678782,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 41,
            "author_fullname": "t2_vsz5kd9o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "VRAM bandwidth of the 3060 is less than 400GB/s while the 3090 is just shy of 1TB/s.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k34gj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;VRAM bandwidth of the 3060 is less than 400GB/s while the 3090 is just shy of 1TB/s.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5k34gj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753678782,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 41
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ktzpg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "getmevodka",
                      "can_mod_post": false,
                      "created_utc": 1753693821,
                      "send_replies": true,
                      "parent_id": "t1_n5k5qbi",
                      "score": 1,
                      "author_fullname": "t2_7uoa6r1b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "the 3090 double is still a good idea if you want to integrate llm prompts for the art by loading the model via llama onto the second card while using comfy on the first. just saying, that's how I did it for months. =)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ktzpg",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the 3090 double is still a good idea if you want to integrate llm prompts for the art by loading the model via llama onto the second card while using comfy on the first. just saying, that&amp;#39;s how I did it for months. =)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb77c7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5ktzpg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753693821,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5k5qbi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1753680104,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 21,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you're doing Ai Img/Vid Gen stuff in comfy, then you need to stop and look at your requirements. There is no ability, what so ever, to pool VRAM in that ball park at this time. (Yes, you can offload like 5% of the load maybe with vae/encoders)\n\nSo if your ideal model is 14GB, 3060s can't even begin to run it without doing swapping and taking a 100x performance loss.\n\nLLMs are much more forgiving and free form, comparatively. There is a real argument that stacking 3060s could be the better play here. Depends on use case. But figure out what you're actually targeting in Img Gen stuff first. And the safer option is go 3090s anyways.\n\nIf you knew, 100%, that your workflow was spamming SDXL images all day, then 8 3060s will output, in parallel, about 2x more images than 2x 3090s. Each individual image would take longer to complete, but on a bulk job 3060s win. On an interactive designer, awaiting an image to complete before your next move, 3090 wins. Unless you need 8 samples faster instead of 2 samples faster.\n\nSo, hard to say, but you can figure this out ahead of time with better research into what you want to actually do. And if \"do whatever\" is the answer, 3090s 100%.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k5qbi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re doing Ai Img/Vid Gen stuff in comfy, then you need to stop and look at your requirements. There is no ability, what so ever, to pool VRAM in that ball park at this time. (Yes, you can offload like 5% of the load maybe with vae/encoders)&lt;/p&gt;\n\n&lt;p&gt;So if your ideal model is 14GB, 3060s can&amp;#39;t even begin to run it without doing swapping and taking a 100x performance loss.&lt;/p&gt;\n\n&lt;p&gt;LLMs are much more forgiving and free form, comparatively. There is a real argument that stacking 3060s could be the better play here. Depends on use case. But figure out what you&amp;#39;re actually targeting in Img Gen stuff first. And the safer option is go 3090s anyways.&lt;/p&gt;\n\n&lt;p&gt;If you knew, 100%, that your workflow was spamming SDXL images all day, then 8 3060s will output, in parallel, about 2x more images than 2x 3090s. Each individual image would take longer to complete, but on a bulk job 3060s win. On an interactive designer, awaiting an image to complete before your next move, 3090 wins. Unless you need 8 samples faster instead of 2 samples faster.&lt;/p&gt;\n\n&lt;p&gt;So, hard to say, but you can figure this out ahead of time with better research into what you want to actually do. And if &amp;quot;do whatever&amp;quot; is the answer, 3090s 100%.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5k5qbi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753680104,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 21
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5k6rh0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Rich_Repeat_22",
            "can_mod_post": false,
            "created_utc": 1753680648,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 10,
            "author_fullname": "t2_viufiki6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "8x3060 aren't viable because you need to connect them some how 😂\n\n2 x 3090 are better option as you can use NVLINK and also can hook them on a X670E/870E board having 2 PCIe to the GPU, and use 8+8 lanes config.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k6rh0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;8x3060 aren&amp;#39;t viable because you need to connect them some how 😂&lt;/p&gt;\n\n&lt;p&gt;2 x 3090 are better option as you can use NVLINK and also can hook them on a X670E/870E board having 2 PCIe to the GPU, and use 8+8 lanes config.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5k6rh0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753680648,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5lk3ac",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "EthanMiner",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5kayky",
                                                    "score": 2,
                                                    "author_fullname": "t2_1vqwb1k7",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "I had it in two 3090s and didn’t notice a difference in anything, but I wasn’t fine tuning.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5lk3ac",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I had it in two 3090s and didn’t notice a difference in anything, but I wasn’t fine tuning.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mb77c7",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5lk3ac/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753706257,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753706257,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5kayky",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "SillyLilBear",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5kasa6",
                                          "score": 5,
                                          "author_fullname": "t2_wjjtz",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "From what I have seen it has little impact for inference as the cards are not really taking to each other like they do with fine tuning.  But nvlink for the 3090 is pretty cheap and it won’t hurt.  Just not a must have",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5kayky",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;From what I have seen it has little impact for inference as the cards are not really taking to each other like they do with fine tuning.  But nvlink for the 3090 is pretty cheap and it won’t hurt.  Just not a must have&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mb77c7",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5kayky/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753682896,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753682896,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 5
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5kasa6",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Wheynelau",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5k96z2",
                                "score": 2,
                                "author_fullname": "t2_vezlk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Wouldn't it hurt tensor parallelism?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5kasa6",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wouldn&amp;#39;t it hurt tensor parallelism?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mb77c7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5kasa6/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753682802,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753682802,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5lquu3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Imaginary_Bench_7294",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5k96z2",
                                "score": 2,
                                "author_fullname": "t2_fling4kx",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Don't know if OP edited or not, but the post does explicitly say \"training.\"",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5lquu3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t know if OP edited or not, but the post does explicitly say &amp;quot;training.&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mb77c7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5lquu3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753708645,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753708645,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5k96z2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "SillyLilBear",
                      "can_mod_post": false,
                      "created_utc": 1753681945,
                      "send_replies": true,
                      "parent_id": "t1_n5k2lfq",
                      "score": 11,
                      "author_fullname": "t2_wjjtz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "nvlink isn't really that big of a deal unless he is fine tuning.  For inference, the difference is insignificant.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5k96z2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;nvlink isn&amp;#39;t really that big of a deal unless he is fine tuning.  For inference, the difference is insignificant.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mb77c7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5k96z2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753681945,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 11
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5k2lfq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "getmevodka",
            "can_mod_post": false,
            "created_utc": 1753678519,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 16,
            "author_fullname": "t2_7uoa6r1b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "short answer is get the two 3090 cards and a nvlink bridge. but dont think you can use more than one card in comfy.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k2lfq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;short answer is get the two 3090 cards and a nvlink bridge. but dont think you can use more than one card in comfy.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5k2lfq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753678519,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 16
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5k4tga",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AnonsAnonAnonagain",
            "can_mod_post": false,
            "created_utc": 1753679629,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 8,
            "author_fullname": "t2_pi37jsdc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "3090s wayyy better than the 3060s",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k4tga",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;3090s wayyy better than the 3060s&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5k4tga/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753679629,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5kdgnh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "LumpyWelds",
            "can_mod_post": false,
            "created_utc": 1753684277,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 5,
            "author_fullname": "t2_32hdazgq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We are just guessing until you tell us what you want to do with them.\n\nIf you don't care about speed, go the 8 3060s route.\n\nIf you want to run models closer to the online ones in accuracy, get the 8 3060s with 96GB.\n\nIf you need to process something heavy like image generation and need it fast, get the 2 3090's.\n\nIf you don't have free electricity and are going to run 24/7 , which would you prefer? 8x3060s at 1400W or 2 3090's at 700W",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kdgnh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We are just guessing until you tell us what you want to do with them.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t care about speed, go the 8 3060s route.&lt;/p&gt;\n\n&lt;p&gt;If you want to run models closer to the online ones in accuracy, get the 8 3060s with 96GB.&lt;/p&gt;\n\n&lt;p&gt;If you need to process something heavy like image generation and need it fast, get the 2 3090&amp;#39;s.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t have free electricity and are going to run 24/7 , which would you prefer? 8x3060s at 1400W or 2 3090&amp;#39;s at 700W&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5kdgnh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753684277,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5k9v6d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SvenVargHimmel",
            "can_mod_post": false,
            "created_utc": 1753682303,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 3,
            "author_fullname": "t2_orer5qiaz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Forget about pooling the vram, and get the 2 x 3090s , in comfyui (through swarm) you get twice as much throughput by running jobs in parallel\n\n\nYou'll be able to pack other models onto the 2nd card",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k9v6d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Forget about pooling the vram, and get the 2 x 3090s , in comfyui (through swarm) you get twice as much throughput by running jobs in parallel&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ll be able to pack other models onto the 2nd card&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5k9v6d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753682303,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5khhfp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "moncallikta",
            "can_mod_post": false,
            "created_utc": 1753686489,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 2,
            "author_fullname": "t2_15ju4l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Go with 2x3090. Getting enough PCIe lanes for 8 GPUs is tricky, as well as figuring out a way to mount the GPUs in a case (most likely would have to mount them on a stand outside the case). Dual 3090 on the other hand is doable in a suitable gaming PC case. Power requirements will also be easier to satisfy with dual 3090.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5khhfp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Go with 2x3090. Getting enough PCIe lanes for 8 GPUs is tricky, as well as figuring out a way to mount the GPUs in a case (most likely would have to mount them on a stand outside the case). Dual 3090 on the other hand is doable in a suitable gaming PC case. Power requirements will also be easier to satisfy with dual 3090.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5khhfp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753686489,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5lke02",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "texasdude11",
            "can_mod_post": false,
            "created_utc": 1753706368,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 2,
            "author_fullname": "t2_ya9qn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "TLDR: 2x3090 &gt; 8x3060\nMemory Bandwidth \nPower\nLess PCIE to PCIE communication \nPossibility of nvlink\nFuture upgradibility\nLess clutter\nFaster\nMore CUDA cores\n.\n.\n.\n.\nOnly con, less aggregated VRAM which can be made up for in future but you cannot reverse it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5lke02",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: 2x3090 &amp;gt; 8x3060\nMemory Bandwidth \nPower\nLess PCIE to PCIE communication \nPossibility of nvlink\nFuture upgradibility\nLess clutter\nFaster\nMore CUDA cores\n.\n.\n.\n.\nOnly con, less aggregated VRAM which can be made up for in future but you cannot reverse it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5lke02/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753706368,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5lv8uq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Imaginary_Bench_7294",
            "can_mod_post": false,
            "created_utc": 1753710098,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 2,
            "author_fullname": "t2_fling4kx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hardware logistics aside, the only benefit that using several smaller GPUs offers is the total max VRAM.\n\nThat being said, image gen is pretty much single GPU only.\n\nInference for LLMs is more than capable of splitting across GPUs. Though overall speed is largely dependent on RAM bandwidth. Smaller cards have lower bandwidths.\n\nTraining speed for an LLM on multiple GPUs is bottlenecked by the GPU to GPU transfer rates. This is where NVlink helps with the 3090's. The NVlink, when using Linux, allows the GPUs to bypass the PCIe bus and communicate faster. Even if the smaller cards had just as high of compute and memory speed of the 3090, the multiple transfers over PCIe will eat into the training speeds. I run dual 3090's and have trained LLM LoRA's. On Ubuntu, I see between 30% and 40% faster training when using NVlink vs the PCIe connection.\n\nUnless you _*really*_ want a project on your hands, I highly suggest just going with the 3090 cards and picking up an NVlink (if you really need it).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5lv8uq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hardware logistics aside, the only benefit that using several smaller GPUs offers is the total max VRAM.&lt;/p&gt;\n\n&lt;p&gt;That being said, image gen is pretty much single GPU only.&lt;/p&gt;\n\n&lt;p&gt;Inference for LLMs is more than capable of splitting across GPUs. Though overall speed is largely dependent on RAM bandwidth. Smaller cards have lower bandwidths.&lt;/p&gt;\n\n&lt;p&gt;Training speed for an LLM on multiple GPUs is bottlenecked by the GPU to GPU transfer rates. This is where NVlink helps with the 3090&amp;#39;s. The NVlink, when using Linux, allows the GPUs to bypass the PCIe bus and communicate faster. Even if the smaller cards had just as high of compute and memory speed of the 3090, the multiple transfers over PCIe will eat into the training speeds. I run dual 3090&amp;#39;s and have trained LLM LoRA&amp;#39;s. On Ubuntu, I see between 30% and 40% faster training when using NVlink vs the PCIe connection.&lt;/p&gt;\n\n&lt;p&gt;Unless you &lt;em&gt;&lt;em&gt;really&lt;/em&gt;&lt;/em&gt; want a project on your hands, I highly suggest just going with the 3090 cards and picking up an NVlink (if you really need it).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5lv8uq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753710098,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5kd6w0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "prompt_seeker",
            "can_mod_post": false,
            "created_utc": 1753684131,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 4,
            "author_fullname": "t2_7xpjy9r9p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have 4x3090 and 4x3060. Go 2x3090.\n\nIt is very difficult connect 8 GPUs, becuase of num of PCIe lanes, power consumsion, temperature control.\nAnd in case of ComfyUI, you can only use max 2x GPUs parallelly at the moment.\nIn case of LLM, models going to around 32B or very big MoE, so 96GB of VRAM is too much or too small.",
            "edited": 1753684495,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kd6w0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have 4x3090 and 4x3060. Go 2x3090.&lt;/p&gt;\n\n&lt;p&gt;It is very difficult connect 8 GPUs, becuase of num of PCIe lanes, power consumsion, temperature control.\nAnd in case of ComfyUI, you can only use max 2x GPUs parallelly at the moment.\nIn case of LLM, models going to around 32B or very big MoE, so 96GB of VRAM is too much or too small.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5kd6w0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753684131,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ki6vq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TokenRingAI",
            "can_mod_post": false,
            "created_utc": 1753686888,
            "send_replies": true,
            "parent_id": "t3_1mb77c7",
            "score": 1,
            "author_fullname": "t2_1twuope92q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would get 1x 3090 and 4x3060.\n\nThe 4x 3060 will run your LM in tensor parallel 4, and the 3090 will be used for your other tasks. The 4x 3060 should give you 1x the prompt performance and 2x the token performance and 2x the memory vs a 3090. I have had good results doing this with either 4x or 8x 5060 TI which is a similar type card.\n\nPut them in separate computers. The 3090 in your desktop to run image tasks, and the 3060s in a cheap old xeon server board with 7+ slots. Otherwise you end up with a riser cable and PSU mess\n\nWith some hackery, VLLM should also be able to run pipeline parallel on your network, with the 3090 and 4x3060 combined for 72G vram, if you really need the extra memory for a particular model.\n\nThis setup is very flexible.\n\nFor a few dollars, you can rent 4x3060 or 4060 on vast.ai to try this out. I recommend you do that before buying the cards",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ki6vq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would get 1x 3090 and 4x3060.&lt;/p&gt;\n\n&lt;p&gt;The 4x 3060 will run your LM in tensor parallel 4, and the 3090 will be used for your other tasks. The 4x 3060 should give you 1x the prompt performance and 2x the token performance and 2x the memory vs a 3090. I have had good results doing this with either 4x or 8x 5060 TI which is a similar type card.&lt;/p&gt;\n\n&lt;p&gt;Put them in separate computers. The 3090 in your desktop to run image tasks, and the 3060s in a cheap old xeon server board with 7+ slots. Otherwise you end up with a riser cable and PSU mess&lt;/p&gt;\n\n&lt;p&gt;With some hackery, VLLM should also be able to run pipeline parallel on your network, with the 3090 and 4x3060 combined for 72G vram, if you really need the extra memory for a particular model.&lt;/p&gt;\n\n&lt;p&gt;This setup is very flexible.&lt;/p&gt;\n\n&lt;p&gt;For a few dollars, you can rent 4x3060 or 4060 on vast.ai to try this out. I recommend you do that before buying the cards&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb77c7/2x_rtx_3090_24gb_or_8x_3060_12gb/n5ki6vq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753686888,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb77c7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]