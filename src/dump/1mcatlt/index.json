[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi all, I'm new to working with LLMs, especially when it comes to fine-tuning or customizing them for domain-specific use cases.\n\nRight now, I'm exploring how to build a **Prompt : Expected-Output** style dataset for fine-tuning a lightweight language model (\\~1–1.5B parameters).  \nThe goal is to enable the model to analyze code files and identify specific patterns within them. However, the twist is that some false positives or edge cases can only be flagged correctly when you consider the file path or context of the file in the project — not just the raw code.\n\nSo essentially, the input to the model would be:\n\n    &lt;file-path&gt;\\n&lt;code-contents&gt;\n\nThe output would be a custom JSON.\n\nThis would help the model learn more nuanced behaviors that static rules often miss.\n\nAre there any tools, workflows, or existing pipelines that can semi-automate dataset generation like this — especially ones that leverage existing models (e.g., Claude, Gemini, GPT-4, etc.) to help with generating prompt (+ CoT).\n\nI'm trying to avoid doing the entire dataset manually if there's a smart way to leverage existing models/tools to bootstrap it.\n\nThanks — any suggestions or pointers would go a long way.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Creating a High Quality Dataset for Instruction Fine-Tuning",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mcatlt",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_jchazvrjf",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753794085,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all, I&amp;#39;m new to working with LLMs, especially when it comes to fine-tuning or customizing them for domain-specific use cases.&lt;/p&gt;\n\n&lt;p&gt;Right now, I&amp;#39;m exploring how to build a &lt;strong&gt;Prompt : Expected-Output&lt;/strong&gt; style dataset for fine-tuning a lightweight language model (~1–1.5B parameters).&lt;br/&gt;\nThe goal is to enable the model to analyze code files and identify specific patterns within them. However, the twist is that some false positives or edge cases can only be flagged correctly when you consider the file path or context of the file in the project — not just the raw code.&lt;/p&gt;\n\n&lt;p&gt;So essentially, the input to the model would be:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;&amp;lt;file-path&amp;gt;\\n&amp;lt;code-contents&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;The output would be a custom JSON.&lt;/p&gt;\n\n&lt;p&gt;This would help the model learn more nuanced behaviors that static rules often miss.&lt;/p&gt;\n\n&lt;p&gt;Are there any tools, workflows, or existing pipelines that can semi-automate dataset generation like this — especially ones that leverage existing models (e.g., Claude, Gemini, GPT-4, etc.) to help with generating prompt (+ CoT).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to avoid doing the entire dataset manually if there&amp;#39;s a smart way to leverage existing models/tools to bootstrap it.&lt;/p&gt;\n\n&lt;p&gt;Thanks — any suggestions or pointers would go a long way.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mcatlt",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "unnxt30",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mcatlt/creating_a_high_quality_dataset_for_instruction/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcatlt/creating_a_high_quality_dataset_for_instruction/",
            "subreddit_subscribers": 506973,
            "created_utc": 1753794085,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]