[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Looking for some \"best practices\" for this new 30B A3B to squeeze the most out of it with my 4090. Normally I'm pretty up to date on this stuff but I'm a month or so behind the times. I'll share where I'm at and hopefully somebody's got some suggestions :).\n\nI'm sitting on 64gb ram/24gb vram (4090). I'm open to running this thing in ik\\_llama, tabby, vllm, whatever works best really. I have a mix of needs - ideally I'd like to have the best of all worlds (fast, low latency, high throughput), but I know it's all a bit of a \"pick two\" situation usually.\n\nI've got VLLM set up. Looks like I can run an AWQ quant of this thing at 8192 context fully in 24gb vram. If I bump down to an 8 bit KV Cache, I can fit 16,000 context.\n\nWith that setup with 16k context:\n\nOverall tokens/sec (single user, single request): 181.30t/s\n\nMean latency: 2.88s\n\nMean Time to First Token: 0.046s\n\nMax Batching tokens/s: 2,549.14t/s (100 requests)\n\nThat's not terrible as-is, and can hit the kinds of high throughput I need (2500 tokens per second is great, and even the single user 181t/s is snappy), but, I'm curious what my options are out there because I wouldn't mind adding a way to run this with much higher context limits. Like... if I can find a way to run it at an appreciable speed with 128k+ context I'd -love- that, even if that was only a single-user setup. Seems like I could do that with something like ik\\_llama, a ggml 4 or 8 bit 30b a3b, and my 24gb vram card holding part of the model with the rest offloaded into regular ram. Anybody running this thing on ik\\_llama want to chime in with some idea of how its performing and how you'r setting it up?  \n  \nOpen to any advice. I'd like to get this thing running as best I can for both a single user AND for batch-use (I'm fine with it being two separate setups, I can run them when needed appropriately).",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best way to run the Qwen3 30b A3B coder/instruct models for HIGH throughput and/or HIGH context? (on a single 4090)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mf3wr0",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.81,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ddyte",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754072100,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Looking for some &amp;quot;best practices&amp;quot; for this new 30B A3B to squeeze the most out of it with my 4090. Normally I&amp;#39;m pretty up to date on this stuff but I&amp;#39;m a month or so behind the times. I&amp;#39;ll share where I&amp;#39;m at and hopefully somebody&amp;#39;s got some suggestions :).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sitting on 64gb ram/24gb vram (4090). I&amp;#39;m open to running this thing in ik_llama, tabby, vllm, whatever works best really. I have a mix of needs - ideally I&amp;#39;d like to have the best of all worlds (fast, low latency, high throughput), but I know it&amp;#39;s all a bit of a &amp;quot;pick two&amp;quot; situation usually.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ve got VLLM set up. Looks like I can run an AWQ quant of this thing at 8192 context fully in 24gb vram. If I bump down to an 8 bit KV Cache, I can fit 16,000 context.&lt;/p&gt;\n\n&lt;p&gt;With that setup with 16k context:&lt;/p&gt;\n\n&lt;p&gt;Overall tokens/sec (single user, single request): 181.30t/s&lt;/p&gt;\n\n&lt;p&gt;Mean latency: 2.88s&lt;/p&gt;\n\n&lt;p&gt;Mean Time to First Token: 0.046s&lt;/p&gt;\n\n&lt;p&gt;Max Batching tokens/s: 2,549.14t/s (100 requests)&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s not terrible as-is, and can hit the kinds of high throughput I need (2500 tokens per second is great, and even the single user 181t/s is snappy), but, I&amp;#39;m curious what my options are out there because I wouldn&amp;#39;t mind adding a way to run this with much higher context limits. Like... if I can find a way to run it at an appreciable speed with 128k+ context I&amp;#39;d -love- that, even if that was only a single-user setup. Seems like I could do that with something like ik_llama, a ggml 4 or 8 bit 30b a3b, and my 24gb vram card holding part of the model with the rest offloaded into regular ram. Anybody running this thing on ik_llama want to chime in with some idea of how its performing and how you&amp;#39;r setting it up?  &lt;/p&gt;\n\n&lt;p&gt;Open to any advice. I&amp;#39;d like to get this thing running as best I can for both a single user AND for batch-use (I&amp;#39;m fine with it being two separate setups, I can run them when needed appropriately).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mf3wr0",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "teachersecret",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/",
            "subreddit_subscribers": 508541,
            "created_utc": 1754072100,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ed2gq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "teachersecret",
                      "can_mod_post": false,
                      "created_utc": 1754074328,
                      "send_replies": true,
                      "parent_id": "t1_n6e9456",
                      "score": 2,
                      "author_fullname": "t2_ddyte",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I was actually specifically using it in this instance for synthetic data gen. That said - interesting!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ed2gq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was actually specifically using it in this instance for synthetic data gen. That said - interesting!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf3wr0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6ed2gq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754074328,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ex9xs",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "fp4guru",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6euww9",
                                "score": 2,
                                "author_fullname": "t2_1tp8zldw5g",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It reached max context and open-webui chats started to fail.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ex9xs",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It reached max context and open-webui chats started to fail.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mf3wr0",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6ex9xs/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754080341,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754080341,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6euww9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "teachersecret",
                      "can_mod_post": false,
                      "created_utc": 1754079642,
                      "send_replies": true,
                      "parent_id": "t1_n6e9456",
                      "score": 2,
                      "author_fullname": "t2_ddyte",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I am coming back to this because for some reason it stuck in my mind so I went and did a little testing... and... Why were you capped at 3 users with real-world use cases? It seems like it could handle significantly more than 3 users. I did some bench testing and I can't find a workload where it's so limited.\n\nAm I missing something? What kind of workload are you doing?",
                      "edited": 1754080105,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6euww9",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am coming back to this because for some reason it stuck in my mind so I went and did a little testing... and... Why were you capped at 3 users with real-world use cases? It seems like it could handle significantly more than 3 users. I did some bench testing and I can&amp;#39;t find a workload where it&amp;#39;s so limited.&lt;/p&gt;\n\n&lt;p&gt;Am I missing something? What kind of workload are you doing?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf3wr0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6euww9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754079642,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6e9456",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "fp4guru",
            "can_mod_post": false,
            "created_utc": 1754073166,
            "send_replies": true,
            "parent_id": "t3_1mf3wr0",
            "score": 5,
            "author_fullname": "t2_1tp8zldw5g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "4090 can serve up to 3 users with real use cases. We tested it in an enterprise environment with Dev users. You need A100 80gb to actually do anything. 2500 tokens per second can only be reached if you are using it for synthetic data generation.",
            "edited": 1754073426,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6e9456",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;4090 can serve up to 3 users with real use cases. We tested it in an enterprise environment with Dev users. You need A100 80gb to actually do anything. 2500 tokens per second can only be reached if you are using it for synthetic data generation.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6e9456/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754073166,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf3wr0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6escmr",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "eloquentemu",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6ejwof",
                                                    "score": 1,
                                                    "author_fullname": "t2_lpdsy",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "A quick `llama-batched-bench` to compare LLAMA_SET_ROWS = 0 or 1 gives\n\n|    PP |     TG |    B |   N_KV | PP ROWS=0 t/s | PP ROWS=1 t/s | TG ROWS=0 t/s | TG ROWS=1 t/s |\n|-------|--------|------|--------|---------------|---------------|---------------|---------------|\n|   512 |    512 |    1 |   1024 |       4164.16 |       4126.67 |        170.66 |        190.68 |\n|   512 |    512 |    4 |   4096 |       4284.84 |       4493.47 |        285.41 |        302.42 |\n|   512 |    512 |   16 |  16384 |       4036.63 |       4444.20 |        665.68 |        718.39 |\n|   512 |    512 |   32 |  32768 |       3674.78 |       4479.74 |       1020.82 |       1172.28 |\n|   256 |    256 |   64 |  32768 |       3748.84 |       4457.20 |       1558.02 |       1866.95 |\n\nThis is for Q4_K_M and note I had to drop the pp/tg to 512 to fit the context for batch=64 on GPU.\n\nSo it's a small gain.  However, I _think_ that the real improvement is in the real `llama-server` where there otherwise needs to be synchronization between the requests and stuff.  However, I'm not equipped to test that right now... sorry.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6escmr",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A quick &lt;code&gt;llama-batched-bench&lt;/code&gt; to compare LLAMA_SET_ROWS = 0 or 1 gives&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;PP&lt;/th&gt;\n&lt;th&gt;TG&lt;/th&gt;\n&lt;th&gt;B&lt;/th&gt;\n&lt;th&gt;N_KV&lt;/th&gt;\n&lt;th&gt;PP ROWS=0 t/s&lt;/th&gt;\n&lt;th&gt;PP ROWS=1 t/s&lt;/th&gt;\n&lt;th&gt;TG ROWS=0 t/s&lt;/th&gt;\n&lt;th&gt;TG ROWS=1 t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1024&lt;/td&gt;\n&lt;td&gt;4164.16&lt;/td&gt;\n&lt;td&gt;4126.67&lt;/td&gt;\n&lt;td&gt;170.66&lt;/td&gt;\n&lt;td&gt;190.68&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;4&lt;/td&gt;\n&lt;td&gt;4096&lt;/td&gt;\n&lt;td&gt;4284.84&lt;/td&gt;\n&lt;td&gt;4493.47&lt;/td&gt;\n&lt;td&gt;285.41&lt;/td&gt;\n&lt;td&gt;302.42&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;16&lt;/td&gt;\n&lt;td&gt;16384&lt;/td&gt;\n&lt;td&gt;4036.63&lt;/td&gt;\n&lt;td&gt;4444.20&lt;/td&gt;\n&lt;td&gt;665.68&lt;/td&gt;\n&lt;td&gt;718.39&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;512&lt;/td&gt;\n&lt;td&gt;32&lt;/td&gt;\n&lt;td&gt;32768&lt;/td&gt;\n&lt;td&gt;3674.78&lt;/td&gt;\n&lt;td&gt;4479.74&lt;/td&gt;\n&lt;td&gt;1020.82&lt;/td&gt;\n&lt;td&gt;1172.28&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;256&lt;/td&gt;\n&lt;td&gt;256&lt;/td&gt;\n&lt;td&gt;64&lt;/td&gt;\n&lt;td&gt;32768&lt;/td&gt;\n&lt;td&gt;3748.84&lt;/td&gt;\n&lt;td&gt;4457.20&lt;/td&gt;\n&lt;td&gt;1558.02&lt;/td&gt;\n&lt;td&gt;1866.95&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;This is for Q4_K_M and note I had to drop the pp/tg to 512 to fit the context for batch=64 on GPU.&lt;/p&gt;\n\n&lt;p&gt;So it&amp;#39;s a small gain.  However, I &lt;em&gt;think&lt;/em&gt; that the real improvement is in the real &lt;code&gt;llama-server&lt;/code&gt; where there otherwise needs to be synchronization between the requests and stuff.  However, I&amp;#39;m not equipped to test that right now... sorry.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mf3wr0",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6escmr/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754078878,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754078878,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6ejwof",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "json12",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6efrbh",
                                          "score": 1,
                                          "author_fullname": "t2_13ljzj",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "How much speed up do you get from that command?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6ejwof",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How much speed up do you get from that command?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mf3wr0",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6ejwof/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754076347,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754076347,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6efrbh",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6ecv2x",
                                "score": 3,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It does but I think practically the speed ups aren't as good as other engines.  I think gains drop off after 2-4 sessions, but it's under active development.  [See here](https://github.com/ggml-org/llama.cpp/pull/14363) and while that's merged you still need to set `LLAMA_SET_ROWS=1` for the benefit and it won't work in all cases.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6efrbh",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It does but I think practically the speed ups aren&amp;#39;t as good as other engines.  I think gains drop off after 2-4 sessions, but it&amp;#39;s under active development.  &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14363\"&gt;See here&lt;/a&gt; and while that&amp;#39;s merged you still need to set &lt;code&gt;LLAMA_SET_ROWS=1&lt;/code&gt; for the benefit and it won&amp;#39;t work in all cases.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mf3wr0",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6efrbh/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754075118,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754075118,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ecv2x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "teachersecret",
                      "can_mod_post": false,
                      "created_utc": 1754074267,
                      "send_replies": true,
                      "parent_id": "t1_n6ecjou",
                      "score": 2,
                      "author_fullname": "t2_ddyte",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'll give that a shot, I wanted a long-context system to test, and 100t/s sounds fine. Does llama.cpp also support batching these days? I'll have to eyeball it and see if I can get a max throughput out of her.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ecv2x",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll give that a shot, I wanted a long-context system to test, and 100t/s sounds fine. Does llama.cpp also support batching these days? I&amp;#39;ll have to eyeball it and see if I can get a max throughput out of her.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf3wr0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6ecv2x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754074267,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6eqv9p",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Alby407",
                      "can_mod_post": false,
                      "created_utc": 1754078433,
                      "send_replies": true,
                      "parent_id": "t1_n6ecjou",
                      "score": 1,
                      "author_fullname": "t2_ptl3un7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "How does this affect the quality of the output?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6eqv9p",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How does this affect the quality of the output?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf3wr0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6eqv9p/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754078433,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6fthq2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Foreign-Beginning-49",
                      "can_mod_post": false,
                      "created_utc": 1754090759,
                      "send_replies": true,
                      "parent_id": "t1_n6ecjou",
                      "score": 1,
                      "author_fullname": "t2_83u2l6o4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Omg that's incredible can't wait to try this later. Qwen be cooking lads! Frontier can eat mud i am gonna put this thing into orbit on a rpi. Lol thanks again",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6fthq2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Omg that&amp;#39;s incredible can&amp;#39;t wait to try this later. Qwen be cooking lads! Frontier can eat mud i am gonna put this thing into orbit on a rpi. Lol thanks again&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf3wr0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6fthq2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754090759,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ecjou",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tomz17",
            "can_mod_post": false,
            "created_utc": 1754074174,
            "send_replies": true,
            "parent_id": "t3_1mf3wr0",
            "score": 2,
            "author_fullname": "t2_1mhx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "llama.cpp + unsloth Qwen3-Coder-30B-A3B-Instruct-UD-Q4\\_K\\_XL.gguf\n\nwith settings\n\n\\-c 131072 -fa -ctk q8\\_0 -ctv q8\\_0\n\nwill fit a 128k context in 24GB and run &gt; 100t/s tg, \\~2k t/s pp on a 3090 @ 250 watts.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ecjou",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llama.cpp + unsloth Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf&lt;/p&gt;\n\n&lt;p&gt;with settings&lt;/p&gt;\n\n&lt;p&gt;-c 131072 -fa -ctk q8_0 -ctv q8_0&lt;/p&gt;\n\n&lt;p&gt;will fit a 128k context in 24GB and run &amp;gt; 100t/s tg, ~2k t/s pp on a 3090 @ 250 watts.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6ecjou/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754074174,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf3wr0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6eddnh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "teachersecret",
                      "can_mod_post": false,
                      "created_utc": 1754074418,
                      "send_replies": true,
                      "parent_id": "t1_n6ea72f",
                      "score": 2,
                      "author_fullname": "t2_ddyte",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I suspect it would be fast on your 3090, yeah, but hell, 700 from 8 parallel requests isn't bad!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6eddnh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I suspect it would be fast on your 3090, yeah, but hell, 700 from 8 parallel requests isn&amp;#39;t bad!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf3wr0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6eddnh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754074418,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ea72f",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "michaelsoft__binbows",
            "can_mod_post": false,
            "created_utc": 1754073483,
            "send_replies": true,
            "parent_id": "t3_1mf3wr0",
            "score": 1,
            "author_fullname": "t2_iifi6ul2l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "wow 2500 t/s that's a lot. I am using sglang (a two month old docker setup now) with my 3090 and with 8 requests in parallel i'm hitting nearly 700, and i thought that was incredible. sounds like 1000 or more might be possible (though when i pushed past 8 it was not giving me more speed), or maybe i need to try vllm too...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ea72f",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;wow 2500 t/s that&amp;#39;s a lot. I am using sglang (a two month old docker setup now) with my 3090 and with 8 requests in parallel i&amp;#39;m hitting nearly 700, and i thought that was incredible. sounds like 1000 or more might be possible (though when i pushed past 8 it was not giving me more speed), or maybe i need to try vllm too...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6ea72f/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754073483,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf3wr0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6enhpd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "teachersecret",
                      "can_mod_post": false,
                      "created_utc": 1754077420,
                      "send_replies": true,
                      "parent_id": "t1_n6em93g",
                      "score": 0,
                      "author_fullname": "t2_ddyte",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm attempting to do mass-generation of text, literally pulling over two thousand tokens per second out of the model using mass-batch gen with 100 parallel requests. It's not a \"can it load in lmstudio?\" request - I'm interested in high performance mass-level inference, as best I can manage it on my 4090.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6enhpd",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m attempting to do mass-generation of text, literally pulling over two thousand tokens per second out of the model using mass-batch gen with 100 parallel requests. It&amp;#39;s not a &amp;quot;can it load in lmstudio?&amp;quot; request - I&amp;#39;m interested in high performance mass-level inference, as best I can manage it on my 4090.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf3wr0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6enhpd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754077420,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6em93g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1754077049,
            "send_replies": true,
            "parent_id": "t3_1mf3wr0",
            "score": 1,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why don't you use LM Studio and manually set how many layers you offload to your GPU ? You can adjust until she literally cries...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6em93g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why don&amp;#39;t you use LM Studio and manually set how many layers you offload to your GPU ? You can adjust until she literally cries...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6em93g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754077049,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf3wr0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6f0qju",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tmvr",
            "can_mod_post": false,
            "created_utc": 1754081372,
            "send_replies": true,
            "parent_id": "t3_1mf3wr0",
            "score": 1,
            "author_fullname": "t2_11qlhv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The only way to fit in higher context is to use lower quants. With FA and KV at Q8 you can fit 128K context into 24GB VRAM using a quant that is about 14GB in size or smaller.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6f0qju",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The only way to fit in higher context is to use lower quants. With FA and KV at Q8 you can fit 128K context into 24GB VRAM using a quant that is about 14GB in size or smaller.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf3wr0/best_way_to_run_the_qwen3_30b_a3b_coderinstruct/n6f0qju/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754081372,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf3wr0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]