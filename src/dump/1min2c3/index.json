[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I tested 500 math problems from HuggingFaceH4/MATH-500 and discovered something surprising: Qwen3's Chinese Chain-of-Thought achieves 97% accuracy using only 61% of the tokens its English CoT needs. The efficiency gap grows with problem complexity - for the hardest problems (Level 5), Chinese needs just 65% of English tokens.\n\nThis contradicts most research showing English as the more efficient reasoning language in LLMs. The difference appears to stem from training data: English CoT is exploratory and self-doubting (\"Wait, let me check...\"), while Chinese CoT is direct and confident. Same problem, completely different reasoning styles.\n\nKey findings:\n- Overall: Chinese uses 40% fewer tokens for same accuracy\n- Efficiency scales: 7% advantage on easy problems → 35% on hardest\n- English hit token limits on 15.4% of problems; Chinese only 0.6%\n- When given more tokens, English can match accuracy but still uses 40% more\n\nFull analysis with methodology, case studies, and reproducible code: https://github.com/PastaPastaPasta/llm-chinese-english\n\nTested on: qwen3-30b-a3b-thinking-2507-mlx@6bit via LM Studio",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Qwen3 Uses 40% Fewer Tokens When Reasoning in Chinese vs English",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 104,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1min2c3",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "ups": 45,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ynfaa",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 45,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/3yM1YJj_ZpYksP-TJ0pe0cKsv0FxUcxVWGobejapRrc.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754431944,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tested 500 math problems from HuggingFaceH4/MATH-500 and discovered something surprising: Qwen3&amp;#39;s Chinese Chain-of-Thought achieves 97% accuracy using only 61% of the tokens its English CoT needs. The efficiency gap grows with problem complexity - for the hardest problems (Level 5), Chinese needs just 65% of English tokens.&lt;/p&gt;\n\n&lt;p&gt;This contradicts most research showing English as the more efficient reasoning language in LLMs. The difference appears to stem from training data: English CoT is exploratory and self-doubting (&amp;quot;Wait, let me check...&amp;quot;), while Chinese CoT is direct and confident. Same problem, completely different reasoning styles.&lt;/p&gt;\n\n&lt;p&gt;Key findings:\n- Overall: Chinese uses 40% fewer tokens for same accuracy\n- Efficiency scales: 7% advantage on easy problems → 35% on hardest\n- English hit token limits on 15.4% of problems; Chinese only 0.6%\n- When given more tokens, English can match accuracy but still uses 40% more&lt;/p&gt;\n\n&lt;p&gt;Full analysis with methodology, case studies, and reproducible code: &lt;a href=\"https://github.com/PastaPastaPasta/llm-chinese-english\"&gt;https://github.com/PastaPastaPasta/llm-chinese-english&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Tested on: qwen3-30b-a3b-thinking-2507-mlx@6bit via LM Studio&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/y6r4oreky9hf1.png",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/y6r4oreky9hf1.png?auto=webp&amp;s=71fd8247b4a71b5bb0f025593e91ae20102b4ded",
                    "width": 1050,
                    "height": 784
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9411b4817b468e3653bbb24545bb86567f9aff4e",
                      "width": 108,
                      "height": 80
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e313d8547c3684bce868ef32001e0a0497675930",
                      "width": 216,
                      "height": 161
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=96b46effef3077a8bd28468cc14a3ae3c9d9b9ba",
                      "width": 320,
                      "height": 238
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=55636863c8127aeded3b9275c0615ec7fa28ce29",
                      "width": 640,
                      "height": 477
                    },
                    {
                      "url": "https://preview.redd.it/y6r4oreky9hf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c5d0e8e53cdf6fb3450d968f45dd71a96670748",
                      "width": 960,
                      "height": 716
                    }
                  ],
                  "variants": {},
                  "id": "NGrFEOFC_35j6Uz_6zEFlEZETBI5wWBfnbcLP3PZU4I"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1min2c3",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "PastaBlizzard",
            "discussion_type": null,
            "num_comments": 20,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/",
            "stickied": false,
            "url": "https://i.redd.it/y6r4oreky9hf1.png",
            "subreddit_subscribers": 511885,
            "created_utc": 1754431944,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7717ls",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Theio666",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n76ar3r",
                                          "score": 2,
                                          "author_fullname": "t2_ikhuo",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Idk about qwen, but r1 specifically had one of GRPO rewards to not switch languages in answer, so the models are aligned to use the same language across the whole answer.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7717ls",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Idk about qwen, but r1 specifically had one of GRPO rewards to not switch languages in answer, so the models are aligned to use the same language across the whole answer.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1min2c3",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n7717ls/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754467432,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754467432,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n76zf0f",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "-p-e-w-",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n76ar3r",
                                          "score": 1,
                                          "author_fullname": "t2_dkgrhaet",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "The problem is that “reasoning” isn’t thinking. It’s a context pre-fill mechanism. It’s actually more closely related to remembering than to reasoning, and should probably be called that.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n76zf0f",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The problem is that “reasoning” isn’t thinking. It’s a context pre-fill mechanism. It’s actually more closely related to remembering than to reasoning, and should probably be called that.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1min2c3",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n76zf0f/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754466418,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754466418,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n76ar3r",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Cool-Chemical-5629",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7658yk",
                                "score": 2,
                                "author_fullname": "t2_qz1qjc86",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I actually tried to just ask it in the past. Didn't work.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n76ar3r",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I actually tried to just ask it in the past. Didn&amp;#39;t work.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1min2c3",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n76ar3r/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754453814,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754453814,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7658yk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "droptableadventures",
                      "can_mod_post": false,
                      "created_utc": 1754451514,
                      "send_replies": true,
                      "parent_id": "t1_n75b91g",
                      "score": 2,
                      "author_fullname": "t2_52zg0eoq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This feels blindingly obvious but I guess you could try asking it. I wonder if it would.\n\nAlternatively you could try constrained generation - make the sampler disregard any character that isn't Chinese between &lt;think&gt; and &lt;/think&gt;.\n\nOr some continued pre-training might be able to make it happen.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7658yk",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This feels blindingly obvious but I guess you could try asking it. I wonder if it would.&lt;/p&gt;\n\n&lt;p&gt;Alternatively you could try constrained generation - make the sampler disregard any character that isn&amp;#39;t Chinese between &amp;lt;think&amp;gt; and &amp;lt;/think&amp;gt;.&lt;/p&gt;\n\n&lt;p&gt;Or some continued pre-training might be able to make it happen.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1min2c3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n7658yk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754451514,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n77gsqn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Cool-Chemical-5629",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n772gzt",
                                "score": 1,
                                "author_fullname": "t2_qz1qjc86",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Tried, sadly didn't work. I was hoping that maybe someone else already solved it and could share a working solution, but I guess that's just my wishful thinking.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n77gsqn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tried, sadly didn&amp;#39;t work. I was hoping that maybe someone else already solved it and could share a working solution, but I guess that&amp;#39;s just my wishful thinking.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1min2c3",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n77gsqn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754476068,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754476068,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n772gzt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "getmevodka",
                      "can_mod_post": false,
                      "created_utc": 1754468161,
                      "send_replies": true,
                      "parent_id": "t1_n75b91g",
                      "score": 1,
                      "author_fullname": "t2_7uoa6r1b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "system prompt it ?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n772gzt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;system prompt it ?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1min2c3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n772gzt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754468161,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n77rhep",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pvt_Twinkietoes",
                      "can_mod_post": false,
                      "created_utc": 1754480873,
                      "send_replies": true,
                      "parent_id": "t1_n75b91g",
                      "score": 1,
                      "author_fullname": "t2_3k9qfjsr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Translate your question into mandarin, then translate back with the results.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n77rhep",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Translate your question into mandarin, then translate back with the results.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1min2c3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n77rhep/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754480873,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n75b91g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Cool-Chemical-5629",
            "can_mod_post": false,
            "created_utc": 1754440772,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 12,
            "author_fullname": "t2_qz1qjc86",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Now to figure out how to make it reason in Chinese, but give final answer in English...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75b91g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Now to figure out how to make it reason in Chinese, but give final answer in English...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n75b91g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754440772,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 12
          }
        },
        {
          "kind": "t1",
          "data": {
            "total_awards_received": 0,
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "ups": 7,
            "removal_reason": null,
            "link_id": "t3_1min2c3",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n763r2c",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "RoyalCities",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n762eou",
                                                    "score": 1,
                                                    "author_fullname": "t2_5hq9z0rq",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "You totally could expand this out my dude depending on the findings as well.\n\nSo let's say IF chinese COT performs better for much more deterministic or more straightforward tasks and going to English just ends up being a waste of tokens you could make a hybrid model out of it.\n\nMy theory is English CoT is a total waste of tokens at a certain cut off point but since the language MAY be more rich with more nuance - it COULD perform better than Chinese cot as complexity rises. But once again. Is total overkill on less complex tasks. \n\nIf proven true...well a model could do a zero shot on a problem from the get go.\n\nThis answer is then refed to the model where is asks itself how confident it is on reply. If it is deterministic or a simple algebra problem(say needing limited cot) it can do the cot using Chinese tokens because it would be a waste to do it in English. \n\nBUT if it determines it is low confidence on its reply AND there is much deeper complexity then it can do CoT in English. \n\nYou then get the best of both worlds where your not wasting tokens on some problems that may not need it. \n\nIt takes like 1 extra call but could have savings down the road. Sorta like a confidence check + reasoning check i.e. should I reason more and based on the problem which CoT is needed based on performance.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n763r2c",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You totally could expand this out my dude depending on the findings as well.&lt;/p&gt;\n\n&lt;p&gt;So let&amp;#39;s say IF chinese COT performs better for much more deterministic or more straightforward tasks and going to English just ends up being a waste of tokens you could make a hybrid model out of it.&lt;/p&gt;\n\n&lt;p&gt;My theory is English CoT is a total waste of tokens at a certain cut off point but since the language MAY be more rich with more nuance - it COULD perform better than Chinese cot as complexity rises. But once again. Is total overkill on less complex tasks. &lt;/p&gt;\n\n&lt;p&gt;If proven true...well a model could do a zero shot on a problem from the get go.&lt;/p&gt;\n\n&lt;p&gt;This answer is then refed to the model where is asks itself how confident it is on reply. If it is deterministic or a simple algebra problem(say needing limited cot) it can do the cot using Chinese tokens because it would be a waste to do it in English. &lt;/p&gt;\n\n&lt;p&gt;BUT if it determines it is low confidence on its reply AND there is much deeper complexity then it can do CoT in English. &lt;/p&gt;\n\n&lt;p&gt;You then get the best of both worlds where your not wasting tokens on some problems that may not need it. &lt;/p&gt;\n\n&lt;p&gt;It takes like 1 extra call but could have savings down the road. Sorta like a confidence check + reasoning check i.e. should I reason more and based on the problem which CoT is needed based on performance.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1min2c3",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n763r2c/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754450912,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754450912,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n762eou",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "PastaBlizzard",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n760jhm",
                                          "score": 2,
                                          "author_fullname": "t2_ynfaa",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Indeed! I talk more about this in the GitHub I linked. Some of the next things I plan to investigate include harder datasets to see if the exploratory or straight to the point CoT results in better performance.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n762eou",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Indeed! I talk more about this in the GitHub I linked. Some of the next things I plan to investigate include harder datasets to see if the exploratory or straight to the point CoT results in better performance.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1min2c3",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n762eou/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754450382,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754450382,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n760jhm",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "RoyalCities",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n74n955",
                                "score": 2,
                                "author_fullname": "t2_5hq9z0rq",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Interesting. This could point to how how different the cot training dataset is. English is probably trained off of very verbose sorta phrasing \"let me see,\" \"how about this.\" But maybe the Chinese dataset is far more logical / matter of fact just due to how their language is structured or how they reason through a problem? Very neat finding\n\nEnglish may be more verbose but you may have a richer cot reasoning. Chinese more direct with less uncertainty. \n\nI'm sure their would be a way to test accuracy here - give it identical problems im both Chinese and English then see itf the reasoning is on both is accurate or diverges. Like is the fact it uses less tokens more efficient or more accurate etc.",
                                "edited": 1754449952,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n760jhm",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting. This could point to how how different the cot training dataset is. English is probably trained off of very verbose sorta phrasing &amp;quot;let me see,&amp;quot; &amp;quot;how about this.&amp;quot; But maybe the Chinese dataset is far more logical / matter of fact just due to how their language is structured or how they reason through a problem? Very neat finding&lt;/p&gt;\n\n&lt;p&gt;English may be more verbose but you may have a richer cot reasoning. Chinese more direct with less uncertainty. &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure their would be a way to test accuracy here - give it identical problems im both Chinese and English then see itf the reasoning is on both is accurate or diverges. Like is the fact it uses less tokens more efficient or more accurate etc.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1min2c3",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n760jhm/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754449660,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754449660,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n74n955",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "PastaBlizzard",
                      "can_mod_post": false,
                      "created_utc": 1754432721,
                      "send_replies": true,
                      "parent_id": "t1_n74m63z",
                      "score": 5,
                      "author_fullname": "t2_ynfaa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, this is why the primary focus is on the difference in the substance of their Chain of Thought as well as the increasing efficiency with harder problems. If we were simply looking at token differences due to encoding differences and how tokens are counted we would expect a constant efficiency ratio, not one that decreases with harder problems.\n\nWhat we do see is that while easy problems are solved in Chinese with 90% of the token usage as English, hard problems solved in Chinese use only 60% of the tokens as English.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74n955",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, this is why the primary focus is on the difference in the substance of their Chain of Thought as well as the increasing efficiency with harder problems. If we were simply looking at token differences due to encoding differences and how tokens are counted we would expect a constant efficiency ratio, not one that decreases with harder problems.&lt;/p&gt;\n\n&lt;p&gt;What we do see is that while easy problems are solved in Chinese with 90% of the token usage as English, hard problems solved in Chinese use only 60% of the tokens as English.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1min2c3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n74n955/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754432721,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7713hl",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "-p-e-w-",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n770hz7",
                                          "score": 1,
                                          "author_fullname": "t2_dkgrhaet",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Not quite. CJK is one of the reasons why LLM vocabularies are so large, yet inefficient. Having high-information characters bloats the vocabulary and requires large embedding spaces to represent that information. If we had only ASCII to deal with, LLMs would look very different.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7713hl",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not quite. CJK is one of the reasons why LLM vocabularies are so large, yet inefficient. Having high-information characters bloats the vocabulary and requires large embedding spaces to represent that information. If we had only ASCII to deal with, LLMs would look very different.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1min2c3",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n7713hl/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754467367,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754467367,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n770hz7",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "wwabbbitt",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n76zu2b",
                                "score": 1,
                                "author_fullname": "t2_51z2oqmy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It's the mapping to tokens that matter, not the mapping to number of bytes",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n770hz7",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s the mapping to tokens that matter, not the mapping to number of bytes&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1min2c3",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n770hz7/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754467032,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754467032,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n76zu2b",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "-p-e-w-",
                      "can_mod_post": false,
                      "created_utc": 1754466657,
                      "send_replies": true,
                      "parent_id": "t1_n74m63z",
                      "score": 1,
                      "author_fullname": "t2_dkgrhaet",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; It can take 4 characters to say the same thing that requires 15 characters in English.\n\nBut representing those characters takes up to 4 bytes each, vs 1 byte for basic Latin characters. The information density is similar, and indeed comparative linguistics has demonstrated that the information density of most human languages is roughly the same.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n76zu2b",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;It can take 4 characters to say the same thing that requires 15 characters in English.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;But representing those characters takes up to 4 bytes each, vs 1 byte for basic Latin characters. The information density is similar, and indeed comparative linguistics has demonstrated that the information density of most human languages is roughly the same.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1min2c3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n76zu2b/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754466657,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n74m63z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "DELETED",
            "no_follow": false,
            "author": "[deleted]",
            "can_mod_post": false,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 7,
            "approved_by": null,
            "report_reasons": null,
            "all_awardings": [],
            "subreddit_id": "t5_81eyvm",
            "body": "[deleted]",
            "edited": 1754432583,
            "downs": 0,
            "author_flair_css_class": null,
            "collapsed": true,
            "is_submitter": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;",
            "gildings": {},
            "collapsed_reason": null,
            "associated_award": null,
            "stickied": false,
            "subreddit_type": "public",
            "can_gild": false,
            "top_awarded_type": null,
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n74m63z/",
            "num_reports": null,
            "locked": false,
            "name": "t1_n74m63z",
            "created": 1754432360,
            "subreddit": "LocalLLaMA",
            "author_flair_text": null,
            "treatment_tags": [],
            "created_utc": 1754432360,
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "mod_note": null,
            "distinguished": null
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76ge1v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "archtekton",
            "can_mod_post": false,
            "created_utc": 1754456367,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 2,
            "author_fullname": "t2_1fv89v6an9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "非常好",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76ge1v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;非常好&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n76ge1v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754456367,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n75e4vp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "gamblingapocalypse",
            "can_mod_post": false,
            "created_utc": 1754441761,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 2,
            "author_fullname": "t2_fz3utn30",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "So you’re saying I have to learn Chinese huh? ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75e4vp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So you’re saying I have to learn Chinese huh? &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n75e4vp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754441761,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76rlrs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "o5mfiHTNsH748KVq",
            "can_mod_post": false,
            "created_utc": 1754462094,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 1,
            "author_fullname": "t2_e11zi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Real Chinese humans use less tokens when they speak English too. Non-native English speaking Chinese people often skip our English filler words because they’re unnecessary in Mandarin.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76rlrs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Real Chinese humans use less tokens when they speak English too. Non-native English speaking Chinese people often skip our English filler words because they’re unnecessary in Mandarin.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n76rlrs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754462094,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76szod",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jonasaba",
            "can_mod_post": false,
            "created_utc": 1754462855,
            "send_replies": true,
            "parent_id": "t3_1min2c3",
            "score": 1,
            "author_fullname": "t2_1umovk3s4e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "English is definitely not there lost efficient. \n\nMandarin Chinese is a lot more information dense in script. But it is obviously not the most efficient either. \n\nI think a lot also depends on tokenization. Ideally there should also be separate tokens with no analog in language reserved for thinking, whose internal meetings must be evolved by the training process.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76szod",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;English is definitely not there lost efficient. &lt;/p&gt;\n\n&lt;p&gt;Mandarin Chinese is a lot more information dense in script. But it is obviously not the most efficient either. &lt;/p&gt;\n\n&lt;p&gt;I think a lot also depends on tokenization. Ideally there should also be separate tokens with no analog in language reserved for thinking, whose internal meetings must be evolved by the training process.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1min2c3/qwen3_uses_40_fewer_tokens_when_reasoning_in/n76szod/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754462855,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1min2c3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]