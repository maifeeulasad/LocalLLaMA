[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.\n\nI'd like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like *\"how do I work on product1's source code?\" or \"What is the serial communication protocol between product2 and product3?\", \"how am I supposed to interact with product3?\"*, and so on. \n\nNo coding questions, more like general guidance and onboarding, which is doable even by small models I think.\n\nIn the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.\n\nSome thoughts:\n\n* Putting all the raw data in the same request for a flagship model easily exceeds the context limit\n* Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I'm looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I'm just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.\n* I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.\n* Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.\n* To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: *\"The user has requested: $original_prompt. Answer the user's question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3\"*. Unless LM Studio is the most ghetto RAG implementation in existence and there's a lot of much nicer options, I honestly wouldn't want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn't even a match means it just poisoned the context. Honestly if it wasn't for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.\n\nAnyway what's your advice?\n\nEDIT: despite the title, I'm open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there's some other solution that solves this problem in a smart way (ie not just \"run ElasticSearch\", but something that can connect the dots on its own like an LLM does) I'm happy to hear about it.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How do I train a good LLM on my company's doc in order to answer easy questions?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mbviok",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_93yn32gx",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753744987,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753744434,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I work at a tiny hardware company that has a lot of products (legacy and new) which means a lot of doc, about 3M lines of text across a wiki, READMEs in git repos, source code doc (sometimes concepts in some class in a header file), Word/PDF docs.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to have a LLM that is aware of our products and internal details, in order for employees to be able to get answers to questions like &lt;em&gt;&amp;quot;how do I work on product1&amp;#39;s source code?&amp;quot; or &amp;quot;What is the serial communication protocol between product2 and product3?&amp;quot;, &amp;quot;how am I supposed to interact with product3?&amp;quot;&lt;/em&gt;, and so on. &lt;/p&gt;\n\n&lt;p&gt;No coding questions, more like general guidance and onboarding, which is doable even by small models I think.&lt;/p&gt;\n\n&lt;p&gt;In the absence of the manpower to properly organize and curate the doc, I would like to know the best way I could have an LLM ingest this information.&lt;/p&gt;\n\n&lt;p&gt;Some thoughts:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Putting all the raw data in the same request for a flagship model easily exceeds the context limit&lt;/li&gt;\n&lt;li&gt;Creating a slim ~100k token document to use as the absolutely essential context for a flagship model (perhaps with links to larger documents, basically a curated sitemap) would take me at least 2 weeks. Plus the burden of maintaining. I&amp;#39;m looking for something that can take a document dump I can automatically create from a bash script that amalgamates the relevant documents. I&amp;#39;m just looking for something that is better than the status quo, this is a nice-to-have, not a business thing.&lt;/li&gt;\n&lt;li&gt;I have an idle Xeon server with 48GB DDR4 RAM free, if I wanted to run a local model. But from what I can see all local models have a low context cap.&lt;/li&gt;\n&lt;li&gt;Should I pay some Llama3 8B finetune service to make my own GGUF, or a LORA, trained on our data? I have zero experience with this stuff but it seems like a good option.&lt;/li&gt;\n&lt;li&gt;To preempt the RAG suggestions: I tried this in LM Studio with a single document. It was pure trash. Basically what it does is feed the document to some RAG db, then query the top 3 results that match the user prompt, then changes the LLM prompt to be: &lt;em&gt;&amp;quot;The user has requested: $original_prompt. Answer the user&amp;#39;s question. The following citations may be relevant: 1. $RAG1  2. $RAG2  3. $RAG3&amp;quot;&lt;/em&gt;. Unless LM Studio is the most ghetto RAG implementation in existence and there&amp;#39;s a lot of much nicer options, I honestly wouldn&amp;#39;t want to deal with RAG again. The fact that it gave 3 citations even when the 3rd one wasn&amp;#39;t even a match means it just poisoned the context. Honestly if it wasn&amp;#39;t for you guys praising RAG all the time I would have called it a marketing gimmick based on my (admittedly limited) experience.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Anyway what&amp;#39;s your advice?&lt;/p&gt;\n\n&lt;p&gt;EDIT: despite the title, I&amp;#39;m open to any sort of suggestions. I wrote the title after the idea of finetuning came to me, but if there&amp;#39;s some other solution that solves this problem in a smart way (ie not just &amp;quot;run ElasticSearch&amp;quot;, but something that can connect the dots on its own like an LLM does) I&amp;#39;m happy to hear about it.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mbviok",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "dtdisapointingresult",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/",
            "subreddit_subscribers": 506191,
            "created_utc": 1753744434,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5phjpf",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "RhubarbSimilar1683",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5pddu9",
                                "score": 2,
                                "author_fullname": "t2_1k4sjdwzk2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "a rag pipeline is the answer.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5phjpf",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;a rag pipeline is the answer.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mbviok",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5phjpf/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753748527,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753748527,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5pddu9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "dtdisapointingresult",
                      "can_mod_post": false,
                      "created_utc": 1753747101,
                      "send_replies": true,
                      "parent_id": "t1_n5p9r5y",
                      "score": 1,
                      "author_fullname": "t2_93yn32gx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; The model needs to process the users question, and then output a list of search terms, globs, regex, to do the look up, to be used to gather the context.\n\nThat's the manually curated sitemap I referred to, no? The LLM can't do it on its own, it can't browse our wiki, our file server, our git repos.\n\n&gt;In addition, you can run a background process on your docs, which goes through your documentation, and sends it to a cheap LLM, to create the 'master document' you referred to\n\nI don't see how an LLM can turn 3M lines of text into an efficient ~10k summary. Not only that, it would need to connect dots between unrelated documents that provide different degrees of clarification about a subject, and understand how they fit with each other.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5pddu9",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The model needs to process the users question, and then output a list of search terms, globs, regex, to do the look up, to be used to gather the context.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That&amp;#39;s the manually curated sitemap I referred to, no? The LLM can&amp;#39;t do it on its own, it can&amp;#39;t browse our wiki, our file server, our git repos.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;In addition, you can run a background process on your docs, which goes through your documentation, and sends it to a cheap LLM, to create the &amp;#39;master document&amp;#39; you referred to&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I don&amp;#39;t see how an LLM can turn 3M lines of text into an efficient ~10k summary. Not only that, it would need to connect dots between unrelated documents that provide different degrees of clarification about a subject, and understand how they fit with each other.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mbviok",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5pddu9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753747101,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5p9r5y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TokenRingAI",
            "can_mod_post": false,
            "created_utc": 1753745909,
            "send_replies": true,
            "parent_id": "t3_1mbviok",
            "score": 2,
            "author_fullname": "t2_1twuope92q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Model training isn't the answer, tool calls or structured outputs are the answer. The model needs to process the users question, and then output a list of search terms, globs, regex, to do the look up, to be used to gather the context. The more options you offer the LLM the more likely it will find the right stuff. The details matter here, and will make or break your search interface.\n\nIn addition, you can run a background process on your docs, which goes through your documentation, and sends it to a cheap LLM, to create the 'master document' you referred to, which can be referenced when responding to the users prompt, which will give the LLM a lot of context about what your company does, what your product names are and what features are in them, etc., which will generally help a lot to guide the model, which isn't going to know anything by default. You don't need to create that manually, an LLM can do that if you guide it",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5p9r5y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Model training isn&amp;#39;t the answer, tool calls or structured outputs are the answer. The model needs to process the users question, and then output a list of search terms, globs, regex, to do the look up, to be used to gather the context. The more options you offer the LLM the more likely it will find the right stuff. The details matter here, and will make or break your search interface.&lt;/p&gt;\n\n&lt;p&gt;In addition, you can run a background process on your docs, which goes through your documentation, and sends it to a cheap LLM, to create the &amp;#39;master document&amp;#39; you referred to, which can be referenced when responding to the users prompt, which will give the LLM a lot of context about what your company does, what your product names are and what features are in them, etc., which will generally help a lot to guide the model, which isn&amp;#39;t going to know anything by default. You don&amp;#39;t need to create that manually, an LLM can do that if you guide it&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5p9r5y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753745909,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbviok",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5phpex",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CyberNuub",
                      "can_mod_post": false,
                      "created_utc": 1753748582,
                      "send_replies": true,
                      "parent_id": "t1_n5p6t6f",
                      "score": 1,
                      "author_fullname": "t2_tdeyjjm5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Cool, but not free for his or my purposes. Thanks for the link though. I wonder how hard it would be to build a local agent solution... hm...",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5phpex",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Cool, but not free for his or my purposes. Thanks for the link though. I wonder how hard it would be to build a local agent solution... hm...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mbviok",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5phpex/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753748582,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5p6t6f",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "literallyCrevice",
            "can_mod_post": false,
            "created_utc": 1753744946,
            "send_replies": true,
            "parent_id": "t3_1mbviok",
            "score": 1,
            "author_fullname": "t2_4rygnrpn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "[https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval](https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5p6t6f",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval\"&gt;https://www.llamaindex.ai/blog/rag-is-dead-long-live-agentic-retrieval&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5p6t6f/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753744946,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbviok",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5pmdlg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "-dysangel-",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5plm65",
                                "score": 1,
                                "author_fullname": "t2_12ggykute6",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I suspect if you looked at stuff like LangChain and n8n you could find some ready made components that you can mix and match together. Personally I usually like building stuff from scratch to learn about it, and to have more control.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5pmdlg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I suspect if you looked at stuff like LangChain and n8n you could find some ready made components that you can mix and match together. Personally I usually like building stuff from scratch to learn about it, and to have more control.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mbviok",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5pmdlg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753750193,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753750193,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5plm65",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "dtdisapointingresult",
                      "can_mod_post": false,
                      "created_utc": 1753749935,
                      "send_replies": true,
                      "parent_id": "t1_n5pctiz",
                      "score": 1,
                      "author_fullname": "t2_93yn32gx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the advice. I guess there's no ready-made solution huh? (I say this while realizing that this is a relatively simple setup that people 5 years ago would have killed for, but remember I'm a casual user compared to you, so everything will require a time investment to learn, write wrapper scripts, etc)\n\nI found a selfhosted tool called R2R that's been posted on here and was gonna give it a try first, but it doesn't support Chroma, it uses Postgres. Do you think Chroma produces better results or is just an example you gave? And it seems like R2R is \"OpenAI-first\" (despite supporting local models) in that no doc is provided for which local models should be used, and people are complaining that their tool-calling using some popular local models is hard.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5plm65",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the advice. I guess there&amp;#39;s no ready-made solution huh? (I say this while realizing that this is a relatively simple setup that people 5 years ago would have killed for, but remember I&amp;#39;m a casual user compared to you, so everything will require a time investment to learn, write wrapper scripts, etc)&lt;/p&gt;\n\n&lt;p&gt;I found a selfhosted tool called R2R that&amp;#39;s been posted on here and was gonna give it a try first, but it doesn&amp;#39;t support Chroma, it uses Postgres. Do you think Chroma produces better results or is just an example you gave? And it seems like R2R is &amp;quot;OpenAI-first&amp;quot; (despite supporting local models) in that no doc is provided for which local models should be used, and people are complaining that their tool-calling using some popular local models is hard.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mbviok",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5plm65/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753749935,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5pctiz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753746913,
            "send_replies": true,
            "parent_id": "t3_1mbviok",
            "score": 1,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "RAG just means \"load some data from somewhere\" really.. yes, it's going to be the best option if you have 3 *million* lines of information.\n\nIf it were me, I'd do something like this\n\n\\- use a proper vector database like ChromaDB\n\n\\- load sections into the database a paragraph at a time, and store the original text as meta-data. You'll be able to match by semantic meaning, or exact text\n\n\\- query the database to find relevant results\n\n\\- have an agent review the results (I'd imagine you want way more than 3 depending on what you're doing and how big the stored sections are), discarding irrelevant memories, and providing relevant ones\n\n\\- pass the cleaned, *relevant* context to your chat agent",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5pctiz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;RAG just means &amp;quot;load some data from somewhere&amp;quot; really.. yes, it&amp;#39;s going to be the best option if you have 3 &lt;em&gt;million&lt;/em&gt; lines of information.&lt;/p&gt;\n\n&lt;p&gt;If it were me, I&amp;#39;d do something like this&lt;/p&gt;\n\n&lt;p&gt;- use a proper vector database like ChromaDB&lt;/p&gt;\n\n&lt;p&gt;- load sections into the database a paragraph at a time, and store the original text as meta-data. You&amp;#39;ll be able to match by semantic meaning, or exact text&lt;/p&gt;\n\n&lt;p&gt;- query the database to find relevant results&lt;/p&gt;\n\n&lt;p&gt;- have an agent review the results (I&amp;#39;d imagine you want way more than 3 depending on what you&amp;#39;re doing and how big the stored sections are), discarding irrelevant memories, and providing relevant ones&lt;/p&gt;\n\n&lt;p&gt;- pass the cleaned, &lt;em&gt;relevant&lt;/em&gt; context to your chat agent&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5pctiz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753746913,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mbviok",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5peg7q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1753747467,
            "send_replies": true,
            "parent_id": "t3_1mbviok",
            "score": 1,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "RAG",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5peg7q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;RAG&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5peg7q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753747467,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbviok",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ph0q4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lesser-than",
            "can_mod_post": false,
            "created_utc": 1753748349,
            "send_replies": true,
            "parent_id": "t3_1mbviok",
            "score": 1,
            "author_fullname": "t2_98d256k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "like others have said, RAG or a custom tool that caches common questions and gives canned answers.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ph0q4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;like others have said, RAG or a custom tool that caches common questions and gives canned answers.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5ph0q4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753748349,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbviok",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5pph7x",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "triynizzles1",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5pmvdl",
                                "score": 1,
                                "author_fullname": "t2_zr0g49ixt",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "To answer your second question first, it’s out of my control and is the resource that the company gives us. 🤪\n\nI haven’t had much success with fine-tuning. In my opinion, it leads to changing the AI models personality more than it does adding to their intelligence. If the data isn’t in the QA pairs used when fine tuning, then it wont be in the output when using the model.\n\n\nYou can get something up and running pretty easily by asking Claude 4 or Gemini 2.5 pro to build you a RAG system written in python using streamlit as the GUI and (ollama, vllm, llamacpp) as the inference engine.\n\nMaybe this will give you a good idea on how viable it would be to develop further.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5pph7x",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To answer your second question first, it’s out of my control and is the resource that the company gives us. 🤪&lt;/p&gt;\n\n&lt;p&gt;I haven’t had much success with fine-tuning. In my opinion, it leads to changing the AI models personality more than it does adding to their intelligence. If the data isn’t in the QA pairs used when fine tuning, then it wont be in the output when using the model.&lt;/p&gt;\n\n&lt;p&gt;You can get something up and running pretty easily by asking Claude 4 or Gemini 2.5 pro to build you a RAG system written in python using streamlit as the GUI and (ollama, vllm, llamacpp) as the inference engine.&lt;/p&gt;\n\n&lt;p&gt;Maybe this will give you a good idea on how viable it would be to develop further.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mbviok",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5pph7x/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753751253,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753751253,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5pmvdl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "dtdisapointingresult",
                      "can_mod_post": false,
                      "created_utc": 1753750364,
                      "send_replies": true,
                      "parent_id": "t1_n5pjkdu",
                      "score": 1,
                      "author_fullname": "t2_93yn32gx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Is your vision to have this be external facing for clients or internal only for employees?\n\nInternal only. This is just for basically 5 people.\n\nIt does seem like I will be forced to look into RAG. I'm just surprised so far everyone is saying this, without exception. Is a finetune such a bad choice?\n\nOut of curiosity why would you bother using RAG if your entire doc totals 80k tokens? That comfortably sits within the peak performance range of major LLMs. Instead of praying the RAG returns the correct results, you could guarantee the LLM gives the best answer by giving it everything. Cost concerns?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5pmvdl",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Is your vision to have this be external facing for clients or internal only for employees?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Internal only. This is just for basically 5 people.&lt;/p&gt;\n\n&lt;p&gt;It does seem like I will be forced to look into RAG. I&amp;#39;m just surprised so far everyone is saying this, without exception. Is a finetune such a bad choice?&lt;/p&gt;\n\n&lt;p&gt;Out of curiosity why would you bother using RAG if your entire doc totals 80k tokens? That comfortably sits within the peak performance range of major LLMs. Instead of praying the RAG returns the correct results, you could guarantee the LLM gives the best answer by giving it everything. Cost concerns?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mbviok",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5pmvdl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753750364,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5pjkdu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753749225,
            "send_replies": true,
            "parent_id": "t3_1mbviok",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is your vision to have this be external facing for clients or internal only for employees?\n\nRAG will probably be your best choice. However its easier said than done if you are building it by hand.  The challenge in my opinion is dialing in the parameters of the search ex number of results, size of each result, overlap, finding the best embedding model etc.\n\n\nYou could also try building a system where you select what file you want to extract data from. For example, if you know your question is about serial numbers then you can check the box to load the document about serial numbers into the context window. Or if your question is about source code, then you check the box for your source code document. This would eliminate the need to build a full rag system and hopefully documents would be able to fit in the context window of the model you are using.\n\nIf a single file is still too large for the context window, You could build a hybrid version. you select what files you want to be added to the vector database, then ask your question about the data. You can specify how much data is returned by the vector database so that you do not overfill your context window.\n\n\n\nI haven’t had good luck building QA pairs and then fine tuning a model. The knowledge held in the QA pairs are the bottleneck.\n\n\nIf you can use cloud services, use google’s NotebookLM and call it a day. Its a RAG system and quite impressive. I use it at my work, 26 documents about 80k tokens.. haven’t had any issues accept for when the documents have conflicting information. Sometimes it helps to know the answer ahead of time… but if you already know the answer, then it’s not worth asking an AI XD.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5pjkdu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is your vision to have this be external facing for clients or internal only for employees?&lt;/p&gt;\n\n&lt;p&gt;RAG will probably be your best choice. However its easier said than done if you are building it by hand.  The challenge in my opinion is dialing in the parameters of the search ex number of results, size of each result, overlap, finding the best embedding model etc.&lt;/p&gt;\n\n&lt;p&gt;You could also try building a system where you select what file you want to extract data from. For example, if you know your question is about serial numbers then you can check the box to load the document about serial numbers into the context window. Or if your question is about source code, then you check the box for your source code document. This would eliminate the need to build a full rag system and hopefully documents would be able to fit in the context window of the model you are using.&lt;/p&gt;\n\n&lt;p&gt;If a single file is still too large for the context window, You could build a hybrid version. you select what files you want to be added to the vector database, then ask your question about the data. You can specify how much data is returned by the vector database so that you do not overfill your context window.&lt;/p&gt;\n\n&lt;p&gt;I haven’t had good luck building QA pairs and then fine tuning a model. The knowledge held in the QA pairs are the bottleneck.&lt;/p&gt;\n\n&lt;p&gt;If you can use cloud services, use google’s NotebookLM and call it a day. Its a RAG system and quite impressive. I use it at my work, 26 documents about 80k tokens.. haven’t had any issues accept for when the documents have conflicting information. Sometimes it helps to know the answer ahead of time… but if you already know the answer, then it’s not worth asking an AI XD.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5pjkdu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753749225,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbviok",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5p6pz4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HilLiedTroopsDied",
            "can_mod_post": false,
            "created_utc": 1753744917,
            "send_replies": true,
            "parent_id": "t3_1mbviok",
            "score": 1,
            "author_fullname": "t2_1snfn3ui",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Option 1 - easiest = RAG  \nOption 2 - need a curated dataset in proper \\*paca format, then lora something like llama 3.2",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5p6pz4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Option 1 - easiest = RAG&lt;br/&gt;\nOption 2 - need a curated dataset in proper *paca format, then lora something like llama 3.2&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbviok/how_do_i_train_a_good_llm_on_my_companys_doc_in/n5p6pz4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753744917,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbviok",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]