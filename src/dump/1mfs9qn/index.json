[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using `llama.cpp` with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.\n\n**KV Quantization**\n\n* **KV cache quantization matters a lot**. If you're offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. ~~Use q5\\_1 for a good balance of memory usage and performance~~. It works well in PPL tests and in practice. **UPDATE:** K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:\n\n|CTK - CTD|PPL|STD|VRAM|\n|:-|:-|:-|:-|\n|q8\\_0 - q8\\_0|6.9016|0.04818|10.1GB|\n|q8\\_0 - q4\\_0|6.9104|0.04822|9.6GB|\n|q4\\_0 - q8\\_0|7.1241|0.04963|9.6GB|\n|q5\\_1 - q5\\_1|6.9664|0.04872|9.5GB|\n\n* **TLDR:** looks like q8\\_0 q4\\_0 is a very nice tradeoff in terms of accuracy and vram usage\n\n**Offloading Strategy**\n\n* You're bottlenecked by your **system RAM bandwidth** when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.\n* Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n* If you dont understand what the regex does, just feed it to and llm and it'll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.\n\n**Memory Tuning for CPU Offloading**\n\n* System memory speed has a major impact on throughput when using partial offloading.\n* Run your RAM at the highest stable speed. Overclock and tighten timings if you're comfortable doing so.\n* On **AM4** platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.\n* On **AM5**, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.\n* Poor memory tuning will bottleneck your CPU offloading even with a fast processor.\n\n**ubatch (Prompt Batch Size)**\n\n* Higher `ubatch` values significantly improve prompt processing (PP) performance.\n* Try values like `768` or `1024`. You’ll use more VRAM, but it’s often worth it for the speedup.\n* If you’re VRAM-limited, lower this until it fits.\n\n**Extra Performance Boost**\n\n* Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA\\_SET\\_ROWS=1 ./llama-server -md /path/to/model etc.\n\n**Speculative Decoding Tips (SD)**\n\nSpeculative decoding is supported in `llama.cpp`, but there are a couple important caveats:\n\n1. **KV cache quant affects acceptance rate heavily.** Using `q4_0` for the draft model’s KV cache *halves* the acceptance rate in my testing. Use ~~q5\\_1 or even~~ `q8_0`for the draft model KV cache for much better performance. **UPDATE:** \\-ctkd q8\\_0 -ctvd q4\\_0 works like a charm and saves vram. K is much more sensitive to quantization.\n2. **Draft model context handling is broken after filling the draft KV cache.** Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.\n3. **Draft parameters matter a lot**. In my testing, using `--draft-p-min 0.85 --draft-min 2 --draft-max 12` gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.\n\nFor SD, try using **Qwen 3 0.6B** as the draft model. It’s fast and works well, as long as you avoid the issues above.\n\nIf you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "[GUIDE] Running Qwen-30B (Coder/Instruct/Thinking) with CPU-GPU Partial Offloading - Tips, Tricks, and Optimizations",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfs9qn",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.95,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 124,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_66tlmx2l",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 124,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754223074,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754145881,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This post is a collection of practical tips and performance insights for running Qwen-30B (either Coder-Instruct or Thinking) locally using &lt;code&gt;llama.cpp&lt;/code&gt; with partial CPU-GPU offloading. After testing various configurations, quantizations, and setups, here’s what actually works.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;KV Quantization&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quantization matters a lot&lt;/strong&gt;. If you&amp;#39;re offloading layers to CPU, RAM usage can spike hard unless you quantize the KV cache. &lt;del&gt;Use q5_1 for a good balance of memory usage and performance&lt;/del&gt;. It works well in PPL tests and in practice. &lt;strong&gt;UPDATE:&lt;/strong&gt; K seems to be much more sensitive to quantization. I ran some ppl tests on 40k context and here are the results:&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;CTK - CTD&lt;/th&gt;\n&lt;th align=\"left\"&gt;PPL&lt;/th&gt;\n&lt;th align=\"left\"&gt;STD&lt;/th&gt;\n&lt;th align=\"left\"&gt;VRAM&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9016&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04818&lt;/td&gt;\n&lt;td align=\"left\"&gt;10.1GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q8_0 - q4_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9104&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04822&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q4_0 - q8_0&lt;/td&gt;\n&lt;td align=\"left\"&gt;7.1241&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04963&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.6GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;q5_1 - q5_1&lt;/td&gt;\n&lt;td align=\"left\"&gt;6.9664&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.04872&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; looks like q8_0 q4_0 is a very nice tradeoff in terms of accuracy and vram usage&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Offloading Strategy&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;You&amp;#39;re bottlenecked by your &lt;strong&gt;system RAM bandwidth&lt;/strong&gt; when offloading to CPU. Offload as few layers as possible. Ideally, offload only enough to make the model fit in VRAM.&lt;/li&gt;\n&lt;li&gt;Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/li&gt;\n&lt;li&gt;If you dont understand what the regex does, just feed it to and llm and it&amp;#39;ll break it down how it works and how you can tweak it for your vram amount. ofc it requires some experimentation to find the right number of layers.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Memory Tuning for CPU Offloading&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;System memory speed has a major impact on throughput when using partial offloading.&lt;/li&gt;\n&lt;li&gt;Run your RAM at the highest stable speed. Overclock and tighten timings if you&amp;#39;re comfortable doing so.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM4&lt;/strong&gt; platforms, run 1:1 FCLK:MCLK. Example: 3600 MT/s RAM = 1800 MHz FCLK.&lt;/li&gt;\n&lt;li&gt;On &lt;strong&gt;AM5&lt;/strong&gt;, make sure UCLK:MCLK is 1:1. Keep FCLK above 2000 MHz.&lt;/li&gt;\n&lt;li&gt;Poor memory tuning will bottleneck your CPU offloading even with a fast processor.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;ubatch (Prompt Batch Size)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Higher &lt;code&gt;ubatch&lt;/code&gt; values significantly improve prompt processing (PP) performance.&lt;/li&gt;\n&lt;li&gt;Try values like &lt;code&gt;768&lt;/code&gt; or &lt;code&gt;1024&lt;/code&gt;. You’ll use more VRAM, but it’s often worth it for the speedup.&lt;/li&gt;\n&lt;li&gt;If you’re VRAM-limited, lower this until it fits.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra Performance Boost&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Set this environment variable for a 5–10% performance gain:Launch like this: LLAMA_SET_ROWS=1 ./llama-server -md /path/to/model etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Speculative Decoding Tips (SD)&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Speculative decoding is supported in &lt;code&gt;llama.cpp&lt;/code&gt;, but there are a couple important caveats:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;KV cache quant affects acceptance rate heavily.&lt;/strong&gt; Using &lt;code&gt;q4_0&lt;/code&gt; for the draft model’s KV cache &lt;em&gt;halves&lt;/em&gt; the acceptance rate in my testing. Use &lt;del&gt;q5_1 or even&lt;/del&gt; &lt;code&gt;q8_0&lt;/code&gt;for the draft model KV cache for much better performance. &lt;strong&gt;UPDATE:&lt;/strong&gt; -ctkd q8_0 -ctvd q4_0 works like a charm and saves vram. K is much more sensitive to quantization.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft model context handling is broken after filling the draft KV cache.&lt;/strong&gt; Once the draft model’s context fills up, performance tanks. Right now it’s better to run the draft with full context size. Reducing it actually hurts.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Draft parameters matter a lot&lt;/strong&gt;. In my testing, using &lt;code&gt;--draft-p-min 0.85 --draft-min 2 --draft-max 12&lt;/code&gt; gives noticeably better results for code generation. These control how many draft tokens are proposed per step and how aggressive the speculative decoder is.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For SD, try using &lt;strong&gt;Qwen 3 0.6B&lt;/strong&gt; as the draft model. It’s fast and works well, as long as you avoid the issues above.&lt;/p&gt;\n\n&lt;p&gt;If you’ve got more tips or want help tuning your setup, feel free to add to the thread. I want this thread to become a collection of tips and tricks and best practices for running partial offloading on llama.cpp&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mfs9qn",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "AliNT77",
            "discussion_type": null,
            "num_comments": 57,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/",
            "subreddit_subscribers": 509626,
            "created_utc": 1754145881,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6m0ta8",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "AuspiciousApple",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6jc56m",
                                "score": 4,
                                "author_fullname": "t2_bndbg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Super cool to see people pushing things to the limit like this",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6m0ta8",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Super cool to see people pushing things to the limit like this&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6m0ta8/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754179037,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754179037,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6njedg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "xrailgun",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6jc56m",
                                "score": 2,
                                "author_fullname": "t2_kggm5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "So is k = q5_1 and v = q4_1 the way to go?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6njedg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So is k = q5_1 and v = q4_1 the way to go?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6njedg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754202408,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754202408,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6jc56m",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754147030,
                      "send_replies": true,
                      "parent_id": "t1_n6j8wva",
                      "score": 20,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "now this is exactly what i was looking for when i made the post. \n\nyou're right i just tested the draft model and -ctvd q4\\_0 does not drop the acceptance rate... great catch",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6jc56m",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;now this is exactly what i was looking for when i made the post. &lt;/p&gt;\n\n&lt;p&gt;you&amp;#39;re right i just tested the draft model and -ctvd q4_0 does not drop the acceptance rate... great catch&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jc56m/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754147030,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 20
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ogw6q",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754221375,
                      "send_replies": true,
                      "parent_id": "t1_n6j8wva",
                      "score": 2,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ok i just ran a few ppl tests:\n\nLLAMA_SET_ROWS=1 ./llama-perplexity -m ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot \"blk.(1[1-9]|[23][0-9]|4[0-6]).ffn_.*._exps.=CPU\" -fa -ub 4096 -b 8192 -c 40960 --seed 1337 -f ../../wikitext-2-raw/wiki.test.raw -ctk q8_0 -ctv q4_0\n\nCtk-ctd : ppl std     vram\n\nq8_0- q8_0 :  6.9016 0.04818.       10.1GB\n\nq8_0- q4_0:  6.9104 0.04822.       9.6GB\n\nq4_0- q8_0 :  7.1241  0.04963.       9.6GB\n\nq5_1- q5_1 :    6.9664 0.04872       9.5GB\n\n\nq8_0 q4_0 seems like the way to go",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ogw6q",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ok i just ran a few ppl tests:&lt;/p&gt;\n\n&lt;p&gt;LLAMA&lt;em&gt;SET_ROWS=1 ./llama-perplexity -m ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot &amp;quot;blk.(1[1-9]|[23][0-9]|4[0-6]).ffn&lt;/em&gt;.*._exps.=CPU&amp;quot; -fa -ub 4096 -b 8192 -c 40960 --seed 1337 -f ../../wikitext-2-raw/wiki.test.raw -ctk q8_0 -ctv q4_0&lt;/p&gt;\n\n&lt;p&gt;Ctk-ctd : ppl std     vram&lt;/p&gt;\n\n&lt;p&gt;q8_0- q8_0 :  6.9016 0.04818.       10.1GB&lt;/p&gt;\n\n&lt;p&gt;q8_0- q4_0:  6.9104 0.04822.       9.6GB&lt;/p&gt;\n\n&lt;p&gt;q4_0- q8_0 :  7.1241  0.04963.       9.6GB&lt;/p&gt;\n\n&lt;p&gt;q5_1- q5_1 :    6.9664 0.04872       9.5GB&lt;/p&gt;\n\n&lt;p&gt;q8_0 q4_0 seems like the way to go&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6ogw6q/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754221375,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6j8wva",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "boringcynicism",
            "can_mod_post": false,
            "created_utc": 1754146000,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 42,
            "author_fullname": "t2_1hz0lz0k5i",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Tip: Don't use the same quantization for K and V. K is way more sensitive!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6j8wva",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tip: Don&amp;#39;t use the same quantization for K and V. K is way more sensitive!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6j8wva/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754146000,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 42
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6msdir",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "EugenePopcorn",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6mab8w",
                                                    "score": 1,
                                                    "author_fullname": "t2_g6hpxxgss",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "IIRC it ignores the iGPU's dedicated memory partition and just allocates system memory instead.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6msdir",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;IIRC it ignores the iGPU&amp;#39;s dedicated memory partition and just allocates system memory instead.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfs9qn",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6msdir/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754189599,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754189599,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6mab8w",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "steezy13312",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6lk7bt",
                                          "score": 1,
                                          "author_fullname": "t2_rfjj2",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "What does GGML_VK_PREFER_HOST_MEMORY do?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6mab8w",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What does GGML_VK_PREFER_HOST_MEMORY do?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfs9qn",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6mab8w/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754182629,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754182629,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6sgeg1",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "EugenePopcorn",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6s3xtm",
                                                    "score": 1,
                                                    "author_fullname": "t2_g6hpxxgss",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Lama.cpp's default behavior is to ignore the usually pretty useless iGPU. You can override it by setting `GGML_VK_VISIBLE_DEVICES` to `0`, or `0,1`.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6sgeg1",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Lama.cpp&amp;#39;s default behavior is to ignore the usually pretty useless iGPU. You can override it by setting &lt;code&gt;GGML_VK_VISIBLE_DEVICES&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;, or &lt;code&gt;0,1&lt;/code&gt;.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfs9qn",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6sgeg1/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754268428,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754268428,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6s3xtm",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "AliNT77",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6lk7bt",
                                          "score": 1,
                                          "author_fullname": "t2_66tlmx2l",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I have not tried it. I did try compiling with vulkan support but the igpu was not identified.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6s3xtm",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have not tried it. I did try compiling with vulkan support but the igpu was not identified.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfs9qn",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6s3xtm/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754264011,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754264011,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6lk7bt",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "EugenePopcorn",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6jgyx9",
                                "score": 1,
                                "author_fullname": "t2_g6hpxxgss",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "What results do you get when offloading to your iGPU instead? 8600G for example goes from 50-&gt;100 tok/s prompt processing by using the 860m iGPU instead of CPU. TG goes from 15-&gt;20.\n\nExample\n\nGGML_VK_PREFER_HOST_MEMORY=1 LLAMA_SET_ROWS=0 GGML_VK_VISIBLE_DEVICES=0 ./llama-batched-bench -m ~/Models/Qwen3-Coder-30B-A3B-Instruct-UD-Q6_K_XL.gguf -ngl 99 -npp 1024 -ntg 256 -fa -npl 1 -c 32000 -ctk q8_0 -ctv q8_0\n\n| PP | TG | B | N_KV | T_PP s | S_PP t/s | T_TG s | S_TG t/s | T s | S t/s |\n|-------|--------|------|--------|----------|----------|----------|----------|----------|----------|\n| 1024 | 256 | 1 | 1280 | 10.361 | 98.83 | 13.254 | 19.31 | 23.615 | 54.20 |",
                                "edited": 1754180807,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6lk7bt",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What results do you get when offloading to your iGPU instead? 8600G for example goes from 50-&amp;gt;100 tok/s prompt processing by using the 860m iGPU instead of CPU. TG goes from 15-&amp;gt;20.&lt;/p&gt;\n\n&lt;p&gt;Example&lt;/p&gt;\n\n&lt;p&gt;GGML_VK_PREFER_HOST_MEMORY=1 LLAMA_SET_ROWS=0 GGML_VK_VISIBLE_DEVICES=0 ./llama-batched-bench -m ~/Models/Qwen3-Coder-30B-A3B-Instruct-UD-Q6_K_XL.gguf -ngl 99 -npp 1024 -ntg 256 -fa -npl 1 -c 32000 -ctk q8_0 -ctv q8_0&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;PP&lt;/th&gt;\n&lt;th&gt;TG&lt;/th&gt;\n&lt;th&gt;B&lt;/th&gt;\n&lt;th&gt;N_KV&lt;/th&gt;\n&lt;th&gt;T_PP s&lt;/th&gt;\n&lt;th&gt;S_PP t/s&lt;/th&gt;\n&lt;th&gt;T_TG s&lt;/th&gt;\n&lt;th&gt;S_TG t/s&lt;/th&gt;\n&lt;th&gt;T s&lt;/th&gt;\n&lt;th&gt;S t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;1024&lt;/td&gt;\n&lt;td&gt;256&lt;/td&gt;\n&lt;td&gt;1&lt;/td&gt;\n&lt;td&gt;1280&lt;/td&gt;\n&lt;td&gt;10.361&lt;/td&gt;\n&lt;td&gt;98.83&lt;/td&gt;\n&lt;td&gt;13.254&lt;/td&gt;\n&lt;td&gt;19.31&lt;/td&gt;\n&lt;td&gt;23.615&lt;/td&gt;\n&lt;td&gt;54.20&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6lk7bt/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754173151,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754173151,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6jgyx9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754148542,
                      "send_replies": true,
                      "parent_id": "t1_n6jdxgj",
                      "score": 6,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "that's actually a good idea. now that I think about it the title of post didn't really have to mention the 30B or even qwen3 for that matter all of these tips are applicable to every MoE model of any size.  \n\nmine is this on 5600G 32GB RTX3080 10GB: \n\nLLAMA\\_SET\\_ROWS=1 ./llama-server --api-key 1 -a qwen3 -m \\~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4\\_NL.gguf -ngl 999 -ot \"blk.(1\\[8-9\\]|\\[2-4\\]\\[0-9\\]).ffn\\_.\\*.\\_exps.=CPU\" -ub 768 -b 4096 -c 40960 -ctk q5\\_1 -ctv q5\\_1 -fa",
                      "edited": 1754154201,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6jgyx9",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;that&amp;#39;s actually a good idea. now that I think about it the title of post didn&amp;#39;t really have to mention the 30B or even qwen3 for that matter all of these tips are applicable to every MoE model of any size.  &lt;/p&gt;\n\n&lt;p&gt;mine is this on 5600G 32GB RTX3080 10GB: &lt;/p&gt;\n\n&lt;p&gt;LLAMA_SET_ROWS=1 ./llama-server --api-key 1 -a qwen3 -m ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot &amp;quot;blk.(1[8-9]|[2-4][0-9]).ffn_.*._exps.=CPU&amp;quot; -ub 768 -b 4096 -c 40960 -ctk q5_1 -ctv q5_1 -fa&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jgyx9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754148542,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6jdxgj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Alby407",
            "can_mod_post": false,
            "created_utc": 1754147582,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 15,
            "author_fullname": "t2_ptl3un7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thank you!  \nMaybe we can create a collection of setups and respective `llama-server` configurations?  \nDoes anyone have one for 64GB RAM and 24GB VRAM (RTX 4090)?",
            "edited": 1754147781,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jdxgj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you!&lt;br/&gt;\nMaybe we can create a collection of setups and respective &lt;code&gt;llama-server&lt;/code&gt; configurations?&lt;br/&gt;\nDoes anyone have one for 64GB RAM and 24GB VRAM (RTX 4090)?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jdxgj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754147582,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 15
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6jb6es",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754146729,
                      "send_replies": true,
                      "parent_id": "t1_n6j8vkr",
                      "score": 7,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "this should work nicely:   \nLLAMA\\_SET\\_ROWS=1 ./llama-server --api-key 1 -a qwen3 -m \\~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4\\_NL.gguf -ngl 999 -ot \"blk.(\\[2-9\\]|\\[1-4\\]\\[0-9\\]).ffn\\_.\\*.\\_exps.=CPU\" -ub 512 -b 4096 -c 40960 -ctk q5\\_1 -ctv q5\\_1 -fa \n\noffloads the experts from layer 2 to the cpu. 40k ctx with q5_1. uses 3.8GB vram on my system",
                      "edited": 1754152136,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6jb6es",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;this should work nicely:&lt;br/&gt;\nLLAMA_SET_ROWS=1 ./llama-server --api-key 1 -a qwen3 -m ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot &amp;quot;blk.([2-9]|[1-4][0-9]).ffn_.*._exps.=CPU&amp;quot; -ub 512 -b 4096 -c 40960 -ctk q5_1 -ctv q5_1 -fa &lt;/p&gt;\n\n&lt;p&gt;offloads the experts from layer 2 to the cpu. 40k ctx with q5_1. uses 3.8GB vram on my system&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jb6es/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754146729,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6j8vkr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "nevermore12154",
            "can_mod_post": false,
            "created_utc": 1754145988,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 5,
            "author_fullname": "t2_4eyo6q0e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "hi do you have any particular preset for 4gb vram/32gb ram? many thanks",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6j8vkr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;hi do you have any particular preset for 4gb vram/32gb ram? many thanks&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6j8vkr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754145988,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6jbytq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArchdukeofHyperbole",
            "can_mod_post": false,
            "created_utc": 1754146974,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 3,
            "author_fullname": "t2_1p41v97q5d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I hear there's a way to convert transformer models to rwkv. If that's true, I hope someone makes a qwen 30B conversion. It would simply memory management. On my meager 6GB gpu, prompt processing and generation tps were the same when starting a fresh conversation and compared to when the conversation was at 20K tokens. Max context is 1M, but I ran out of things to throw at it after a while. Got nowhere near 1M.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jbytq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I hear there&amp;#39;s a way to convert transformer models to rwkv. If that&amp;#39;s true, I hope someone makes a qwen 30B conversion. It would simply memory management. On my meager 6GB gpu, prompt processing and generation tps were the same when starting a fresh conversation and compared to when the conversation was at 20K tokens. Max context is 1M, but I ran out of things to throw at it after a while. Got nowhere near 1M.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jbytq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754146974,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6k26nm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754155306,
                      "send_replies": true,
                      "parent_id": "t1_n6k1kch",
                      "score": 3,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "https://github.com/ggml-org/llama.cpp/pull/14285",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6k26nm",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14285\"&gt;https://github.com/ggml-org/llama.cpp/pull/14285&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6k26nm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754155306,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ksgrh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754163886,
                      "send_replies": true,
                      "parent_id": "t1_n6k1kch",
                      "score": 1,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It really depends on the workload, for ai coding tools like RooCode, having high PP speeds makes a huge difference in the overall experience.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ksgrh",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It really depends on the workload, for ai coding tools like RooCode, having high PP speeds makes a huge difference in the overall experience.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6ksgrh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754163886,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6k1kch",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "regstuff",
            "can_mod_post": false,
            "created_utc": 1754155121,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 3,
            "author_fullname": "t2_a4ve8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Can someone explain the llama set rows thing?\nAlso I find the optimal ub size is actually a smaller value like 256 in my case. Am using the thinking model, and I find that I'm generating way more tokens than prompt processing cuz my prompts are mostly short. So i'd rather cut ub size a bit and jam another ffn or two into gpu. That gives me an extra 10% generation speed. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6k1kch",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can someone explain the llama set rows thing?\nAlso I find the optimal ub size is actually a smaller value like 256 in my case. Am using the thinking model, and I find that I&amp;#39;m generating way more tokens than prompt processing cuz my prompts are mostly short. So i&amp;#39;d rather cut ub size a bit and jam another ffn or two into gpu. That gives me an extra 10% generation speed. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6k1kch/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754155121,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ohggx",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754221638,
                      "send_replies": true,
                      "parent_id": "t1_n6o6x0z",
                      "score": 1,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "LLAMA\\_SET\\_ROWS=1 ./llama-server \\~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4\\_NL.gguf -ngl 999 -ot \"blk.(1\\[8-9\\]|\\[23\\]\\[0-9\\]|4\\[0-7\\]).ffn\\_.\\*.\\_exps.=CPU\" -ub 1024 -b 4096 -c 40960 -ctk q8\\_0 -ctv q4\\_0 -fa\n\ngive this a try and report back. No worries!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ohggx",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LLAMA_SET_ROWS=1 ./llama-server ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot &amp;quot;blk.(1[8-9]|[23][0-9]|4[0-7]).ffn_.*._exps.=CPU&amp;quot; -ub 1024 -b 4096 -c 40960 -ctk q8_0 -ctv q4_0 -fa&lt;/p&gt;\n\n&lt;p&gt;give this a try and report back. No worries!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6ohggx/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754221638,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6o6x0z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ImStruggles",
            "can_mod_post": false,
            "created_utc": 1754216139,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 2,
            "author_fullname": "t2_3u8qfeku",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Any preset/advice for \n\n  \n10GB Nvidia Vram, 64GB DDR4 3600 (PC4 28800) CL16 16-16-16-36, 5950X AM4\n\n  \nTysm for this guide 🙏",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6o6x0z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Any preset/advice for &lt;/p&gt;\n\n&lt;p&gt;10GB Nvidia Vram, 64GB DDR4 3600 (PC4 28800) CL16 16-16-16-36, 5950X AM4&lt;/p&gt;\n\n&lt;p&gt;Tysm for this guide 🙏&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6o6x0z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754216139,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6kvr30",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Ne00n",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6kkca5",
                                                    "score": 1,
                                                    "author_fullname": "t2_6d9eitt",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Either its unhappy because of the CUDA version, despite me having the correct one in stalled. On Windows with WSL, it complains about invalid cache settings, or just crashes because it runs out of VRAM on start. Neither the 4GB config here works nor the 8GB one for me.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6kvr30",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Either its unhappy because of the CUDA version, despite me having the correct one in stalled. On Windows with WSL, it complains about invalid cache settings, or just crashes because it runs out of VRAM on start. Neither the 4GB config here works nor the 8GB one for me.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfs9qn",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kvr30/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754165008,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754165008,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                },
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6s25vv",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Ne00n",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6kkca5",
                                                    "score": 1,
                                                    "author_fullname": "t2_6d9eitt",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Okay this works for me.\n\nllama.cpp/build/bin/llama-server --api-key 1 -a qwen3 \\\\\n\n\\-m /mnt/f/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q4\\_K\\_XL.gguf \\\\\n\n\\--jinja -ngl 99 --threads -1 --ctx-size 32684 \\\\\n\n\\--temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 \\\\\n\n\t \\-ot \"blk.(\\[2-9\\]|\\[1-4\\]\\[0-9\\]).ffn\\_.\\*.\\_exps.=CPU\" --host [0.0.0.0](http://0.0.0.0)\n\nHowever, no cache is used, otherwise llama just crashed outright, this way it nearly maxes out my VRAM usage, that's an issue though. I have no idea what cache settings to use though.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6s25vv",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Okay this works for me.&lt;/p&gt;\n\n&lt;p&gt;llama.cpp/build/bin/llama-server --api-key 1 -a qwen3 \\&lt;/p&gt;\n\n&lt;p&gt;-m /mnt/f/models/Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf \\&lt;/p&gt;\n\n&lt;p&gt;--jinja -ngl 99 --threads -1 --ctx-size 32684 \\&lt;/p&gt;\n\n&lt;p&gt;--temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 \\&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt; \\-ot &amp;quot;blk.(\\[2-9\\]|\\[1-4\\]\\[0-9\\]).ffn\\_.\\*.\\_exps.=CPU&amp;quot; --host [0.0.0.0](http://0.0.0.0)\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;However, no cache is used, otherwise llama just crashed outright, this way it nearly maxes out my VRAM usage, that&amp;#39;s an issue though. I have no idea what cache settings to use though.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfs9qn",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6s25vv/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754263388,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754263388,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6kkca5",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "AliNT77",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6kh6ca",
                                          "score": 1,
                                          "author_fullname": "t2_66tlmx2l",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Did you build llama.cpp properly? with cuda support I mean",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6kkca5",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Did you build llama.cpp properly? with cuda support I mean&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfs9qn",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kkca5/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754161157,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754161157,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6kh6ca",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Ne00n",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6js2wi",
                                "score": 1,
                                "author_fullname": "t2_6d9eitt",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Thanks but it won't use my GPU at all, so basically same result as with ollama out of the box.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6kh6ca",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks but it won&amp;#39;t use my GPU at all, so basically same result as with ollama out of the box.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kh6ca/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754160084,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754160084,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6js2wi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754152086,
                      "send_replies": true,
                      "parent_id": "t1_n6jnd3u",
                      "score": 3,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "this should work nicely:\nLLAMA_SET_ROWS=1 ./llama-server -m ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot \"blk.(1[0-9]|[1-4][0-9]).ffn_.*._exps.=CPU\" -ub 512 -b 4096 -c 40960 -ctk q5_1 -ctv q5_1 -fa\n\noffloads the experts from layer 11 onwards. No guarantee to fit tho I haven’t tried it. Play around with the ot regex to maximize the vram usage",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6js2wi",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;this should work nicely:\nLLAMA&lt;em&gt;SET_ROWS=1 ./llama-server -m ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot &amp;quot;blk.(1[0-9]|[1-4][0-9]).ffn&lt;/em&gt;.*._exps.=CPU&amp;quot; -ub 512 -b 4096 -c 40960 -ctk q5_1 -ctv q5_1 -fa&lt;/p&gt;\n\n&lt;p&gt;offloads the experts from layer 11 onwards. No guarantee to fit tho I haven’t tried it. Play around with the ot regex to maximize the vram usage&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6js2wi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754152086,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6jnd3u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ne00n",
            "can_mod_post": false,
            "created_utc": 1754150573,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 2,
            "author_fullname": "t2_6d9eitt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "hi, do you have a preset for 32gig and 8GB VRAM? thanks",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jnd3u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;hi, do you have a preset for 32gig and 8GB VRAM? thanks&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jnd3u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754150573,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6jy0os",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "sannysanoff",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6jr3ww",
                                "score": 3,
                                "author_fullname": "t2_35blh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "plz share your temperature and other settings.\n\nupd: i found official temperature and other settings, seemingly works better, topic is closed.",
                                "edited": 1754154610,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6jy0os",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;plz share your temperature and other settings.&lt;/p&gt;\n\n&lt;p&gt;upd: i found official temperature and other settings, seemingly works better, topic is closed.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jy0os/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754154005,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754154005,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6jr3ww",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754151774,
                      "send_replies": true,
                      "parent_id": "t1_n6jo5n8",
                      "score": 2,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I use it with RooCode and works surprisingly well. Punches wayyy above it’s weight.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6jr3ww",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use it with RooCode and works surprisingly well. Punches wayyy above it’s weight.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jr3ww/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754151774,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6k0o74",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "knownboyofno",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6jxtsg",
                                          "score": 1,
                                          "author_fullname": "t2_5y9divj7",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Got ya. I have used Qwen 3 Coder 30B 3A with VLLM in RooCode where I sent the temp to 0.15-0.7. It was able to do diff edits but would sometimes need to do the whole file because it failed about 30% of the time.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6k0o74",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Got ya. I have used Qwen 3 Coder 30B 3A with VLLM in RooCode where I sent the temp to 0.15-0.7. It was able to do diff edits but would sometimes need to do the whole file because it failed about 30% of the time.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfs9qn",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6k0o74/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754154851,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754154851,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6keydd",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "boringcynicism",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6jxtsg",
                                          "score": 1,
                                          "author_fullname": "t2_1hz0lz0k5i",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "These new models aren't very good with aider. The small Coder models are even worse than the normal ones. They're too optimized for agentic coding, which is not what aider is.\n\nYou can see this in the official announcement: the Coder model quotes a lower aider score than the generic one.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6keydd",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These new models aren&amp;#39;t very good with aider. The small Coder models are even worse than the normal ones. They&amp;#39;re too optimized for agentic coding, which is not what aider is.&lt;/p&gt;\n\n&lt;p&gt;You can see this in the official announcement: the Coder model quotes a lower aider score than the generic one.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfs9qn",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6keydd/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754159346,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754159346,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6jxtsg",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "sannysanoff",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6jueve",
                                "score": 2,
                                "author_fullname": "t2_35blh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I am using aider, which I use for work, with larger models like: qwen3-coder-400B, kimi k2, deepseek v3 and of course various closed source ones, too. So I can tell when diffs are produced correctly and when not. Maybe need some temperature setting though for 30B model.\n\nupd: i found official temperature and other settings, seemingly works better, topic is closed.",
                                "edited": 1754154625,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6jxtsg",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am using aider, which I use for work, with larger models like: qwen3-coder-400B, kimi k2, deepseek v3 and of course various closed source ones, too. So I can tell when diffs are produced correctly and when not. Maybe need some temperature setting though for 30B model.&lt;/p&gt;\n\n&lt;p&gt;upd: i found official temperature and other settings, seemingly works better, topic is closed.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jxtsg/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754153942,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754153942,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6jueve",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "knownboyofno",
                      "can_mod_post": false,
                      "created_utc": 1754152828,
                      "send_replies": true,
                      "parent_id": "t1_n6jo5n8",
                      "score": 1,
                      "author_fullname": "t2_5y9divj7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What settings did you use? Did you have this problem with Aider or another one like Cline/RooCode/KiloCode? Have you tried [Qwen Code](https://github.com/QwenLM/qwen-code) It is a CLI tool like Claude Code but is a branch of [Gemini CLI](https://github.com/google-gemini/gemini-cli).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6jueve",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What settings did you use? Did you have this problem with Aider or another one like Cline/RooCode/KiloCode? Have you tried &lt;a href=\"https://github.com/QwenLM/qwen-code\"&gt;Qwen Code&lt;/a&gt; It is a CLI tool like Claude Code but is a branch of &lt;a href=\"https://github.com/google-gemini/gemini-cli\"&gt;Gemini CLI&lt;/a&gt;.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jueve/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754152828,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6jxz5m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "sannysanoff",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6jut31",
                                "score": 1,
                                "author_fullname": "t2_35blh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "that was not the point of my post. I said it errs even in small contexts.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6jxz5m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;that was not the point of my post. I said it errs even in small contexts.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jxz5m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754153991,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754153991,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6jut31",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AdamDhahabi",
                      "can_mod_post": false,
                      "created_utc": 1754152955,
                      "send_replies": true,
                      "parent_id": "t1_n6jo5n8",
                      "score": 1,
                      "author_fullname": "t2_x5lnbc2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "30K is nothing in such agentic workflows. Read here [https://www.reddit.com/r/LocalLLaMA/comments/1mfe77f/claude\\_code\\_limit\\_reached\\_super\\_quickly/](https://www.reddit.com/r/LocalLLaMA/comments/1mfe77f/claude_code_limit_reached_super_quickly/)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6jut31",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;30K is nothing in such agentic workflows. Read here &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mfe77f/claude_code_limit_reached_super_quickly/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mfe77f/claude_code_limit_reached_super_quickly/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jut31/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754152955,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6jo5n8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sannysanoff",
            "can_mod_post": false,
            "created_utc": 1754150828,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_35blh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have genuine question. I tried 30B-coder model in alibaba cloud. I was using aider, but basically any agent would have same issues.\n\nWhen doing LLM-assisted coding, edits to the code are done as search/replace pairs (old code -&gt; new code).\n\nThis model, in its native quantization, struggled to quote my code to be replaced with new code. Basically, search/replace failed more often than not.\n\nMy context size was around 30K tokens max. And these errors, making any LLM-assisted coding process fail.\n\nQuestions: \n\nWhat use of this model can I make in my coding scenario? It cannot edit code. What are people using it for?\n\nOr, what am I doing wrong?\n\nThanks in advance.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jo5n8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have genuine question. I tried 30B-coder model in alibaba cloud. I was using aider, but basically any agent would have same issues.&lt;/p&gt;\n\n&lt;p&gt;When doing LLM-assisted coding, edits to the code are done as search/replace pairs (old code -&amp;gt; new code).&lt;/p&gt;\n\n&lt;p&gt;This model, in its native quantization, struggled to quote my code to be replaced with new code. Basically, search/replace failed more often than not.&lt;/p&gt;\n\n&lt;p&gt;My context size was around 30K tokens max. And these errors, making any LLM-assisted coding process fail.&lt;/p&gt;\n\n&lt;p&gt;Questions: &lt;/p&gt;\n\n&lt;p&gt;What use of this model can I make in my coding scenario? It cannot edit code. What are people using it for?&lt;/p&gt;\n\n&lt;p&gt;Or, what am I doing wrong?&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6jo5n8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754150828,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6js2bk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "UsualResult",
            "can_mod_post": false,
            "created_utc": 1754152080,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_10iarzku",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This is a great guide. I'm surprised how poor the ollama documentation is for most of their config files. I've since gotten frustrated with it and moved to a combination of llama-swap / llama.cpp and it's much easier to benchmark and configure models for maximum speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6js2bk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a great guide. I&amp;#39;m surprised how poor the ollama documentation is for most of their config files. I&amp;#39;ve since gotten frustrated with it and moved to a combination of llama-swap / llama.cpp and it&amp;#39;s much easier to benchmark and configure models for maximum speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6js2bk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754152080,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6kpkkm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "RemindMeBot",
                      "can_mod_post": false,
                      "created_utc": 1754162897,
                      "send_replies": true,
                      "parent_id": "t1_n6kpgs3",
                      "score": 1,
                      "author_fullname": "t2_gbm4p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I will be messaging you in 2 days on [**2025-08-04 19:27:41 UTC**](http://www.wolframalpha.com/input/?i=2025-08-04%2019:27:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kpgs3/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1mfs9qn%2Fguide_running_qwen30b_coderinstructthinking_with%2Fn6kpgs3%2F%5D%0A%0ARemindMe%21%202025-08-04%2019%3A27%3A41%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%201mfs9qn)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6kpkkm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I will be messaging you in 2 days on &lt;a href=\"http://www.wolframalpha.com/input/?i=2025-08-04%2019:27:41%20UTC%20To%20Local%20Time\"&gt;&lt;strong&gt;2025-08-04 19:27:41 UTC&lt;/strong&gt;&lt;/a&gt; to remind you of &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kpgs3/?context=3\"&gt;&lt;strong&gt;this link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1mfs9qn%2Fguide_running_qwen30b_coderinstructthinking_with%2Fn6kpgs3%2F%5D%0A%0ARemindMe%21%202025-08-04%2019%3A27%3A41%20UTC\"&gt;&lt;strong&gt;CLICK THIS LINK&lt;/strong&gt;&lt;/a&gt; to send a PM to also be reminded and to reduce spam.&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Parent commenter can &lt;/sup&gt; &lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Delete%20Comment&amp;amp;message=Delete%21%201mfs9qn\"&gt;&lt;sup&gt;delete this message to hide from others.&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/\"&gt;&lt;sup&gt;Info&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here\"&gt;&lt;sup&gt;Custom&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=List%20Of%20Reminders&amp;amp;message=MyReminders%21\"&gt;&lt;sup&gt;Your Reminders&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=Watchful1&amp;amp;subject=RemindMeBot%20Feedback\"&gt;&lt;sup&gt;Feedback&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kpkkm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754162897,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6kpgs3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "l0nedigit",
            "can_mod_post": false,
            "created_utc": 1754162861,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_ri3e9d6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "!RemindMe 2 days",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kpgs3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;!RemindMe 2 days&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kpgs3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754162861,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6kpvyj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AliNT77",
            "can_mod_post": false,
            "created_utc": 1754163006,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_66tlmx2l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Has anyone done any tests with PCIE Gen4 vs Gen3 speeds? Unfortunately my 5600g is limited to gen3…",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kpvyj",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Has anyone done any tests with PCIE Gen4 vs Gen3 speeds? Unfortunately my 5600g is limited to gen3…&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kpvyj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754163006,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6o47vd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754214566,
                      "send_replies": true,
                      "parent_id": "t1_n6kvaha",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "You can use my powershell script for building ik_llama under windows: https://github.com/Danmoreng/local-qwen3-coder-env",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6o47vd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can use my powershell script for building ik_llama under windows: &lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6o47vd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754214566,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6kvaha",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mohammacl",
            "can_mod_post": false,
            "created_utc": 1754164852,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_6f7xjt0x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I tried to use ik llama but apparently i need to recompile it for some of the flags and params to work. Is there any llama cpp binary that just works for partial offloading?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kvaha",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I tried to use ik llama but apparently i need to recompile it for some of the flags and params to work. Is there any llama cpp binary that just works for partial offloading?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6kvaha/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754164852,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6l1vod",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ConversationNice3225",
                      "can_mod_post": false,
                      "created_utc": 1754167031,
                      "send_replies": true,
                      "parent_id": "t1_n6l1ug4",
                      "score": 2,
                      "author_fullname": "t2_84upswup",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Per Unsloth's [documentation](https://docs.unsloth.ai/basics/qwen3-2507#improving-generation-speed), offloads all the MoE to CPU:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \".ffn\\_.\\*\\_exps.=CPU\"  \npp512 |        339.48 ± 6.70  \ntg128 |         23.82 ± 1.48\n\nOffloads both the UP and DOWN experts to CPU:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \".ffn\\_(up|down)\\_exps.=CPU\"  \npp512 |       478.74 ± 12.12  \ntg128 |         26.31 ± 1.11\n\nOffloads only the UP experts to CPU:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \".ffn\\_(up)\\_exps.=CPU\"  \npp512 |       868.27 ± 19.74  \ntg128 |         38.39 ± 1.03\n\nOffloads only the DOWN experts to CPU:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \".ffn\\_(down)\\_exps.=CPU\"  \npp512 |       818.52 ± 11.85  \ntg128 |         37.06 ± 1.01\n\nThis is where I started targeting only the attention and normal tensors for offloading, but keeping everything else (I think...regex is a little confusing).\n\nAll attention and normal tensors offloaded:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \"\\\\.(attn\\_.\\*|.\\*\\_norm)\\\\.=CPU\"  \npp512 |      2457.93 ± 27.35  \ntg128 |         16.56 ± 1.12\n\nJust the attention tensors for offloading:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \"\\\\.attn\\_.\\*\\\\.=CPU\"  \npp512 |      2543.25 ± 27.13  \ntg128 |         20.20 ± 0.83\n\nJust the normal tensors for offloading:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \".\\*\\_norm\\\\.=CPU\"  \npp512 |      3364.83 ± 57.36  \ntg128 |         30.63 ± 1.97\n\nThis is also from Unsloths documentation for selective layers being offloaded:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0 -ot \"\\\\.(6|7|8|9|\\[0-9\\]\\[0-9\\]|\\[0-9\\]\\[0-9\\]\\[0-9\\])\\\\.ffn(gate|up|down)\\_exps.=CPU\"  \npp512 |        384.38 ± 2.41  \ntg128 |         26.60 ± 1.76",
                      "edited": 1754167334,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6l1vod",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Per Unsloth&amp;#39;s &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-2507#improving-generation-speed\"&gt;documentation&lt;/a&gt;, offloads all the MoE to CPU:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;br/&gt;\npp512 |        339.48 ± 6.70&lt;br/&gt;\ntg128 |         23.82 ± 1.48&lt;/p&gt;\n\n&lt;p&gt;Offloads both the UP and DOWN experts to CPU:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot;&lt;br/&gt;\npp512 |       478.74 ± 12.12&lt;br/&gt;\ntg128 |         26.31 ± 1.11&lt;/p&gt;\n\n&lt;p&gt;Offloads only the UP experts to CPU:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot;&lt;br/&gt;\npp512 |       868.27 ± 19.74&lt;br/&gt;\ntg128 |         38.39 ± 1.03&lt;/p&gt;\n\n&lt;p&gt;Offloads only the DOWN experts to CPU:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;.ffn_(down)_exps.=CPU&amp;quot;&lt;br/&gt;\npp512 |       818.52 ± 11.85&lt;br/&gt;\ntg128 |         37.06 ± 1.01&lt;/p&gt;\n\n&lt;p&gt;This is where I started targeting only the attention and normal tensors for offloading, but keeping everything else (I think...regex is a little confusing).&lt;/p&gt;\n\n&lt;p&gt;All attention and normal tensors offloaded:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;\\.(attn_.*|.*_norm)\\.=CPU&amp;quot;&lt;br/&gt;\npp512 |      2457.93 ± 27.35&lt;br/&gt;\ntg128 |         16.56 ± 1.12&lt;/p&gt;\n\n&lt;p&gt;Just the attention tensors for offloading:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;\\.attn_.*\\.=CPU&amp;quot;&lt;br/&gt;\npp512 |      2543.25 ± 27.13&lt;br/&gt;\ntg128 |         20.20 ± 0.83&lt;/p&gt;\n\n&lt;p&gt;Just the normal tensors for offloading:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;.*_norm\\.=CPU&amp;quot;&lt;br/&gt;\npp512 |      3364.83 ± 57.36&lt;br/&gt;\ntg128 |         30.63 ± 1.97&lt;/p&gt;\n\n&lt;p&gt;This is also from Unsloths documentation for selective layers being offloaded:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0 -ot &amp;quot;\\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\\.ffn(gate|up|down)_exps.=CPU&amp;quot;&lt;br/&gt;\npp512 |        384.38 ± 2.41&lt;br/&gt;\ntg128 |         26.60 ± 1.76&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6l1vod/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754167031,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6l1ug4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ConversationNice3225",
            "can_mod_post": false,
            "created_utc": 1754167020,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_84upswup",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I was actually messing around with various offloading strategies this morning! I'm running this on Windows 11 (10.0.26100.4652), AMD 5900X, 32GB (2x16GB) DDR4-3600, RTX 4090 running on driver version 576.57 (CUDA Toolkit 12.9 Update 1), using Llama.cpp b5966. Tested using Unsloths \"Qwen3-30B-A3B-Thinking-2507-UD-Q4\\_K\\_XL.gguf\" via llama-bench:\n\nThis is the full Q4 model in VRAM, no offloading, this is the fastest it can go and is our baseline for the numbers below:  \n\\-fa 1 -ngl 99 -ctk q8\\_0 -ctv q8\\_0 -mmp 0  \npp512 |      3494.38 ± 22.37  \ntg128 |        160.09 ± 1.42\n\nI'd like to also note that I can set a 100k context, albeit using the slightly different but effectivly the same options when using llama-server, before I start going OOM and it spills over into system RAM. The below results are simply testing how much of a negative impact there is for offloading various layers and experts to CPU/system RAM. My intent was not to shoehorn the model into 8/12/16GB of VRAM. I usually don't go below Q8\\_0 on KV cache, my experience is that the chats deteriorate too much at lower quants (or at least Q4 is not great). I don't have VRAM usage documented, however they should more or less be in order of least to most aggressive on VRAM usage.",
            "edited": 1754167444,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6l1ug4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was actually messing around with various offloading strategies this morning! I&amp;#39;m running this on Windows 11 (10.0.26100.4652), AMD 5900X, 32GB (2x16GB) DDR4-3600, RTX 4090 running on driver version 576.57 (CUDA Toolkit 12.9 Update 1), using Llama.cpp b5966. Tested using Unsloths &amp;quot;Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf&amp;quot; via llama-bench:&lt;/p&gt;\n\n&lt;p&gt;This is the full Q4 model in VRAM, no offloading, this is the fastest it can go and is our baseline for the numbers below:&lt;br/&gt;\n-fa 1 -ngl 99 -ctk q8_0 -ctv q8_0 -mmp 0&lt;br/&gt;\npp512 |      3494.38 ± 22.37&lt;br/&gt;\ntg128 |        160.09 ± 1.42&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d like to also note that I can set a 100k context, albeit using the slightly different but effectivly the same options when using llama-server, before I start going OOM and it spills over into system RAM. The below results are simply testing how much of a negative impact there is for offloading various layers and experts to CPU/system RAM. My intent was not to shoehorn the model into 8/12/16GB of VRAM. I usually don&amp;#39;t go below Q8_0 on KV cache, my experience is that the chats deteriorate too much at lower quants (or at least Q4 is not great). I don&amp;#39;t have VRAM usage documented, however they should more or less be in order of least to most aggressive on VRAM usage.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6l1ug4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754167020,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6o4j2i",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754214750,
                      "send_replies": true,
                      "parent_id": "t1_n6lnaw7",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I don’t think my runscript settings are ideal yet, they need some tweaking (I took them from another Reddit comment) but I have a similar setup with 32GB RAM and 12GB VRAM and get ~38 t/s with them. https://github.com/Danmoreng/local-qwen3-coder-env?tab=readme-ov-file#example-of-the-shipped-launch-line",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6o4j2i",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don’t think my runscript settings are ideal yet, they need some tweaking (I took them from another Reddit comment) but I have a similar setup with 32GB RAM and 12GB VRAM and get ~38 t/s with them. &lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env?tab=readme-ov-file#example-of-the-shipped-launch-line\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env?tab=readme-ov-file#example-of-the-shipped-launch-line&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6o4j2i/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754214750,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6lnaw7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "JawGBoi",
            "can_mod_post": false,
            "created_utc": 1754174250,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_2ys57apx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\\&gt; Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\\\.(1\\[6-9\\]|\\[2-4\\]\\[0-9\\])\\\\.ffn\\_.\\*.\\_=CPU\n\nI'm not sure how this tuning should be done.\n\nI have 12gb vram and 64gb of ram. What configuration would be best for this? Currently have `Qwen3-Coder-30B-A3B-Instruct-UD-Q5_K_XL` downloaded but can use a different quant if need be.\n\nmany thanks",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lnaw7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; Start with this offload pattern:This offloads only the FFNs of layers 16 through 49. Tune this range based on your GPU’s VRAM limit. More offloading = slower inference.blk\\.(1[6-9]|[2-4][0-9])\\.ffn_.*._=CPU&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure how this tuning should be done.&lt;/p&gt;\n\n&lt;p&gt;I have 12gb vram and 64gb of ram. What configuration would be best for this? Currently have &lt;code&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q5_K_XL&lt;/code&gt; downloaded but can use a different quant if need be.&lt;/p&gt;\n\n&lt;p&gt;many thanks&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6lnaw7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754174250,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6o0xje",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Independent-Desk5910",
            "can_mod_post": false,
            "created_utc": 1754212578,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_1t8nko6ocb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How do you get speculative decoding to work? I tried with unsloth's 30B Q4\\_K\\_S as the main and 1.7B Q4\\_K\\_M as the draft and just got this error:\n\n    llama.cpp/src/llama-batch.cpp:38: GGML_ASSERT(batch.n_tokens &gt; 0) failed\n\nAnd yes, I did try setting batch size manually.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6o0xje",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do you get speculative decoding to work? I tried with unsloth&amp;#39;s 30B Q4_K_S as the main and 1.7B Q4_K_M as the draft and just got this error:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama.cpp/src/llama-batch.cpp:38: GGML_ASSERT(batch.n_tokens &amp;gt; 0) failed\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;And yes, I did try setting batch size manually.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6o0xje/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754212578,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6oahl5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754218159,
                      "send_replies": true,
                      "parent_id": "t1_n6o3w7w",
                      "score": 2,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have tested ik_llama extensively. The TG speed gain when using -fmoe is around the same as set_rows on llama.cpp so TG is pretty much the same, PP is a bit faster on ik_llama but not that much.\n\nThe main advantage of ik_llama right now is supporting IQK quants (ubergarm uploads them to hf). They are on the pareto front in terms of ppl/size. The IQ4_KSS quants from ubergarm are both smaller and better than IQ4_NL.\n\nBut my main problem is the lack of SD in ik_llama.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6oahl5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have tested ik_llama extensively. The TG speed gain when using -fmoe is around the same as set_rows on llama.cpp so TG is pretty much the same, PP is a bit faster on ik_llama but not that much.&lt;/p&gt;\n\n&lt;p&gt;The main advantage of ik_llama right now is supporting IQK quants (ubergarm uploads them to hf). They are on the pareto front in terms of ppl/size. The IQ4_KSS quants from ubergarm are both smaller and better than IQ4_NL.&lt;/p&gt;\n\n&lt;p&gt;But my main problem is the lack of SD in ik_llama.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6oahl5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754218159,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n6pccoe",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "AliNT77",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n6oomet",
                                                              "score": 1,
                                                              "author_fullname": "t2_66tlmx2l",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Have you tried ubuntu? That’s what i’m using.   Also your -ot looks very wrong to me. Do you intend to offload ffns from block20-47 to the cpu and the first regex is to keep the first 20 on the gpu? If so that makes sense maybe but try this one -ot instead of the two.\n\n-ot \"blk.(2[7-9]|[3][0-9]|4[0-7]).ffn_.*._exps.=CPU”\n\nThis one offloads the ffn tensors from the last 20 blocks to the cpu, everything else will be on the gpu.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n6pccoe",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Have you tried ubuntu? That’s what i’m using.   Also your -ot looks very wrong to me. Do you intend to offload ffns from block20-47 to the cpu and the first regex is to keep the first 20 on the gpu? If so that makes sense maybe but try this one -ot instead of the two.&lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;blk.(2[7-9]|[3][0-9]|4[0-7]).ffn_.*._exps.=CPU”&lt;/p&gt;\n\n&lt;p&gt;This one offloads the ffn tensors from the last 20 blocks to the cpu, everything else will be on the gpu.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mfs9qn",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6pccoe/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754232843,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754232843,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6oomet",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Danmoreng",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6oic29",
                                                    "score": 1,
                                                    "author_fullname": "t2_7z26p",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Hm...the fastest I can get is ~36 t/s with 11.6GB VRAM used and these parameters: \n`LLAMA_SET_ROWS=1 ./llama-server --model ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf --threads 8 -fa -c 65536 -ub 1024 -ctk q8_0 -ctv q4_0 -ot 'blk.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19).ffn.*exps=CUDA0' -ot 'exps=CPU' -ngl 999 --temp 0.6 --top-p 0.95 --top-k 20 --presence-penalty 1.5`\n\nNote that I'm running this under Windows inside powershell, just converted the command for you to bash if you want to try out as well.\n\nWhen I try adding in the draft model, my RAM usage goes up to almost 30GB and performance drops to ~24 t/s.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6oomet",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hm...the fastest I can get is ~36 t/s with 11.6GB VRAM used and these parameters: \n&lt;code&gt;LLAMA_SET_ROWS=1 ./llama-server --model ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf --threads 8 -fa -c 65536 -ub 1024 -ctk q8_0 -ctv q4_0 -ot &amp;#39;blk.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19).ffn.*exps=CUDA0&amp;#39; -ot &amp;#39;exps=CPU&amp;#39; -ngl 999 --temp 0.6 --top-p 0.95 --top-k 20 --presence-penalty 1.5&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Note that I&amp;#39;m running this under Windows inside powershell, just converted the command for you to bash if you want to try out as well.&lt;/p&gt;\n\n&lt;p&gt;When I try adding in the draft model, my RAM usage goes up to almost 30GB and performance drops to ~24 t/s.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mfs9qn",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6oomet/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754224729,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754224729,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6oic29",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "AliNT77",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6oeo0q",
                                          "score": 1,
                                          "author_fullname": "t2_66tlmx2l",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "give this a try on the mainline llama.cpp with iq4\\_nl quant :\n\nLLAMA\\_SET\\_ROWS=1 ./llama-server -m \\~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4\\_NL.gguf -ngl 999 -ot \"blk.(1\\[9-9\\]|\\[23\\]\\[0-9\\]|4\\[0-7\\]).ffn\\_.\\*.\\_exps.=CPU\" -ub 1024 -b 4096 -c 40960 -ctk q8\\_0 -ctv q4\\_0 -fa\n\nuses 9.5GB VRAM on my setup.\n\nI'm getting 48tps tg128 and 877tps pp1024",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6oic29",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;give this a try on the mainline llama.cpp with iq4_nl quant :&lt;/p&gt;\n\n&lt;p&gt;LLAMA_SET_ROWS=1 ./llama-server -m ~/dev/Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf -ngl 999 -ot &amp;quot;blk.(1[9-9]|[23][0-9]|4[0-7]).ffn_.*._exps.=CPU&amp;quot; -ub 1024 -b 4096 -c 40960 -ctk q8_0 -ctv q4_0 -fa&lt;/p&gt;\n\n&lt;p&gt;uses 9.5GB VRAM on my setup.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m getting 48tps tg128 and 877tps pp1024&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfs9qn",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6oic29/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754222041,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754222041,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6oeo0q",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Danmoreng",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6oao6t",
                                "score": 1,
                                "author_fullname": "t2_7z26p",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That sounds great. Well 38 was already way above the 20 I got from LMStudio so I was very happy about that. If I can get the same with original llama.cpp even better tbh. I'll do a bit more benchmarking myself now.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6oeo0q",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That sounds great. Well 38 was already way above the 20 I got from LMStudio so I was very happy about that. If I can get the same with original llama.cpp even better tbh. I&amp;#39;ll do a bit more benchmarking myself now.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6oeo0q/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754220301,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754220301,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6oao6t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754218259,
                      "send_replies": true,
                      "parent_id": "t1_n6o3w7w",
                      "score": 2,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "38tps in what? Also how much vram are you using? \n\n38 sounds very low for your setup. I get 48 with IQ4KSS on 5600G 3800MT/s ram and rtx 3080 10GB",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6oao6t",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;38tps in what? Also how much vram are you using? &lt;/p&gt;\n\n&lt;p&gt;38 sounds very low for your setup. I get 48 with IQ4KSS on 5600G 3800MT/s ram and rtx 3080 10GB&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6oao6t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754218259,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6o3w7w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Danmoreng",
            "can_mod_post": false,
            "created_utc": 1754214373,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 1,
            "author_fullname": "t2_7z26p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Did you test ik_llama.cpp vs llama.cpp as well? Gave me really nice results on my hardware (Ryzen 5 7600/32GB DDR5/RTX 4070 Ti 12GB =&gt; 38 t/s). I believe my settings can be tuned further however, will give your recommendations a try.\n\nhttps://github.com/Danmoreng/local-qwen3-coder-env",
            "edited": 1754216714,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6o3w7w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Did you test ik_llama.cpp vs llama.cpp as well? Gave me really nice results on my hardware (Ryzen 5 7600/32GB DDR5/RTX 4070 Ti 12GB =&amp;gt; 38 t/s). I believe my settings can be tuned further however, will give your recommendations a try.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6o3w7w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754214373,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6o4dqo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754214662,
                      "send_replies": true,
                      "parent_id": "t1_n6ls6bj",
                      "score": 3,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "LMStudio does not let you modify all the settings needed for performance.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6o4dqo",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;LMStudio does not let you modify all the settings needed for performance.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6o4dqo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754214662,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ojb5o",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "sToeTer",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6oiq8g",
                                "score": 1,
                                "author_fullname": "t2_nlp8v",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Nice thank you, gonna try that! :)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ojb5o",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice thank you, gonna try that! :)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfs9qn",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6ojb5o/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754222486,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754222486,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6oiq8g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AliNT77",
                      "can_mod_post": false,
                      "created_utc": 1754222224,
                      "send_replies": true,
                      "parent_id": "t1_n6ls6bj",
                      "score": 2,
                      "author_fullname": "t2_66tlmx2l",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "sorry for the confusion but these tips are only for llama.cpp and ik\\_llama.cpp.\n\nthe one thing you can use on LMStudio is to turn on flash attention, then set K quantization to q8\\_0 and V quantization to q4\\_0 to save a lot of vram",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6oiq8g",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;sorry for the confusion but these tips are only for llama.cpp and ik_llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;the one thing you can use on LMStudio is to turn on flash attention, then set K quantization to q8_0 and V quantization to q4_0 to save a lot of vram&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfs9qn",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6oiq8g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754222224,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ls6bj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sToeTer",
            "can_mod_post": false,
            "created_utc": 1754175980,
            "send_replies": true,
            "parent_id": "t3_1mfs9qn",
            "score": 0,
            "author_fullname": "t2_nlp8v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Might get downvoted, but is it possible to translate this to a more noob friendly way?( =Just the settings in LM Studio, please :D )\n\n( additional info: I got a Ryzen 7800x3d, 32GB RAM, RTX 4070Super 12GB VRAM)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ls6bj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Might get downvoted, but is it possible to translate this to a more noob friendly way?( =Just the settings in LM Studio, please :D )&lt;/p&gt;\n\n&lt;p&gt;( additional info: I got a Ryzen 7800x3d, 32GB RAM, RTX 4070Super 12GB VRAM)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfs9qn/guide_running_qwen30b_coderinstructthinking_with/n6ls6bj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754175980,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfs9qn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]