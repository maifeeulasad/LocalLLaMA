[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’ve just finished building a desk-side powerhouse and I want to run a community-driven series of inference latency benchmarks (in ms) on the latest high-performance open-source LLMs (&gt; 70 B parameters), including models like QWEN, GLM 4.5, and others you recommend. \n\nI’m also keen to try any RAM/CPU tricks for models that may not fit entirely in GPU VRAM—e.g. K-Transformers, paged-KV llama.cpp, host-offload, thread-affinity hacks, etc.\n\nIf your interested in a benchmark send me: \n\n- Model (&gt; 70 B): NUQUEN-xxx, GLM 4.5,etc\n- Quantization: Q4_K, Q5_K, BF16, FP16, 4-bit, etc.\n- Engine: vLLM, k-Transformers, llama.cpp (paged KV), Text Generation Inference (TGI), etc.\n- Context length: e.g. 32 K, 100 K tokens\n- Batch size: default 1 unless you specify otherwise\nAny Tricks: any settings or hacks to leverage host RAM, CPU offload, etc.\n\nI’ll report back with:\n- Generation latency (e.g. tg128)\n- Prompt-processing throughput (e.g. pp512)\n\nMy system specs:\n\nCPU: AMD EPYC 9255 (24 C / 48 T)\nRAM: 12 × 64 GB DDR5-6000 ≈ 760 GiB\nGPUs: 2 × RTX PRO 6000 Blackwell Max-Q (96 GB VRAM each)\nOS: Pop!_OS",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Offering Benchmarks on my New RIG for Larger Models",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhoaxs",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_5l4zmzcw",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754338256,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’ve just finished building a desk-side powerhouse and I want to run a community-driven series of inference latency benchmarks (in ms) on the latest high-performance open-source LLMs (&amp;gt; 70 B parameters), including models like QWEN, GLM 4.5, and others you recommend. &lt;/p&gt;\n\n&lt;p&gt;I’m also keen to try any RAM/CPU tricks for models that may not fit entirely in GPU VRAM—e.g. K-Transformers, paged-KV llama.cpp, host-offload, thread-affinity hacks, etc.&lt;/p&gt;\n\n&lt;p&gt;If your interested in a benchmark send me: &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Model (&amp;gt; 70 B): NUQUEN-xxx, GLM 4.5,etc&lt;/li&gt;\n&lt;li&gt;Quantization: Q4_K, Q5_K, BF16, FP16, 4-bit, etc.&lt;/li&gt;\n&lt;li&gt;Engine: vLLM, k-Transformers, llama.cpp (paged KV), Text Generation Inference (TGI), etc.&lt;/li&gt;\n&lt;li&gt;Context length: e.g. 32 K, 100 K tokens&lt;/li&gt;\n&lt;li&gt;Batch size: default 1 unless you specify otherwise\nAny Tricks: any settings or hacks to leverage host RAM, CPU offload, etc.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I’ll report back with:\n- Generation latency (e.g. tg128)\n- Prompt-processing throughput (e.g. pp512)&lt;/p&gt;\n\n&lt;p&gt;My system specs:&lt;/p&gt;\n\n&lt;p&gt;CPU: AMD EPYC 9255 (24 C / 48 T)\nRAM: 12 × 64 GB DDR5-6000 ≈ 760 GiB\nGPUs: 2 × RTX PRO 6000 Blackwell Max-Q (96 GB VRAM each)\nOS: Pop!_OS&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mhoaxs",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Infamous_Jaguar_2151",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754338256,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xxk7o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "pololueco",
            "can_mod_post": false,
            "created_utc": 1754342141,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 4,
            "author_fullname": "t2_9valyhvm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "How much power do you draw at idle and running?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xxk7o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How much power do you draw at idle and running?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6xxk7o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754342141,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6yy435",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Infamous_Jaguar_2151",
                      "can_mod_post": false,
                      "created_utc": 1754354240,
                      "send_replies": true,
                      "parent_id": "t1_n6yw4q1",
                      "score": 1,
                      "author_fullname": "t2_5l4zmzcw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah definitely gonna try ik-llama.cpp",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6yy435",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah definitely gonna try ik-llama.cpp&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhoaxs",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6yy435/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754354240,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6yw4q1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dear-Argument7658",
            "can_mod_post": false,
            "created_utc": 1754353539,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 2,
            "author_fullname": "t2_y3hxemhj4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nice rig! I would love to see GLM 4.5 and Kimi K2 with ik\\_llama.cpp. I have an RTX A6000 as well; send me a message if you want/need some tuning tips for ik\\_llama.cpp with CPU offload. If you want to run agentic workflows, I can recommend GLM 4.5-Air on vLLM. I see 90-140 token/s at lower context windows for single requests and &gt;3000 for batched, using the AWQ quantized model on Blackwell.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yw4q1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice rig! I would love to see GLM 4.5 and Kimi K2 with ik_llama.cpp. I have an RTX A6000 as well; send me a message if you want/need some tuning tips for ik_llama.cpp with CPU offload. If you want to run agentic workflows, I can recommend GLM 4.5-Air on vLLM. I see 90-140 token/s at lower context windows for single requests and &amp;gt;3000 for batched, using the AWQ quantized model on Blackwell.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6yw4q1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754353539,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ydvpw",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "MelodicRecognition7",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6xr6n5",
                                "score": 1,
                                "author_fullname": "t2_1eex9ug5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "https://old.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/ I think the same applies to the 5th gen",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ydvpw",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1fcy8x6/memory_bandwidth_values_stream_triad_benchmark/&lt;/a&gt; I think the same applies to the 5th gen&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mhoaxs",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6ydvpw/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754347360,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754347360,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6xr6n5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Infamous_Jaguar_2151",
                      "can_mod_post": false,
                      "created_utc": 1754340266,
                      "send_replies": true,
                      "parent_id": "t1_n6xmoqe",
                      "score": 3,
                      "author_fullname": "t2_5l4zmzcw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I wanted the memory bandwidth without spending more than I need to.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6xr6n5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I wanted the memory bandwidth without spending more than I need to.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhoaxs",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6xr6n5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754340266,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6xmoqe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754338998,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "what is your motherboard model? And why just 24 cores?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xmoqe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;what is your motherboard model? And why just 24 cores?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6xmoqe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754338998,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6xnva3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "IrisColt",
            "can_mod_post": false,
            "created_utc": 1754339327,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 1,
            "author_fullname": "t2_c2f558x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thanks!!! I always appreciate these posts. :)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6xnva3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks!!! I always appreciate these posts. :)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6xnva3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754339327,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y22a6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "no_no_no_oh_yes",
            "can_mod_post": false,
            "created_utc": 1754343516,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 1,
            "author_fullname": "t2_vgzf6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I was looking for a very similar setup, looking forward to know the benchmarks.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y22a6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was looking for a very similar setup, looking forward to know the benchmarks.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6y22a6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754343516,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y7k5n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Secure_Reflection409",
            "can_mod_post": false,
            "created_utc": 1754345259,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 1,
            "author_fullname": "t2_by77ogdhr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nice rig.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y7k5n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice rig.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6y7k5n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754345259,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6y7x0h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Eden1506",
            "can_mod_post": false,
            "created_utc": 1754345374,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 1,
            "author_fullname": "t2_2ezqqypt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Running qwen3 235b and deepseek R1 at q4 on cpu only.\n\n2 RTX PRO 6000 are awesome but honestly too far out of my budget but an AMD Epyc system with 256gb might be an option and I would love to know what kind of token speed you can possibly get with just cpu interference on such a system.\n\nThanks for testing and sharing!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6y7x0h",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Running qwen3 235b and deepseek R1 at q4 on cpu only.&lt;/p&gt;\n\n&lt;p&gt;2 RTX PRO 6000 are awesome but honestly too far out of my budget but an AMD Epyc system with 256gb might be an option and I would love to know what kind of token speed you can possibly get with just cpu interference on such a system.&lt;/p&gt;\n\n&lt;p&gt;Thanks for testing and sharing!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6y7x0h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754345374,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yfh6b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "blackwell_tart",
            "can_mod_post": false,
            "created_utc": 1754347899,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 1,
            "author_fullname": "t2_1t7r9dkpud",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The official Qwen3 235B A22B GPTQ INT4 in vLLM would be interesting. We run it here on a 128-core Epyc 9745 with a pair of Blackwell PRO Workstation GPUs, not the Max-Q. We also run 12x 64GB DDR5-6400, but the Gigabyte MZ33-AR1 rev. 3.x motherboard maxes out at 5200 MT/s. Power comes from a 2800W Super Flower Leadex PSU that we run from 240V (this is USA).\n\nI do not have access to do testing for another week due to vacation, but once back I can repeat your tests. What I can tell you is that for simple chat with small prompts we are seeing 77 tokens/sec.\n\nThank you for offering to do this. I'm curious how RAM speed impacts things, if at all.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yfh6b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The official Qwen3 235B A22B GPTQ INT4 in vLLM would be interesting. We run it here on a 128-core Epyc 9745 with a pair of Blackwell PRO Workstation GPUs, not the Max-Q. We also run 12x 64GB DDR5-6400, but the Gigabyte MZ33-AR1 rev. 3.x motherboard maxes out at 5200 MT/s. Power comes from a 2800W Super Flower Leadex PSU that we run from 240V (this is USA).&lt;/p&gt;\n\n&lt;p&gt;I do not have access to do testing for another week due to vacation, but once back I can repeat your tests. What I can tell you is that for simple chat with small prompts we are seeing 77 tokens/sec.&lt;/p&gt;\n\n&lt;p&gt;Thank you for offering to do this. I&amp;#39;m curious how RAM speed impacts things, if at all.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6yfh6b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754347899,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6yxfun",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "segmond",
            "can_mod_post": false,
            "created_utc": 1754354000,
            "send_replies": true,
            "parent_id": "t3_1mhoaxs",
            "score": 1,
            "author_fullname": "t2_ah13x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "run the glm-4.5-air q8.  i'm getting starting from 45tk/sec tg and then it drops as it goes along,  fastest pp I have seen is 720tk/sec.  my system is a decade old x99 platform with decade old intel 2.4ghz cpu and ddr 2400mhz quad channel  I dream of rtx blackwell 6000, on such an epyc platform.   your specs is a dream!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6yxfun",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;run the glm-4.5-air q8.  i&amp;#39;m getting starting from 45tk/sec tg and then it drops as it goes along,  fastest pp I have seen is 720tk/sec.  my system is a decade old x99 platform with decade old intel 2.4ghz cpu and ddr 2400mhz quad channel  I dream of rtx blackwell 6000, on such an epyc platform.   your specs is a dream!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhoaxs/offering_benchmarks_on_my_new_rig_for_larger/n6yxfun/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754354000,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhoaxs",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]