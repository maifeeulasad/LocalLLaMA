[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I've been experimenting with llama.cpp's RPC with two machines.\n\nDuring inference it generates traffic of about 650 KBytes/token (from the master node to the RPC node) and 45 KBytes/token (opposite direction).\n\nThis is much more than I expected, as my understanding was that only activations at boundary layers are transferred, and each token is basically a few KB of data. \n\nWhy is there such high continuous traffic during inference?\n(I'm aware that model loading is a network-heavy task, but this is after that)\n\n(other info)\n\n- model: Qwen3-Coder-30B-A3B:Q8_0\n- both machines have 32GB VRAM\n- the layers 0-24 are offloaded to the RPC node, 25-48 on master (according to logs)\n- amount of traffic per token slightly increases as more token are decoded\n\nthe full command (on the master) is:\n\n    llama-server\n      -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -dt 0.1 --cache-reuse 256 -fa\n      --host 0.0.0.0 --port 11435 --jinja\n      -c 8192 -n 32768 -ngl 99 -v\n      --rpc 10.42.0.94:50094\n      -m model.gguf\n\nThanks in advance!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "High traffic when *inferencing* in llama.cpp's RPC mode?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mk5spn",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_3szjkcbx",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754586488,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been experimenting with llama.cpp&amp;#39;s RPC with two machines.&lt;/p&gt;\n\n&lt;p&gt;During inference it generates traffic of about 650 KBytes/token (from the master node to the RPC node) and 45 KBytes/token (opposite direction).&lt;/p&gt;\n\n&lt;p&gt;This is much more than I expected, as my understanding was that only activations at boundary layers are transferred, and each token is basically a few KB of data. &lt;/p&gt;\n\n&lt;p&gt;Why is there such high continuous traffic during inference?\n(I&amp;#39;m aware that model loading is a network-heavy task, but this is after that)&lt;/p&gt;\n\n&lt;p&gt;(other info)&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;model: Qwen3-Coder-30B-A3B:Q8_0&lt;/li&gt;\n&lt;li&gt;both machines have 32GB VRAM&lt;/li&gt;\n&lt;li&gt;the layers 0-24 are offloaded to the RPC node, 25-48 on master (according to logs)&lt;/li&gt;\n&lt;li&gt;amount of traffic per token slightly increases as more token are decoded&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;the full command (on the master) is:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;llama-server\n  -sm row --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0 -dt 0.1 --cache-reuse 256 -fa\n  --host 0.0.0.0 --port 11435 --jinja\n  -c 8192 -n 32768 -ngl 99 -v\n  --rpc 10.42.0.94:50094\n  -m model.gguf\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mk5spn",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ilhud9s",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/",
            "subreddit_subscribers": 513813,
            "created_utc": 1754586488,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7g9bhu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "RhubarbSimilar1683",
            "can_mod_post": false,
            "created_utc": 1754587006,
            "send_replies": true,
            "parent_id": "t3_1mk5spn",
            "score": 1,
            "author_fullname": "t2_1k4sjdwzk2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would advice asking this on llama.cpp github discussions.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7g9bhu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would advice asking this on llama.cpp github discussions.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/n7g9bhu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754587006,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk5spn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ggxpu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DorphinPack",
            "can_mod_post": false,
            "created_utc": 1754589052,
            "send_replies": true,
            "parent_id": "t3_1mk5spn",
            "score": 1,
            "author_fullname": "t2_zebuyjw9s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Asking in the repo is a good idea but there are also some fundamentals that will help you communicate with them better. Not sure how far in your learning journey you are so forgive me if I‚Äôm retreading ground.\n\nToken prediction speed will scale inversely with the number of tokens in context. The first token is the result of looking at the entire context and predicting what comes next. That token is added to the context and the process repeats. The rest is all chat templates (start/stop tokens to control flow, max_tokens to limit output, etc).\n\nLooking deeper, the context is not a list of tokens. It is a data structure called the KV cache which encodes the tokens, their positions and relationships between them. 3Blue1Brown‚Äôs multi-headed attention video (and the one/two before it for background) reaaaalllly help here. Constructing the initial cache is ‚Äúprompt processing‚Äù but you also need to compute new Q/K values for each additional token as well as the V values relative to every other token already in the cache. Each new token still requires a bit of prompt processing and more tokens means more total operations. Those combinatorics are where the gradual slowdown comes from.\n\nFinally, there is some research from Deepmind into ‚Äúmixture of recursions‚Äù that aims to guide computation towards more effort on difficult tokens and minimal effort on easy tokens. While not entirely relevant it does highlight that these baby steps into advanced text prediction are inefficient because we dump the same compute and storage resources into each part of the work equally. We can definitely get smarter, which should help optimize things like RPC.\n\nThe importance of that depth comes out when you compare it to where most of us start in our mental model: a simple assembly line where some model+prompt monolith passes in-progress tokens down the layers until they‚Äôre sent to the user. Under that model, your intuition makes a lot of sense. Should help you triangulate, I hope! You are correct that there is more than just the tokens flowing. \n\nPlease do come back and update with anything you learn! RPC inference is fascinating, especially as someone who nerdily always wished it made sense to buy surplus infiniband gear üòÇ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ggxpu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Asking in the repo is a good idea but there are also some fundamentals that will help you communicate with them better. Not sure how far in your learning journey you are so forgive me if I‚Äôm retreading ground.&lt;/p&gt;\n\n&lt;p&gt;Token prediction speed will scale inversely with the number of tokens in context. The first token is the result of looking at the entire context and predicting what comes next. That token is added to the context and the process repeats. The rest is all chat templates (start/stop tokens to control flow, max_tokens to limit output, etc).&lt;/p&gt;\n\n&lt;p&gt;Looking deeper, the context is not a list of tokens. It is a data structure called the KV cache which encodes the tokens, their positions and relationships between them. 3Blue1Brown‚Äôs multi-headed attention video (and the one/two before it for background) reaaaalllly help here. Constructing the initial cache is ‚Äúprompt processing‚Äù but you also need to compute new Q/K values for each additional token as well as the V values relative to every other token already in the cache. Each new token still requires a bit of prompt processing and more tokens means more total operations. Those combinatorics are where the gradual slowdown comes from.&lt;/p&gt;\n\n&lt;p&gt;Finally, there is some research from Deepmind into ‚Äúmixture of recursions‚Äù that aims to guide computation towards more effort on difficult tokens and minimal effort on easy tokens. While not entirely relevant it does highlight that these baby steps into advanced text prediction are inefficient because we dump the same compute and storage resources into each part of the work equally. We can definitely get smarter, which should help optimize things like RPC.&lt;/p&gt;\n\n&lt;p&gt;The importance of that depth comes out when you compare it to where most of us start in our mental model: a simple assembly line where some model+prompt monolith passes in-progress tokens down the layers until they‚Äôre sent to the user. Under that model, your intuition makes a lot of sense. Should help you triangulate, I hope! You are correct that there is more than just the tokens flowing. &lt;/p&gt;\n\n&lt;p&gt;Please do come back and update with anything you learn! RPC inference is fascinating, especially as someone who nerdily always wished it made sense to buy surplus infiniband gear üòÇ&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/n7ggxpu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754589052,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk5spn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7i2jnh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "segmond",
            "can_mod_post": false,
            "created_utc": 1754606160,
            "send_replies": true,
            "parent_id": "t3_1mk5spn",
            "score": 1,
            "author_fullname": "t2_ah13x",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There's a host buffer that needs to be used for every token prediction, for every token generated data on the RPC data is being moved back and forth.   What kills you is the latency.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7i2jnh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s a host buffer that needs to be used for every token prediction, for every token generated data on the RPC data is being moved back and forth.   What kills you is the latency.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/n7i2jnh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754606160,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mk5spn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7jeitt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Desperate-Sir-5088",
            "can_mod_post": false,
            "created_utc": 1754623414,
            "send_replies": true,
            "parent_id": "t3_1mk5spn",
            "score": 1,
            "author_fullname": "t2_1dhesoqqtu",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's why NVIDIA's SPARC equipped Mellanox Infiniband 200GB x 2port - It also supports RDMA communication.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7jeitt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s why NVIDIA&amp;#39;s SPARC equipped Mellanox Infiniband 200GB x 2port - It also supports RDMA communication.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk5spn/high_traffic_when_inferencing_in_llamacpps_rpc/n7jeitt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754623414,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk5spn",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]