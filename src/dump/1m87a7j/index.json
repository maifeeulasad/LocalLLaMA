[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi all,   \nI'm considering deploying the Qwen3-Coder-480B-A35B-Instruct model locally I can't afford more than a used workstation with the following specs:\n\n* **2× Intel Xeon Platinum 8176** (So, the total cores = 56 , total threads = 112)\n* **DDR4-2666 ECC RAM**\n* **24 Vram**  (so I think it'll be CPU-only inference)\n\nThis model is a 480B Mixture-of-Experts setup with 35B active parameters per task and supports up to 256K context length (extendable to 1M via YaRN).\n\nI'm specifically looking to understand:\n\n* Expected **tokens per second** for quantized versions: **Q8, Q6, Q4**\n* Whether **any of these quantizations** can achieve from **20** to **30 tokens/sec** on my setup\n* Viability of CPU-only inference for **agentic workflows** or **long-context tasks**\n* Tips for optimizing performance (e.g. quantization strategy, thread tuning, KV cache tweaks)\n\nIf you've run this model or similar setups, I'd love to hear your benchmarks or advice",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What token rate can I expect running Qwen3-Coder-480B-A35B-Instruct on dual Xeon Platinum 8176 CPUs?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m87a7j",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.57,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_p2gnozjh",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753370500,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;br/&gt;\nI&amp;#39;m considering deploying the Qwen3-Coder-480B-A35B-Instruct model locally I can&amp;#39;t afford more than a used workstation with the following specs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;2× Intel Xeon Platinum 8176&lt;/strong&gt; (So, the total cores = 56 , total threads = 112)&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;DDR4-2666 ECC RAM&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;24 Vram&lt;/strong&gt;  (so I think it&amp;#39;ll be CPU-only inference)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This model is a 480B Mixture-of-Experts setup with 35B active parameters per task and supports up to 256K context length (extendable to 1M via YaRN).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m specifically looking to understand:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Expected &lt;strong&gt;tokens per second&lt;/strong&gt; for quantized versions: &lt;strong&gt;Q8, Q6, Q4&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Whether &lt;strong&gt;any of these quantizations&lt;/strong&gt; can achieve from &lt;strong&gt;20&lt;/strong&gt; to &lt;strong&gt;30 tokens/sec&lt;/strong&gt; on my setup&lt;/li&gt;\n&lt;li&gt;Viability of CPU-only inference for &lt;strong&gt;agentic workflows&lt;/strong&gt; or &lt;strong&gt;long-context tasks&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;Tips for optimizing performance (e.g. quantization strategy, thread tuning, KV cache tweaks)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;If you&amp;#39;ve run this model or similar setups, I&amp;#39;d love to hear your benchmarks or advice&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m87a7j",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "WashWarm8360",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/",
            "subreddit_subscribers": 504023,
            "created_utc": 1753370500,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n4xjrrq",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "eloquentemu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n4x8xx1",
                                          "score": 2,
                                          "author_fullname": "t2_lpdsy",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "You are sort of correct, but also doing it wrong.  Precisely because you can't offload a meaningful amount of these large MoE the preferred method of hybrid inference is to offload all non-experts to the GPU rather than layers.  For me at least the result is about ~50% improved token generation, though the prompt processing suffers slightly (~10% worse).",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n4xjrrq",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You are sort of correct, but also doing it wrong.  Precisely because you can&amp;#39;t offload a meaningful amount of these large MoE the preferred method of hybrid inference is to offload all non-experts to the GPU rather than layers.  For me at least the result is about ~50% improved token generation, though the prompt processing suffers slightly (~10% worse).&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m87a7j",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4xjrrq/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753376453,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753376453,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n4x8xx1",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "curios-al",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4x5gul",
                                "score": 1,
                                "author_fullname": "t2_te425vy7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For prompt processing somewhat. For token generation - nope. You have to have all weights readily available even if you will use only part of them and GPU fits less than 5% of weights in q8. Which means its help will be negligible. ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4x8xx1",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For prompt processing somewhat. For token generation - nope. You have to have all weights readily available even if you will use only part of them and GPU fits less than 5% of weights in q8. Which means its help will be negligible. &lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m87a7j",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4x8xx1/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753373494,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753373494,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4x5gul",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "SourceCodeplz",
                      "can_mod_post": false,
                      "created_utc": 1753372540,
                      "send_replies": true,
                      "parent_id": "t1_n4x3x1p",
                      "score": 1,
                      "author_fullname": "t2_608qiyuv",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yep. But the VRAM would help a lot, considering this is a MoE model.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4x5gul",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep. But the VRAM would help a lot, considering this is a MoE model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m87a7j",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4x5gul/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753372540,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4xsw2e",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FullstackSensei",
                      "can_mod_post": false,
                      "created_utc": 1753378873,
                      "send_replies": true,
                      "parent_id": "t1_n4x3x1p",
                      "score": 1,
                      "author_fullname": "t2_17n3nqtj56",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Xeon Scalable 1 (Skylake-SP) has six memory channels. At 2666 that's 128GB/s. Q4 offers very little degradation VS Q8 but runs much faster.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4xsw2e",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Xeon Scalable 1 (Skylake-SP) has six memory channels. At 2666 that&amp;#39;s 128GB/s. Q4 offers very little degradation VS Q8 but runs much faster.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m87a7j",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4xsw2e/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753378873,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4x3x1p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "curios-al",
            "can_mod_post": false,
            "created_utc": 1753372116,
            "send_replies": true,
            "parent_id": "t3_1m87a7j",
            "score": 5,
            "author_fullname": "t2_te425vy7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "According to Google the memory bandwidth of the corresponding Xeon is 102Gb/s. At q8 model requires processing of 35Gb of data per token. So you will get max 3 tk/s at q8 (102 / 35 ~= 3) and max 6 tk/s at q4. \n\n\nIs it extremely hard math to calc estimate yourself?",
            "edited": 1753372383,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4x3x1p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;According to Google the memory bandwidth of the corresponding Xeon is 102Gb/s. At q8 model requires processing of 35Gb of data per token. So you will get max 3 tk/s at q8 (102 / 35 ~= 3) and max 6 tk/s at q4. &lt;/p&gt;\n\n&lt;p&gt;Is it extremely hard math to calc estimate yourself?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4x3x1p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753372116,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m87a7j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n4zyz2i",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "a_beautiful_rhind",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n4zhfb5",
                                                              "score": 1,
                                                              "author_fullname": "t2_h5utwre7",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "From a quick search now, the AVX2 code loads in 256 and the \"fancy\" code loads in 512. There's 512F use in gemm and FA but not the quant CPP files.\n\nI don't know if that means that we are only executing 1 256 instruction per clock instead of 2 or if that is handled by the compiler, depends on the quant, etc.\n\nThis is what it says about vnni:\n\n&gt;It introduces new instructions that merge multiple operations into a single instruction, thereby improving performance by reducing the number of clock cycles required for certain computations.\n\nSounds more relevant for prompt processing than t/g. It's also missing VBMI and some counting instruction. \n\nWithout benchmarks, am at a loss if this stuff is fluff or not, and if it's outweighed by being able to overclock your ram.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n4zyz2i",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;From a quick search now, the AVX2 code loads in 256 and the &amp;quot;fancy&amp;quot; code loads in 512. There&amp;#39;s 512F use in gemm and FA but not the quant CPP files.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know if that means that we are only executing 1 256 instruction per clock instead of 2 or if that is handled by the compiler, depends on the quant, etc.&lt;/p&gt;\n\n&lt;p&gt;This is what it says about vnni:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;It introduces new instructions that merge multiple operations into a single instruction, thereby improving performance by reducing the number of clock cycles required for certain computations.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Sounds more relevant for prompt processing than t/g. It&amp;#39;s also missing VBMI and some counting instruction. &lt;/p&gt;\n\n&lt;p&gt;Without benchmarks, am at a loss if this stuff is fluff or not, and if it&amp;#39;s outweighed by being able to overclock your ram.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1m87a7j",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4zyz2i/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753402137,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753402137,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n4zhfb5",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "FullstackSensei",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n4zbhxu",
                                                    "score": 2,
                                                    "author_fullname": "t2_17n3nqtj56",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "That's kind of my point. AVX-512 on Skylake/Cascade Lake doesn't bring any real world benefits for LLMs. Each core has two AVX2/FMA ports, each 256 bits wide. The load and store ports can handle 512 bits max per clock. So, each core can dispatch two AVX2/FMA instructions per clock, saturating the load/store bandwidth.\n\nBack when Skylake-SP was released, I read a lot of reviews and benchmarks about it and the consensus about AVX-512 was that it basically brought no benefit in memory bound workloads because of the limited load/store bandwidth. There was also some severe clock throttling that was mostly resolved in Cascade Lake, but the core architecture wasn't changed vs Skylake-SP. \n\nThere might be some bandwidth savings if VNNI can reduce the number of instructions needed to perform a given operation, but I doubt that show any benefits in Cascade Lake, since it's just a minor tweak of the Skylake core without any major silicon changes.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n4zhfb5",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s kind of my point. AVX-512 on Skylake/Cascade Lake doesn&amp;#39;t bring any real world benefits for LLMs. Each core has two AVX2/FMA ports, each 256 bits wide. The load and store ports can handle 512 bits max per clock. So, each core can dispatch two AVX2/FMA instructions per clock, saturating the load/store bandwidth.&lt;/p&gt;\n\n&lt;p&gt;Back when Skylake-SP was released, I read a lot of reviews and benchmarks about it and the consensus about AVX-512 was that it basically brought no benefit in memory bound workloads because of the limited load/store bandwidth. There was also some severe clock throttling that was mostly resolved in Cascade Lake, but the core architecture wasn&amp;#39;t changed vs Skylake-SP. &lt;/p&gt;\n\n&lt;p&gt;There might be some bandwidth savings if VNNI can reduce the number of instructions needed to perform a given operation, but I doubt that show any benefits in Cascade Lake, since it&amp;#39;s just a minor tweak of the Skylake core without any major silicon changes.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m87a7j",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4zhfb5/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753396298,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753396298,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n4zbhxu",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "a_beautiful_rhind",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n4z2wyo",
                                          "score": 1,
                                          "author_fullname": "t2_h5utwre7",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "A lot of stuff is sectioned off via \"has_fancy_simd\". What I heard from IK himself, is that most of the ops still take the AVX path if you don't have *all* the instructions.\n\nTo me that implies that generally AVX512 isn't utilized and speed is left on the cutting room floor. Some searching through the code would probably confirm.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n4zbhxu",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A lot of stuff is sectioned off via &amp;quot;has_fancy_simd&amp;quot;. What I heard from IK himself, is that most of the ops still take the AVX path if you don&amp;#39;t have &lt;em&gt;all&lt;/em&gt; the instructions.&lt;/p&gt;\n\n&lt;p&gt;To me that implies that generally AVX512 isn&amp;#39;t utilized and speed is left on the cutting room floor. Some searching through the code would probably confirm.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m87a7j",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4zbhxu/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753394400,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753394400,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n4z2wyo",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "FullstackSensei",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4yfljq",
                                "score": 2,
                                "author_fullname": "t2_17n3nqtj56",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Does VNNI bring any benefit given the memory bandwidth of the platform? We discussed the lack of VNNI before, but Cascade Lake ES still brings improved clock speeds in AVX workloads and faster memory speed compared to Skylake. If the memory controller is saturated using AVX2/512F, what's the benefit of VNNI in llama.cpp?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4z2wyo",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does VNNI bring any benefit given the memory bandwidth of the platform? We discussed the lack of VNNI before, but Cascade Lake ES still brings improved clock speeds in AVX workloads and faster memory speed compared to Skylake. If the memory controller is saturated using AVX2/512F, what&amp;#39;s the benefit of VNNI in llama.cpp?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m87a7j",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4z2wyo/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753391836,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753391836,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4yfljq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "a_beautiful_rhind",
                      "can_mod_post": false,
                      "created_utc": 1753385272,
                      "send_replies": true,
                      "parent_id": "t1_n4xu8tl",
                      "score": 1,
                      "author_fullname": "t2_h5utwre7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That puppy not real cascade lake, unfortunately. llama.cpp demands the VNNI, VBNI instructions for the good stuff.\n\nWould be curious at op's MLC benchmarks to compare if an upgrade is worth it. I get over 200 on allreads between the two.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4yfljq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That puppy not real cascade lake, unfortunately. llama.cpp demands the VNNI, VBNI instructions for the good stuff.&lt;/p&gt;\n\n&lt;p&gt;Would be curious at op&amp;#39;s MLC benchmarks to compare if an upgrade is worth it. I get over 200 on allreads between the two.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m87a7j",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4yfljq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753385272,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4xu8tl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1753379234,
            "send_replies": true,
            "parent_id": "t3_1m87a7j",
            "score": 2,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Do yourself a favor and upgrade your CPU to Cascade Lake ES. You can get 24 cores for under 100. Just search ebay for Intel QQ89. The extra cores in the 8176 don't make a difference anyway. Cascade Lake clocks considerably higher under AVX loads, and most importantly the memory controller gets a bump to 2933, taking your memory bandwidth to 140GB/s. With a smidge of luck, you can overclock your current RAM to run at 2933.\n\nTk/s will depend on what GPU you have for those 24GB, what quantization is acceptable to you (depending on your use case), and how much context you'll need.\n\nI haven't tried 480B, but 235B Q4_K_XL runs at almost 5 tk/s on a single Epyc 7648 with 2666 memory and one 3090 with ~5k context",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4xu8tl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do yourself a favor and upgrade your CPU to Cascade Lake ES. You can get 24 cores for under 100. Just search ebay for Intel QQ89. The extra cores in the 8176 don&amp;#39;t make a difference anyway. Cascade Lake clocks considerably higher under AVX loads, and most importantly the memory controller gets a bump to 2933, taking your memory bandwidth to 140GB/s. With a smidge of luck, you can overclock your current RAM to run at 2933.&lt;/p&gt;\n\n&lt;p&gt;Tk/s will depend on what GPU you have for those 24GB, what quantization is acceptable to you (depending on your use case), and how much context you&amp;#39;ll need.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t tried 480B, but 235B Q4_K_XL runs at almost 5 tk/s on a single Epyc 7648 with 2666 memory and one 3090 with ~5k context&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4xu8tl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753379234,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m87a7j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4xucoz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753379263,
            "send_replies": true,
            "parent_id": "t3_1m87a7j",
            "score": 2,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; Expected tokens per second for quantized versions: Q8, Q6, Q4\n\nHard to predict with the influence of NUMA, which isn't perfectly supported.\n\nThat said, my machine gets ~10.7t/s CPU only.  That's with 500GBps RAM and your system would have 127GBps per socket so you could expect ~2.5t/s.  Numbers I see indicate that NUMA and GPU _individually_ offer about a 50% speed boost, but together I can't say (remember the GPU is only attached to 1 CPU).  So, maaayybee 6t/s?\n\nThat's at Q4.  You can basically divide by 2 to get Q8 and Q6 is in between.\n\nYour bigger problem will probably be prompt processing.  Totally wild guess but I'd say it'll be in the 25t/s range?  Should only take a day or so to process 1M tokens :).  I get 40t/s but that's on 48c with a more modern CPU with a higher power budget.  I doubt hyperthreading will provide any boost since this is AVX-bound and don't use it myself.  Mind that processing get slower with longer context; these numbers are all at context length = 0.\n\n&gt; Whether any of these quantizations can achieve from 20 to 30 tokens/sec on my setup\n\nI saw a dual socket Epyc Turin with 24ch memory and a RTX Pro 6000 Blackwell (so a ~$20k build) benchmark a similar model at ~20t/s at Q4.  So even 20 is pretty out of reach, sorry.  Going below Q4 could help in theory, but would more rapidly degrade functional performance and might not even help speed that much.\n\n&gt; Viability of CPU-only inference for agentic workflows or long-context tasks\n\nIt works but it's slow.  I haven't dipped into it to hard, TBH, so I can't say if it's _too_ slow. But with that system... probably?\n\n&gt; Tips for optimizing performance (e.g. quantization strategy, thread tuning, KV cache tweaks)\n\nBasically none.  I suspect that if you are an exceptional developer than wants to devote some weeks to it, there is definitely room to improve the code _in theory_.  But only like... 50% in tg and 100% in PP.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4xucoz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Expected tokens per second for quantized versions: Q8, Q6, Q4&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Hard to predict with the influence of NUMA, which isn&amp;#39;t perfectly supported.&lt;/p&gt;\n\n&lt;p&gt;That said, my machine gets ~10.7t/s CPU only.  That&amp;#39;s with 500GBps RAM and your system would have 127GBps per socket so you could expect ~2.5t/s.  Numbers I see indicate that NUMA and GPU &lt;em&gt;individually&lt;/em&gt; offer about a 50% speed boost, but together I can&amp;#39;t say (remember the GPU is only attached to 1 CPU).  So, maaayybee 6t/s?&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s at Q4.  You can basically divide by 2 to get Q8 and Q6 is in between.&lt;/p&gt;\n\n&lt;p&gt;Your bigger problem will probably be prompt processing.  Totally wild guess but I&amp;#39;d say it&amp;#39;ll be in the 25t/s range?  Should only take a day or so to process 1M tokens :).  I get 40t/s but that&amp;#39;s on 48c with a more modern CPU with a higher power budget.  I doubt hyperthreading will provide any boost since this is AVX-bound and don&amp;#39;t use it myself.  Mind that processing get slower with longer context; these numbers are all at context length = 0.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Whether any of these quantizations can achieve from 20 to 30 tokens/sec on my setup&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I saw a dual socket Epyc Turin with 24ch memory and a RTX Pro 6000 Blackwell (so a ~$20k build) benchmark a similar model at ~20t/s at Q4.  So even 20 is pretty out of reach, sorry.  Going below Q4 could help in theory, but would more rapidly degrade functional performance and might not even help speed that much.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Viability of CPU-only inference for agentic workflows or long-context tasks&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It works but it&amp;#39;s slow.  I haven&amp;#39;t dipped into it to hard, TBH, so I can&amp;#39;t say if it&amp;#39;s &lt;em&gt;too&lt;/em&gt; slow. But with that system... probably?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Tips for optimizing performance (e.g. quantization strategy, thread tuning, KV cache tweaks)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Basically none.  I suspect that if you are an exceptional developer than wants to devote some weeks to it, there is definitely room to improve the code &lt;em&gt;in theory&lt;/em&gt;.  But only like... 50% in tg and 100% in PP.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4xucoz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753379263,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m87a7j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4yj9w8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1753386313,
            "send_replies": true,
            "parent_id": "t3_1m87a7j",
            "score": 1,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;Whether any of these quantizations can achieve from 20 to 30 tokens/sec on my setup\n\nWith one GPU? No way. You need newer generation xeons with AMX instructions or recent epyc with more channels. Then the GPU carries the prompt processing and the memory does the text gen. \n\nWith 4x3090, I can get decent 10-12tps on this platform if I keep the quants around 250gb and meticulously throw tensors on the GPUs. Pure CPU inference is like 3.5t/s. Granted, I still didn't try https://github.com/ztxz16/fastllm to see if it's better than ik_llama in terms of speeds.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4yj9w8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Whether any of these quantizations can achieve from 20 to 30 tokens/sec on my setup&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;With one GPU? No way. You need newer generation xeons with AMX instructions or recent epyc with more channels. Then the GPU carries the prompt processing and the memory does the text gen. &lt;/p&gt;\n\n&lt;p&gt;With 4x3090, I can get decent 10-12tps on this platform if I keep the quants around 250gb and meticulously throw tensors on the GPUs. Pure CPU inference is like 3.5t/s. Granted, I still didn&amp;#39;t try &lt;a href=\"https://github.com/ztxz16/fastllm\"&gt;https://github.com/ztxz16/fastllm&lt;/a&gt; to see if it&amp;#39;s better than ik_llama in terms of speeds.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4yj9w8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753386313,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m87a7j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4zoymx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SillyLilBear",
            "can_mod_post": false,
            "created_utc": 1753398787,
            "send_replies": true,
            "parent_id": "t3_1m87a7j",
            "score": 1,
            "author_fullname": "t2_wjjtz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "probably 1 token/s if you are lucky",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4zoymx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;probably 1 token/s if you are lucky&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m87a7j/what_token_rate_can_i_expect_running/n4zoymx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753398787,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m87a7j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]