[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.\n\nBack then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.\n\nThis is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.\n\nThere is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.\n\nDeepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it's fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.\n\nWith the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn't been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that's a long while ago...\n\nI also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.\n\nThis does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the *really* large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.\n\nI suppose I'm partially reminiscing, and partially trying to start a dialogue on where the \"sweet spot\" for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.\n\nAre \\~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?\n\n**EDIT:** If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Are ~70B Models Going Out of Fashion?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1majfwi",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.92,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 106,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_cyw8u51dt",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 106,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753617926,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753613850,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.&lt;/p&gt;\n\n&lt;p&gt;Back then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.&lt;/p&gt;\n\n&lt;p&gt;This is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.&lt;/p&gt;\n\n&lt;p&gt;There is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.&lt;/p&gt;\n\n&lt;p&gt;Deepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it&amp;#39;s fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.&lt;/p&gt;\n\n&lt;p&gt;With the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn&amp;#39;t been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that&amp;#39;s a long while ago...&lt;/p&gt;\n\n&lt;p&gt;I also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.&lt;/p&gt;\n\n&lt;p&gt;This does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the &lt;em&gt;really&lt;/em&gt; large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.&lt;/p&gt;\n\n&lt;p&gt;I suppose I&amp;#39;m partially reminiscing, and partially trying to start a dialogue on where the &amp;quot;sweet spot&amp;quot; for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.&lt;/p&gt;\n\n&lt;p&gt;Are ~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1majfwi",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "HvskyAI",
            "discussion_type": null,
            "num_comments": 72,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
            "subreddit_subscribers": 505616,
            "created_utc": 1753613850,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": {
                                                                "kind": "Listing",
                                                                "data": {
                                                                  "after": null,
                                                                  "dist": null,
                                                                  "modhash": "",
                                                                  "geo_filter": "",
                                                                  "children": [
                                                                    {
                                                                      "kind": "t1",
                                                                      "data": {
                                                                        "subreddit_id": "t5_81eyvm",
                                                                        "approved_at_utc": null,
                                                                        "author_is_blocked": false,
                                                                        "comment_type": null,
                                                                        "awarders": [],
                                                                        "mod_reason_by": null,
                                                                        "banned_by": null,
                                                                        "author_flair_type": "text",
                                                                        "total_awards_received": 0,
                                                                        "subreddit": "LocalLLaMA",
                                                                        "author_flair_template_id": null,
                                                                        "distinguished": null,
                                                                        "likes": null,
                                                                        "replies": "",
                                                                        "user_reports": [],
                                                                        "saved": false,
                                                                        "id": "n5iic4d",
                                                                        "banned_at_utc": null,
                                                                        "mod_reason_title": null,
                                                                        "gilded": 0,
                                                                        "archived": false,
                                                                        "collapsed_reason_code": null,
                                                                        "no_follow": true,
                                                                        "author": "Aaronski1974",
                                                                        "can_mod_post": false,
                                                                        "send_replies": true,
                                                                        "parent_id": "t1_n5fetwb",
                                                                        "score": 1,
                                                                        "author_fullname": "t2_145gqt",
                                                                        "approved_by": null,
                                                                        "mod_note": null,
                                                                        "all_awardings": [],
                                                                        "collapsed": false,
                                                                        "body": "I’m running kimi at 4 bits as a code review engine on a ddr4 Xeon box.  It was around 2k$, had 1.5tb of ddr4, 112 cores(dual socket).  I get 2 tokens a second, use 580gb of ram, and it works perfectly.  Users check in their code, triggers a code review, and 10 minutes later they get a notification their code is reviewed and they can look at the review, and chat with it in realtime.   The chat uses qwen3-30b-2ba and gets 15 tok/second.  \n\nSpeed is important for chat, less so for other tasks. There’s no perfect model.",
                                                                        "edited": false,
                                                                        "gildings": {},
                                                                        "author_flair_css_class": null,
                                                                        "name": "t1_n5iic4d",
                                                                        "is_submitter": false,
                                                                        "downs": 0,
                                                                        "author_flair_richtext": [],
                                                                        "author_patreon_flair": false,
                                                                        "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m running kimi at 4 bits as a code review engine on a ddr4 Xeon box.  It was around 2k$, had 1.5tb of ddr4, 112 cores(dual socket).  I get 2 tokens a second, use 580gb of ram, and it works perfectly.  Users check in their code, triggers a code review, and 10 minutes later they get a notification their code is reviewed and they can look at the review, and chat with it in realtime.   The chat uses qwen3-30b-2ba and gets 15 tok/second.  &lt;/p&gt;\n\n&lt;p&gt;Speed is important for chat, less so for other tasks. There’s no perfect model.&lt;/p&gt;\n&lt;/div&gt;",
                                                                        "removal_reason": null,
                                                                        "collapsed_reason": null,
                                                                        "link_id": "t3_1majfwi",
                                                                        "associated_award": null,
                                                                        "stickied": false,
                                                                        "author_premium": false,
                                                                        "can_gild": false,
                                                                        "top_awarded_type": null,
                                                                        "unrepliable_reason": null,
                                                                        "author_flair_text_color": null,
                                                                        "score_hidden": false,
                                                                        "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5iic4d/",
                                                                        "subreddit_type": "public",
                                                                        "locked": false,
                                                                        "report_reasons": null,
                                                                        "created": 1753656528,
                                                                        "author_flair_text": null,
                                                                        "treatment_tags": [],
                                                                        "created_utc": 1753656528,
                                                                        "subreddit_name_prefixed": "r/LocalLLaMA",
                                                                        "controversiality": 0,
                                                                        "depth": 6,
                                                                        "author_flair_background_color": null,
                                                                        "collapsed_because_crowd_control": null,
                                                                        "mod_reports": [],
                                                                        "num_reports": null,
                                                                        "ups": 1
                                                                      }
                                                                    }
                                                                  ],
                                                                  "before": null
                                                                }
                                                              },
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n5fetwb",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "HvskyAI",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n5fbbzc",
                                                              "score": 3,
                                                              "author_fullname": "t2_cyw8u51dt",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "That's true, but in all fairness, these models aren't exactly cheap to train either. Despite all the hype, these companies do eventually need to turn a profit from these extremely costly investments. \n\nI wonder what performance is like for V3/R1 off of an EPYC board with DDR5 RAM... \n\nUltimately, it's the local LLM community that's going to have to adapt to these architectural changes, not the other way around.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n5fetwb",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s true, but in all fairness, these models aren&amp;#39;t exactly cheap to train either. Despite all the hype, these companies do eventually need to turn a profit from these extremely costly investments. &lt;/p&gt;\n\n&lt;p&gt;I wonder what performance is like for V3/R1 off of an EPYC board with DDR5 RAM... &lt;/p&gt;\n\n&lt;p&gt;Ultimately, it&amp;#39;s the local LLM community that&amp;#39;s going to have to adapt to these architectural changes, not the other way around.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1majfwi",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fetwb/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753622039,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753622039,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 3
                                                            }
                                                          },
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n5g9njl",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "eli_pizza",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n5fbbzc",
                                                              "score": 1,
                                                              "author_fullname": "t2_1pdeyk44rl",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Small developers, I dunno. But lots of big money to be made with good models that run on-device with consumer hardware.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n5g9njl",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Small developers, I dunno. But lots of big money to be made with good models that run on-device with consumer hardware.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1majfwi",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5g9njl/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753631921,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753631921,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5fbbzc",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": false,
                                                    "author": "Faintly_glowing_fish",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5f8xxa",
                                                    "score": 7,
                                                    "author_fullname": "t2_97avhniv",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "My sad and cynical theory is that no one really care about small developers anymore.  The only reason they still open source models is to help them sell to big customers that require self host.  They ain’t even gonna train things they won’t sell.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5fbbzc",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My sad and cynical theory is that no one really care about small developers anymore.  The only reason they still open source models is to help them sell to big customers that require self host.  They ain’t even gonna train things they won’t sell.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1majfwi",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fbbzc/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753620749,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753620749,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 7
                                                  }
                                                },
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n5hh9tv",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "TheRealMasonMac",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n5ffxns",
                                                              "score": 2,
                                                              "author_fullname": "t2_101haj",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Developers are probably using LLMs hosted on-site by their company. I don't think a lot of software engineers care *that* much about LLMs right now to buy a server for a local model.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n5hh9tv",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Developers are probably using LLMs hosted on-site by their company. I don&amp;#39;t think a lot of software engineers care &lt;em&gt;that&lt;/em&gt; much about LLMs right now to buy a server for a local model.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1majfwi",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5hh9tv/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753644780,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753644780,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 2
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5ffxns",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": false,
                                                    "author": "maxstader",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5f8xxa",
                                                    "score": 5,
                                                    "author_fullname": "t2_d2nov",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Software engineers use apple silicon a lot. I think the number of people that would be able to run inference on these are more than you think.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5ffxns",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Software engineers use apple silicon a lot. I think the number of people that would be able to run inference on these are more than you think.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1majfwi",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ffxns/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753622437,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753622437,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 5
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5f8xxa",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "HvskyAI",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5f7sv6",
                                          "score": 7,
                                          "author_fullname": "t2_cyw8u51dt",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yeah, I have noticed that in the larger MOE models. I suppose as they scale inference infrastructure, large dense models are just not efficient in terms of the VRAM required. Servers are already chock full of abundant system RAM, and it's a lot cheaper than buying nodes with the newest Nvidia chips. \n\nIt's also a good point that the \"24GB\\~48GB VRAM home LLM enthusiast\" market segment isn't exactly at the top of the priorities list when it comes to the major players developing SOTA models.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5f8xxa",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, I have noticed that in the larger MOE models. I suppose as they scale inference infrastructure, large dense models are just not efficient in terms of the VRAM required. Servers are already chock full of abundant system RAM, and it&amp;#39;s a lot cheaper than buying nodes with the newest Nvidia chips. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also a good point that the &amp;quot;24GB~48GB VRAM home LLM enthusiast&amp;quot; market segment isn&amp;#39;t exactly at the top of the priorities list when it comes to the major players developing SOTA models.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1majfwi",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f8xxa/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753619809,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753619809,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 7
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f7sv6",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Faintly_glowing_fish",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5ezkzk",
                                "score": 13,
                                "author_fullname": "t2_97avhniv",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "They don’t use much more active parameters than ~30B in moe either.  It’s expensive to serve and not useful for big players.  I doubt any mainline commercial models have much more than 32B active parameters.   But for local deploy maybe 70-100G is still the best. Companies are just not gonna spend money for something not useful to them",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f7sv6",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They don’t use much more active parameters than ~30B in moe either.  It’s expensive to serve and not useful for big players.  I doubt any mainline commercial models have much more than 32B active parameters.   But for local deploy maybe 70-100G is still the best. Companies are just not gonna spend money for something not useful to them&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f7sv6/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753619343,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753619343,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 13
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ezkzk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753615647,
                      "send_replies": true,
                      "parent_id": "t1_n5ey860",
                      "score": 24,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Interesting. I suppose there are architectural reasons why the Qwen team went with MOE for anything past 32B. \n\nIf this and Deepseek, Kimi, etc. are anything to go off of, it would appear that the move away from dense models is occurring for good. Perhaps, as mentioned by another comment, the new paradigm for local inference is running a server motherboard with lots of channels for fast system RAM and larger MOE models.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ezkzk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting. I suppose there are architectural reasons why the Qwen team went with MOE for anything past 32B. &lt;/p&gt;\n\n&lt;p&gt;If this and Deepseek, Kimi, etc. are anything to go off of, it would appear that the move away from dense models is occurring for good. Perhaps, as mentioned by another comment, the new paradigm for local inference is running a server motherboard with lots of channels for fast system RAM and larger MOE models.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ezkzk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753615647,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 24
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gdj0z",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "svachalek",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5fy76n",
                                "score": 5,
                                "author_fullname": "t2_6cey3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Humans are not LLMs. Our brains are still far more complex and far more efficient.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gdj0z",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Humans are not LLMs. Our brains are still far more complex and far more efficient.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5gdj0z/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753633066,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753633066,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5fy76n",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": "LOW_SCORE",
                      "no_follow": true,
                      "author": "Emotional-Metal4879",
                      "can_mod_post": false,
                      "created_utc": 1753628462,
                      "send_replies": true,
                      "parent_id": "t1_n5ey860",
                      "score": -7,
                      "author_fullname": "t2_psquw0767",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Wow Does this mean human's logic needs only 30B to describe while human's knowledge needs more than hundreds of B?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5fy76n",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wow Does this mean human&amp;#39;s logic needs only 30B to describe while human&amp;#39;s knowledge needs more than hundreds of B?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": "comment score below threshold",
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fy76n/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753628462,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": true,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -7
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ey860",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "brown2green",
            "can_mod_post": false,
            "created_utc": 1753614968,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 97,
            "author_fullname": "t2_f010l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://x.com/JustinLin610/status/1934809653004939705\n\n&gt;For dense models larger than 30B, it is a bit hard to optimize effectiveness and efficiency (either training or inference). We prefer to use MoE for large models.\n\nQwen researcher Junyang Lin.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ey860",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/JustinLin610/status/1934809653004939705\"&gt;https://x.com/JustinLin610/status/1934809653004939705&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;For dense models larger than 30B, it is a bit hard to optimize effectiveness and efficiency (either training or inference). We prefer to use MoE for large models.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Qwen researcher Junyang Lin.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ey860/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753614968,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 97
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5geoe6",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "BulkyPlay7704",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5ff825",
                                "score": 1,
                                "author_fullname": "t2_1ti9nuwlx8",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Even more so with MOE, those of us with high ram laptops want a 70b model, to fit into 32gb, because the current 30b moe is fast but could be smarter.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5geoe6",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Even more so with MOE, those of us with high ram laptops want a 70b model, to fit into 32gb, because the current 30b moe is fast but could be smarter.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5geoe6/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753633403,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753633403,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ff825",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753622180,
                      "send_replies": true,
                      "parent_id": "t1_n5fa579",
                      "score": 7,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, but it does appear that due to the demands of serving inference at scale, MOE architecture for SOTA models are here to stay. It is much more efficient to serve in comparison to a large dense model. \n\nIt may be time to look at taking those 5090s and putting them on a server motherboard with lots of fast RAM.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ff825",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, but it does appear that due to the demands of serving inference at scale, MOE architecture for SOTA models are here to stay. It is much more efficient to serve in comparison to a large dense model. &lt;/p&gt;\n\n&lt;p&gt;It may be time to look at taking those 5090s and putting them on a server motherboard with lots of fast RAM.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ff825/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753622180,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5fa579",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Herr_Drosselmeyer",
            "can_mod_post": false,
            "created_utc": 1753620291,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 20,
            "author_fullname": "t2_1zr9gwsn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I feel you. I have 64GB available through two 5090s and that's basically perfect for 70b models at Q5 or Q4, so I'd love more of them. I'd especially like Mistral Medium, which I suspect is a 70b to be released, but sadly, they don't and only give us small and large instead.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fa579",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I feel you. I have 64GB available through two 5090s and that&amp;#39;s basically perfect for 70b models at Q5 or Q4, so I&amp;#39;d love more of them. I&amp;#39;d especially like Mistral Medium, which I suspect is a 70b to be released, but sadly, they don&amp;#39;t and only give us small and large instead.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fa579/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753620291,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 20
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fuqld",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ArsNeph",
            "can_mod_post": false,
            "created_utc": 1753627378,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 16,
            "author_fullname": "t2_vt0xkv60d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It's honestly a tragedy that Mistral Medium was never released as Open source, aside from the original Miqu prototype. Based on its benchmarks, it would have been a huge deal for open source, and just in general, I think most people would have preferred it to the open release of Mistral large. In the overall space, the shift in focus to coding is understandable, as it is the number one most common use case according to open router, but it bothers me that most companies don't split their models into a main and coder model, instead overfitting their models on code and neglecting world knowledge and creative writing.\n\nI also think that it's terrible that Hunyuan 80B MoE turned out to be a mediocre model, it could have been a massive win for local. What I've been seeing lately is more and more people just opting to run the large MoEs at a really low quant, and splitting to system RAM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fuqld",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s honestly a tragedy that Mistral Medium was never released as Open source, aside from the original Miqu prototype. Based on its benchmarks, it would have been a huge deal for open source, and just in general, I think most people would have preferred it to the open release of Mistral large. In the overall space, the shift in focus to coding is understandable, as it is the number one most common use case according to open router, but it bothers me that most companies don&amp;#39;t split their models into a main and coder model, instead overfitting their models on code and neglecting world knowledge and creative writing.&lt;/p&gt;\n\n&lt;p&gt;I also think that it&amp;#39;s terrible that Hunyuan 80B MoE turned out to be a mediocre model, it could have been a massive win for local. What I&amp;#39;ve been seeing lately is more and more people just opting to run the large MoEs at a really low quant, and splitting to system RAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fuqld/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753627378,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 16
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5fjtm0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "FullstackSensei",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5fbe5v",
                                "score": 6,
                                "author_fullname": "t2_17n3nqtj56",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "[On a single Epyc 7642 with 8x64GB DDR4-2666 and three 3090s](https://www.reddit.com/r/LocalLLaMA/s/UOhX0xssFB) I get about 2.4tk/s on DS Q4_K_XL, and about the same with K2 at Q2_K_XL. That rig has 3x 3090s but I used only one for offloading and left two for context. This is using ik_llama.cpp. Don't recall PP speed. Whether that is usable will depend on your use case. I use [this simple script](https://www.reddit.com/r/unsloth/s/bKD3Bq6IbF) to batch-brainstorm ideas with those models. So technically I don't even need a GPU to speed up inference for this use case. On that same rig I get close to 5tk/s with Qwen 3 235B at Q4_K_XL.\n\nI just got yesterday the remaining bits to upgrade my [Quad P40 rig](https://www.reddit.com/r/LocalLLaMA/s/7uIBD6AOJk) to eight P40s. Should finish that in the next couple of weeks, depending on time availability. I expect this to get 10tk/s or even more with Qwen3 235B Q4.\n\nThe Mi50 build is at least two months away. Just ordered the cards a couple of days ago. Also ordered an X11DPG-QT with some bent pins. It was 1/3 the price of working ones on ebay. Had success fixing such boards before, so I figured I'd take a chance. Will transplant my two Xeon ES CPUs, RAM and SSDs into that board. It will be home to those five Mi50s. Will be interesting to see how this one compare to the P40s for large MoE models.\n\nMy plan is to basically have multiple large models running at the same time (DS V3/R1 or K2, Q3 235B and Q3 Coder 480B) on those rigs and have an agentic pipeline that let's me use them as a (very junior) software developer to whom I feed detailed descriptions of what I need done and how I want it done and let them churn at it while I'm busy with life. Even if current models can't get there, I'm betting in 6 months or so we'll get models that can do that.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5fjtm0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/UOhX0xssFB\"&gt;On a single Epyc 7642 with 8x64GB DDR4-2666 and three 3090s&lt;/a&gt; I get about 2.4tk/s on DS Q4_K_XL, and about the same with K2 at Q2_K_XL. That rig has 3x 3090s but I used only one for offloading and left two for context. This is using ik_llama.cpp. Don&amp;#39;t recall PP speed. Whether that is usable will depend on your use case. I use &lt;a href=\"https://www.reddit.com/r/unsloth/s/bKD3Bq6IbF\"&gt;this simple script&lt;/a&gt; to batch-brainstorm ideas with those models. So technically I don&amp;#39;t even need a GPU to speed up inference for this use case. On that same rig I get close to 5tk/s with Qwen 3 235B at Q4_K_XL.&lt;/p&gt;\n\n&lt;p&gt;I just got yesterday the remaining bits to upgrade my &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/7uIBD6AOJk\"&gt;Quad P40 rig&lt;/a&gt; to eight P40s. Should finish that in the next couple of weeks, depending on time availability. I expect this to get 10tk/s or even more with Qwen3 235B Q4.&lt;/p&gt;\n\n&lt;p&gt;The Mi50 build is at least two months away. Just ordered the cards a couple of days ago. Also ordered an X11DPG-QT with some bent pins. It was 1/3 the price of working ones on ebay. Had success fixing such boards before, so I figured I&amp;#39;d take a chance. Will transplant my two Xeon ES CPUs, RAM and SSDs into that board. It will be home to those five Mi50s. Will be interesting to see how this one compare to the P40s for large MoE models.&lt;/p&gt;\n\n&lt;p&gt;My plan is to basically have multiple large models running at the same time (DS V3/R1 or K2, Q3 235B and Q3 Coder 480B) on those rigs and have an agentic pipeline that let&amp;#39;s me use them as a (very junior) software developer to whom I feed detailed descriptions of what I need done and how I want it done and let them churn at it while I&amp;#39;m busy with life. Even if current models can&amp;#39;t get there, I&amp;#39;m betting in 6 months or so we&amp;#39;ll get models that can do that.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fjtm0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753623814,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753623814,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5fbe5v",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753620773,
                      "send_replies": true,
                      "parent_id": "t1_n5f53x7",
                      "score": 1,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, I did take a look at that tweet, as well. It would appear that serving inference at scales doesn't lend itself well to large, dense model architectures. If so, it would appear that the general move towards MOE is here for good when it comes to frontier models. \n\nAs I read more and more of these comments, it makes me think that dense models crammed into multi-GPU consumer setups is a poor solution for local inference in the future. As such, it's very interesting to me that you mention running Deepseek V3/R1 off of a server board. \n\nWhat kind of prompt ingestion/token generation speeds were you generally seeing with models of that size, and did offloading some layers to GPU make a large difference? \n\nI'm currently running 2 x 3090 with EXL quants, and I would be interested in hearing how you find combined multi-GPU + multi-channel system RAM performance to be for these extremely large MOE models such as Deepseek or Kimi K2. Looking at Q4 quants of R1, it seems that inference on a 512GB RAM server motherboard would be theoretically possible. I'd just worry if the speeds would be usable, even with a couple of 3090s thrown in... \n\nIf you have any general numbers on performance, I'd be very interested!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5fbe5v",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, I did take a look at that tweet, as well. It would appear that serving inference at scales doesn&amp;#39;t lend itself well to large, dense model architectures. If so, it would appear that the general move towards MOE is here for good when it comes to frontier models. &lt;/p&gt;\n\n&lt;p&gt;As I read more and more of these comments, it makes me think that dense models crammed into multi-GPU consumer setups is a poor solution for local inference in the future. As such, it&amp;#39;s very interesting to me that you mention running Deepseek V3/R1 off of a server board. &lt;/p&gt;\n\n&lt;p&gt;What kind of prompt ingestion/token generation speeds were you generally seeing with models of that size, and did offloading some layers to GPU make a large difference? &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m currently running 2 x 3090 with EXL quants, and I would be interested in hearing how you find combined multi-GPU + multi-channel system RAM performance to be for these extremely large MOE models such as Deepseek or Kimi K2. Looking at Q4 quants of R1, it seems that inference on a 512GB RAM server motherboard would be theoretically possible. I&amp;#39;d just worry if the speeds would be usable, even with a couple of 3090s thrown in... &lt;/p&gt;\n\n&lt;p&gt;If you have any general numbers on performance, I&amp;#39;d be very interested!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fbe5v/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753620773,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5f53x7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1753618210,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 13,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The first thing that came to my mind when reading the title was the tweet by the Qwen researcher that brown2green mentioned.\n\nWe often forget that AI labs aren't thinking of us plebeans when they design the architecture of their models. Their primary concern is how to deploy the trained model at scale and effectively. Larger dense models aren't so friendly to A100/H100 deployments, especially with large contexts and heavy batching.\n\nPersonally, I'm very happy with the move to MoE. Deepseek V3/R1 and Kimi K2 run quite well on a cheap engineering sample 2nd gen Xeon Scalable (Cascade Lake) or Epyc Rome with one or two GPUs. They also run well on older GPUs. I just bought five Mi50s and working on upgrading my P40 rig to 8 GPUs (had them since over a year). Both will run run Qwen 3 235B or similar class MoE models quite happily at Q4 with large contexts. I've scrapped upgrading my triple 3090 rig with a fourth 3090, and will sell that fourth 3090. Funny enough, four Mi50s cost as much as said 3090. The only reason I'm keeping the 3090 rig is for diffusion/image/video models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f53x7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The first thing that came to my mind when reading the title was the tweet by the Qwen researcher that brown2green mentioned.&lt;/p&gt;\n\n&lt;p&gt;We often forget that AI labs aren&amp;#39;t thinking of us plebeans when they design the architecture of their models. Their primary concern is how to deploy the trained model at scale and effectively. Larger dense models aren&amp;#39;t so friendly to A100/H100 deployments, especially with large contexts and heavy batching.&lt;/p&gt;\n\n&lt;p&gt;Personally, I&amp;#39;m very happy with the move to MoE. Deepseek V3/R1 and Kimi K2 run quite well on a cheap engineering sample 2nd gen Xeon Scalable (Cascade Lake) or Epyc Rome with one or two GPUs. They also run well on older GPUs. I just bought five Mi50s and working on upgrading my P40 rig to 8 GPUs (had them since over a year). Both will run run Qwen 3 235B or similar class MoE models quite happily at Q4 with large contexts. I&amp;#39;ve scrapped upgrading my triple 3090 rig with a fourth 3090, and will sell that fourth 3090. Funny enough, four Mi50s cost as much as said 3090. The only reason I&amp;#39;m keeping the 3090 rig is for diffusion/image/video models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f53x7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753618210,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 13
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "richtext",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5f45rl",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "jacek2023",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5f1o0y",
                                                    "score": 2,
                                                    "author_fullname": "t2_vqgbql9w",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "you can see some details and benchmarks of 2x3090+2x3060\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp\\_benchmarks\\_on\\_72gb\\_vram\\_setup\\_2x\\_3090\\_2x/](https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/)\n\nyesterday I was running 235B in Q3 and also got around 10t/s \n\nhowever smaller MoEs are fast\n\npeople on reddit recommend extremely expensive mobos all the time, maybe they are happy with them, I don't really care",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5f45rl",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [
                                                      {
                                                        "e": "text",
                                                        "t": "llama.cpp"
                                                      }
                                                    ],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you can see some details and benchmarks of 2x3090+2x3060&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;yesterday I was running 235B in Q3 and also got around 10t/s &lt;/p&gt;\n\n&lt;p&gt;however smaller MoEs are fast&lt;/p&gt;\n\n&lt;p&gt;people on reddit recommend extremely expensive mobos all the time, maybe they are happy with them, I don&amp;#39;t really care&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1majfwi",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": "light",
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f45rl/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753617805,
                                                    "author_flair_text": "llama.cpp",
                                                    "collapsed": false,
                                                    "created_utc": 1753617805,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": "#bbbdbf",
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5f1o0y",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "HvskyAI",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5ey6tx",
                                          "score": 3,
                                          "author_fullname": "t2_cyw8u51dt",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "48GB (2 x 3090) runs 70B at \\~4.25BPW just fine, assuming EXL2/3 quants are being used with some K/V cache quantization. 32B leaves a *lot* of room, even after accounting for context and an embedding model, etc. \n\nWhat kind of system RAM are you running on that mobo, if you don't mind me asking? Do you find that GPU offloads make a large difference with MOE models that spill over into system RAM? \n\nIf large-ish dense models are on the way out and much larger MOE models will be the norm, server boards with fast system RAM may be the way to go over adding more 3090s, as far as local inference goes...",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5f1o0y",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;48GB (2 x 3090) runs 70B at ~4.25BPW just fine, assuming EXL2/3 quants are being used with some K/V cache quantization. 32B leaves a &lt;em&gt;lot&lt;/em&gt; of room, even after accounting for context and an embedding model, etc. &lt;/p&gt;\n\n&lt;p&gt;What kind of system RAM are you running on that mobo, if you don&amp;#39;t mind me asking? Do you find that GPU offloads make a large difference with MOE models that spill over into system RAM? &lt;/p&gt;\n\n&lt;p&gt;If large-ish dense models are on the way out and much larger MOE models will be the norm, server boards with fast system RAM may be the way to go over adding more 3090s, as far as local inference goes...&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1majfwi",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f1o0y/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753616660,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753616660,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5ihiwo",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "DinoAmino",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5ey6tx",
                                          "score": 2,
                                          "author_fullname": "t2_j1v7f",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "&gt; Yesterday I called it finetune and another person commented that's it's not just finetune... ;)\n\nSo it's a fine-prune then?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5ihiwo",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Yesterday I called it finetune and another person commented that&amp;#39;s it&amp;#39;s not just finetune... ;)&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;So it&amp;#39;s a fine-prune then?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1majfwi",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ihiwo/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753656240,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753656240,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ey6tx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "jacek2023",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5expjj",
                                "score": 11,
                                "author_fullname": "t2_vqgbql9w",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yesterday I called it finetune and another person commented that's it's not just finetune... ;)\n\nTo answer more to your topic - I use 3x3090 currently, my x399 has 4 slots but the only reason to add fourth 3090 are bigger quants of MoE models. \n\nTwo 3090s are enough for 32B dense models. With three you can have more fun with Nemotron 49B and older 70B models.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ey6tx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday I called it finetune and another person commented that&amp;#39;s it&amp;#39;s not just finetune... ;)&lt;/p&gt;\n\n&lt;p&gt;To answer more to your topic - I use 3x3090 currently, my x399 has 4 slots but the only reason to add fourth 3090 are bigger quants of MoE models. &lt;/p&gt;\n\n&lt;p&gt;Two 3090s are enough for 32B dense models. With three you can have more fun with Nemotron 49B and older 70B models.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ey6tx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753614950,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753614950,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 11
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5expjj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753614704,
                      "send_replies": true,
                      "parent_id": "t1_n5ewc8o",
                      "score": 23,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Well, I don't know if I'd call a L3.3-70B model that's received further post-training and pruning a direct successor. It's more so a high-performance finetune. \n\nThe closest thing to a LLaMA 3.3 70B successor is LLaMA 4 Scout, which is both much larger (109B) and an MOE. I suppose that's what I'm getting at with the post - there is no real direct successor in the dense 70B-parameter range. \n\nThat being said, Nemotron 49B does look interesting, and I've heard good things about Nvidia's tuned models in the past. Thanks for the recommendation.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5expjj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, I don&amp;#39;t know if I&amp;#39;d call a L3.3-70B model that&amp;#39;s received further post-training and pruning a direct successor. It&amp;#39;s more so a high-performance finetune. &lt;/p&gt;\n\n&lt;p&gt;The closest thing to a LLaMA 3.3 70B successor is LLaMA 4 Scout, which is both much larger (109B) and an MOE. I suppose that&amp;#39;s what I&amp;#39;m getting at with the post - there is no real direct successor in the dense 70B-parameter range. &lt;/p&gt;\n\n&lt;p&gt;That being said, Nemotron 49B does look interesting, and I&amp;#39;ve heard good things about Nvidia&amp;#39;s tuned models in the past. Thanks for the recommendation.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5expjj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753614704,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 23
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ewc8o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1753613994,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 28,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nemotron 49B was released yesterday, it's a successor of LLaMA 3.3 70B",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ewc8o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nemotron 49B was released yesterday, it&amp;#39;s a successor of LLaMA 3.3 70B&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ewc8o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753613994,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 28
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fmjvl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1753624734,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 10,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MoE is better for providers who want to serve more requests and have vram but only so much compute. MoE is worse for us because we get less active parameters for the vram. MoE models store a lot of knowledge but not that great in terms of raw intelligence.\n\nThey have cheaper training vs the 70b and we have inability to finetune. Safety gets to stay in. Another provider win.\n\nPeople talk of maxing out ram, but performance is only really good with ik_llama and more expensive platforms. Plus you get dinged on the prompt processing. Reasoning models compound the issue having to generate a lot more tokens.\n\nLots here cheer this change, likely because small models got better and they get more of them. They didn't have hardware for those models and they don't have it now. \n\nYour 48gb isn't in a \"weird\" spot. If anything, you need two extra. In hybrid inference, the more you place on the GPU, the faster it will go. Unless you're trapped in small model purgatory because of the host system. Also, none of those old models stopped working. Mistral large BTFO these hunyuan, dots, and all that other bullshit. From this entire crop, at best we got 235b, deepseek and kimi for general use. Even if you stuck with the 30b, you can now run them at 8bit with lots of context.\n\nBesides MoE, there's also the massive stem and benchmark focus in new models. They sacrifice general knowledge and common sense to chase worthless points. Amusingly when RAG is offered as a solution, it always ignores the poor long context performance and the pain of having to reprocess on partially offloaded weights. \n\nDunno where it all ends up going, but having tried a bunch of the newer models, they seem like dumb obese parrots and a bit of a waste. But hey, it's their money.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fmjvl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MoE is better for providers who want to serve more requests and have vram but only so much compute. MoE is worse for us because we get less active parameters for the vram. MoE models store a lot of knowledge but not that great in terms of raw intelligence.&lt;/p&gt;\n\n&lt;p&gt;They have cheaper training vs the 70b and we have inability to finetune. Safety gets to stay in. Another provider win.&lt;/p&gt;\n\n&lt;p&gt;People talk of maxing out ram, but performance is only really good with ik_llama and more expensive platforms. Plus you get dinged on the prompt processing. Reasoning models compound the issue having to generate a lot more tokens.&lt;/p&gt;\n\n&lt;p&gt;Lots here cheer this change, likely because small models got better and they get more of them. They didn&amp;#39;t have hardware for those models and they don&amp;#39;t have it now. &lt;/p&gt;\n\n&lt;p&gt;Your 48gb isn&amp;#39;t in a &amp;quot;weird&amp;quot; spot. If anything, you need two extra. In hybrid inference, the more you place on the GPU, the faster it will go. Unless you&amp;#39;re trapped in small model purgatory because of the host system. Also, none of those old models stopped working. Mistral large BTFO these hunyuan, dots, and all that other bullshit. From this entire crop, at best we got 235b, deepseek and kimi for general use. Even if you stuck with the 30b, you can now run them at 8bit with lots of context.&lt;/p&gt;\n\n&lt;p&gt;Besides MoE, there&amp;#39;s also the massive stem and benchmark focus in new models. They sacrifice general knowledge and common sense to chase worthless points. Amusingly when RAG is offered as a solution, it always ignores the poor long context performance and the pain of having to reprocess on partially offloaded weights. &lt;/p&gt;\n\n&lt;p&gt;Dunno where it all ends up going, but having tried a bunch of the newer models, they seem like dumb obese parrots and a bit of a waste. But hey, it&amp;#39;s their money.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fmjvl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753624734,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f0obb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753616183,
                      "send_replies": true,
                      "parent_id": "t1_n5eyupr",
                      "score": 6,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah, I suppose we're spoiled for choice nowadays compared to a couple of years ago. I remember when 4096 context on LLaMA 2 was hailed as a revolutionary advance around here. And you know, at the time, it really was a big deal. We've come a long way. \n\n  \nAssuming that a rig is using a server motherboard with a sufficient number of memory channels, what kinds of prompt ingestion and token generation speeds are we realistically seeing with fairly fast RAM and very large MOE models? \n\nFor example, I see that DeekSeek-R1 at IQ4-KS comes in at around 368 GiB (so around \\~406GB, give or take). Can this realistically run at usable speeds on a 512GB DDR5 RAM system?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f0obb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, I suppose we&amp;#39;re spoiled for choice nowadays compared to a couple of years ago. I remember when 4096 context on LLaMA 2 was hailed as a revolutionary advance around here. And you know, at the time, it really was a big deal. We&amp;#39;ve come a long way. &lt;/p&gt;\n\n&lt;p&gt;Assuming that a rig is using a server motherboard with a sufficient number of memory channels, what kinds of prompt ingestion and token generation speeds are we realistically seeing with fairly fast RAM and very large MOE models? &lt;/p&gt;\n\n&lt;p&gt;For example, I see that DeekSeek-R1 at IQ4-KS comes in at around 368 GiB (so around ~406GB, give or take). Can this realistically run at usable speeds on a 512GB DDR5 RAM system?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f0obb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753616183,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5eyupr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Klutzy-Snow8016",
            "can_mod_post": false,
            "created_utc": 1753615282,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 11,
            "author_fullname": "t2_1d5l610jz3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Pretty much, yeah. The new meta is getting a bunch of regular RAM and running the big MOEs anyway, and (if you have a consumer-grade motherboard) learning to be patient while you wait for the generation.\n\nHonestly, it's not too bad. Remember when GPT-4 through ChatGPT would slow down to like 1 or 2 tokens per second when demand was high? And the version of GPT-4 with 32k context was a big deal and API keys for it were coveted? Now you can have basically that at home, but better and with more control.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5eyupr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much, yeah. The new meta is getting a bunch of regular RAM and running the big MOEs anyway, and (if you have a consumer-grade motherboard) learning to be patient while you wait for the generation.&lt;/p&gt;\n\n&lt;p&gt;Honestly, it&amp;#39;s not too bad. Remember when GPT-4 through ChatGPT would slow down to like 1 or 2 tokens per second when demand was high? And the version of GPT-4 with 32k context was a big deal and API keys for it were coveted? Now you can have basically that at home, but better and with more control.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eyupr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753615282,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f7pbi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "vegatx40",
                      "can_mod_post": false,
                      "created_utc": 1753619302,
                      "send_replies": true,
                      "parent_id": "t1_n5f0v9b",
                      "score": 1,
                      "author_fullname": "t2_18dhiarv40",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Makes sense",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f7pbi",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Makes sense&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f7pbi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753619302,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5f0v9b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "anzzax",
            "can_mod_post": false,
            "created_utc": 1753616277,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 9,
            "author_fullname": "t2_zloia",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think this is a good sign, the focus of open models is shifting from home enthusiasts to small and medium-sized businesses, which means adoption is happening. The 72B dense model is a really odd size: it’s not affordable for most home nerds with a single 24GB GPU, and it’s not efficient for businesses with real workloads and batched inference needs. In my opinion, there’s no practical value in dense models larger than 32B. What’s happening now isn’t just a reasonable response to demand - it’s grounded in actual, justified use cases.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f0v9b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think this is a good sign, the focus of open models is shifting from home enthusiasts to small and medium-sized businesses, which means adoption is happening. The 72B dense model is a really odd size: it’s not affordable for most home nerds with a single 24GB GPU, and it’s not efficient for businesses with real workloads and batched inference needs. In my opinion, there’s no practical value in dense models larger than 32B. What’s happening now isn’t just a reasonable response to demand - it’s grounded in actual, justified use cases.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f0v9b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753616277,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5eycjs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Only-Letterhead-3411",
            "can_mod_post": false,
            "created_utc": 1753615031,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 6,
            "author_fullname": "t2_pbfqmgf8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There is no way we can know if model makers will stick to small models from now on or we'll get 70b sized models again. If you ask me, the chance of them sticking to 20-30B range is higher than 70B range. A QWQ 72B would be great. But not many people owns hardware to run it. And Qwen3 creators sounded like they aren't really interested in using their resources for that size models. I mean, qwen3 30B MoE can even run on a potato and it is smarter than L3.3 70B. I'm totally fine with the plan of them doing very big models and then distilling them into super smart small models",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5eycjs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is no way we can know if model makers will stick to small models from now on or we&amp;#39;ll get 70b sized models again. If you ask me, the chance of them sticking to 20-30B range is higher than 70B range. A QWQ 72B would be great. But not many people owns hardware to run it. And Qwen3 creators sounded like they aren&amp;#39;t really interested in using their resources for that size models. I mean, qwen3 30B MoE can even run on a potato and it is smarter than L3.3 70B. I&amp;#39;m totally fine with the plan of them doing very big models and then distilling them into super smart small models&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eycjs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753615031,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5g611u",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "MR_-_501",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5f2uui",
                                "score": 1,
                                "author_fullname": "t2_14fb6edg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "If you want to use a large batch size through something like VLLM the extra VRAM also helps out",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5g611u",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you want to use a large batch size through something like VLLM the extra VRAM also helps out&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5g611u/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753630853,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753630853,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5fawh2",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "NNN_Throwaway2",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5f77bo",
                                                    "score": 2,
                                                    "author_fullname": "t2_8rrihts9",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "If I recall, the first I heard of it was from unsloth and how they benchmark their dynamic quants.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5fawh2",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If I recall, the first I heard of it was from unsloth and how they benchmark their dynamic quants.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1majfwi",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fawh2/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753620586,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753620586,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5f77bo",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "HvskyAI",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5f4ynx",
                                          "score": 1,
                                          "author_fullname": "t2_cyw8u51dt",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "That's an interesting point you raise. My views on quantization and perplexity increases may be dated, as I'm basing this all off of information that is quite old (by LLM standards). \n\nI wasn't aware that perplexity was found to be an ineffective measure of the detrimental effects of quantization. Would you happen to have a source on that, so I could take a look? I'd be very interested if you could point me in the general direction. \n\nIt's also true that long-context performance varies with different models. I'm going off of the RULER benchmark, which I now see is quite old. Perhaps more recent models are doing better on this front, and are not displaying the tendencies of past models where retrieval accuracy would drop off sharply at some small fraction of the stated maximum context window. \n\nAs for K/V cache quantization, I am on TabbyAPI as a back end, so the cache quantization is weighted, and not just naive truncation. I would think that this makes some tangible difference, but for tasks where high precision is necessary, I'd agree that higher quants (in both weights and cache) are indeed preferable.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5f77bo",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s an interesting point you raise. My views on quantization and perplexity increases may be dated, as I&amp;#39;m basing this all off of information that is quite old (by LLM standards). &lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t aware that perplexity was found to be an ineffective measure of the detrimental effects of quantization. Would you happen to have a source on that, so I could take a look? I&amp;#39;d be very interested if you could point me in the general direction. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also true that long-context performance varies with different models. I&amp;#39;m going off of the RULER benchmark, which I now see is quite old. Perhaps more recent models are doing better on this front, and are not displaying the tendencies of past models where retrieval accuracy would drop off sharply at some small fraction of the stated maximum context window. &lt;/p&gt;\n\n&lt;p&gt;As for K/V cache quantization, I am on TabbyAPI as a back end, so the cache quantization is weighted, and not just naive truncation. I would think that this makes some tangible difference, but for tasks where high precision is necessary, I&amp;#39;d agree that higher quants (in both weights and cache) are indeed preferable.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1majfwi",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f77bo/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753619095,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753619095,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f4ynx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "NNN_Throwaway2",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5f2uui",
                                "score": 1,
                                "author_fullname": "t2_8rrihts9",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For something like qwq with good long context performance, you can absolutely make use of the VRAM. 48GB also opens up the possibility to load multiple models at once, which can be useful for agentic tasks, for example. I also think you're understating the impact of quantization (both of weights and kv cache). This has typically been estimated by perplexity, which has been found more recently to be an imperfect way of quantifying this impact.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f4ynx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For something like qwq with good long context performance, you can absolutely make use of the VRAM. 48GB also opens up the possibility to load multiple models at once, which can be useful for agentic tasks, for example. I also think you&amp;#39;re understating the impact of quantization (both of weights and kv cache). This has typically been estimated by perplexity, which has been found more recently to be an imperfect way of quantifying this impact.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f4ynx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753618148,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753618148,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f2uui",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753617216,
                      "send_replies": true,
                      "parent_id": "t1_n5f1074",
                      "score": 5,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, that would be one way to go about things. However, because perplexity does not increase linearly with lower quants, there's only so much marginal increase in precision to be gained from running larger quantizations. The same could be said for K/V cache. \n\nOf course, going from a tiny (\\~2.5BPW) quant to 4BPW or 4.25BPW will be a massive improvement. But going from 4.25BPW to 5BPW? 6BPW? From what I've seen (and I'm happy to be proven wrong here), the returns are increasingly diminishing, and results are nearly indistinguishable from full precision at around \\~8BPW. \n\nUnless the use case is something like mathematics or coding, I'm doubtful that there would be a good value proposition for the VRAM overhead.\n\nMore context is hardly useful in practice, as well, since most models degrade in performance quite acutely long before their stated maximum context window. \n\nAll else being equal, being able to run a larger parameter-count model at a slightly lower (=&gt;4BPW) quant is preferable for me, as opposed to running, say, 32B at 8BPW with no K/V cache quantization. \n\nThis is all use case-dependent, of course.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f2uui",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, that would be one way to go about things. However, because perplexity does not increase linearly with lower quants, there&amp;#39;s only so much marginal increase in precision to be gained from running larger quantizations. The same could be said for K/V cache. &lt;/p&gt;\n\n&lt;p&gt;Of course, going from a tiny (~2.5BPW) quant to 4BPW or 4.25BPW will be a massive improvement. But going from 4.25BPW to 5BPW? 6BPW? From what I&amp;#39;ve seen (and I&amp;#39;m happy to be proven wrong here), the returns are increasingly diminishing, and results are nearly indistinguishable from full precision at around ~8BPW. &lt;/p&gt;\n\n&lt;p&gt;Unless the use case is something like mathematics or coding, I&amp;#39;m doubtful that there would be a good value proposition for the VRAM overhead.&lt;/p&gt;\n\n&lt;p&gt;More context is hardly useful in practice, as well, since most models degrade in performance quite acutely long before their stated maximum context window. &lt;/p&gt;\n\n&lt;p&gt;All else being equal, being able to run a larger parameter-count model at a slightly lower (=&amp;gt;4BPW) quant is preferable for me, as opposed to running, say, 32B at 8BPW with no K/V cache quantization. &lt;/p&gt;\n\n&lt;p&gt;This is all use case-dependent, of course.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f2uui/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753617216,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5f1074",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "NNN_Throwaway2",
            "can_mod_post": false,
            "created_utc": 1753616342,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 14,
            "author_fullname": "t2_8rrihts9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Huh??? How is 48GB VRAM \"too large\"? Just run more context and a higher quant. Its well established at this point that running kv quantization and smaller quants affects output and can degrade quality.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f1074",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Huh??? How is 48GB VRAM &amp;quot;too large&amp;quot;? Just run more context and a higher quant. Its well established at this point that running kv quantization and smaller quants affects output and can degrade quality.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f1074/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753616342,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 14
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f7177",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "steezy13312",
            "can_mod_post": false,
            "created_utc": 1753619025,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 4,
            "author_fullname": "t2_rfjj2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We're definitely in MoE summer",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f7177",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re definitely in MoE summer&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f7177/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753619025,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f7myd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Former-Ad-5757",
            "can_mod_post": false,
            "created_utc": 1753619275,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 4,
            "author_fullname": "t2_ihsdiwk6k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes. The training was much longer and more expensive ( it contains simply more combinations) while it didn’t really add much more intelligence.\n\nMore context / reasoning / moe works much better than simply releasing 70b and it is cheaper to train and cheaper to run.\n\nThe need for 48 vram is still there, it is just too fill more context.\nI see you said somewhere else that context is unreliable. But that is just on finding the needle tests etc. It sounds stupid, but just by repeating some things over and over ( aka reasoning / cot ) you work around those problems. The model will understand your original intent ok, because it has been expressed in 20 ways. But it will also pick up extra content ( not all mind you but some, still better than none ).\n\nAlso what some people don’t seem to understand is that an llm is a reasoning machine not a factual machine, it works best if you bring your own facts into the context.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f7myd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes. The training was much longer and more expensive ( it contains simply more combinations) while it didn’t really add much more intelligence.&lt;/p&gt;\n\n&lt;p&gt;More context / reasoning / moe works much better than simply releasing 70b and it is cheaper to train and cheaper to run.&lt;/p&gt;\n\n&lt;p&gt;The need for 48 vram is still there, it is just too fill more context.\nI see you said somewhere else that context is unreliable. But that is just on finding the needle tests etc. It sounds stupid, but just by repeating some things over and over ( aka reasoning / cot ) you work around those problems. The model will understand your original intent ok, because it has been expressed in 20 ways. But it will also pick up extra content ( not all mind you but some, still better than none ).&lt;/p&gt;\n\n&lt;p&gt;Also what some people don’t seem to understand is that an llm is a reasoning machine not a factual machine, it works best if you bring your own facts into the context.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f7myd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753619275,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f595a",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "No_Afternoon_4260",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5eymmf",
                                "score": 5,
                                "author_fullname": "t2_cj9kap4bx",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "And how do you run a 35B if you don't have at least 48gb of vram\n\n&gt;The whole point of 48GB was to run 70B at a 4-bit quant\n\nBecause 48gb vram allow 70B with very small ctx size. Just to give you an idea you can run mistral small q8 at 50k ctx on 48gb vram, it's not that much",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f595a",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And how do you run a 35B if you don&amp;#39;t have at least 48gb of vram&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The whole point of 48GB was to run 70B at a 4-bit quant&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Because 48gb vram allow 70B with very small ctx size. Just to give you an idea you can run mistral small q8 at 50k ctx on 48gb vram, it&amp;#39;s not that much&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f595a/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753618271,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753618271,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5eymmf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753615171,
                      "send_replies": true,
                      "parent_id": "t1_n5exki3",
                      "score": 7,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Interesting discussion! I went ahead and gave it a read. \n\nIt's intriguing to see people still recommending 48GB while acknowledging that there have been no significant \\~70B releases in quite some time. The whole point of 48GB was to run 70B at a 4-bit quant as to have an acceptable perplexity increase from quantization. \n\nI suppose if multi-hundred billion-parameter MOE models are going to be the norm moving forward, the conversation may move towards building inference rigs with server motherboards and fast, multi-channel system RAM rather than continuing to focus on increases in VRAM. Thanks for the input!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5eymmf",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting discussion! I went ahead and gave it a read. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s intriguing to see people still recommending 48GB while acknowledging that there have been no significant ~70B releases in quite some time. The whole point of 48GB was to run 70B at a 4-bit quant as to have an acceptable perplexity increase from quantization. &lt;/p&gt;\n\n&lt;p&gt;I suppose if multi-hundred billion-parameter MOE models are going to be the norm moving forward, the conversation may move towards building inference rigs with server motherboards and fast, multi-channel system RAM rather than continuing to focus on increases in VRAM. Thanks for the input!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eymmf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753615171,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5fjn6h",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "createthiscom",
                      "can_mod_post": false,
                      "created_utc": 1753623752,
                      "send_replies": true,
                      "parent_id": "t1_n5exki3",
                      "score": 1,
                      "author_fullname": "t2_ozxxf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I’m using every bit of the 96gb on my blackwell 6000 pro. More is always better because you can always offload more layers to the GPU.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5fjn6h",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m using every bit of the 96gb on my blackwell 6000 pro. More is always better because you can always offload more layers to the GPU.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fjn6h/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753623752,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5exki3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "fgoricha",
            "can_mod_post": false,
            "created_utc": 1753614631,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 6,
            "author_fullname": "t2_40xsg56g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I asked a similar question yesterday about vram sweet spot. Most people seemed to think 48gb vram is still relevant, but more vram is better. I think MOE and small dense models will be the trend going forward",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5exki3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I asked a similar question yesterday about vram sweet spot. Most people seemed to think 48gb vram is still relevant, but more vram is better. I think MOE and small dense models will be the trend going forward&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5exki3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753614631,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f4rj0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "c3real2k",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5f3vgv",
                                "score": 3,
                                "author_fullname": "t2_h7qvk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Hm, yes, Command-A was alright if I remember correctly. Might have to give it a spin again.\n\nI can't say all that much about \"serious\" M4 setups, since I'm running the base M4s (16GB + 24GB), the worst possible configuration for inference. Prompt processing is slow, as well as token generation. Ironically, the only models bearable (for me) on those are small MoE's like Qwen3 30B A3B :D",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f4rj0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hm, yes, Command-A was alright if I remember correctly. Might have to give it a spin again.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t say all that much about &amp;quot;serious&amp;quot; M4 setups, since I&amp;#39;m running the base M4s (16GB + 24GB), the worst possible configuration for inference. Prompt processing is slow, as well as token generation. Ironically, the only models bearable (for me) on those are small MoE&amp;#39;s like Qwen3 30B A3B :D&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f4rj0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753618065,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753618065,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f3vgv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753617677,
                      "send_replies": true,
                      "parent_id": "t1_n5f12d5",
                      "score": 2,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I suppose of the recent releases, there's Cohere's Command-A. But that's been about it, from what I can tell. Most new releases appear to be smaller dense models or *much* larger MOE models. \n\nHow are you finding the M4 to be for inference? I've heard that prompt ingestion can be rather slow, but I've never run inference on Apple silicon, myself.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f3vgv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I suppose of the recent releases, there&amp;#39;s Cohere&amp;#39;s Command-A. But that&amp;#39;s been about it, from what I can tell. Most new releases appear to be smaller dense models or &lt;em&gt;much&lt;/em&gt; larger MOE models. &lt;/p&gt;\n\n&lt;p&gt;How are you finding the M4 to be for inference? I&amp;#39;ve heard that prompt ingestion can be rather slow, but I&amp;#39;ve never run inference on Apple silicon, myself.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f3vgv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753617677,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5f12d5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "c3real2k",
            "can_mod_post": false,
            "created_utc": 1753616371,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 7,
            "author_fullname": "t2_h7qvk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I **yearn** for something modern and dense in the 70-130B range. Those smaller models (24-30B) might be highly optimized for specific tasks, but honestly, suck for creative writing (I might be exaggerating here a bit).\n\nNow I'm running a franken-rig of my GPU server and two MacMinis to somehow squeeze the lobotomized 90GB of Qwen3 235B@IQ3 XS into reasonably fast RAM to get what is essentially a 72B dense equivalent (which would fit nicely with a much less aggressive quantization into the 80GB VRAM my GPU server hosts, or at a reasonable 4bit quant for users with 48GB).\n\nSo, I have a gigantic 235B MoE of what would be a 72B dense model running, not gaining anything from the potential speed gains ('cause base M4's memory speed, prompt processing, ... is slow AF) and (while writing is nice) now having problems with code generation because of the low quant. Meaning I have to switch models every now and then.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f12d5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I &lt;strong&gt;yearn&lt;/strong&gt; for something modern and dense in the 70-130B range. Those smaller models (24-30B) might be highly optimized for specific tasks, but honestly, suck for creative writing (I might be exaggerating here a bit).&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m running a franken-rig of my GPU server and two MacMinis to somehow squeeze the lobotomized 90GB of Qwen3 235B@IQ3 XS into reasonably fast RAM to get what is essentially a 72B dense equivalent (which would fit nicely with a much less aggressive quantization into the 80GB VRAM my GPU server hosts, or at a reasonable 4bit quant for users with 48GB).&lt;/p&gt;\n\n&lt;p&gt;So, I have a gigantic 235B MoE of what would be a 72B dense model running, not gaining anything from the potential speed gains (&amp;#39;cause base M4&amp;#39;s memory speed, prompt processing, ... is slow AF) and (while writing is nice) now having problems with code generation because of the low quant. Meaning I have to switch models every now and then.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f12d5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753616371,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fa5la",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "PassengerPigeon343",
            "can_mod_post": false,
            "created_utc": 1753620295,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 2,
            "author_fullname": "t2_v42t9zy1t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I originally got 48gb for running local Llama 3.3 70b, but after Gemma 3 27b came out it became my local default using a Q8_0 quant at 32k context. I get consistently great results and find the extra context extremely valuable. I could run a smaller quant but it is fast and gives me enough headroom for now so I haven’t played with it further.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fa5la",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I originally got 48gb for running local Llama 3.3 70b, but after Gemma 3 27b came out it became my local default using a Q8_0 quant at 32k context. I get consistently great results and find the extra context extremely valuable. I could run a smaller quant but it is fast and gives me enough headroom for now so I haven’t played with it further.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fa5la/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753620295,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f7x5t",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "HvskyAI",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5f1siq",
                                "score": 1,
                                "author_fullname": "t2_cyw8u51dt",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I agree - there are emergent properties with larger models that aren't necessarily reflected in benchmarks. \n\nAlso, if a given model at n parameters is superior to an older model with a larger number of parameters, then that increase in performance would presumably carry over into a larger-parameter version of the newer model, with 2n parameters, for example, and so on and so forth. \n\nSo while smaller modern models may outperform larger and older models, the question then becomes one of why not have larger *and* newer models, all else being equal.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f7x5t",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I agree - there are emergent properties with larger models that aren&amp;#39;t necessarily reflected in benchmarks. &lt;/p&gt;\n\n&lt;p&gt;Also, if a given model at n parameters is superior to an older model with a larger number of parameters, then that increase in performance would presumably carry over into a larger-parameter version of the newer model, with 2n parameters, for example, and so on and so forth. &lt;/p&gt;\n\n&lt;p&gt;So while smaller modern models may outperform larger and older models, the question then becomes one of why not have larger &lt;em&gt;and&lt;/em&gt; newer models, all else being equal.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f7x5t/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753619393,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753619393,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f1siq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AppearanceHeavy6724",
                      "can_mod_post": false,
                      "created_utc": 1753616719,
                      "send_replies": true,
                      "parent_id": "t1_n5eyn0q",
                      "score": 10,
                      "author_fullname": "t2_uz37qfx5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is not this simple, bigger models still are better in ways not captured by benchmark; for example they, within same family of models,  almost always better at long context; they almost always better at world knowledge too.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f1siq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is not this simple, bigger models still are better in ways not captured by benchmark; for example they, within same family of models,  almost always better at long context; they almost always better at world knowledge too.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f1siq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753616719,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 10
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f90qu",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "custodiam99",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5f7kcr",
                                "score": 1,
                                "author_fullname": "t2_nqnhgqqf5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I tried it. For my use case it is not that good or not really better.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f90qu",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I tried it. For my use case it is not that good or not really better.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f90qu/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753619841,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753619841,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f7kcr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ParaboloidalCrest",
                      "can_mod_post": false,
                      "created_utc": 1753619246,
                      "send_replies": true,
                      "parent_id": "t1_n5eyn0q",
                      "score": 3,
                      "author_fullname": "t2_nc2u4f7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "You haven't tried Nemotron yet (even the non-thinking one). It's been better than any 32b model for me.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f7kcr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You haven&amp;#39;t tried Nemotron yet (even the non-thinking one). It&amp;#39;s been better than any 32b model for me.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f7kcr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753619246,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5eyn0q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "custodiam99",
            "can_mod_post": false,
            "created_utc": 1753615176,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 2,
            "author_fullname": "t2_nqnhgqqf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen3 14b is better than any 70b model I tried, so I think structured training data and MOE is the future with smaller models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5eyn0q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 14b is better than any 70b model I tried, so I think structured training data and MOE is the future with smaller models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eyn0q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753615176,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f39lf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Eden1506",
            "can_mod_post": false,
            "created_utc": 1753617405,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_2ezqqypt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It solely depends on what the companies/institutions release.\n\nCreating a model from scratch costs millions and isn't something the community can causally do or decide by itself. \n\nIn addition MOE models are far more cost efficient for companies to run making it natural for them to change focus from monolithic models to moe models.\n\nBuying 128 gb of ddr5 ram costs around the same as a single 16gb GPU and allows one to run qwen3 235b at 3-4 tokens/s",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f39lf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It solely depends on what the companies/institutions release.&lt;/p&gt;\n\n&lt;p&gt;Creating a model from scratch costs millions and isn&amp;#39;t something the community can causally do or decide by itself. &lt;/p&gt;\n\n&lt;p&gt;In addition MOE models are far more cost efficient for companies to run making it natural for them to change focus from monolithic models to moe models.&lt;/p&gt;\n\n&lt;p&gt;Buying 128 gb of ddr5 ram costs around the same as a single 16gb GPU and allows one to run qwen3 235b at 3-4 tokens/s&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f39lf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753617405,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f5xc5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Zealousideal_Nail288",
            "can_mod_post": false,
            "created_utc": 1753618558,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_3lwcs91h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For most part its bigger and less Quantisized better \n\n\nMistral 7b the first that really did blow me away \nIs decent imo \nBut can it write good stories, no \nCan it do programming,no \nCan it do other languages, no \n\n\nMeanwhile the 70b deepseek destillation claims to be able to program, speak all languages in the World, and is ok in Writing Stories (but very monotone)\n\n\n i have seen a decent ammount of comments saying they dont touch anything below 120b for writing \nSo if people have been using 200b+ models they wont go back to 70b \n\n\nAnd for groups and cooperations its much easier to aim for big models because they already start off better than small models ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f5xc5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For most part its bigger and less Quantisized better &lt;/p&gt;\n\n&lt;p&gt;Mistral 7b the first that really did blow me away \nIs decent imo \nBut can it write good stories, no \nCan it do programming,no \nCan it do other languages, no &lt;/p&gt;\n\n&lt;p&gt;Meanwhile the 70b deepseek destillation claims to be able to program, speak all languages in the World, and is ok in Writing Stories (but very monotone)&lt;/p&gt;\n\n&lt;p&gt; i have seen a decent ammount of comments saying they dont touch anything below 120b for writing \nSo if people have been using 200b+ models they wont go back to 70b &lt;/p&gt;\n\n&lt;p&gt;And for groups and cooperations its much easier to aim for big models because they already start off better than small models &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f5xc5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753618558,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fboib",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "asobalife",
            "can_mod_post": false,
            "created_utc": 1753620882,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_1im6ui0j",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you sure about the ratio?\n\nI admit to likely doing something wrong, but with an A10 (24 gb vram) I struggle with 14B.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fboib",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you sure about the ratio?&lt;/p&gt;\n\n&lt;p&gt;I admit to likely doing something wrong, but with an A10 (24 gb vram) I struggle with 14B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fboib/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753620882,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ff5g6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Expensive-Apricot-25",
            "can_mod_post": false,
            "created_utc": 1753622154,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_idqkwio0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Llama 4 scout has the same performance of llama 3.3 70b, except it has the best open source vision, and it is far faster",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ff5g6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Llama 4 scout has the same performance of llama 3.3 70b, except it has the best open source vision, and it is far faster&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ff5g6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753622154,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ffwm0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dudensen",
            "can_mod_post": false,
            "created_utc": 1753622427,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_6vcmk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There is hunyuan moe 80b. I think at that size there will be mostly moe moving forward.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ffwm0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is hunyuan moe 80b. I think at that size there will be mostly moe moving forward.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ffwm0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753622427,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fgs33",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Conversation9561",
            "can_mod_post": false,
            "created_utc": 1753622741,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_jqxb4pte",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MoE is just faster for the compute bound.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fgs33",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MoE is just faster for the compute bound.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fgs33/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753622741,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fkspm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sub_RedditTor",
            "can_mod_post": false,
            "created_utc": 1753624144,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_oy3c84euj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The 48GB card can still give you much better context window size than 24GB ever will ..",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fkspm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The 48GB card can still give you much better context window size than 24GB ever will ..&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fkspm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753624144,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fm94d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Waste_Hotel5834",
            "can_mod_post": false,
            "created_utc": 1753624634,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_ti1n2lb1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "With the rise of reasoning models, one needs much larger context windows than before. To run the same 70B/Q4 models, today you probably should choose 64GB than 48GB.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fm94d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;With the rise of reasoning models, one needs much larger context windows than before. To run the same 70B/Q4 models, today you probably should choose 64GB than 48GB.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5fm94d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753624634,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5itja1",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "ttkciar",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5ide9j",
                                          "score": 2,
                                          "author_fullname": "t2_cpegz",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks for this.  I have llama.cpp using SWA, but will see if there are additional parameters I should be giving it.\n\nAlso, I just checked, and realized I hadn't updated my \"production\" inference system's llama.cpp since May.  It might be too old to have all the crunchy goodness.  Will update and see if that makes a difference.\n\n**Edited to add:** Yep, after updating llama.cpp, Gemma3-27B Q4_K_M only needs 46.3GB for 128K context!  I'll fiddle with it to see how much will fit in 32GB.  Thanks again for the tip!",
                                          "edited": 1753664047,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5itja1",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for this.  I have llama.cpp using SWA, but will see if there are additional parameters I should be giving it.&lt;/p&gt;\n\n&lt;p&gt;Also, I just checked, and realized I hadn&amp;#39;t updated my &amp;quot;production&amp;quot; inference system&amp;#39;s llama.cpp since May.  It might be too old to have all the crunchy goodness.  Will update and see if that makes a difference.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edited to add:&lt;/strong&gt; Yep, after updating llama.cpp, Gemma3-27B Q4_K_M only needs 46.3GB for 128K context!  I&amp;#39;ll fiddle with it to see how much will fit in 32GB.  Thanks again for the tip!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1majfwi",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5itja1/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753660434,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1753660434,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ide9j",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "henfiber",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5iaw76",
                                "score": 2,
                                "author_fullname": "t2_lw9me25",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "91GB sounds like a lot. Have you enabled interleaved [Sliding Window Attention](https://www.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/) (iSWA)?\n\nAlso [this thread](https://www.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/)\n\nIt reduces kv cache size to 1/6 without quality degradation.\n\nAccording to the OP in the [above thread](https://www.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/), you can fit the whole 128k context size even on 24GB cards with iSWA, a smaller batch size (e.g. -b 64), and kv quanitzation (Q8\\_0). 96k without kv cache quanitzation.",
                                "edited": 1753655662,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ide9j",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;91GB sounds like a lot. Have you enabled interleaved &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kqye2t/sliding_window_attention_support_merged_into/\"&gt;Sliding Window Attention&lt;/a&gt; (iSWA)?&lt;/p&gt;\n\n&lt;p&gt;Also &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/\"&gt;this thread&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;It reduces kv cache size to 1/6 without quality degradation.&lt;/p&gt;\n\n&lt;p&gt;According to the OP in the &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1krr7hn/how_to_get_the_most_from_llamacpps_iswa_support/\"&gt;above thread&lt;/a&gt;, you can fit the whole 128k context size even on 24GB cards with iSWA, a smaller batch size (e.g. -b 64), and kv quanitzation (Q8_0). 96k without kv cache quanitzation.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ide9j/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753654824,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753654824,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5iaw76",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ttkciar",
                      "can_mod_post": false,
                      "created_utc": 1753653985,
                      "send_replies": true,
                      "parent_id": "t1_n5glxeu",
                      "score": 1,
                      "author_fullname": "t2_cpegz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yep, this.  Gemma3-27B Q4_K_M wants about 91GB for full 128K context, which I have to trim down to 4K to get inference to fit in my 32GB MI60.\n\nIf I upped my VRAM to 48GB, I could increase that limit quite a bit, making RAG on the MI60 more practical.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5iaw76",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep, this.  Gemma3-27B Q4_K_M wants about 91GB for full 128K context, which I have to trim down to 4K to get inference to fit in my 32GB MI60.&lt;/p&gt;\n\n&lt;p&gt;If I upped my VRAM to 48GB, I could increase that limit quite a bit, making RAG on the MI60 more practical.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5iaw76/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753653985,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5glxeu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "henfiber",
            "can_mod_post": false,
            "created_utc": 1753635504,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_lw9me25",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You need VRAM/RAM also for longer context, it's useful that you have some space left for that.\n\nYou may also run multiple models in parallel, such as speect-to-text, text-to-speect, image generation, architect-coder pairs, vision or omni models etc.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5glxeu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You need VRAM/RAM also for longer context, it&amp;#39;s useful that you have some space left for that.&lt;/p&gt;\n\n&lt;p&gt;You may also run multiple models in parallel, such as speect-to-text, text-to-speect, image generation, architect-coder pairs, vision or omni models etc.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5glxeu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753635504,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5gz94p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "dat_cosmo_cat",
            "can_mod_post": false,
            "created_utc": 1753639351,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_2e6gzozr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Well the LLM depth and width can scale out to however much vram there is available. Teams putting forth truly competitive models must have solid training data pipelines, so overfitting is not a concern. Nvidia has fully monopolized the compute node options for deep learning, so just check the vram cap of the latest HGX, DGX, or MGX —and this is the direction you can expect model sizes to go in (and how they have been going historically). \n\nThe large jump from 70B -&gt; deepseek sized models aligns pretty closely with the NVSwitch baseboard release and availability; which allowed us to pool 8x GPU vRAM instead of 2x/4x with NVLink bridges. On the median side (teams stuck on PCIe), sizes scaled with the jumps from Volta / Turing (~20GB) to Ampere / hopper(~40GB/80GB). The loss of SLI on affordable cards will probably anchor open models to 80GB caps for awhile, barring some major research breakthroughs on inference optimization (which are absolutely possible).   ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gz94p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well the LLM depth and width can scale out to however much vram there is available. Teams putting forth truly competitive models must have solid training data pipelines, so overfitting is not a concern. Nvidia has fully monopolized the compute node options for deep learning, so just check the vram cap of the latest HGX, DGX, or MGX —and this is the direction you can expect model sizes to go in (and how they have been going historically). &lt;/p&gt;\n\n&lt;p&gt;The large jump from 70B -&amp;gt; deepseek sized models aligns pretty closely with the NVSwitch baseboard release and availability; which allowed us to pool 8x GPU vRAM instead of 2x/4x with NVLink bridges. On the median side (teams stuck on PCIe), sizes scaled with the jumps from Volta / Turing (~20GB) to Ampere / hopper(~40GB/80GB). The loss of SLI on affordable cards will probably anchor open models to 80GB caps for awhile, barring some major research breakthroughs on inference optimization (which are absolutely possible).   &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5gz94p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753639351,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hluyq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "perelmanych",
            "can_mod_post": false,
            "created_utc": 1753646188,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_63q8kong",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Huge MOE models is the way to go forward for companies. These models are easier to train and are faster to run on server grade hardware. So I think you are completely right and the days of dense models over 32B parameters are counted.\n\nIt is funny that AMD Strix Halo arrived with 128Gb shared memory it was a big deal for running Llama 3.3 70B and now it is basically DOA product for LLM. Thanks god I was patient enough not to rush buying one. I really like DS-V3 style and now researching the best options to run it on tight budget. It looks like 5t/s is the best I can hope for, lol.",
            "edited": 1753647227,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hluyq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Huge MOE models is the way to go forward for companies. These models are easier to train and are faster to run on server grade hardware. So I think you are completely right and the days of dense models over 32B parameters are counted.&lt;/p&gt;\n\n&lt;p&gt;It is funny that AMD Strix Halo arrived with 128Gb shared memory it was a big deal for running Llama 3.3 70B and now it is basically DOA product for LLM. Thanks god I was patient enough not to rush buying one. I really like DS-V3 style and now researching the best options to run it on tight budget. It looks like 5t/s is the best I can hope for, lol.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5hluyq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753646188,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5huj24",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "private_wombat",
            "can_mod_post": false,
            "created_utc": 1753648832,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_yay53kf1d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For 128GB of RAM on an M3 Max, what’s the best current option for local use?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5huj24",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For 128GB of RAM on an M3 Max, what’s the best current option for local use?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5huj24/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753648832,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ic2f7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Am-Insurgent",
            "can_mod_post": false,
            "created_utc": 1753654376,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_1frrwq6sq2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It seems like the best bet rn for that range is AM‑Distill‑Qwen‑72B . Things change fast in this space.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ic2f7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It seems like the best bet rn for that range is AM‑Distill‑Qwen‑72B . Things change fast in this space.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ic2f7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753654376,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ingnn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "reneil1337",
            "can_mod_post": false,
            "created_utc": 1753658308,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_16jq2z",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try Anubis 70b v1.1 its amazing I pull larger stuff from apis but Anubis is my daily driver at home",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ingnn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try Anubis 70b v1.1 its amazing I pull larger stuff from apis but Anubis is my daily driver at home&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ingnn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753658308,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5iqiv6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1753659375,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Can relate, to a degree.  I don't have enough VRAM to infer with 70B models, but have always treated them as an escalation option when my usual models in the 24B-32B range aren't smart enough for a specific prompt.\n\nIt has been suggested to me that with my CPU inference rig (dual v3 Xeons, 256GB of RAM) I really should be using large MoE instead of a dense 70B, and I think that suggestion has merit.  I've downloaded a couple of the recent large MoE, but haven't had time or spare compute to evaluate them, yet (my work has monopolized both my time and my homelab with other tasks for a couple of weeks now).\n\nEven if these MoEs can replace Tulu3-70 for STEM tasks, though, I'm still going to be using it for figuring out the prompt(s) I want Tulu3-405B to infer on overnight.  On the **other** hand, if these large MoEs can replace Tulu3-405B too, that would obviate that need for the 70B as well.\n\nWe will see.  There's an \"embiggened\" Qwen3-72B waiting for someone to instruction-train, and I look forward to seeing how that works out.  As long as there is demand for 70B-class models, I think the open source community will find a way to fill the niche, either by passthrough-merging smaller models, or distilling them from larger ones.\n\nPersonally I suspect the \"real\" future in dynamic models is MoA (like PHATGOOSE), and not MoE, but right now MoE is having its day in the sun.  I dream of a 32-adapter Tulu3-405B MoA, but that will be some years in the coming, I think (and we might well have better dense STEM models to use as the MoA base by then).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5iqiv6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can relate, to a degree.  I don&amp;#39;t have enough VRAM to infer with 70B models, but have always treated them as an escalation option when my usual models in the 24B-32B range aren&amp;#39;t smart enough for a specific prompt.&lt;/p&gt;\n\n&lt;p&gt;It has been suggested to me that with my CPU inference rig (dual v3 Xeons, 256GB of RAM) I really should be using large MoE instead of a dense 70B, and I think that suggestion has merit.  I&amp;#39;ve downloaded a couple of the recent large MoE, but haven&amp;#39;t had time or spare compute to evaluate them, yet (my work has monopolized both my time and my homelab with other tasks for a couple of weeks now).&lt;/p&gt;\n\n&lt;p&gt;Even if these MoEs can replace Tulu3-70 for STEM tasks, though, I&amp;#39;m still going to be using it for figuring out the prompt(s) I want Tulu3-405B to infer on overnight.  On the &lt;strong&gt;other&lt;/strong&gt; hand, if these large MoEs can replace Tulu3-405B too, that would obviate that need for the 70B as well.&lt;/p&gt;\n\n&lt;p&gt;We will see.  There&amp;#39;s an &amp;quot;embiggened&amp;quot; Qwen3-72B waiting for someone to instruction-train, and I look forward to seeing how that works out.  As long as there is demand for 70B-class models, I think the open source community will find a way to fill the niche, either by passthrough-merging smaller models, or distilling them from larger ones.&lt;/p&gt;\n\n&lt;p&gt;Personally I suspect the &amp;quot;real&amp;quot; future in dynamic models is MoA (like PHATGOOSE), and not MoE, but right now MoE is having its day in the sun.  I dream of a 32-adapter Tulu3-405B MoA, but that will be some years in the coming, I think (and we might well have better dense STEM models to use as the MoA base by then).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5iqiv6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753659375,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5exeox",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTshop_ai",
            "can_mod_post": false,
            "created_utc": 1753614548,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 0,
            "author_fullname": "t2_rkmud0isr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "TLDR: yes!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5exeox",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: yes!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5exeox/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753614548,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]