[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.\n\nBack then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.\n\nThis is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.\n\nThere is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.\n\nDeepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it's fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.\n\nWith the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn't been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that's a long while ago...\n\nI also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.\n\nThis does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the *really* large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.\n\nI suppose I'm partially reminiscing, and partially trying to start a dialogue on where the \"sweet spot\" for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.\n\nAre \\~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?\n\n**EDIT:** If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Are ~70B Models Going Out of Fashion?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1majfwi",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.79,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 15,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_cyw8u51dt",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 15,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753617926,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753613850,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Around a year and a half on from my post about 24GB vs 48GB VRAM, I personally find that the scene has changed a lot in terms of what sizes of models are popularly available and used.&lt;/p&gt;\n\n&lt;p&gt;Back then, 48GB VRAM for 70B models at 4BPW was more or less the gold standard for local inference. This is back when The Bloke was still releasing quants and Midnight Miqu was the holy grail for creative writing.&lt;/p&gt;\n\n&lt;p&gt;This is practically ancient history in the LLM space, but some of you surely recall this period just as well as I do.&lt;/p&gt;\n\n&lt;p&gt;There is now a much greater diversity of model parameter sizes available in terms of open-weights models, and the frontier of performance has continually been pushed forward. That being said, I find that newer open-weights models are either narrower in scope and smaller in parameter size, or generally much more competent but prohibitively large to be run locally for most.&lt;/p&gt;\n\n&lt;p&gt;Deepseek R1 and V3 are good examples of this, as is the newer Kimi K2. At 671B parameters and 1T parameters, respectively, I think it&amp;#39;s fair to assume that most users of these models are doing so via API rather than hosting locally. Even with an MOE architecture, they are simply too large to be hosted locally at reasonable speeds by enthusiasts. This is reminiscent of the situation with LLaMA 405B, in my opinion.&lt;/p&gt;\n\n&lt;p&gt;With the launch of LLaMA 4 being a bust and Qwen3 only going up to 32B in terms of dense models, perhaps there just hasn&amp;#39;t been a solid 70/72B model released in quite some time? The last model that really made a splash in this parameter range was Qwen2.5 72B, and that&amp;#39;s a long while ago...&lt;/p&gt;\n\n&lt;p&gt;I also find that most finetunes are still working with L3.3 as a base, which speaks to the recent lack of available models in this parameter range.&lt;/p&gt;\n\n&lt;p&gt;This does leave 48GB VRAM in a bit of a weird spot - too large for the small/medium-models, and too small for the &lt;em&gt;really&lt;/em&gt; large models. Perhaps a migration to a general preference for an MOE architecture is a natural consequence of the ever-increasing demand for VRAM and compute, or this is just a temporary lull in the output of the major labs training open-weights models which will come to pass eventually.&lt;/p&gt;\n\n&lt;p&gt;I suppose I&amp;#39;m partially reminiscing, and partially trying to start a dialogue on where the &amp;quot;sweet spot&amp;quot; for local models is nowadays. It would appear that the age of 70B/4BPW/48GB VRAM being the consensus has come to an end.&lt;/p&gt;\n\n&lt;p&gt;Are ~70B dense models going out of fashion for good? Or do you think this is just a temporary lull amidst a general move towards preference for MOE architectures?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; If very large MOE models will be the norm moving forward, perhaps building a server motherboard with large amounts of fast multi-channel system RAM is preferable to continually adding consumer GPUs to accrue larger amounts of VRAM for local inference (seeing as the latter is an approach that is primarily aimed at dense models that fit entirely into VRAM).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1majfwi",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "HvskyAI",
            "discussion_type": null,
            "num_comments": 28,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/",
            "subreddit_subscribers": 505252,
            "created_utc": 1753613850,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ezkzk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753615647,
                      "send_replies": true,
                      "parent_id": "t1_n5ey860",
                      "score": 1,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Interesting. I suppose there are architectural reasons why the Qwen team went with MOE for anything past 32B. \n\nIf this and Deepseek, Kimi, etc. are anything to go off of, it would appear that the move away from dense models is occurring for good. Perhaps, as mentioned by another comment, the new paradigm for local inference is running a server motherboard with lots of channels for fast system RAM and larger MOE models.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ezkzk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting. I suppose there are architectural reasons why the Qwen team went with MOE for anything past 32B. &lt;/p&gt;\n\n&lt;p&gt;If this and Deepseek, Kimi, etc. are anything to go off of, it would appear that the move away from dense models is occurring for good. Perhaps, as mentioned by another comment, the new paradigm for local inference is running a server motherboard with lots of channels for fast system RAM and larger MOE models.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ezkzk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753615647,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ey860",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "brown2green",
            "can_mod_post": false,
            "created_utc": 1753614968,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 11,
            "author_fullname": "t2_f010l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://x.com/JustinLin610/status/1934809653004939705\n\n&gt;For dense models larger than 30B, it is a bit hard to optimize effectiveness and efficiency (either training or inference). We prefer to use MoE for large models.\n\nQwen researcher Junyang Lin.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ey860",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://x.com/JustinLin610/status/1934809653004939705\"&gt;https://x.com/JustinLin610/status/1934809653004939705&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;For dense models larger than 30B, it is a bit hard to optimize effectiveness and efficiency (either training or inference). We prefer to use MoE for large models.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Qwen researcher Junyang Lin.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ey860/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753614968,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "richtext",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5f45rl",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "jacek2023",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5f1o0y",
                                                    "score": 1,
                                                    "author_fullname": "t2_vqgbql9w",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "you can see some details and benchmarks of 2x3090+2x3060\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp\\_benchmarks\\_on\\_72gb\\_vram\\_setup\\_2x\\_3090\\_2x/](https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/)\n\nyesterday I was running 235B in Q3 and also got around 10t/s \n\nhowever smaller MoEs are fast\n\npeople on reddit recommend extremely expensive mobos all the time, maybe they are happy with them, I don't really care",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5f45rl",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [
                                                      {
                                                        "e": "text",
                                                        "t": "llama.cpp"
                                                      }
                                                    ],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you can see some details and benchmarks of 2x3090+2x3060&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;yesterday I was running 235B in Q3 and also got around 10t/s &lt;/p&gt;\n\n&lt;p&gt;however smaller MoEs are fast&lt;/p&gt;\n\n&lt;p&gt;people on reddit recommend extremely expensive mobos all the time, maybe they are happy with them, I don&amp;#39;t really care&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1majfwi",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": "light",
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f45rl/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753617805,
                                                    "author_flair_text": "llama.cpp",
                                                    "collapsed": false,
                                                    "created_utc": 1753617805,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": "#bbbdbf",
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5f1o0y",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "HvskyAI",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5ey6tx",
                                          "score": 1,
                                          "author_fullname": "t2_cyw8u51dt",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "48GB (2 x 3090) runs 70B at \\~4.25BPW just fine, assuming EXL2/3 quants are being used with some K/V cache quantization. 32B leaves a *lot* of room, even after accounting for context and an embedding model, etc. \n\nWhat kind of system RAM are you running on that mobo, if you don't mind me asking? Do you find that GPU offloads make a large difference with MOE models that spill over into system RAM? \n\nIf large-ish dense models are on the way out and much larger MOE models will be the norm, server boards with fast system RAM may be the way to go over adding more 3090s, as far as local inference goes...",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5f1o0y",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;48GB (2 x 3090) runs 70B at ~4.25BPW just fine, assuming EXL2/3 quants are being used with some K/V cache quantization. 32B leaves a &lt;em&gt;lot&lt;/em&gt; of room, even after accounting for context and an embedding model, etc. &lt;/p&gt;\n\n&lt;p&gt;What kind of system RAM are you running on that mobo, if you don&amp;#39;t mind me asking? Do you find that GPU offloads make a large difference with MOE models that spill over into system RAM? &lt;/p&gt;\n\n&lt;p&gt;If large-ish dense models are on the way out and much larger MOE models will be the norm, server boards with fast system RAM may be the way to go over adding more 3090s, as far as local inference goes...&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1majfwi",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f1o0y/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753616660,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753616660,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ey6tx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "jacek2023",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5expjj",
                                "score": 5,
                                "author_fullname": "t2_vqgbql9w",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yesterday I called it finetune and another person commented that's it's not just finetune... ;)\n\nTo answer more to your topic - I use 3x3090 currently, my x399 has 4 slots but the only reason to add fourth 3090 are bigger quants of MoE models. \n\nTwo 3090s are enough for 32B dense models. With three you can have more fun with Nemotron 49B and older 70B models.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ey6tx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday I called it finetune and another person commented that&amp;#39;s it&amp;#39;s not just finetune... ;)&lt;/p&gt;\n\n&lt;p&gt;To answer more to your topic - I use 3x3090 currently, my x399 has 4 slots but the only reason to add fourth 3090 are bigger quants of MoE models. &lt;/p&gt;\n\n&lt;p&gt;Two 3090s are enough for 32B dense models. With three you can have more fun with Nemotron 49B and older 70B models.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ey6tx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753614950,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753614950,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 5
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5expjj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753614704,
                      "send_replies": true,
                      "parent_id": "t1_n5ewc8o",
                      "score": 6,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Well, I don't know if I'd call a L3.3-70B model that's received further post-training and pruning a direct successor. It's more so a high-performance finetune. \n\nThe closest thing to a LLaMA 3.3 70B successor is LLaMA 4 Scout, which is both much larger (109B) and an MOE. I suppose that's what I'm getting at with the post - there is no real direct successor in the dense 70B-parameter range. \n\nThat being said, Nemotron 49B does look interesting, and I've heard good things about Nvidia's tuned models in the past. Thanks for the recommendation.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5expjj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, I don&amp;#39;t know if I&amp;#39;d call a L3.3-70B model that&amp;#39;s received further post-training and pruning a direct successor. It&amp;#39;s more so a high-performance finetune. &lt;/p&gt;\n\n&lt;p&gt;The closest thing to a LLaMA 3.3 70B successor is LLaMA 4 Scout, which is both much larger (109B) and an MOE. I suppose that&amp;#39;s what I&amp;#39;m getting at with the post - there is no real direct successor in the dense 70B-parameter range. &lt;/p&gt;\n\n&lt;p&gt;That being said, Nemotron 49B does look interesting, and I&amp;#39;ve heard good things about Nvidia&amp;#39;s tuned models in the past. Thanks for the recommendation.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5expjj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753614704,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ewc8o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1753613994,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 10,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nemotron 49B was released yesterday, it's a successor of LLaMA 3.3 70B",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ewc8o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nemotron 49B was released yesterday, it&amp;#39;s a successor of LLaMA 3.3 70B&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5ewc8o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753613994,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f595a",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "No_Afternoon_4260",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5eymmf",
                                "score": 1,
                                "author_fullname": "t2_cj9kap4bx",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "And how do you run a 35B if you don't have at least 48gb of vram\n\n&gt;The whole point of 48GB was to run 70B at a 4-bit quant\n\nBecause 48gb vram allow 70B with very small ctx size. Just to give you an idea you can run mistral small q8 at 50k ctx on 48gb vram, it's not that much",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f595a",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;And how do you run a 35B if you don&amp;#39;t have at least 48gb of vram&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;The whole point of 48GB was to run 70B at a 4-bit quant&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Because 48gb vram allow 70B with very small ctx size. Just to give you an idea you can run mistral small q8 at 50k ctx on 48gb vram, it&amp;#39;s not that much&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f595a/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753618271,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753618271,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5eymmf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753615171,
                      "send_replies": true,
                      "parent_id": "t1_n5exki3",
                      "score": 5,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Interesting discussion! I went ahead and gave it a read. \n\nIt's intriguing to see people still recommending 48GB while acknowledging that there have been no significant \\~70B releases in quite some time. The whole point of 48GB was to run 70B at a 4-bit quant as to have an acceptable perplexity increase from quantization. \n\nI suppose if multi-hundred billion-parameter MOE models are going to be the norm moving forward, the conversation may move towards building inference rigs with server motherboards and fast, multi-channel system RAM rather than continuing to focus on increases in VRAM. Thanks for the input!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5eymmf",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting discussion! I went ahead and gave it a read. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s intriguing to see people still recommending 48GB while acknowledging that there have been no significant ~70B releases in quite some time. The whole point of 48GB was to run 70B at a 4-bit quant as to have an acceptable perplexity increase from quantization. &lt;/p&gt;\n\n&lt;p&gt;I suppose if multi-hundred billion-parameter MOE models are going to be the norm moving forward, the conversation may move towards building inference rigs with server motherboards and fast, multi-channel system RAM rather than continuing to focus on increases in VRAM. Thanks for the input!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eymmf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753615171,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5exki3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "fgoricha",
            "can_mod_post": false,
            "created_utc": 1753614631,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 5,
            "author_fullname": "t2_40xsg56g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I asked a similar question yesterday about vram sweet spot. Most people seemed to think 48gb vram is still relevant, but more vram is better. I think MOE and small dense models will be the trend going forward",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5exki3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I asked a similar question yesterday about vram sweet spot. Most people seemed to think 48gb vram is still relevant, but more vram is better. I think MOE and small dense models will be the trend going forward&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5exki3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753614631,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5f77bo",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "HvskyAI",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5f4ynx",
                                          "score": 1,
                                          "author_fullname": "t2_cyw8u51dt",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "That's an interesting point you raise. My views on quantization and perplexity increases may be dated, as I'm basing this all off of information that is quite old (by LLM standards). \n\nI wasn't aware that perplexity was found to be an ineffective measure of the detrimental effects of quantization. Would you happen to have a source on that, so I could take a look? I'd be very interested if you could point me in the general direction. \n\nIt's also true that long-context performance varies with different models. I'm going off of the RULER benchmark, which I now see is quite old. Perhaps more recent models are doing better on this front, and are not displaying the tendencies of past models where retrieval accuracy would drop off sharply at some small fraction of the stated maximum context window. \n\nAs for K/V cache quantization, I am on TabbyAPI as a back end, so the cache quantization is weighted, and not just naive truncation. I would think that this makes some tangible difference, but for tasks where high precision is necessary, I'd agree that higher quants (in both weights and cache) are indeed preferable.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5f77bo",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s an interesting point you raise. My views on quantization and perplexity increases may be dated, as I&amp;#39;m basing this all off of information that is quite old (by LLM standards). &lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t aware that perplexity was found to be an ineffective measure of the detrimental effects of quantization. Would you happen to have a source on that, so I could take a look? I&amp;#39;d be very interested if you could point me in the general direction. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also true that long-context performance varies with different models. I&amp;#39;m going off of the RULER benchmark, which I now see is quite old. Perhaps more recent models are doing better on this front, and are not displaying the tendencies of past models where retrieval accuracy would drop off sharply at some small fraction of the stated maximum context window. &lt;/p&gt;\n\n&lt;p&gt;As for K/V cache quantization, I am on TabbyAPI as a back end, so the cache quantization is weighted, and not just naive truncation. I would think that this makes some tangible difference, but for tasks where high precision is necessary, I&amp;#39;d agree that higher quants (in both weights and cache) are indeed preferable.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1majfwi",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f77bo/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753619095,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753619095,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f4ynx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "NNN_Throwaway2",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5f2uui",
                                "score": 2,
                                "author_fullname": "t2_8rrihts9",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For something like qwq with good long context performance, you can absolutely make use of the VRAM. 48GB also opens up the possibility to load multiple models at once, which can be useful for agentic tasks, for example. I also think you're understating the impact of quantization (both of weights and kv cache). This has typically been estimated by perplexity, which has been found more recently to be an imperfect way of quantifying this impact.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f4ynx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For something like qwq with good long context performance, you can absolutely make use of the VRAM. 48GB also opens up the possibility to load multiple models at once, which can be useful for agentic tasks, for example. I also think you&amp;#39;re understating the impact of quantization (both of weights and kv cache). This has typically been estimated by perplexity, which has been found more recently to be an imperfect way of quantifying this impact.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f4ynx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753618148,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753618148,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f2uui",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753617216,
                      "send_replies": true,
                      "parent_id": "t1_n5f1074",
                      "score": 2,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, that would be one way to go about things. However, because perplexity does not increase linearly with lower quants, there's only so much marginal increase in precision to be gained from running larger quantizations. The same could be said for K/V cache. \n\nOf course, going from a tiny (\\~2.5BPW) quant to 4BPW or 4.25BPW will be a massive improvement. But going from 4.25BPW to 5BPW? 6BPW? From what I've seen (and I'm happy to be proven wrong here), the returns are increasingly diminishing, and results are nearly indistinguishable from full precision at around \\~8BPW. \n\nUnless the use case is something like mathematics or coding, I'm doubtful that there would be a good value proposition for the VRAM overhead.\n\nMore context is hardly useful in practice, as well, since most models degrade in performance quite acutely long before their stated maximum context window. \n\nAll else being equal, being able to run a larger parameter-count model at a slightly lower (=&gt;4BPW) quant is preferable for me, as opposed to running, say, 32B at 8BPW with no K/V cache quantization. \n\nThis is all use case-dependent, of course.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f2uui",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, that would be one way to go about things. However, because perplexity does not increase linearly with lower quants, there&amp;#39;s only so much marginal increase in precision to be gained from running larger quantizations. The same could be said for K/V cache. &lt;/p&gt;\n\n&lt;p&gt;Of course, going from a tiny (~2.5BPW) quant to 4BPW or 4.25BPW will be a massive improvement. But going from 4.25BPW to 5BPW? 6BPW? From what I&amp;#39;ve seen (and I&amp;#39;m happy to be proven wrong here), the returns are increasingly diminishing, and results are nearly indistinguishable from full precision at around ~8BPW. &lt;/p&gt;\n\n&lt;p&gt;Unless the use case is something like mathematics or coding, I&amp;#39;m doubtful that there would be a good value proposition for the VRAM overhead.&lt;/p&gt;\n\n&lt;p&gt;More context is hardly useful in practice, as well, since most models degrade in performance quite acutely long before their stated maximum context window. &lt;/p&gt;\n\n&lt;p&gt;All else being equal, being able to run a larger parameter-count model at a slightly lower (=&amp;gt;4BPW) quant is preferable for me, as opposed to running, say, 32B at 8BPW with no K/V cache quantization. &lt;/p&gt;\n\n&lt;p&gt;This is all use case-dependent, of course.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f2uui/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753617216,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5f1074",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "NNN_Throwaway2",
            "can_mod_post": false,
            "created_utc": 1753616342,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 5,
            "author_fullname": "t2_8rrihts9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Huh??? How is 48GB VRAM \"too large\"? Just run more context and a higher quant. Its well established at this point that running kv quantization and smaller quants affects output and can degrade quality.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f1074",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Huh??? How is 48GB VRAM &amp;quot;too large&amp;quot;? Just run more context and a higher quant. Its well established at this point that running kv quantization and smaller quants affects output and can degrade quality.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f1074/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753616342,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5eycjs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Only-Letterhead-3411",
            "can_mod_post": false,
            "created_utc": 1753615031,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 4,
            "author_fullname": "t2_pbfqmgf8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There is no way we can know if model makers will stick to small models from now on or we'll get 70b sized models again. If you ask me, the chance of them sticking to 20-30B range is higher than 70B range. A QWQ 72B would be great. But not many people owns hardware to run it. And Qwen3 creators sounded like they aren't really interested in using their resources for that size models. I mean, qwen3 30B MoE can even run on a potato and it is smarter than L3.3 70B. I'm totally fine with the plan of them doing very big models and then distilling them into super smart small models",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5eycjs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There is no way we can know if model makers will stick to small models from now on or we&amp;#39;ll get 70b sized models again. If you ask me, the chance of them sticking to 20-30B range is higher than 70B range. A QWQ 72B would be great. But not many people owns hardware to run it. And Qwen3 creators sounded like they aren&amp;#39;t really interested in using their resources for that size models. I mean, qwen3 30B MoE can even run on a potato and it is smarter than L3.3 70B. I&amp;#39;m totally fine with the plan of them doing very big models and then distilling them into super smart small models&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eycjs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753615031,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f0obb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753616183,
                      "send_replies": true,
                      "parent_id": "t1_n5eyupr",
                      "score": 2,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yeah, I suppose we're spoiled for choice nowadays compared to a couple of years ago. I remember when 4096 context on LLaMA 2 was hailed as a revolutionary advance around here. And you know, at the time, it really was a big deal. We've come a long way. \n\n  \nAssuming that a rig is using a server motherboard with a sufficient number of memory channels, what kinds of prompt ingestion and token generation speeds are we realistically seeing with fairly fast RAM and very large MOE models? \n\nFor example, I see that DeekSeek-R1 at IQ4-KS comes in at around 368 GiB (so around \\~406GB, give or take). Can this realistically run at usable speeds on a 512GB DDR5 RAM system?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f0obb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, I suppose we&amp;#39;re spoiled for choice nowadays compared to a couple of years ago. I remember when 4096 context on LLaMA 2 was hailed as a revolutionary advance around here. And you know, at the time, it really was a big deal. We&amp;#39;ve come a long way. &lt;/p&gt;\n\n&lt;p&gt;Assuming that a rig is using a server motherboard with a sufficient number of memory channels, what kinds of prompt ingestion and token generation speeds are we realistically seeing with fairly fast RAM and very large MOE models? &lt;/p&gt;\n\n&lt;p&gt;For example, I see that DeekSeek-R1 at IQ4-KS comes in at around 368 GiB (so around ~406GB, give or take). Can this realistically run at usable speeds on a 512GB DDR5 RAM system?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f0obb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753616183,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5eyupr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Klutzy-Snow8016",
            "can_mod_post": false,
            "created_utc": 1753615282,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 3,
            "author_fullname": "t2_1d5l610jz3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Pretty much, yeah. The new meta is getting a bunch of regular RAM and running the big MOEs anyway, and (if you have a consumer-grade motherboard) learning to be patient while you wait for the generation.\n\nHonestly, it's not too bad. Remember when GPT-4 through ChatGPT would slow down to like 1 or 2 tokens per second when demand was high? And the version of GPT-4 with 32k context was a big deal and API keys for it were coveted? Now you can have basically that at home, but better and with more control.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5eyupr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pretty much, yeah. The new meta is getting a bunch of regular RAM and running the big MOEs anyway, and (if you have a consumer-grade motherboard) learning to be patient while you wait for the generation.&lt;/p&gt;\n\n&lt;p&gt;Honestly, it&amp;#39;s not too bad. Remember when GPT-4 through ChatGPT would slow down to like 1 or 2 tokens per second when demand was high? And the version of GPT-4 with 32k context was a big deal and API keys for it were coveted? Now you can have basically that at home, but better and with more control.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eyupr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753615282,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f53x7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1753618210,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 2,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The first thing that came to my mind when reading the title was the tweet by the Qwen researcher that brown2green mentioned.\n\nWe often forget that AI labs aren't thinking of us plebeans when they design the architecture of their models. Their primary concern is how to deploy the trained model at scale and effectively. Larger dense models aren't so friendly to A100/H100 deployments, especially with large contexts and heavy batching.\n\nPersonally, I'm very happy with the move to MoE. Deepseek V3/R1 and Kimi K2 run quite well on a cheap engineering sample 2nd gen Xeon Scalable (Cascade Lake) or Epyc Rome with one or two GPUs. They also run well on older GPUs. I just bought five Mi50s and working on upgrading my P40 rig to 8 GPUs (had them since over a year). Both will run run Qwen 3 235B or similar class MoE models quite happily at Q4 with large contexts. I've scrapped upgrading my triple 3090 rig with a fourth 3090, and will sell that fourth 3090. Funny enough, four Mi50s cost as much as said 3090. The only reason I'm keeping the 3090 rig is for diffusion/image/video models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f53x7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The first thing that came to my mind when reading the title was the tweet by the Qwen researcher that brown2green mentioned.&lt;/p&gt;\n\n&lt;p&gt;We often forget that AI labs aren&amp;#39;t thinking of us plebeans when they design the architecture of their models. Their primary concern is how to deploy the trained model at scale and effectively. Larger dense models aren&amp;#39;t so friendly to A100/H100 deployments, especially with large contexts and heavy batching.&lt;/p&gt;\n\n&lt;p&gt;Personally, I&amp;#39;m very happy with the move to MoE. Deepseek V3/R1 and Kimi K2 run quite well on a cheap engineering sample 2nd gen Xeon Scalable (Cascade Lake) or Epyc Rome with one or two GPUs. They also run well on older GPUs. I just bought five Mi50s and working on upgrading my P40 rig to 8 GPUs (had them since over a year). Both will run run Qwen 3 235B or similar class MoE models quite happily at Q4 with large contexts. I&amp;#39;ve scrapped upgrading my triple 3090 rig with a fourth 3090, and will sell that fourth 3090. Funny enough, four Mi50s cost as much as said 3090. The only reason I&amp;#39;m keeping the 3090 rig is for diffusion/image/video models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f53x7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753618210,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f1siq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AppearanceHeavy6724",
                      "can_mod_post": false,
                      "created_utc": 1753616719,
                      "send_replies": true,
                      "parent_id": "t1_n5eyn0q",
                      "score": 2,
                      "author_fullname": "t2_uz37qfx5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is not this simple, bigger models still are better in ways not captured by benchmark; for example they, within same family of models,  almost always better at long context; they almost always better at world knowledge too.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f1siq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is not this simple, bigger models still are better in ways not captured by benchmark; for example they, within same family of models,  almost always better at long context; they almost always better at world knowledge too.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f1siq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753616719,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5eyn0q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "custodiam99",
            "can_mod_post": false,
            "created_utc": 1753615176,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 3,
            "author_fullname": "t2_nqnhgqqf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen3 14b is better than any 70b model I tried, so I think structured training data and MOE is the future with smaller models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5eyn0q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 14b is better than any 70b model I tried, so I think structured training data and MOE is the future with smaller models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5eyn0q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753615176,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5f4rj0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "c3real2k",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5f3vgv",
                                "score": 1,
                                "author_fullname": "t2_h7qvk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Hm, yes, Command-A was alright if I remember correctly. Might have to give it a spin again.\n\nI can't say all that much about \"serious\" M4 setups, since I'm running the base M4s (16GB + 24GB), the worst possible configuration for inference. Prompt processing is slow, as well as token generation. Ironically, the only models bearable (for me) on those are small MoE's like Qwen3 30B A3B :D",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5f4rj0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hm, yes, Command-A was alright if I remember correctly. Might have to give it a spin again.&lt;/p&gt;\n\n&lt;p&gt;I can&amp;#39;t say all that much about &amp;quot;serious&amp;quot; M4 setups, since I&amp;#39;m running the base M4s (16GB + 24GB), the worst possible configuration for inference. Prompt processing is slow, as well as token generation. Ironically, the only models bearable (for me) on those are small MoE&amp;#39;s like Qwen3 30B A3B :D&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1majfwi",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f4rj0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753618065,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753618065,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5f3vgv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HvskyAI",
                      "can_mod_post": false,
                      "created_utc": 1753617677,
                      "send_replies": true,
                      "parent_id": "t1_n5f12d5",
                      "score": 1,
                      "author_fullname": "t2_cyw8u51dt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I suppose of the recent releases, there's Cohere's Command-A. But that's been about it, from what I can tell. Most new releases appear to be smaller dense models or *much* larger MOE models. \n\nHow are you finding the M4 to be for inference? I've heard that prompt ingestion can be rather slow, but I've never run inference on Apple silicon, myself.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5f3vgv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I suppose of the recent releases, there&amp;#39;s Cohere&amp;#39;s Command-A. But that&amp;#39;s been about it, from what I can tell. Most new releases appear to be smaller dense models or &lt;em&gt;much&lt;/em&gt; larger MOE models. &lt;/p&gt;\n\n&lt;p&gt;How are you finding the M4 to be for inference? I&amp;#39;ve heard that prompt ingestion can be rather slow, but I&amp;#39;ve never run inference on Apple silicon, myself.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1majfwi",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f3vgv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753617677,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5f12d5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "c3real2k",
            "can_mod_post": false,
            "created_utc": 1753616371,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 3,
            "author_fullname": "t2_h7qvk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I **yearn** for something modern and dense in the 70-130B range. Those smaller models (24-30B) might be highly optimized for specific tasks, but honestly, suck for creative writing (I might be exaggerating here a bit).\n\nNow I'm running a franken-rig of my GPU server and two MacMinis to somehow squeeze the lobotomized 90GB of Qwen3 235B@IQ3 XS into reasonably fast RAM to get what is essentially a 72B dense equivalent (which would fit nicely with a much less aggressive quantization into the 80GB VRAM my GPU server hosts, or at a reasonable 4bit quant for users with 48GB).\n\nSo, I have a gigantic 235B MoE of what would be a 72B dense model running, not gaining anything from the potential speed gains ('cause base M4's memory speed, prompt processing, ... is slow AF) and (while writing is nice) now having problems with code generation because of the low quant. Meaning I have to switch models every now and then.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f12d5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I &lt;strong&gt;yearn&lt;/strong&gt; for something modern and dense in the 70-130B range. Those smaller models (24-30B) might be highly optimized for specific tasks, but honestly, suck for creative writing (I might be exaggerating here a bit).&lt;/p&gt;\n\n&lt;p&gt;Now I&amp;#39;m running a franken-rig of my GPU server and two MacMinis to somehow squeeze the lobotomized 90GB of Qwen3 235B@IQ3 XS into reasonably fast RAM to get what is essentially a 72B dense equivalent (which would fit nicely with a much less aggressive quantization into the 80GB VRAM my GPU server hosts, or at a reasonable 4bit quant for users with 48GB).&lt;/p&gt;\n\n&lt;p&gt;So, I have a gigantic 235B MoE of what would be a 72B dense model running, not gaining anything from the potential speed gains (&amp;#39;cause base M4&amp;#39;s memory speed, prompt processing, ... is slow AF) and (while writing is nice) now having problems with code generation because of the low quant. Meaning I have to switch models every now and then.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f12d5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753616371,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f39lf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Eden1506",
            "can_mod_post": false,
            "created_utc": 1753617405,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_2ezqqypt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It solely depends on what the companies/institutions release.\n\nCreating a model from scratch costs millions and isn't something the community can causally do or decide by itself. \n\nIn addition MOE models are far more cost efficient for companies to run making it natural for them to change focus from monolithic models to moe models.\n\nBuying 128 gb of ddr5 ram costs around the same as a single 16gb GPU and allows one to run qwen3 235b at 3-4 tokens/s",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f39lf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It solely depends on what the companies/institutions release.&lt;/p&gt;\n\n&lt;p&gt;Creating a model from scratch costs millions and isn&amp;#39;t something the community can causally do or decide by itself. &lt;/p&gt;\n\n&lt;p&gt;In addition MOE models are far more cost efficient for companies to run making it natural for them to change focus from monolithic models to moe models.&lt;/p&gt;\n\n&lt;p&gt;Buying 128 gb of ddr5 ram costs around the same as a single 16gb GPU and allows one to run qwen3 235b at 3-4 tokens/s&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f39lf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753617405,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f5xc5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Zealousideal_Nail288",
            "can_mod_post": false,
            "created_utc": 1753618558,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_3lwcs91h",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For most part its bigger and less Quantisized better\n\n\nMistral 7b the first that really did blow me away\nIs decent imo\nBut can it write good stories, no\nCan it do programming,no\nCan it do other languages, no\n\n\nMeanwhile the 70b deepseek destillation claims to be able to program, speak all languages in the World, and is ok in Writing Stories (but very monotone)\n\n\ni have seen a decent ammount of comments saying they dont touch anything below 120b for writing\nSo if people have been using 200b+ models they wont go back to 70b\n\n\nAnd for groups and cooperations its much easier to aim for big models because they already start off better than small models",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f5xc5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For most part its bigger and less Quantisized better&lt;/p&gt;\n\n&lt;p&gt;Mistral 7b the first that really did blow me away\nIs decent imo\nBut can it write good stories, no\nCan it do programming,no\nCan it do other languages, no&lt;/p&gt;\n\n&lt;p&gt;Meanwhile the 70b deepseek destillation claims to be able to program, speak all languages in the World, and is ok in Writing Stories (but very monotone)&lt;/p&gt;\n\n&lt;p&gt;i have seen a decent ammount of comments saying they dont touch anything below 120b for writing\nSo if people have been using 200b+ models they wont go back to 70b&lt;/p&gt;\n\n&lt;p&gt;And for groups and cooperations its much easier to aim for big models because they already start off better than small models&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f5xc5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753618558,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f7177",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "steezy13312",
            "can_mod_post": false,
            "created_utc": 1753619025,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_rfjj2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "We're definitely in MoE summer",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f7177",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;We&amp;#39;re definitely in MoE summer&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f7177/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753619025,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5exeox",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTshop_ai",
            "can_mod_post": false,
            "created_utc": 1753614548,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_rkmud0isr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "TLDR: yes!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5exeox",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;TLDR: yes!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5exeox/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753614548,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5f0v9b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "anzzax",
            "can_mod_post": false,
            "created_utc": 1753616277,
            "send_replies": true,
            "parent_id": "t3_1majfwi",
            "score": 1,
            "author_fullname": "t2_zloia",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think this is a good sign, the focus of open models is shifting from home enthusiasts to small and medium-sized businesses, which means adoption is happening. The 72B dense model is a really odd size: its not affordable for most home nerds with a single 24GB GPU, and its not efficient for businesses with real workloads and batched inference needs. In my opinion, theres no practical value in dense models larger than 32B. Whats happening now isnt just a reasonable response to demand - its grounded in actual, justified use cases.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5f0v9b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think this is a good sign, the focus of open models is shifting from home enthusiasts to small and medium-sized businesses, which means adoption is happening. The 72B dense model is a really odd size: its not affordable for most home nerds with a single 24GB GPU, and its not efficient for businesses with real workloads and batched inference needs. In my opinion, theres no practical value in dense models larger than 32B. Whats happening now isnt just a reasonable response to demand - its grounded in actual, justified use cases.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1majfwi/are_70b_models_going_out_of_fashion/n5f0v9b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753616277,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1majfwi",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]