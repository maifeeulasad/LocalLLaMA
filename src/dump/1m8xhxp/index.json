[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone,\n\nI’m diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don’t have to answer all!\n\t\n1.\tLLM Frameworks:\nI’m currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?\n\n2.\tAgent Evaluation:\nHow do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?\n\n3.\tAttention Mechanisms:\nI’m familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?\n\n4.\tFine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?\n\n5.\tUnderstanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?\n\n6.\tUsing HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I’ve been using OLAMA. Curious how others are using HuggingFace—especially beyond embeddings.\n\n7.\tFine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you’d recommend?\n\n\nAny book or paper recommendations? (I actively read papers but maybe i see something new)\n\nWould love to hear your approaches or suggestions—thanks in advance!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Guidance on diving deep into LLMs",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8xhxp",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1ocdptcmcg",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753447432,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753445519,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’m diving deeper into the world of Large Language Models (LLMs) and had a many questions I was hoping to get input on from the community. Feel free to give answer to any of my questions! You don’t have to answer all!&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;LLM Frameworks:\nI’m currently using LangChain and recently exploring LangGraph. Are there any other LLM orchestration frameworks which companies are actively using?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Agent Evaluation:\nHow do you approach the evaluation of agents in your pipelines? Any best practices or tools you rely on?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Attention Mechanisms:\nI’m familiar with multi-head attention, sparse attention, and window attention. Are there other noteworthy attention mechanisms worth checking out?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fine-Tuning Methods:\nBesides LoRA and QLoRA, are there other commonly used or emerging techniques for LLM fine-tuning?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Understanding the Basics:\nI read a book on attention and LLMs that came out last September. It covered foundational topics well. Has anything crucial come out since then that might not be in the book?&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Using HuggingFace:\nI mostly use HuggingFace for embedding models, and for local LLMs, I’ve been using OLAMA. Curious how others are using HuggingFace—especially beyond embeddings.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Fine-Tuning Datasets:\nWhere do you typically source data for fine-tuning your models? Are there any reliable public datasets or workflows you’d recommend?&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any book or paper recommendations? (I actively read papers but maybe i see something new)&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your approaches or suggestions—thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m8xhxp",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Far-Run-3778",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753445519,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n52rwv7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Far-Run-3778",
                      "can_mod_post": false,
                      "created_utc": 1753447339,
                      "send_replies": true,
                      "parent_id": "t1_n52qmh0",
                      "score": 1,
                      "author_fullname": "t2_1ocdptcmcg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Definitely, some of the terms here were like totally new to me and thanks a lot! I will definitely have a lot to do for some days now!\n\nIs there any place where you learned about how to get data right from the LLM perspective?",
                      "edited": 1753447620,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52rwv7",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Definitely, some of the terms here were like totally new to me and thanks a lot! I will definitely have a lot to do for some days now!&lt;/p&gt;\n\n&lt;p&gt;Is there any place where you learned about how to get data right from the LLM perspective?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8xhxp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/n52rwv7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753447339,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52qmh0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Theio666",
            "can_mod_post": false,
            "created_utc": 1753446884,
            "send_replies": true,
            "parent_id": "t3_1m8xhxp",
            "score": 2,
            "author_fullname": "t2_ikhuo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "1: never used langchain myself, most our testing is done in raw torch/transformers, sometimes vLLM, but we don't build agentic systems so that's why we don't use langchain.\n\n3: check out GQA and MLA, both are ways to do less heavy MHA. I don't think any modern LLM uses full MHA. \n\n4: commonly used - no, but there's DoRA, reLoRA and countless other methods. Also, separate topic, but RFHF methods are very important nowadays, so if you haven't checked, study dpo ppo grpo - things like these. Specifically recommend MiMO papers, they have pretty in-depth description of GRPO pipeline with auto calibration based on task hardness.\n\n7: depends on what do you need, we spent more than 2 months doing a data preprocessing/formatting/generation pipeline for our audio LLM, and it still requires more work. Getting your data correct is the hardest part of training LLM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52qmh0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;1: never used langchain myself, most our testing is done in raw torch/transformers, sometimes vLLM, but we don&amp;#39;t build agentic systems so that&amp;#39;s why we don&amp;#39;t use langchain.&lt;/p&gt;\n\n&lt;p&gt;3: check out GQA and MLA, both are ways to do less heavy MHA. I don&amp;#39;t think any modern LLM uses full MHA. &lt;/p&gt;\n\n&lt;p&gt;4: commonly used - no, but there&amp;#39;s DoRA, reLoRA and countless other methods. Also, separate topic, but RFHF methods are very important nowadays, so if you haven&amp;#39;t checked, study dpo ppo grpo - things like these. Specifically recommend MiMO papers, they have pretty in-depth description of GRPO pipeline with auto calibration based on task hardness.&lt;/p&gt;\n\n&lt;p&gt;7: depends on what do you need, we spent more than 2 months doing a data preprocessing/formatting/generation pipeline for our audio LLM, and it still requires more work. Getting your data correct is the hardest part of training LLM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/n52qmh0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753446884,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8xhxp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n533co9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "UBIAI",
            "can_mod_post": false,
            "created_utc": 1753451113,
            "send_replies": true,
            "parent_id": "t3_1m8xhxp",
            "score": 2,
            "author_fullname": "t2_32tnavmg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "1. Some other frameworks you might want to check out are Haystack, which is great for building RAG systems; Promptify, which is focused on prompt engineering; and LlamaIndex, which is designed for data frameworks. \n\n2. One approach I’ve seen is to use human evaluation or LLM as-a-judge in the early stages to get a rough idea of performance, and then switch to automated metrics like BLEU, ROUGE, or even more advanced ones like BERTScore as the models mature. It’s also a good idea to evaluate against your specific use case, so custom metrics can be helpful. Checkout this quick tutorial for evals: [https://ubiai.tools/building-and-evaluating-an-ai-agent-startup-strategist-using-langchain-ubiai-openai/](https://ubiai.tools/building-and-evaluating-an-ai-agent-startup-strategist-using-langchain-ubiai-openai/) and this guide: [https://ubiai.gitbook.io/llm-guide/evaluation-of-fine-tuned-models](https://ubiai.gitbook.io/llm-guide/evaluation-of-fine-tuned-models)\n\nFew eval frameworks to check out: [arize.com](http://arize.com) or [confident-ai.com](http://confident-ai.com)\n\n3. I’d recommend looking into Cross Attention\n\n4. Adapter tuning is the main technique everybody uses. It allows you to add a small number of parameters to each layer of the transformer architecture, which can be trained separately from the rest of the model. It’s especially useful when working with smaller datasets.\n\n5. I’d recommend checking out [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/) for a more visual approach to understanding the architecture. For a deeper dive, “Transformers for Natural Language Processing”.\n\n7. For domain-specific models, I usually start with a web crawl. It’s a great way to get a large amount of data quickly. For more sensitive applications, the best way is to source internal data and have human-in-the-loop review it using a [data labeling platform](https://ubiai.tools/the-best-text-annotation-tool-ubiai-for-effortless-data-labeling/)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n533co9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Some other frameworks you might want to check out are Haystack, which is great for building RAG systems; Promptify, which is focused on prompt engineering; and LlamaIndex, which is designed for data frameworks. &lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;One approach I’ve seen is to use human evaluation or LLM as-a-judge in the early stages to get a rough idea of performance, and then switch to automated metrics like BLEU, ROUGE, or even more advanced ones like BERTScore as the models mature. It’s also a good idea to evaluate against your specific use case, so custom metrics can be helpful. Checkout this quick tutorial for evals: &lt;a href=\"https://ubiai.tools/building-and-evaluating-an-ai-agent-startup-strategist-using-langchain-ubiai-openai/\"&gt;https://ubiai.tools/building-and-evaluating-an-ai-agent-startup-strategist-using-langchain-ubiai-openai/&lt;/a&gt; and this guide: &lt;a href=\"https://ubiai.gitbook.io/llm-guide/evaluation-of-fine-tuned-models\"&gt;https://ubiai.gitbook.io/llm-guide/evaluation-of-fine-tuned-models&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Few eval frameworks to check out: &lt;a href=\"http://arize.com\"&gt;arize.com&lt;/a&gt; or &lt;a href=\"http://confident-ai.com\"&gt;confident-ai.com&lt;/a&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;I’d recommend looking into Cross Attention&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Adapter tuning is the main technique everybody uses. It allows you to add a small number of parameters to each layer of the transformer architecture, which can be trained separately from the rest of the model. It’s especially useful when working with smaller datasets.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;I’d recommend checking out &lt;a href=\"https://jalammar.github.io/illustrated-transformer/\"&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt; for a more visual approach to understanding the architecture. For a deeper dive, “Transformers for Natural Language Processing”.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;For domain-specific models, I usually start with a web crawl. It’s a great way to get a large amount of data quickly. For more sensitive applications, the best way is to source internal data and have human-in-the-loop review it using a &lt;a href=\"https://ubiai.tools/the-best-text-annotation-tool-ubiai-for-effortless-data-labeling/\"&gt;data labeling platform&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/n533co9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753451113,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8xhxp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n532olj",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Far-Run-3778",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n531zzd",
                                          "score": 1,
                                          "author_fullname": "t2_1ocdptcmcg",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Glad to hear, if it could be helpful for you!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n532olj",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Glad to hear, if it could be helpful for you!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m8xhxp",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/n532olj/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753450902,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753450902,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n531zzd",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "christ776",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n52yz9s",
                                "score": 1,
                                "author_fullname": "t2_do8g1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Sounds perfect for me , as I'm a beginner as well!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n531zzd",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sounds perfect for me , as I&amp;#39;m a beginner as well!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8xhxp",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/n531zzd/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753450684,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753450684,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n52yz9s",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Far-Run-3778",
                      "can_mod_post": false,
                      "created_utc": 1753449710,
                      "send_replies": true,
                      "parent_id": "t1_n52y9a5",
                      "score": 2,
                      "author_fullname": "t2_1ocdptcmcg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I read \"hands on large Language models\" - I would see it's really really beginner friendly one, like almost no maths but many visualizations,  so i won't suggest that as a complete resource but can be starter or combined with papers",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52yz9s",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I read &amp;quot;hands on large Language models&amp;quot; - I would see it&amp;#39;s really really beginner friendly one, like almost no maths but many visualizations,  so i won&amp;#39;t suggest that as a complete resource but can be starter or combined with papers&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8xhxp",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/n52yz9s/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753449710,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52y9a5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "christ776",
            "can_mod_post": false,
            "created_utc": 1753449474,
            "send_replies": true,
            "parent_id": "t3_1m8xhxp",
            "score": 1,
            "author_fullname": "t2_do8g1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hi, do you mind to share the book title you read in point 5?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52y9a5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi, do you mind to share the book title you read in point 5?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8xhxp/guidance_on_diving_deep_into_llms/n52y9a5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753449474,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8xhxp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]