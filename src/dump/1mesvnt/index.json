[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "(Noob here) I am currently using qwen3:14b and qwen2.5-coder:14b which are okay in general task, general coding &amp; normal tool callings.\n\nBut whenever I add it in IDE/extenstions like KiloCode then it just can't handle it. &amp; Stops without completing task.\n\nIn my personal assistant I have added simple tool callings so it works 80\\~90% of the time.\n\nBut when I add Jan AI (sqeuntional calling &amp; browser navigation) then after just 1 \\~ 2 callings it just goes stopped without completing task.\n\nsame with kilo code when I add on kilo code or another extenstions then it just cannot perform task completely. It just stops.\n\n  \nI want smarter then this llm (if smarter then I am okay with slow token response)\n\n\\--\n\nI was researchig about both. When I researched about 20b MoE and asked AI's so they suggested my 14b is more smart then 30b MoE \n\nand\n\n32b I will become slow (since it will run in ram and cpu, so I want to know how much smart it is? I can just use it alternative of chatgpt, if not smart then doesn't make sense to wait for long time)\n\n\\-----\n\nCurrently my 14b llm gives 25\\~35 tokens per second token output in general (avg)\n\nCurrently I am using ollama (I am sure using llama.cpp will boost the performance significantly)\n\nSince I am using ollama then I am currently using gpus power only.\n\nI am planning to switch to llama.cpp so I can do more customization like using all system resources cpu+gpu) and doing quantization.\n\n\\--\n\nI don't know about quants q, k etc too much (but have shallow knowledge)\n\n  \nif you think in my specs I can run bigger llms with quintization (sorry for spelling) &amp; custom configs so please suggest those models as well\n\n\\--\n\nCan I run 70b model? (obiosuly I need to quantize it, but 70b quantized vs 30b which will be smart and which will be faster?)\n\n  \n\\--- \n\nMax llm size which I can run?\n\nBest setting for my requirement?\n\nWhat should I look for to get even better llms?\n\n\n\n    OS: Ubuntu 22.04.5 LTS x86_64 \n    Host: B450 AORUS ELITE V2 -CF \n    Kernel: 5.15.0-130-generic \n    Uptime: 1 day, 5 hours, 42 mins \n    Packages: 1736 (dpkg) \n    Shell: bash 5.1.16 \n    Resolution: 2560x1440 \n    DE: GNOME 42.9 \n    WM: Mutter \n    WM Theme: Yaru-dark \n    Theme: Adwaita-dark [GTK2/3] \n    Icons: Yaru [GTK2/3] \n    Terminal: gnome-terminal \n    CPU: AMD Ryzen 5 5600G with Radeon Graphics (12) @ 3.900GHz \n    GPU: NVIDIA GeForce RTX 3060 Lite Hash Rate (12GB VRAM)\n    Memory: 21186MiB / 48035MiB \n\n  \n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "(Noob here) Qwen 30b (MoE) vs Qwen 32B which is smartest in coding, reasoning and which faster &amp; smartest? (I have RTX 3060 12GB VRAM + 48 GB RAM)",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 57,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mesvnt",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1b8utegv8t",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/JMnIT-T7tU4TQdi7zHU4o3GCnNbIEcaUSqSB_BhdJVA.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754044055,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Noob here) I am currently using qwen3:14b and qwen2.5-coder:14b which are okay in general task, general coding &amp;amp; normal tool callings.&lt;/p&gt;\n\n&lt;p&gt;But whenever I add it in IDE/extenstions like KiloCode then it just can&amp;#39;t handle it. &amp;amp; Stops without completing task.&lt;/p&gt;\n\n&lt;p&gt;In my personal assistant I have added simple tool callings so it works 80~90% of the time.&lt;/p&gt;\n\n&lt;p&gt;But when I add Jan AI (sqeuntional calling &amp;amp; browser navigation) then after just 1 ~ 2 callings it just goes stopped without completing task.&lt;/p&gt;\n\n&lt;p&gt;same with kilo code when I add on kilo code or another extenstions then it just cannot perform task completely. It just stops.&lt;/p&gt;\n\n&lt;p&gt;I want smarter then this llm (if smarter then I am okay with slow token response)&lt;/p&gt;\n\n&lt;p&gt;--&lt;/p&gt;\n\n&lt;p&gt;I was researchig about both. When I researched about 20b MoE and asked AI&amp;#39;s so they suggested my 14b is more smart then 30b MoE &lt;/p&gt;\n\n&lt;p&gt;and&lt;/p&gt;\n\n&lt;p&gt;32b I will become slow (since it will run in ram and cpu, so I want to know how much smart it is? I can just use it alternative of chatgpt, if not smart then doesn&amp;#39;t make sense to wait for long time)&lt;/p&gt;\n\n&lt;p&gt;-----&lt;/p&gt;\n\n&lt;p&gt;Currently my 14b llm gives 25~35 tokens per second token output in general (avg)&lt;/p&gt;\n\n&lt;p&gt;Currently I am using ollama (I am sure using llama.cpp will boost the performance significantly)&lt;/p&gt;\n\n&lt;p&gt;Since I am using ollama then I am currently using gpus power only.&lt;/p&gt;\n\n&lt;p&gt;I am planning to switch to llama.cpp so I can do more customization like using all system resources cpu+gpu) and doing quantization.&lt;/p&gt;\n\n&lt;p&gt;--&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t know about quants q, k etc too much (but have shallow knowledge)&lt;/p&gt;\n\n&lt;p&gt;if you think in my specs I can run bigger llms with quintization (sorry for spelling) &amp;amp; custom configs so please suggest those models as well&lt;/p&gt;\n\n&lt;p&gt;--&lt;/p&gt;\n\n&lt;p&gt;Can I run 70b model? (obiosuly I need to quantize it, but 70b quantized vs 30b which will be smart and which will be faster?)&lt;/p&gt;\n\n&lt;p&gt;--- &lt;/p&gt;\n\n&lt;p&gt;Max llm size which I can run?&lt;/p&gt;\n\n&lt;p&gt;Best setting for my requirement?&lt;/p&gt;\n\n&lt;p&gt;What should I look for to get even better llms?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;OS: Ubuntu 22.04.5 LTS x86_64 \nHost: B450 AORUS ELITE V2 -CF \nKernel: 5.15.0-130-generic \nUptime: 1 day, 5 hours, 42 mins \nPackages: 1736 (dpkg) \nShell: bash 5.1.16 \nResolution: 2560x1440 \nDE: GNOME 42.9 \nWM: Mutter \nWM Theme: Yaru-dark \nTheme: Adwaita-dark [GTK2/3] \nIcons: Yaru [GTK2/3] \nTerminal: gnome-terminal \nCPU: AMD Ryzen 5 5600G with Radeon Graphics (12) @ 3.900GHz \nGPU: NVIDIA GeForce RTX 3060 Lite Hash Rate (12GB VRAM)\nMemory: 21186MiB / 48035MiB \n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/kwcziz5qudgf1.png",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/kwcziz5qudgf1.png?auto=webp&amp;s=3157ea78a79d5ac07e0cb0136d52f49b04ac7557",
                    "width": 929,
                    "height": 380
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/kwcziz5qudgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ee82b2afc88bd97945a1d776b4636dca0f5e736b",
                      "width": 108,
                      "height": 44
                    },
                    {
                      "url": "https://preview.redd.it/kwcziz5qudgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3c0a3c4e2f87bbbfbfd54f8d868d876e621b80ae",
                      "width": 216,
                      "height": 88
                    },
                    {
                      "url": "https://preview.redd.it/kwcziz5qudgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d49b24e0a44fed79d0d8f99b520c90146be01046",
                      "width": 320,
                      "height": 130
                    },
                    {
                      "url": "https://preview.redd.it/kwcziz5qudgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b9254c90ef792da4f0fe71262c6e728ae0654cfd",
                      "width": 640,
                      "height": 261
                    }
                  ],
                  "variants": {},
                  "id": "DRyixyGCOuvYJ9GLpCPMtiU2SSEAKjFXPknmcG18D9Q"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mesvnt",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "InsideResolve4517",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mesvnt/noob_here_qwen_30b_moe_vs_qwen_32b_which_is/",
            "stickied": false,
            "url": "https://i.redd.it/kwcziz5qudgf1.png",
            "subreddit_subscribers": 508192,
            "created_utc": 1754044055,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6c728t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "getmevodka",
                      "can_mod_post": false,
                      "created_utc": 1754051176,
                      "send_replies": true,
                      "parent_id": "t1_n6bqrk0",
                      "score": 1,
                      "author_fullname": "t2_7uoa6r1b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "this. if possible get q4 k xl. should run best regarding performance/quality match",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6c728t",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;this. if possible get q4 k xl. should run best regarding performance/quality match&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mesvnt",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mesvnt/noob_here_qwen_30b_moe_vs_qwen_32b_which_is/n6c728t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754051176,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6bqrk0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Weird_Researcher_472",
            "can_mod_post": false,
            "created_utc": 1754044277,
            "send_replies": true,
            "parent_id": "t3_1mesvnt",
            "score": 8,
            "author_fullname": "t2_1tytceq8sj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Pick Qwen3 Coder 30B-A3B (unsloth quants)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bqrk0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pick Qwen3 Coder 30B-A3B (unsloth quants)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mesvnt/noob_here_qwen_30b_moe_vs_qwen_32b_which_is/n6bqrk0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754044277,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mesvnt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bs94a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1754045014,
            "send_replies": true,
            "parent_id": "t3_1mesvnt",
            "score": 4,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "32B has always felt smarter and more reliable than the MoE for me. Since the new 32B Coder isn't out yet though, the MoE coder might be better for some use cases currently.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bs94a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;32B has always felt smarter and more reliable than the MoE for me. Since the new 32B Coder isn&amp;#39;t out yet though, the MoE coder might be better for some use cases currently.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mesvnt/noob_here_qwen_30b_moe_vs_qwen_32b_which_is/n6bs94a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754045014,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mesvnt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6btge1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pristine-Woodpecker",
            "can_mod_post": false,
            "created_utc": 1754045588,
            "send_replies": true,
            "parent_id": "t3_1mesvnt",
            "score": 2,
            "author_fullname": "t2_5b972ieo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For non-agentic coding the 32B looks like the winner. Given that you're talking about tool calling, most likely the Qwen3-Coder 30B-A3B.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6btge1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For non-agentic coding the 32B looks like the winner. Given that you&amp;#39;re talking about tool calling, most likely the Qwen3-Coder 30B-A3B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mesvnt/noob_here_qwen_30b_moe_vs_qwen_32b_which_is/n6btge1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754045588,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mesvnt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6c2hry",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "QFGTrialByFire",
            "can_mod_post": false,
            "created_utc": 1754049475,
            "send_replies": true,
            "parent_id": "t3_1mesvnt",
            "score": 2,
            "author_fullname": "t2_1h4o7f23eh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hi I'm surprised you are considering running those as they will just be so slow for that GPU vram when you spill over to system ram but you mention you don't mind if gives good results so maybe it will be ok. I'm relatively new to this as well but from my understanding instead of going for bigger models you might be better of going for specificly fine tuned reasoning+coding models instead. e.g. give Seed-Coder-8B-Reasoning a go. It runs at 9GB vram on my 3080Ti so will fit on your GPU as well. To me it feels like local models are better suited to being finetuned so bigger models isn't always better getting ones fine tuned for a task might be better and faster than just going for a bigger model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c2hry",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi I&amp;#39;m surprised you are considering running those as they will just be so slow for that GPU vram when you spill over to system ram but you mention you don&amp;#39;t mind if gives good results so maybe it will be ok. I&amp;#39;m relatively new to this as well but from my understanding instead of going for bigger models you might be better of going for specificly fine tuned reasoning+coding models instead. e.g. give Seed-Coder-8B-Reasoning a go. It runs at 9GB vram on my 3080Ti so will fit on your GPU as well. To me it feels like local models are better suited to being finetuned so bigger models isn&amp;#39;t always better getting ones fine tuned for a task might be better and faster than just going for a bigger model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mesvnt/noob_here_qwen_30b_moe_vs_qwen_32b_which_is/n6c2hry/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754049475,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mesvnt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6c179m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AleksHop",
            "can_mod_post": false,
            "created_utc": 1754048953,
            "send_replies": true,
            "parent_id": "t3_1mesvnt",
            "score": -4,
            "author_fullname": "t2_8dnu3hmd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Moe will be like 3-4 faster and as dumb as 32b, use normal models",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c179m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Moe will be like 3-4 faster and as dumb as 32b, use normal models&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mesvnt/noob_here_qwen_30b_moe_vs_qwen_32b_which_is/n6c179m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754048953,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mesvnt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -4
          }
        }
      ],
      "before": null
    }
  }
]