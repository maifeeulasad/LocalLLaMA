[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!\\_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.\n\nRight now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:\n\n* Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?\n* Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?\n* How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?\n\nWould love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How are people staging AI training datasets from NVMe → DDR5 → GPU VRAM for fine-tuning on RTX 5090s?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 78,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m6vj8o",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1tltnwoxsz",
            "secure_media": {
              "reddit_video": {
                "bitrate_kbps": 2400,
                "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
                "has_audio": true,
                "height": 720,
                "width": 1280,
                "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
                "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755865865%2CNDBkNDNhZGZjOWI0YTViYTY1YTE1NjdmNGRlOTVkNzk3Njk4NGViY2I2MTcyYzM4YTk3MWFmMTI0NjBkYjA0OQ%3D%3D&amp;v=1&amp;f=sd",
                "duration": 11,
                "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755865865%2CNjU5ODZiODQ0YWQ0ZDgwMTlhNTEwMTdmNjcwODNiZWY1NDJkZWI2NjE0YmZmODU3NTNhNjRlZTRlYjYzZGViNA%3D%3D&amp;v=1&amp;f=sd",
                "is_gif": false,
                "transcoding_status": "completed"
              }
            },
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;format=jpg&amp;v=enabled&amp;lthumb=true&amp;s=7fa7bed1c348998994fed16cd386547e5aac176b",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "hosted:video",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1753232111,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "v.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’m building a structured fine-tuning pipeline for a legal/finance AI assistant (think deal-closure workflows, private equity logic, etc.) using Pop!_OS 22.04 for cleaner NVIDIA driver control and GPU memory isolation. We’re running Torchlight (nightly) builds to fully unlock Blackwell compatibility, along with bitsandbytes 4-bit LoRA for Mistral 7B.&lt;/p&gt;\n\n&lt;p&gt;Right now, we’re testing ways to preload training batches into system RAM to reduce NVMe fetch latency and minimize I/O stalls when feeding the 5090 at full saturation. Curious what others are doing to optimize this path:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Are you using prefetch workers, memory-mapped datasets, or rolling your own RAM buffers?&lt;/li&gt;\n&lt;li&gt;Anyone running into issues with NUMA alignment or memory pressure in 96–128GB DDR5 systems when training on large batches?&lt;/li&gt;\n&lt;li&gt;How do you ensure smooth RAM → VRAM feeding at 5090 throughput without overloading I/O threads?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Would love to compare notes — especially with anyone running multi-token workflows, synthetic pipelines, or structured LoRA chaining. We’re deep into fine-tuning phase for Project Emberlight, so any tips on squeezing max bandwidth out of RAM → GPU VRAM would be killer.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://v.redd.it/m3v13th5vief1",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?format=pjpg&amp;auto=webp&amp;s=8a06bae144080a3451d0fb255b750bcab9e21c69",
                    "width": 1280,
                    "height": 720
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=108&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=930f76891585f27565f3d929f2d1d4df9fbbe6f7",
                      "width": 108,
                      "height": 60
                    },
                    {
                      "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=216&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=c2cc4160f0d866354b83bad0ce200177193907cd",
                      "width": 216,
                      "height": 121
                    },
                    {
                      "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=320&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=337db9c93858d2e6c9db6e22822d525e7600240d",
                      "width": 320,
                      "height": 180
                    },
                    {
                      "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=640&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=1915548352adb0259f40c35397f4626912fc93d4",
                      "width": 640,
                      "height": 360
                    },
                    {
                      "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=960&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=0e2a49235d59469a5f29b50b3c42efc9cc7f4d39",
                      "width": 960,
                      "height": 540
                    },
                    {
                      "url": "https://external-preview.redd.it/NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo.png?width=1080&amp;crop=smart&amp;format=pjpg&amp;auto=webp&amp;s=81350e82df349faf24b9ef86ecafb7b97303ebd3",
                      "width": 1080,
                      "height": 607
                    }
                  ],
                  "variants": {},
                  "id": "NnNtZzZ1aDV2aWVmMQ_TONUx3ShmleBmxHUm5WhhyHrbQHADnnzginEsV9Wo"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m6vj8o",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "DJAI9LAB",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": {
              "reddit_video": {
                "bitrate_kbps": 2400,
                "fallback_url": "https://v.redd.it/m3v13th5vief1/DASH_720.mp4?source=fallback",
                "has_audio": true,
                "height": 720,
                "width": 1280,
                "scrubber_media_url": "https://v.redd.it/m3v13th5vief1/DASH_96.mp4",
                "dash_url": "https://v.redd.it/m3v13th5vief1/DASHPlaylist.mpd?a=1755865865%2CNDBkNDNhZGZjOWI0YTViYTY1YTE1NjdmNGRlOTVkNzk3Njk4NGViY2I2MTcyYzM4YTk3MWFmMTI0NjBkYjA0OQ%3D%3D&amp;v=1&amp;f=sd",
                "duration": 11,
                "hls_url": "https://v.redd.it/m3v13th5vief1/HLSPlaylist.m3u8?a=1755865865%2CNjU5ODZiODQ0YWQ0ZDgwMTlhNTEwMTdmNjcwODNiZWY1NDJkZWI2NjE0YmZmODU3NTNhNjRlZTRlYjYzZGViNA%3D%3D&amp;v=1&amp;f=sd",
                "is_gif": false,
                "transcoding_status": "completed"
              }
            },
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/",
            "stickied": false,
            "url": "https://v.redd.it/m3v13th5vief1",
            "subreddit_subscribers": 503255,
            "created_utc": 1753232111,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": true
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4mre2l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DJAI9LAB",
            "can_mod_post": false,
            "created_utc": 1753232701,
            "send_replies": true,
            "parent_id": "t3_1m6vj8o",
            "score": 1,
            "author_fullname": "t2_1tltnwoxsz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "P.S. This video was obviously on Ubuntu but we have moved to Pop!\\_OS for the better Nvidia drivers going forward, as mentioned in original post.",
            "edited": 1753232960,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4mre2l",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;P.S. This video was obviously on Ubuntu but we have moved to Pop!_OS for the better Nvidia drivers going forward, as mentioned in original post.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/n4mre2l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753232701,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6vj8o",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4mu0cq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "One-Employment3759",
            "can_mod_post": false,
            "created_utc": 1753233602,
            "send_replies": true,
            "parent_id": "t3_1m6vj8o",
            "score": 0,
            "author_fullname": "t2_1f6wnmakwr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Like anything, it's about benchmarking where the delays are.\n\nDouble/triple/ring buffer GPU ram input so the GPU is never waiting for data. Track that this never drops to 0.\n\nReplace slow-ass python preprocessing with rust.\n\nBenchmark what the GPU saturation is when not waiting for input, since architecture and kernels may impact that maximum GPU usage regardless of data loading!\n\nIn the past I have also entirely replaced Pytorch data loaders because they were slow-ass for my use case.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4mu0cq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Like anything, it&amp;#39;s about benchmarking where the delays are.&lt;/p&gt;\n\n&lt;p&gt;Double/triple/ring buffer GPU ram input so the GPU is never waiting for data. Track that this never drops to 0.&lt;/p&gt;\n\n&lt;p&gt;Replace slow-ass python preprocessing with rust.&lt;/p&gt;\n\n&lt;p&gt;Benchmark what the GPU saturation is when not waiting for input, since architecture and kernels may impact that maximum GPU usage regardless of data loading!&lt;/p&gt;\n\n&lt;p&gt;In the past I have also entirely replaced Pytorch data loaders because they were slow-ass for my use case.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/n4mu0cq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753233602,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6vj8o",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4mwhut",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1753234470,
            "send_replies": true,
            "parent_id": "t3_1m6vj8o",
            "score": 5,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What do you mean by \"NUMA alignment\" and \"memory pressure\"??? If that 5090 is running on that desktop in the video, you don't have any NUMA nodes.\n\nWhere your issues might come from is running graphics on that same GPU (again, assuming you're tuning on that desktop). which can cause issues with GPU scheduling and/or GPU cache thrashing.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4mwhut",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What do you mean by &amp;quot;NUMA alignment&amp;quot; and &amp;quot;memory pressure&amp;quot;??? If that 5090 is running on that desktop in the video, you don&amp;#39;t have any NUMA nodes.&lt;/p&gt;\n\n&lt;p&gt;Where your issues might come from is running graphics on that same GPU (again, assuming you&amp;#39;re tuning on that desktop). which can cause issues with GPU scheduling and/or GPU cache thrashing.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m6vj8o/how_are_people_staging_ai_training_datasets_from/n4mwhut/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753234470,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m6vj8o",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        }
      ],
      "before": null
    }
  }
]