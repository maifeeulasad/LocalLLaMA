[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hello guys, I am just new here.\n\nI installed ollama and runing model qwen3:8b  \nWhen I run iot through terminal, I get full utilisation of the GPU (3060 Mobile 60W).  \nbut slow response and bad utilisation when run in VS Code.  \nprovided some of my debug log-\n\nubuntu terminal:\n\n    $ ollama ps\n    NAME        ID              SIZE      PROCESSOR          UNTIL              \n    qwen3:8b    500a1f067a9f    6.5 GB    10%/90% CPU/GPU    4 minutes from now \n    \n    udo journalctl -u ollama -f\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   560.00 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =   791.61 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    16.01 MiB\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 17 (with bs=512), 5 (with bs=1)\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:49:14.189+02:00 level=INFO source=server.go:637 msg=\"llama runner started in 1.51 seconds\"\n    Jul 27 11:49:14 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:49:14 | 200 |  2.029277689s |       127.0.0.1 | POST     \"/api/generate\"\n    Jul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     \"/api/chat\"\n    Jul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     \"/api/chat\"\n\nwhen I run through the continue chat in VS Code\n\n    ollama ps\n    NAME        ID              SIZE     PROCESSOR          UNTIL               \n    qwen3:8b    500a1f067a9f    13 GB    58%/42% CPU/GPU    29 minutes from now \n    \n    sudo journalctl -u ollama -f\n    [sudo] password for abdelrahman: \n    Jul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     \"/api/chat\"\n    Jul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     \"/api/chat\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     321.358µs |       127.0.0.1 | GET      \"/api/tags\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     249.342µs |       127.0.0.1 | GET      \"/api/tags\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   49.584345ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   54.905231ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   57.173959ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   48.834545ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   59.986822ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   63.046354ms |       127.0.0.1 | POST     \"/api/show\"\n    Jul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      18.856µs |       127.0.0.1 | HEAD     \"/\"\n    Jul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      73.667µs |       127.0.0.1 | GET      \"/api/ps\"\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.945+02:00 level=INFO source=server.go:135 msg=\"system memory\" total=\"15.3 GiB\" free=\"10.4 GiB\" free_swap=\"2.3 GiB\"\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.946+02:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=7 layers.split=\"\" memory.available=\"[5.5 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"12.7 GiB\" memory.required.partial=\"5.4 GiB\" memory.required.kv=\"4.5 GiB\" memory.required.allocations=\"[5.4 GiB]\" memory.weights.total=\"4.5 GiB\" memory.weights.repeating=\"4.1 GiB\" memory.weights.nonrepeating=\"486.9 MiB\" memory.graph.full=\"3.0 GiB\" memory.graph.partial=\"3.0 GiB\"\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...\n    Jul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '&lt;|im_start|&gt;...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = ?B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 'Ċ'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 '&lt;|fim_prefix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 '&lt;|fim_suffix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 '&lt;|fim_middle|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load: vocab only - skipping tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.156+02:00 level=INFO source=server.go:438 msg=\"starting llama server\" cmd=\"/home/abdelrahman/install_directory/ollama/bin/ollama runner --model /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32768 --batch-size 512 --n-gpu-layers 7 --threads 8 --no-mmap --parallel 1 --port 35311\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=sched.go:483 msg=\"loaded runners\" count=1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:598 msg=\"waiting for llama runner to start responding\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server not responding\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.165+02:00 level=INFO source=runner.go:815 msg=\"starting go runner\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: found 1 CUDA devices:\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]:   Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CUDA backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cuda.so\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CPU backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cpu-icelake.so\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=runner.go:874 msg=\"Server listening on 127.0.0.1:35311\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060 Laptop GPU) - 5617 MiB free\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '&lt;|im_start|&gt;...\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.408+02:00 level=INFO source=server.go:632 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_train      = 40960\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd           = 4096\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_layer          = 36\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head           = 32\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head_kv        = 8\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_rot            = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa            = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa_pattern    = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_k    = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_v    = 128\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_gqa            = 4\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_k_gqa     = 1024\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_v_gqa     = 1024\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_eps       = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_rms_eps   = 1.0e-06\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_clamp_kqv      = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_max_alibi_bias = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_logit_scale    = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_attn_scale     = 0.0e+00\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ff             = 12288\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert         = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert_used    = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: causal attn      = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: pooling type     = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope type        = 2\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope scaling     = linear\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_base_train  = 1000000.0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_scale_train = 1\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_orig_yarn  = 40960\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope_finetuned   = unknown\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_conv       = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_inner      = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_state      = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_rank      = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_b_c_rms   = 0\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 'Ċ'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 '&lt;|fim_prefix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 '&lt;|fim_suffix|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 '&lt;|fim_middle|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 '&lt;|endoftext|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 '&lt;|im_end|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 '&lt;|fim_pad|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 '&lt;|repo_name|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 '&lt;|file_sep|&gt;'\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_tensors: loading model tensors, this can take a while... (mmap = false)\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      21.813µs |       127.0.0.1 | HEAD     \"/\"\n    Jul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      55.253µs |       127.0.0.1 | GET      \"/api/ps\"\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloading 7 repeating layers to GPU\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloaded 7/37 layers to GPU\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:    CUDA_Host model buffer size =  3804.56 MiB\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:        CUDA0 model buffer size =   839.23 MiB\n    Jul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:          CPU model buffer size =   333.84 MiB\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: constructing llama_context\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_seq_max     = 1\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx         = 32768\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq = 32768\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_batch       = 512\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ubatch      = 512\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: causal_attn   = 1\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: flash_attn    = 0\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_base     = 1000000.0\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_scale    = 1\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq (32768) &lt; n_ctx_train (40960) -- the full capacity of the model will not be utilized\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context:        CPU  output buffer size =     0.60 MiB\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: kv_size = 32768, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32\n    Jul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =  3712.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  = 4608.00 MiB, K (f16): 2304.00 MiB, V (f16): 2304.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =  2328.00 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    72.01 MiB\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 381 (with bs=512), 61 (with bs=1)\n    Jul 27 11:54:11 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:11.175+02:00 level=INFO source=server.go:637 msg=\"llama runner started in 5.02 seconds\n\nthanks in advance.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "low perfomance on Contionue extension Vs code",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mau9os",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_11kzex88x8",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753642277,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hello guys, I am just new here.&lt;/p&gt;\n\n&lt;p&gt;I installed ollama and runing model qwen3:8b&lt;br/&gt;\nWhen I run iot through terminal, I get full utilisation of the GPU (3060 Mobile 60W).&lt;br/&gt;\nbut slow response and bad utilisation when run in VS Code.&lt;br/&gt;\nprovided some of my debug log-&lt;/p&gt;\n\n&lt;p&gt;ubuntu terminal:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;$ ollama ps\nNAME        ID              SIZE      PROCESSOR          UNTIL              \nqwen3:8b    500a1f067a9f    6.5 GB    10%/90% CPU/GPU    4 minutes from now \n\nudo journalctl -u ollama -f\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   560.00 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  =  576.00 MiB, K (f16):  288.00 MiB, V (f16):  288.00 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =   791.61 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    16.01 MiB\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 17 (with bs=512), 5 (with bs=1)\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:49:14.189+02:00 level=INFO source=server.go:637 msg=&amp;quot;llama runner started in 1.51 seconds&amp;quot;\nJul 27 11:49:14 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:49:14 | 200 |  2.029277689s |       127.0.0.1 | POST     &amp;quot;/api/generate&amp;quot;\nJul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\nJul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;when I run through the continue chat in VS Code&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;ollama ps\nNAME        ID              SIZE     PROCESSOR          UNTIL               \nqwen3:8b    500a1f067a9f    13 GB    58%/42% CPU/GPU    29 minutes from now \n\nsudo journalctl -u ollama -f\n[sudo] password for abdelrahman: \nJul 27 11:50:00 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:50:00 | 200 |  4.942696751s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\nJul 27 11:51:40 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:51:40 | 200 | 19.605748657s |       127.0.0.1 | POST     &amp;quot;/api/chat&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     321.358µs |       127.0.0.1 | GET      &amp;quot;/api/tags&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |     249.342µs |       127.0.0.1 | GET      &amp;quot;/api/tags&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   49.584345ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   54.905231ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   57.173959ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:05 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:05 | 200 |   48.834545ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   59.986822ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:53:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:53:06 | 200 |   63.046354ms |       127.0.0.1 | POST     &amp;quot;/api/show&amp;quot;\nJul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      18.856µs |       127.0.0.1 | HEAD     &amp;quot;/&amp;quot;\nJul 27 11:54:01 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:01 | 200 |      73.667µs |       127.0.0.1 | GET      &amp;quot;/api/ps&amp;quot;\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.945+02:00 level=INFO source=server.go:135 msg=&amp;quot;system memory&amp;quot; total=&amp;quot;15.3 GiB&amp;quot; free=&amp;quot;10.4 GiB&amp;quot; free_swap=&amp;quot;2.3 GiB&amp;quot;\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:05.946+02:00 level=INFO source=server.go:175 msg=offload library=cuda layers.requested=-1 layers.model=37 layers.offload=7 layers.split=&amp;quot;&amp;quot; memory.available=&amp;quot;[5.5 GiB]&amp;quot; memory.gpu_overhead=&amp;quot;0 B&amp;quot; memory.required.full=&amp;quot;12.7 GiB&amp;quot; memory.required.partial=&amp;quot;5.4 GiB&amp;quot; memory.required.kv=&amp;quot;4.5 GiB&amp;quot; memory.required.allocations=&amp;quot;[5.4 GiB]&amp;quot; memory.weights.total=&amp;quot;4.5 GiB&amp;quot; memory.weights.repeating=&amp;quot;4.1 GiB&amp;quot; memory.weights.nonrepeating=&amp;quot;486.9 MiB&amp;quot; memory.graph.full=&amp;quot;3.0 GiB&amp;quot; memory.graph.partial=&amp;quot;3.0 GiB&amp;quot;\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [&amp;quot;!&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;&amp;#39;&amp;quot;, ...\nJul 27 11:54:05 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ġ t&amp;quot;,...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- &amp;#39;&amp;lt;|im_start|&amp;gt;...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = ?B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 &amp;#39;Ċ&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 &amp;#39;&amp;lt;|fim_prefix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 &amp;#39;&amp;lt;|fim_suffix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 &amp;#39;&amp;lt;|fim_middle|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load: vocab only - skipping tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.156+02:00 level=INFO source=server.go:438 msg=&amp;quot;starting llama server&amp;quot; cmd=&amp;quot;/home/abdelrahman/install_directory/ollama/bin/ollama runner --model /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f --ctx-size 32768 --batch-size 512 --n-gpu-layers 7 --threads 8 --no-mmap --parallel 1 --port 35311&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=sched.go:483 msg=&amp;quot;loaded runners&amp;quot; count=1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:598 msg=&amp;quot;waiting for llama runner to start responding&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.157+02:00 level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server not responding&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.165+02:00 level=INFO source=runner.go:815 msg=&amp;quot;starting go runner&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: ggml_cuda_init: found 1 CUDA devices:\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]:   Device 0: NVIDIA GeForce RTX 3060 Laptop GPU, compute capability 8.6, VMM: yes\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CUDA backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cuda.so\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_backend: loaded CPU backend from /home/abdelrahman/install_directory/ollama/lib/ollama/libggml-cpu-icelake.so\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=500,600,610,700,750,800,860,870,890,900,1200 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.225+02:00 level=INFO source=runner.go:874 msg=&amp;quot;Server listening on 127.0.0.1:35311&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3060 Laptop GPU) - 5617 MiB free\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from /home/abdelrahman/install_directory/ollama/.ollama/blobs/sha256-a3de86cd1c132c822487ededd47a324c50491393e6565cd14bafa40d0b8e686f (version GGUF V3 (latest))\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   0:                       general.architecture str              = qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   1:                               general.type str              = model\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   3:                           general.basename str              = Qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   4:                         general.size_label str              = 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   5:                            general.license str              = apache-2.0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   7:                       qwen3.context_length u32              = 40960\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [&amp;quot;!&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;&amp;#39;&amp;quot;, ...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [&amp;quot;Ġ Ġ&amp;quot;, &amp;quot;ĠĠ ĠĠ&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;Ġ t&amp;quot;,...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- &amp;#39;&amp;lt;|im_start|&amp;gt;...\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  26:               general.quantization_version u32              = 2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - kv  27:                          general.file_type u32              = 15\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f32:  145 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type  f16:   36 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q4_K:  199 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: llama_model_loader: - type q6_K:   19 tensors\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file format = GGUF V3 (latest)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file type   = Q4_K - Medium\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: file size   = 4.86 GiB (5.10 BPW)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:06.408+02:00 level=INFO source=server.go:632 msg=&amp;quot;waiting for server to become available&amp;quot; status=&amp;quot;llm server loading model&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: special tokens cache size = 26\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load: token to piece cache size = 0.9311 MB\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: arch             = qwen3\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab_only       = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_train      = 40960\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd           = 4096\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_layer          = 36\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head           = 32\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_head_kv        = 8\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_rot            = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa            = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_swa_pattern    = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_k    = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_head_v    = 128\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_gqa            = 4\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_k_gqa     = 1024\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_embd_v_gqa     = 1024\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_eps       = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_norm_rms_eps   = 1.0e-06\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_clamp_kqv      = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_max_alibi_bias = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_logit_scale    = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: f_attn_scale     = 0.0e+00\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ff             = 12288\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert         = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_expert_used    = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: causal attn      = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: pooling type     = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope type        = 2\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope scaling     = linear\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_base_train  = 1000000.0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: freq_scale_train = 1\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_ctx_orig_yarn  = 40960\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: rope_finetuned   = unknown\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_conv       = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_inner      = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_d_state      = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_rank      = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: ssm_dt_b_c_rms   = 0\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model type       = 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: model params     = 8.19 B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: general.name     = Qwen3 8B\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: vocab type       = BPE\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_vocab          = 151936\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: n_merges         = 151387\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: BOS token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOS token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOT token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: PAD token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: LF token         = 198 &amp;#39;Ċ&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PRE token    = 151659 &amp;#39;&amp;lt;|fim_prefix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SUF token    = 151661 &amp;#39;&amp;lt;|fim_suffix|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM MID token    = 151660 &amp;#39;&amp;lt;|fim_middle|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM PAD token    = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM REP token    = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: FIM SEP token    = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151643 &amp;#39;&amp;lt;|endoftext|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151645 &amp;#39;&amp;lt;|im_end|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151662 &amp;#39;&amp;lt;|fim_pad|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151663 &amp;#39;&amp;lt;|repo_name|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: EOG token        = 151664 &amp;#39;&amp;lt;|file_sep|&amp;gt;&amp;#39;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: print_info: max token length = 256\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: load_tensors: loading model tensors, this can take a while... (mmap = false)\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      21.813µs |       127.0.0.1 | HEAD     &amp;quot;/&amp;quot;\nJul 27 11:54:06 abdelrahman-laptop ollama[143402]: [GIN] 2025/07/27 - 11:54:06 | 200 |      55.253µs |       127.0.0.1 | GET      &amp;quot;/api/ps&amp;quot;\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloading 7 repeating layers to GPU\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors: offloaded 7/37 layers to GPU\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:    CUDA_Host model buffer size =  3804.56 MiB\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:        CUDA0 model buffer size =   839.23 MiB\nJul 27 11:54:07 abdelrahman-laptop ollama[143402]: load_tensors:          CPU model buffer size =   333.84 MiB\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: constructing llama_context\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_seq_max     = 1\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx         = 32768\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq = 32768\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_batch       = 512\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ubatch      = 512\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: causal_attn   = 1\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: flash_attn    = 0\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_base     = 1000000.0\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: freq_scale    = 1\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context: n_ctx_per_seq (32768) &amp;lt; n_ctx_train (40960) -- the full capacity of the model will not be utilized\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_context:        CPU  output buffer size =     0.60 MiB\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: kv_size = 32768, type_k = &amp;#39;f16&amp;#39;, type_v = &amp;#39;f16&amp;#39;, n_layer = 36, can_shift = 1, padding = 32\nJul 27 11:54:09 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:      CUDA0 KV buffer size =   896.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified:        CPU KV buffer size =  3712.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_kv_cache_unified: KV self size  = 4608.00 MiB, K (f16): 2304.00 MiB, V (f16): 2304.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:      CUDA0 compute buffer size =  2328.00 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context:  CUDA_Host compute buffer size =    72.01 MiB\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph nodes  = 1374\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: llama_context: graph splits = 381 (with bs=512), 61 (with bs=1)\nJul 27 11:54:11 abdelrahman-laptop ollama[143402]: time=2025-07-27T11:54:11.175+02:00 level=INFO source=server.go:637 msg=&amp;quot;llama runner started in 5.02 seconds\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;thanks in advance.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mau9os",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "0-sigma-0",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mau9os/low_perfomance_on_contionue_extension_vs_code/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mau9os/low_perfomance_on_contionue_extension_vs_code/",
            "subreddit_subscribers": 505617,
            "created_utc": 1753642277,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hbmro",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ForsookComparison",
            "can_mod_post": false,
            "created_utc": 1753643057,
            "send_replies": true,
            "parent_id": "t3_1mau9os",
            "score": 2,
            "author_fullname": "t2_on5es7pe3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Don't use the bundled Ollama install wrappers that Continue's quickstart guide suggests.\n\nUninstall Ollama\n\nInstall Llama CPP\n\nStart up Llama Server\n\nAnd set up Continue's config to point to/use your localhost endpoint",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hbmro",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t use the bundled Ollama install wrappers that Continue&amp;#39;s quickstart guide suggests.&lt;/p&gt;\n\n&lt;p&gt;Uninstall Ollama&lt;/p&gt;\n\n&lt;p&gt;Install Llama CPP&lt;/p&gt;\n\n&lt;p&gt;Start up Llama Server&lt;/p&gt;\n\n&lt;p&gt;And set up Continue&amp;#39;s config to point to/use your localhost endpoint&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau9os/low_perfomance_on_contionue_extension_vs_code/n5hbmro/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753643057,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mau9os",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ibkg4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Clear-Ad-9312",
            "can_mod_post": false,
            "created_utc": 1753654210,
            "send_replies": true,
            "parent_id": "t3_1mau9os",
            "score": 0,
            "author_fullname": "t2_13gn4f8kdq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "my guy, look at the amount of Gigs that are in use that the ps command is showing you. [continue.dev](http://continue.dev) is likely using a lot higher context window.\n\nif you want to improve performance, either:\n\n* get more VRAM(like eGPU or new computer that is acting as a server or for personal/professional use)\n* switch to a smaller model like `qwen3:4b` (or use a lower model quant size, but not recommended)\n* or just reduce the context window from the default that continue is using, Ollama is using 4096 as default now but continue is requesting 32768.\n   * use a more aggressive KV cache quantization(not recommended)\n\nBTW the new ollama 10.0 version is going to come out with new context window/length detail in the `ollama ps` command, because of this specific issue of users not realizing why there is lower performance.\n\ndo `ollama serve -h` to see environment variables you can when you start ollama. I personally turn on `OLLAMA_FLASH_ATTENTION` and use `OLLAMA_KV_CACHE_TYPE` as q8\\_0 for slightly less VRAM usage, it pretty much almost halves the memory used by the context length.\n\nplay around with the context length setting in Continue's config.yaml\n\nI find that with flash attention and kv cache set to q8\\_0, then qwen3:8b can fit in VRAM with 4096 context length. with qwen3:4b, then I can have 8192 context length(so about double)\n\nin my testing Qwen2.5-Coder-7B-Instruct-128K from unsloth, I seem to be able to have 8192 context length fit comfortable in 6 GB.\n\non the other hand, Qwen2.5-Coder-3B-Instruct-128K can handle about 24576 context length in 5.5 GB, yet for some reason 32768 which should be higher is only using 4.2 GB\n\nto debug this I looked at the logs, and I noticed something interesting about 24576 context length, the logs say:  \nllama\\_context: n\\_ctx         = 49152  \nllama\\_context: n\\_ctx\\_per\\_seq = 24576\n\nfor 32768 context length:  \nllama\\_context: n\\_ctx         = 32768  \nllama\\_context: n\\_ctx\\_per\\_seq = 32768\n\nso for some reason, n\\_ctx is doubled with 24576 context length setting. while the 32768 setting is just same. I think it has something to do with how the model architecture works.  \nhowever, if I use 49152 context length, then I get 5.1GB used and the log says:  \nllama\\_context: n\\_ctx         = 49152  \nllama\\_context: n\\_ctx\\_per\\_seq = 49152\n\nreally, there is something special about what size you end up using, so try it out with various options.",
            "edited": 1753659339,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ibkg4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;my guy, look at the amount of Gigs that are in use that the ps command is showing you. &lt;a href=\"http://continue.dev\"&gt;continue.dev&lt;/a&gt; is likely using a lot higher context window.&lt;/p&gt;\n\n&lt;p&gt;if you want to improve performance, either:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;get more VRAM(like eGPU or new computer that is acting as a server or for personal/professional use)&lt;/li&gt;\n&lt;li&gt;switch to a smaller model like &lt;code&gt;qwen3:4b&lt;/code&gt; (or use a lower model quant size, but not recommended)&lt;/li&gt;\n&lt;li&gt;or just reduce the context window from the default that continue is using, Ollama is using 4096 as default now but continue is requesting 32768.\n\n&lt;ul&gt;\n&lt;li&gt;use a more aggressive KV cache quantization(not recommended)&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;BTW the new ollama 10.0 version is going to come out with new context window/length detail in the &lt;code&gt;ollama ps&lt;/code&gt; command, because of this specific issue of users not realizing why there is lower performance.&lt;/p&gt;\n\n&lt;p&gt;do &lt;code&gt;ollama serve -h&lt;/code&gt; to see environment variables you can when you start ollama. I personally turn on &lt;code&gt;OLLAMA_FLASH_ATTENTION&lt;/code&gt; and use &lt;code&gt;OLLAMA_KV_CACHE_TYPE&lt;/code&gt; as q8_0 for slightly less VRAM usage, it pretty much almost halves the memory used by the context length.&lt;/p&gt;\n\n&lt;p&gt;play around with the context length setting in Continue&amp;#39;s config.yaml&lt;/p&gt;\n\n&lt;p&gt;I find that with flash attention and kv cache set to q8_0, then qwen3:8b can fit in VRAM with 4096 context length. with qwen3:4b, then I can have 8192 context length(so about double)&lt;/p&gt;\n\n&lt;p&gt;in my testing Qwen2.5-Coder-7B-Instruct-128K from unsloth, I seem to be able to have 8192 context length fit comfortable in 6 GB.&lt;/p&gt;\n\n&lt;p&gt;on the other hand, Qwen2.5-Coder-3B-Instruct-128K can handle about 24576 context length in 5.5 GB, yet for some reason 32768 which should be higher is only using 4.2 GB&lt;/p&gt;\n\n&lt;p&gt;to debug this I looked at the logs, and I noticed something interesting about 24576 context length, the logs say:&lt;br/&gt;\nllama_context: n_ctx         = 49152&lt;br/&gt;\nllama_context: n_ctx_per_seq = 24576&lt;/p&gt;\n\n&lt;p&gt;for 32768 context length:&lt;br/&gt;\nllama_context: n_ctx         = 32768&lt;br/&gt;\nllama_context: n_ctx_per_seq = 32768&lt;/p&gt;\n\n&lt;p&gt;so for some reason, n_ctx is doubled with 24576 context length setting. while the 32768 setting is just same. I think it has something to do with how the model architecture works.&lt;br/&gt;\nhowever, if I use 49152 context length, then I get 5.1GB used and the log says:&lt;br/&gt;\nllama_context: n_ctx         = 49152&lt;br/&gt;\nllama_context: n_ctx_per_seq = 49152&lt;/p&gt;\n\n&lt;p&gt;really, there is something special about what size you end up using, so try it out with various options.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau9os/low_perfomance_on_contionue_extension_vs_code/n5ibkg4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753654210,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau9os",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]