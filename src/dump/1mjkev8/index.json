[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey all,\n\nI’m feeling overwhelmed by the huge number of options of chat apis and pricing models out there (openai, gemini, grok, ...) - hoping some of you can help me cut through the noise.\n\n# My use case:\n\n* I want to generate thousands of interesting, high-quality wikipedia summaries (i.e., articles **rewritten from longer wikipedia source** texts)\n* Each around **1000 words**\n* I don't need the chat option, it would just be one **singular prompt per article**\n* They would be used in a **tiktok-like knowledge app**\n* I care about cost per article most of all - ideally I can run thousands of these on a small budget\n* Would &lt; 3$ / 1k articles be unrealistic? (it's just a side-project for now)\n\nI have no idea what to look for or what to expect, but i hope some off y'all could help me out.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best AI-API for mass-generating article summaries (fast + cheap)?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mjkev8",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1v4z55qo0o",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754524241,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754523370,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey all,&lt;/p&gt;\n\n&lt;p&gt;I’m feeling overwhelmed by the huge number of options of chat apis and pricing models out there (openai, gemini, grok, ...) - hoping some of you can help me cut through the noise.&lt;/p&gt;\n\n&lt;h1&gt;My use case:&lt;/h1&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I want to generate thousands of interesting, high-quality wikipedia summaries (i.e., articles &lt;strong&gt;rewritten from longer wikipedia source&lt;/strong&gt; texts)&lt;/li&gt;\n&lt;li&gt;Each around &lt;strong&gt;1000 words&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I don&amp;#39;t need the chat option, it would just be one &lt;strong&gt;singular prompt per article&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;They would be used in a &lt;strong&gt;tiktok-like knowledge app&lt;/strong&gt;&lt;/li&gt;\n&lt;li&gt;I care about cost per article most of all - ideally I can run thousands of these on a small budget&lt;/li&gt;\n&lt;li&gt;Would &amp;lt; 3$ / 1k articles be unrealistic? (it&amp;#39;s just a side-project for now)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I have no idea what to look for or what to expect, but i hope some off y&amp;#39;all could help me out.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjkev8",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Actual-Fee9438",
            "discussion_type": null,
            "num_comments": 10,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/",
            "subreddit_subscribers": 512425,
            "created_utc": 1754523370,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bvrrq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OkStatement3655",
                      "can_mod_post": false,
                      "created_utc": 1754525500,
                      "send_replies": true,
                      "parent_id": "t1_n7bu93h",
                      "score": 2,
                      "author_fullname": "t2_e91kxnzq",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "The price for the 1k articles is not unrealist, since 1 word is round about lets say 1.5 tokens and you want 1000 words, therefore 1500 tokens per article and 1.5 Mio. in total for output tokens, which is 25,5 cents on deepinfra for Gemma 3 27b. Now, we need the input tokens. Lets assume that we have 10k tokens per article (Idk If this is accurate) and for 1k articles that is 10 Mio. tokens, which is about 90 cents.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bvrrq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The price for the 1k articles is not unrealist, since 1 word is round about lets say 1.5 tokens and you want 1000 words, therefore 1500 tokens per article and 1.5 Mio. in total for output tokens, which is 25,5 cents on deepinfra for Gemma 3 27b. Now, we need the input tokens. Lets assume that we have 10k tokens per article (Idk If this is accurate) and for 1k articles that is 10 Mio. tokens, which is about 90 cents.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjkev8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7bvrrq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754525500,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7bu93h",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "OkStatement3655",
            "can_mod_post": false,
            "created_utc": 1754524985,
            "send_replies": true,
            "parent_id": "t3_1mjkev8",
            "score": 1,
            "author_fullname": "t2_e91kxnzq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Deepinfra is cheap.Just test the various models and choose the best one.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bu93h",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Deepinfra is cheap.Just test the various models and choose the best one.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7bu93h/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754524985,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjkev8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7buhrh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Starcast",
            "can_mod_post": false,
            "created_utc": 1754525066,
            "send_replies": true,
            "parent_id": "t3_1mjkev8",
            "score": 1,
            "author_fullname": "t2_3kfxu",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Best is going to change over time.. honestly just pick the cheapest one that kinda functions and build your thing. Then re-run the whole dataset once everything else is in place with whatever model is best then. Inference is only getting cheaper over time.\n\nFor this kind of task my first thought would probably be something like Gemini flash but even that might be overkill.\n\nFind a cheap model on openrouter, then rerunning your data is just changing a line of code.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7buhrh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Best is going to change over time.. honestly just pick the cheapest one that kinda functions and build your thing. Then re-run the whole dataset once everything else is in place with whatever model is best then. Inference is only getting cheaper over time.&lt;/p&gt;\n\n&lt;p&gt;For this kind of task my first thought would probably be something like Gemini flash but even that might be overkill.&lt;/p&gt;\n\n&lt;p&gt;Find a cheap model on openrouter, then rerunning your data is just changing a line of code.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7buhrh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754525066,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjkev8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7bzfk1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Dundell",
            "can_mod_post": false,
            "created_utc": 1754526757,
            "send_replies": true,
            "parent_id": "t3_1mjkev8",
            "score": 1,
            "author_fullname": "t2_3gl53gi6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I don't fully understand the request. This seems like you should just build it out some form of python with some article scraper like newspaper4k or some selenium based on the div holding all the wiki relative info per page, and process it through a local llm. \n\nJust give it some soft-tooling Prompting the LLM to put the  summary in tags like \"&lt;summary&gt; this is the 1000 word summary&lt;/summary&gt;\", and then have the python script to process the LLM's returned answer, to then have the python script ignore all text before the last &lt;/think&gt; and to only accept text in the last pair of &lt;summary&gt; &lt;/summary&gt; tags. Then save the txt into a sqlite db or just simply text file with the name of the webpage or title of the page, or tell the llm during the initial prompt to process a title in \"&lt;title&gt; title here &lt;/title&gt;\"\n\nThen do a 3 attempt, once successful, save and move on to the next page. This could be done with gemini flash 2.5 pretty well (free, although limit 10/min and 250 requests/day per account used), or locally with Qwen 3 30B instruct or thinking if you want it on some form of budget with \"some\" creative writing. \n\nI build reports using 20~60 sources processed through my GLM 4.5 Air Q3 locally and it's x5 slower than Gemini 2.5 Flash was, but better quality output in a report.",
            "edited": 1754527603,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bzfk1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t fully understand the request. This seems like you should just build it out some form of python with some article scraper like newspaper4k or some selenium based on the div holding all the wiki relative info per page, and process it through a local llm. &lt;/p&gt;\n\n&lt;p&gt;Just give it some soft-tooling Prompting the LLM to put the  summary in tags like &amp;quot;&amp;lt;summary&amp;gt; this is the 1000 word summary&amp;lt;/summary&amp;gt;&amp;quot;, and then have the python script to process the LLM&amp;#39;s returned answer, to then have the python script ignore all text before the last &amp;lt;/think&amp;gt; and to only accept text in the last pair of &amp;lt;summary&amp;gt; &amp;lt;/summary&amp;gt; tags. Then save the txt into a sqlite db or just simply text file with the name of the webpage or title of the page, or tell the llm during the initial prompt to process a title in &amp;quot;&amp;lt;title&amp;gt; title here &amp;lt;/title&amp;gt;&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Then do a 3 attempt, once successful, save and move on to the next page. This could be done with gemini flash 2.5 pretty well (free, although limit 10/min and 250 requests/day per account used), or locally with Qwen 3 30B instruct or thinking if you want it on some form of budget with &amp;quot;some&amp;quot; creative writing. &lt;/p&gt;\n\n&lt;p&gt;I build reports using 20~60 sources processed through my GLM 4.5 Air Q3 locally and it&amp;#39;s x5 slower than Gemini 2.5 Flash was, but better quality output in a report.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7bzfk1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754526757,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjkev8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7c180z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Common-Bullfrog6380",
            "can_mod_post": false,
            "created_utc": 1754527383,
            "send_replies": true,
            "parent_id": "t3_1mjkev8",
            "score": 1,
            "author_fullname": "t2_1un7flqwar",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've actually really been liking Grok for this. I am not sure about the budget, but its output doesn't sound all ai-ified (em dashes, Its not \\_\\_\\_ its \\_\\_\\_, etc.) compared to ChatGPT",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7c180z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve actually really been liking Grok for this. I am not sure about the budget, but its output doesn&amp;#39;t sound all ai-ified (em dashes, Its not ___ its ___, etc.) compared to ChatGPT&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7c180z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754527383,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjkev8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7c5a43",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HistorianPotential48",
            "can_mod_post": false,
            "created_utc": 1754528779,
            "send_replies": true,
            "parent_id": "t3_1mjkev8",
            "score": 1,
            "author_fullname": "t2_4dzthia7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "tiktok-like knowledge app oh my god as if those shorts on youtube ain't enough\n\nat least you're doing it in your own app so no one needs to suffer, great job",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7c5a43",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;tiktok-like knowledge app oh my god as if those shorts on youtube ain&amp;#39;t enough&lt;/p&gt;\n\n&lt;p&gt;at least you&amp;#39;re doing it in your own app so no one needs to suffer, great job&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7c5a43/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754528779,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjkev8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7bw5ty",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Kronox_100",
            "can_mod_post": false,
            "created_utc": 1754525633,
            "send_replies": true,
            "parent_id": "t3_1mjkev8",
            "score": 0,
            "author_fullname": "t2_16p6kk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "you could use (abuse lol) the free horizon beta in openrouter, i like how it writes and it's really fast.\n\nother options would be something like qwen in cerebras (incredibly fast, you can check it out [here](https://inference.cerebras.ai/))",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bw5ty",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you could use (abuse lol) the free horizon beta in openrouter, i like how it writes and it&amp;#39;s really fast.&lt;/p&gt;\n\n&lt;p&gt;other options would be something like qwen in cerebras (incredibly fast, you can check it out &lt;a href=\"https://inference.cerebras.ai/\"&gt;here&lt;/a&gt;)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7bw5ty/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754525633,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjkev8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7c1di9",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "CalligrapherAlone133",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7c0xrq",
                                "score": 1,
                                "author_fullname": "t2_1nxsxoxkaf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'll help you again. You can use the OpenRouter free models to generate some, and your local to generate some at the same exact time, doubling your velocity.\n\nJust make sure you are asking the smaller models to generate your articles in pieces, so have it generate a few paragraphs, then ask it to continue, and do this 3-4 times till you get the full article. Don't ask it to plop out a full article for you. Then finally you can pop the whole thing to a bigger model and have it refine it. A lot of ways you can go, but I'd absolutely look at doing this locally if you are thinking about mass content generation.",
                                "edited": 1754527652,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7c1di9",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll help you again. You can use the OpenRouter free models to generate some, and your local to generate some at the same exact time, doubling your velocity.&lt;/p&gt;\n\n&lt;p&gt;Just make sure you are asking the smaller models to generate your articles in pieces, so have it generate a few paragraphs, then ask it to continue, and do this 3-4 times till you get the full article. Don&amp;#39;t ask it to plop out a full article for you. Then finally you can pop the whole thing to a bigger model and have it refine it. A lot of ways you can go, but I&amp;#39;d absolutely look at doing this locally if you are thinking about mass content generation.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjkev8",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7c1di9/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754527438,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754527438,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7c0xrq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Actual-Fee9438",
                      "can_mod_post": false,
                      "created_utc": 1754527283,
                      "send_replies": true,
                      "parent_id": "t1_n7c0qjb",
                      "score": 1,
                      "author_fullname": "t2_1v4z55qo0o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "damn",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7c0xrq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;damn&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjkev8",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7c0xrq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754527283,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7c0qjb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CalligrapherAlone133",
            "can_mod_post": false,
            "created_utc": 1754527213,
            "send_replies": true,
            "parent_id": "t3_1mjkev8",
            "score": 0,
            "author_fullname": "t2_1nxsxoxkaf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "No one tell him. Ugh, your post lacks so much technical knowledge that I just don't like you for being a fake dev. Fine, I'll be nice. You can do this with a 8b model for the cost of your own electricity at home.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7c0qjb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No one tell him. Ugh, your post lacks so much technical knowledge that I just don&amp;#39;t like you for being a fake dev. Fine, I&amp;#39;ll be nice. You can do this with a 8b model for the cost of your own electricity at home.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjkev8/best_aiapi_for_massgenerating_article_summaries/n7c0qjb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754527213,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjkev8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]