[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "It's amazing how fast Qwen3 MoE model is. Why isn't MoE architecture more popular? Unless I am missing something and there are more of interesting MoE models released this year? \n\nIs Mixtral still a thing?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "MoE models in 2025",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mao3ym",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_133m0xy6vg",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753627593,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s amazing how fast Qwen3 MoE model is. Why isn&amp;#39;t MoE architecture more popular? Unless I am missing something and there are more of interesting MoE models released this year? &lt;/p&gt;\n\n&lt;p&gt;Is Mixtral still a thing?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mao3ym",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Acrobatic_Cat_3448",
            "discussion_type": null,
            "num_comments": 25,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/",
            "subreddit_subscribers": 505881,
            "created_utc": 1753627593,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5h7nbu",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Zc5Gwu",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5gsprf",
                                          "score": 1,
                                          "author_fullname": "t2_67qrvlir",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I always kind of think of it like active parameters = smartness and total parameters = world knowledge.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5h7nbu",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I always kind of think of it like active parameters = smartness and total parameters = world knowledge.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mao3ym",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5h7nbu/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753641859,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753641859,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gsprf",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "c3real2k",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gmuu6",
                                "score": 3,
                                "author_fullname": "t2_h7qvk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "\\*me trying to read the papers\\*: I like your funny words, magic man!\n\nI always had the (maybe too narrow) view of sqrt(total\\*active) on MoEs. Especially since it seems to align with my real world experience with the smaller MoEs I tried. Qwen 235B was the first where I thought \"That's pretty impressive.\"\n\nWell, maybe it really is time to think about systems with large quantities of conventional RAM then...",
                                "edited": 1753637809,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gsprf",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;*me trying to read the papers*: I like your funny words, magic man!&lt;/p&gt;\n\n&lt;p&gt;I always had the (maybe too narrow) view of sqrt(total*active) on MoEs. Especially since it seems to align with my real world experience with the smaller MoEs I tried. Qwen 235B was the first where I thought &amp;quot;That&amp;#39;s pretty impressive.&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;Well, maybe it really is time to think about systems with large quantities of conventional RAM then...&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mao3ym",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gsprf/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753637471,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753637471,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5hag9b",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Acrobatic_Cat_3448",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gmuu6",
                                "score": 2,
                                "author_fullname": "t2_133m0xy6vg",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Indeed, I see Qwen MoE and non-MoE roughly on par in my uses!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5hag9b",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Indeed, I see Qwen MoE and non-MoE roughly on par in my uses!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mao3ym",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5hag9b/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753642698,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753642698,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gmuu6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1753635774,
                      "send_replies": true,
                      "parent_id": "t1_n5g873k",
                      "score": 11,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; I'd wish there were more models with the dense equivalent, which, at least for me, would be a lot easier to run (i.e. why do I have to have 300GB (V)RAM for what's basically 118B performance?\n\nFor one, because ERNIE-300B-A47B costs about the same as a 47B model to train vs 118B.\n\nBut more than that, I think that the geometric mean estimation has no real basis in fact.  Maybe for early MoE it was an okay estimate, but a lot of research is indicating that MoE can actually outperform dense models in some problem spaces and for some training conditions:\n\n- [this](https://arxiv.org/abs/2506.12119) finds that given the same training compute a 20% active MoE outperforms the dense model, but consumes more training data\n- [this](https://arxiv.org/abs/2410.19034) finds that MoE performs the same as dense models with the same total parameters on knowledge benchmarks but performs like the geometric mean on math\n- [this](https://arxiv.org/abs/2404.05567v1) seems to find their proposed MoE performs pretty similarly to the equivalently sized dense model and definitely outperforms the geometric mean estimated dense model.\n\nIt's pretty active in terms of research, but I don't think it's fair anymore to say that these models could be replaced by smaller dense models so easily.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gmuu6",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I&amp;#39;d wish there were more models with the dense equivalent, which, at least for me, would be a lot easier to run (i.e. why do I have to have 300GB (V)RAM for what&amp;#39;s basically 118B performance?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;For one, because ERNIE-300B-A47B costs about the same as a 47B model to train vs 118B.&lt;/p&gt;\n\n&lt;p&gt;But more than that, I think that the geometric mean estimation has no real basis in fact.  Maybe for early MoE it was an okay estimate, but a lot of research is indicating that MoE can actually outperform dense models in some problem spaces and for some training conditions:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;a href=\"https://arxiv.org/abs/2506.12119\"&gt;this&lt;/a&gt; finds that given the same training compute a 20% active MoE outperforms the dense model, but consumes more training data&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://arxiv.org/abs/2410.19034\"&gt;this&lt;/a&gt; finds that MoE performs the same as dense models with the same total parameters on knowledge benchmarks but performs like the geometric mean on math&lt;/li&gt;\n&lt;li&gt;&lt;a href=\"https://arxiv.org/abs/2404.05567v1\"&gt;this&lt;/a&gt; seems to find their proposed MoE performs pretty similarly to the equivalently sized dense model and definitely outperforms the geometric mean estimated dense model.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;It&amp;#39;s pretty active in terms of research, but I don&amp;#39;t think it&amp;#39;s fair anymore to say that these models could be replaced by smaller dense models so easily.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gmuu6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753635774,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 11
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5gd0uf",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "limapedro",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5gb2rw",
                                          "score": 3,
                                          "author_fullname": "t2_1knw9sib",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "yeah, that's why I wish DDR6 would come sooner. it'll be cheaper buying 128 GB of RAM than buying that equivalent in VRAM, like a lot cheaper.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5gd0uf",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah, that&amp;#39;s why I wish DDR6 would come sooner. it&amp;#39;ll be cheaper buying 128 GB of RAM than buying that equivalent in VRAM, like a lot cheaper.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mao3ym",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gd0uf/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753632917,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753632917,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gb2rw",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "c3real2k",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gage4",
                                "score": 3,
                                "author_fullname": "t2_h7qvk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah, sure. I bet it also scales better at inference time, serving large batches for API customers.\n\nDoesn't help a salty GPU rig owner that slowly realizes that the meta for running LLMs at home might be shifting towards CPU inference with large amounts of conventional memory :D",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gb2rw",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, sure. I bet it also scales better at inference time, serving large batches for API customers.&lt;/p&gt;\n\n&lt;p&gt;Doesn&amp;#39;t help a salty GPU rig owner that slowly realizes that the meta for running LLMs at home might be shifting towards CPU inference with large amounts of conventional memory :D&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mao3ym",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gb2rw/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753632343,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753632343,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5gedbi",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "limapedro",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5ge37s",
                                          "score": 2,
                                          "author_fullname": "t2_1knw9sib",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I know, I was just replying to the comment above, MoEs are optimal for performance/flop.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5gedbi",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I know, I was just replying to the comment above, MoEs are optimal for performance/flop.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mao3ym",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gedbi/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753633314,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753633314,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5ge37s",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "a_beautiful_rhind",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gage4",
                                "score": 1,
                                "author_fullname": "t2_h5utwre7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Because you are really running a smaller model with more knowledge. But knowledge != intelligence. It's the DLSS of LLMs.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5ge37s",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because you are really running a smaller model with more knowledge. But knowledge != intelligence. It&amp;#39;s the DLSS of LLMs.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mao3ym",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5ge37s/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753633232,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753633232,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gage4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "limapedro",
                      "can_mod_post": false,
                      "created_utc": 1753632159,
                      "send_replies": true,
                      "parent_id": "t1_n5g873k",
                      "score": 6,
                      "author_fullname": "t2_1knw9sib",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "MoE is best for speed, labs use them because it's faster to train, it's still crazy than Meta trained a 405B dense model.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gage4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MoE is best for speed, labs use them because it&amp;#39;s faster to train, it&amp;#39;s still crazy than Meta trained a 405B dense model.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gage4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753632159,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5g9ahm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "TaroOk7112",
                      "can_mod_post": false,
                      "created_utc": 1753631813,
                      "send_replies": true,
                      "parent_id": "t1_n5g873k",
                      "score": 1,
                      "author_fullname": "t2_tsjh0dua",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Also dots.llm1 143B 14B.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5g9ahm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also dots.llm1 143B 14B.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5g9ahm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753631813,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5hcfr6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ArchdukeofHyperbole",
                      "can_mod_post": false,
                      "created_utc": 1753643303,
                      "send_replies": true,
                      "parent_id": "t1_n5g873k",
                      "score": 1,
                      "author_fullname": "t2_1p41v97q5d",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ling lite as well ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5hcfr6",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ling lite as well &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5hcfr6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753643303,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5gxtq3",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "MelodicRecognition7",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5gbf9w",
                                          "score": 1,
                                          "author_fullname": "t2_1eex9ug5",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "yes my approach is not exactly correct either because Kimi stands out from the crowd but in my tests others perform similar to 2x their active parameters. Hunyuan for example was a huge disappointment and performed worse than 27B Gemma so it is more like 1.5x13B. Also I think I saw somewhere a formula like 0.1*(total)+(active) so Qwen would be 3+3=6B dense equivalent, Kimi would be 100+32B=132B equivalent and Hunyan 8+13=21B which looks more like what I've experienced.",
                                          "edited": 1753639135,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5gxtq3",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes my approach is not exactly correct either because Kimi stands out from the crowd but in my tests others perform similar to 2x their active parameters. Hunyuan for example was a huge disappointment and performed worse than 27B Gemma so it is more like 1.5x13B. Also I think I saw somewhere a formula like 0.1*(total)+(active) so Qwen would be 3+3=6B dense equivalent, Kimi would be 100+32B=132B equivalent and Hunyan 8+13=21B which looks more like what I&amp;#39;ve experienced.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mao3ym",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gxtq3/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753638934,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753638934,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gbf9w",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "c3real2k",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5gayv7",
                                "score": 2,
                                "author_fullname": "t2_h7qvk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Possible. I used the ol' sqrt(ParamsTotal\\*ParamsActive). \n\nEdit: Although, come to think of it, that wouldn't quite fit with i.e. Kimi. Kimi would therefor only be a 64B equivalent (2\\*32B), which would be disastrous for 1000B total params. Also, from what I read, it's \"much better\" than what one would expect from something in the 60B range.",
                                "edited": 1753632815,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gbf9w",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Possible. I used the ol&amp;#39; sqrt(ParamsTotal*ParamsActive). &lt;/p&gt;\n\n&lt;p&gt;Edit: Although, come to think of it, that wouldn&amp;#39;t quite fit with i.e. Kimi. Kimi would therefor only be a 64B equivalent (2*32B), which would be disastrous for 1000B total params. Also, from what I read, it&amp;#39;s &amp;quot;much better&amp;quot; than what one would expect from something in the 60B range.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mao3ym",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gbf9w/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753632446,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1753632446,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5gayv7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MelodicRecognition7",
                      "can_mod_post": false,
                      "created_utc": 1753632311,
                      "send_replies": true,
                      "parent_id": "t1_n5g873k",
                      "score": 0,
                      "author_fullname": "t2_1eex9ug5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "you are too optimistic with these equivalent values, IMO the dense equivalent of MoE models is about 2x of their active parameters, so A3B is ~6B dense, A22B is ~44B dense and so on.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5gayv7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you are too optimistic with these equivalent values, IMO the dense equivalent of MoE models is about 2x of their active parameters, so A3B is ~6B dense, A22B is ~44B dense and so on.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gayv7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753632311,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5kzj55",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "cantgetthistowork",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5grucm",
                                "score": 1,
                                "author_fullname": "t2_j1i0o",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Hybrid inference has minutes long prompt processing. I would continue scaling GPU only if I could find a way to physically fit more than 15 GPUs on a single machine because the speeds are still unbeatable when you're trying to do something with real world context of 128-256k.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5kzj55",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hybrid inference has minutes long prompt processing. I would continue scaling GPU only if I could find a way to physically fit more than 15 GPUs on a single machine because the speeds are still unbeatable when you&amp;#39;re trying to do something with real world context of 128-256k.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mao3ym",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5kzj55/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753696975,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753696975,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5grucm",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Double_Cause4609",
                      "can_mod_post": false,
                      "created_utc": 1753637221,
                      "send_replies": true,
                      "parent_id": "t1_n5g873k",
                      "score": -1,
                      "author_fullname": "t2_1kubzxt2ww",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Huh?\n\nYou have it backwards.\n\nMoE is waaaaaay easier to run. MoE models open a lot of crazy optimizations like hybrid CPU + GPU inference (don't forget, even if you think you're doing GPU inference, that's generally run on a hypervisor with a CPU sitting there doing nothing), and MoE models actually scale better with batching due to a few weird things involving arithmatic intensity etc, and can actually come closer to full utilization of a GPU. The team from Corsair doing D-Matrix gave a talk on GPU Mode about this and they noted this with a chart that MoE models actually scale better with batching (on traditional hardware) as a random aside in their presentation.\n\nNow, there's a lot of people who didn't realize what way things were going in 2023 (all the information was publicly available; anybody in the know could have known if they chose to research it), and invested super heavily into mid-sized GPU clusters that were optimized for the now antiquated 32B+ dense LLM size and are salty about MoEs becoming popular, but that's more of a skill issue than the fault of an MoE.\n\nBesides, as I noted, there's a lot of optimizations you can do for small and mid scale deployments exploiting even just a bit of CPU to get some really impressive results.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5grucm",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Huh?&lt;/p&gt;\n\n&lt;p&gt;You have it backwards.&lt;/p&gt;\n\n&lt;p&gt;MoE is waaaaaay easier to run. MoE models open a lot of crazy optimizations like hybrid CPU + GPU inference (don&amp;#39;t forget, even if you think you&amp;#39;re doing GPU inference, that&amp;#39;s generally run on a hypervisor with a CPU sitting there doing nothing), and MoE models actually scale better with batching due to a few weird things involving arithmatic intensity etc, and can actually come closer to full utilization of a GPU. The team from Corsair doing D-Matrix gave a talk on GPU Mode about this and they noted this with a chart that MoE models actually scale better with batching (on traditional hardware) as a random aside in their presentation.&lt;/p&gt;\n\n&lt;p&gt;Now, there&amp;#39;s a lot of people who didn&amp;#39;t realize what way things were going in 2023 (all the information was publicly available; anybody in the know could have known if they chose to research it), and invested super heavily into mid-sized GPU clusters that were optimized for the now antiquated 32B+ dense LLM size and are salty about MoEs becoming popular, but that&amp;#39;s more of a skill issue than the fault of an MoE.&lt;/p&gt;\n\n&lt;p&gt;Besides, as I noted, there&amp;#39;s a lot of optimizations you can do for small and mid scale deployments exploiting even just a bit of CPU to get some really impressive results.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5grucm/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753637221,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5i7wrf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "createthiscom",
                      "can_mod_post": false,
                      "created_utc": 1753653024,
                      "send_replies": true,
                      "parent_id": "t1_n5g873k",
                      "score": 0,
                      "author_fullname": "t2_ozxxf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "\\&gt; why do I have to have 300GB (V)RAM for what's basically 118B performance?\n\nI don't think kimi-k2 and qwen3-coder Q4\\_K\\_XL would be nearly as smart without that 300-400gb of system RAM. I've got 96gb of VRAM. Show me a 70B model that gets an equivalent level of intelligence and I'll run it instead. Also, that same model will still run in 48gb VRAM, just not as quickly. I think people are trying to make these models more accessible and they're succeeding.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5i7wrf",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; why do I have to have 300GB (V)RAM for what&amp;#39;s basically 118B performance?&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t think kimi-k2 and qwen3-coder Q4_K_XL would be nearly as smart without that 300-400gb of system RAM. I&amp;#39;ve got 96gb of VRAM. Show me a 70B model that gets an equivalent level of intelligence and I&amp;#39;ll run it instead. Also, that same model will still run in 48gb VRAM, just not as quickly. I think people are trying to make these models more accessible and they&amp;#39;re succeeding.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5i7wrf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753653024,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5g873k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "c3real2k",
            "can_mod_post": false,
            "created_utc": 1753631489,
            "send_replies": true,
            "parent_id": "t3_1mao3ym",
            "score": 13,
            "author_fullname": "t2_h7qvk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'd say it's quite the opposite. Many of the recent models are MoEs (unfortunately imho):\n\n\\- Qwen3 30B A3B (approx. 9B dense equivalent)  \n\\- Qwen3 235B A22B (approx. 72B dense equivalent)  \n\\- Kimi2 1000B A32B (approx. 179B dense equivalent)  \n\\- Hunyuan 80B A13B (approx. 32B dense equivalent)  \n\\- ERNIE 21B A3B (approx. 8B dense equivalent)  \n\\- ERNIE 300B A47B (approx. 118B dense equivalent)  \n\\- AI21 Jamba Large 398B A94B (approx. 193B dense equivalent)  \n\\- AI21 Jamba Mini 52B A12B (approx. 25B dense equivalent)\n\nMaybe there were more, those were at the top of my head (did InternLM also release a MoE?).\n\nI'd wish there were more models with the dense equivalent, which, at least for me, would be a lot easier to run (i.e. why do I have to have 300GB (V)RAM for what's basically 118B performance? I can fit 118B with a decent quant no problem. 300B? Not so much, or heavily quantized...).",
            "edited": 1753631937,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5g873k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d say it&amp;#39;s quite the opposite. Many of the recent models are MoEs (unfortunately imho):&lt;/p&gt;\n\n&lt;p&gt;- Qwen3 30B A3B (approx. 9B dense equivalent)&lt;br/&gt;\n- Qwen3 235B A22B (approx. 72B dense equivalent)&lt;br/&gt;\n- Kimi2 1000B A32B (approx. 179B dense equivalent)&lt;br/&gt;\n- Hunyuan 80B A13B (approx. 32B dense equivalent)&lt;br/&gt;\n- ERNIE 21B A3B (approx. 8B dense equivalent)&lt;br/&gt;\n- ERNIE 300B A47B (approx. 118B dense equivalent)&lt;br/&gt;\n- AI21 Jamba Large 398B A94B (approx. 193B dense equivalent)&lt;br/&gt;\n- AI21 Jamba Mini 52B A12B (approx. 25B dense equivalent)&lt;/p&gt;\n\n&lt;p&gt;Maybe there were more, those were at the top of my head (did InternLM also release a MoE?).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d wish there were more models with the dense equivalent, which, at least for me, would be a lot easier to run (i.e. why do I have to have 300GB (V)RAM for what&amp;#39;s basically 118B performance? I can fit 118B with a decent quant no problem. 300B? Not so much, or heavily quantized...).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5g873k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753631489,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mao3ym",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 13
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5g3qfb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "JacketHistorical2321",
                      "can_mod_post": false,
                      "created_utc": 1753630164,
                      "send_replies": true,
                      "parent_id": "t1_n5g04io",
                      "score": 1,
                      "author_fullname": "t2_bsvkuoyj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "No they aren't.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5g3qfb",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No they aren&amp;#39;t.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mao3ym",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5g3qfb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753630164,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 1,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "total_awards_received": 0,
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "ups": 0,
                      "removal_reason": null,
                      "link_id": "t3_1mao3ym",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5gsmuj",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Double_Cause4609",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5ggw6h",
                                "score": 3,
                                "author_fullname": "t2_1kubzxt2ww",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I think what u/Simple\\_Split5074 meant isn't that \"all existing models above 70B are MoE\", but rather, that all recent and new models in that category are MoE.\n\nAll the models that you listed are quite old by LLM standards, and there's been a huge shift towards MoE as a scaling method, so effectively all recent models above around 30B parameters have been MoE effectively (outside of fine tunes or NAS on existing models like Nemotron).\n\nI'm not sure exactly when the cutoff was, but it seems like most models that are quite large this year have been MoE, other than the most recent which is...Command-A, which released only technically this year (it was literally at the very start).\n\nI think this is probably emblematic of a trend towards MoE going forward and there's probably not going to be that many new dense models outside of specific orgs that need dense models for some internal reason.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5gsmuj",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think what &lt;a href=\"/u/Simple\"&gt;u/Simple&lt;/a&gt;_Split5074 meant isn&amp;#39;t that &amp;quot;all existing models above 70B are MoE&amp;quot;, but rather, that all recent and new models in that category are MoE.&lt;/p&gt;\n\n&lt;p&gt;All the models that you listed are quite old by LLM standards, and there&amp;#39;s been a huge shift towards MoE as a scaling method, so effectively all recent models above around 30B parameters have been MoE effectively (outside of fine tunes or NAS on existing models like Nemotron).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure exactly when the cutoff was, but it seems like most models that are quite large this year have been MoE, other than the most recent which is...Command-A, which released only technically this year (it was literally at the very start).&lt;/p&gt;\n\n&lt;p&gt;I think this is probably emblematic of a trend towards MoE going forward and there&amp;#39;s probably not going to be that many new dense models outside of specific orgs that need dense models for some internal reason.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mao3ym",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5gsmuj/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753637448,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753637448,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ggw6h",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": "DELETED",
                      "no_follow": true,
                      "author": "[deleted]",
                      "can_mod_post": false,
                      "send_replies": true,
                      "parent_id": "t1_n5g04io",
                      "score": 0,
                      "approved_by": null,
                      "report_reasons": null,
                      "all_awardings": [],
                      "subreddit_id": "t5_81eyvm",
                      "body": "[deleted]",
                      "edited": false,
                      "author_flair_css_class": null,
                      "collapsed": true,
                      "downs": 0,
                      "is_submitter": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "associated_award": null,
                      "stickied": false,
                      "subreddit_type": "public",
                      "can_gild": false,
                      "top_awarded_type": null,
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5ggw6h/",
                      "num_reports": null,
                      "locked": false,
                      "name": "t1_n5ggw6h",
                      "created": 1753634043,
                      "subreddit": "LocalLLaMA",
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "created_utc": 1753634043,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "mod_note": null,
                      "distinguished": null
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5g04io",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Simple_Split5074",
            "can_mod_post": false,
            "created_utc": 1753629066,
            "send_replies": true,
            "parent_id": "t3_1mao3ym",
            "score": 2,
            "author_fullname": "t2_13b59iulvl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "All the models above 70b are MoE so not sure what exactly you mean",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5g04io",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;All the models above 70b are MoE so not sure what exactly you mean&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5g04io/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753629066,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mao3ym",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5k5c38",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Acrobatic_Cat_3448",
            "can_mod_post": false,
            "created_utc": 1753679897,
            "send_replies": true,
            "parent_id": "t3_1mao3ym",
            "score": 1,
            "author_fullname": "t2_133m0xy6vg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is there a handy way to estimate the quality of a MoE vs non-MoE model?\n\nQwen3 30B A3B is much better than a 3B model, and often close to Qwen3-30B.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5k5c38",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is there a handy way to estimate the quality of a MoE vs non-MoE model?&lt;/p&gt;\n\n&lt;p&gt;Qwen3 30B A3B is much better than a 3B model, and often close to Qwen3-30B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5k5c38/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753679897,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mao3ym",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5kqqfz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mart-McUH",
            "can_mod_post": false,
            "created_utc": 1753691899,
            "send_replies": true,
            "parent_id": "t3_1mao3ym",
            "score": 1,
            "author_fullname": "t2_q3eqbw2b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If anything MoE is too popular nowadays, no new open dense models in 70B+ released recently afaik.\n\nAnd at least for me MoE underperforms. Eg 70B L3 even in 4bpw is still better for in creative writing/RP at actually understanding text and what is happening. MoE's today (unless huge ones) just have too little active parameters.\n\nBut 8x22B Mixtral (or WizardLM 2) was actually good at it too (at least for that age), but that one had 44B active parameters which is nowadays unseen unless it is really huge MoE and impractical to run locally.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5kqqfz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If anything MoE is too popular nowadays, no new open dense models in 70B+ released recently afaik.&lt;/p&gt;\n\n&lt;p&gt;And at least for me MoE underperforms. Eg 70B L3 even in 4bpw is still better for in creative writing/RP at actually understanding text and what is happening. MoE&amp;#39;s today (unless huge ones) just have too little active parameters.&lt;/p&gt;\n\n&lt;p&gt;But 8x22B Mixtral (or WizardLM 2) was actually good at it too (at least for that age), but that one had 44B active parameters which is nowadays unseen unless it is really huge MoE and impractical to run locally.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5kqqfz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753691899,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mao3ym",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "body": "It should be more popular. It's amazing",
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5fw6a1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Illustrious-Dot-6888",
            "can_mod_post": false,
            "created_utc": 1753627830,
            "send_replies": true,
            "parent_id": "t3_1mao3ym",
            "score": 2,
            "author_fullname": "t2_gelzgtkby",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "author_cakeday": true,
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5fw6a1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It should be more popular. It&amp;#39;s amazing&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mao3ym/moe_models_in_2025/n5fw6a1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753627830,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mao3ym",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]