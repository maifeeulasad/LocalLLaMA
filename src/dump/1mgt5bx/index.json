[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm not a professional LLMer by any means, but I figured I'd lay out my little journey and the findings along the way.\n\nWhen I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It's so easy to get started and also to pull models.\n\nThe real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.\n\nI had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own \"go template\" setup.\n\nWell, although I don't know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.\n\nI decided to try out llama.cpp, with my own build. Well, it actually wasn't that bad. I was able to get ROCm installed pretty easy and from there it didn't take long to figure out how to compile llama.cpp. \n\nllama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.\n\nAfter playing around with llama.cpp for a while, I was totally hooked. I ended up running into \"llama-swap\", which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. \n\nNow I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.\n\nAdditionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap's config file instead of having to mess around with Modelfiles and so on.\n\nI also really appreciated that I could just keep a directory of .gguf files instead of the \"layering\" system (seemingly inspired by Docker) used by ollama.\n\nI had heard it said that ollama is \"just\" a llama.cpp wrapper, which isn't entirely fair, because they seem to do a lot of work to make it easy to install...\n\nIt was also pretty easy to tie random programs into \"llama-server\" since it is mostly compliant with OpenAI's APIs.\n\nAnyway, if you're out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:\n\nInterested in a quick and easy setup, don't want to fiddle with parameters too much: ollama. It's really, really easy to get going, has really decent cross platform support and it's easy to pull a model if it's on ollama.ai.\n\nIf you're a Linux goon and don't mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn't entirely sure what they were doing.\n\nI kind of see both of these as a tool on my tool-belt, and I'll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I'm 100% llama-swap/llama.cpp and having a blast!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "A rambling post on ollama / llama.cpp and when to use each. Pros and cons and everything in between.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgt5bx",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.55,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_10iarzku",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754251812,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not a professional LLMer by any means, but I figured I&amp;#39;d lay out my little journey and the findings along the way.&lt;/p&gt;\n\n&lt;p&gt;When I first saw you could run local models on your own hardware, I thought it was so interesting. I played around with ollama a bit, had a lot of fun tying it into random things. It&amp;#39;s so easy to get started and also to pull models.&lt;/p&gt;\n\n&lt;p&gt;The real kicker was when I got a 2x AMD MI50 setup. I was having trouble getting decent prompt processing speed out of it. I spent a lot of time looking at the ollama logs on debug, trying to figure out what was going on.&lt;/p&gt;\n\n&lt;p&gt;I had also had a few troubles with using models from huggingface, and I finally figured out that ollama uses its own &amp;quot;go template&amp;quot; setup.&lt;/p&gt;\n\n&lt;p&gt;Well, although I don&amp;#39;t know a super-lot about LLMs, I have spent a lot of time with Linux and editing random config files and compiling things is no big deal.&lt;/p&gt;\n\n&lt;p&gt;I decided to try out llama.cpp, with my own build. Well, it actually wasn&amp;#39;t that bad. I was able to get ROCm installed pretty easy and from there it didn&amp;#39;t take long to figure out how to compile llama.cpp. &lt;/p&gt;\n\n&lt;p&gt;llama.cpp ended up being awesome. I was using the llama-quantize tool to try to quantize my own models, the llama-bench utility was REALLY handy with running experiments with various parameters.&lt;/p&gt;\n\n&lt;p&gt;After playing around with llama.cpp for a while, I was totally hooked. I ended up running into &amp;quot;llama-swap&amp;quot;, which helps me kind of reproduce the setup I have with ollama, where you can load/unload random models on the fly. &lt;/p&gt;\n\n&lt;p&gt;Now I was able to easily use almost any model on huggingface, without much worry about if the templates were going to work.&lt;/p&gt;\n\n&lt;p&gt;Additionally, I found it way easier to play around with various parameters, because it just turned into a quick edit on llama-swap&amp;#39;s config file instead of having to mess around with Modelfiles and so on.&lt;/p&gt;\n\n&lt;p&gt;I also really appreciated that I could just keep a directory of .gguf files instead of the &amp;quot;layering&amp;quot; system (seemingly inspired by Docker) used by ollama.&lt;/p&gt;\n\n&lt;p&gt;I had heard it said that ollama is &amp;quot;just&amp;quot; a llama.cpp wrapper, which isn&amp;#39;t entirely fair, because they seem to do a lot of work to make it easy to install...&lt;/p&gt;\n\n&lt;p&gt;It was also pretty easy to tie random programs into &amp;quot;llama-server&amp;quot; since it is mostly compliant with OpenAI&amp;#39;s APIs.&lt;/p&gt;\n\n&lt;p&gt;Anyway, if you&amp;#39;re out there wondering the ups and downs with ollama/llama.cpp, my guideline would be this:&lt;/p&gt;\n\n&lt;p&gt;Interested in a quick and easy setup, don&amp;#39;t want to fiddle with parameters too much: ollama. It&amp;#39;s really, really easy to get going, has really decent cross platform support and it&amp;#39;s easy to pull a model if it&amp;#39;s on ollama.ai.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re a Linux goon and don&amp;#39;t mind fiddling with config files and compiling your own llama.cpp (although they have binaries), llama.cpp wins hands down for the wide range of configurations you can apply. You get a LOT LOT LOT more control. I also found the documentation of llama.cpp a lot better, where ollama kind of just breezed over some knobs, and I wasn&amp;#39;t entirely sure what they were doing.&lt;/p&gt;\n\n&lt;p&gt;I kind of see both of these as a tool on my tool-belt, and I&amp;#39;ll probably feel free to reach for either one, depending on what my needs are in the future, but for now, I&amp;#39;m 100% llama-swap/llama.cpp and having a blast!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mgt5bx",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "UsualResult",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/",
            "subreddit_subscribers": 509913,
            "created_utc": 1754251812,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6rr2yv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Tyme4Trouble",
                      "can_mod_post": false,
                      "created_utc": 1754259611,
                      "send_replies": true,
                      "parent_id": "t1_n6rcrfs",
                      "score": 5,
                      "author_fullname": "t2_973amyap",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "If you like llama.cpp just wait until you figure out vLLM or SGLang.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6rr2yv",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you like llama.cpp just wait until you figure out vLLM or SGLang.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgt5bx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rr2yv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754259611,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6rnlkg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "UsualResult",
                      "can_mod_post": false,
                      "created_utc": 1754258451,
                      "send_replies": true,
                      "parent_id": "t1_n6rcrfs",
                      "score": 2,
                      "author_fullname": "t2_10iarzku",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; This is supposed to be the easier solution, why is it so much harder?!\n\nI have 0% hate for ollama. They traded off configurability for ease of use. Who knows, maybe if I didn't have that initial really fast setup and use of ollama, maybe I wouldn't have made it to llama.cpp.\n\nI look at ollama like a quick prefab house - drop it in the lot and move in. You don't expect it to exactly fit your needs. It's like a quick pop-up tent in LLM land.\n\nFor those of us that want to move the dial all the way (or nearly all the way) to \"total control\" we can use llama.cpp.\n\nI think there is a time and place for both of them.\n\nIt looks like ollama might be trending towards closing up, well, it's great we have an ecosystem with quite a few choices for running models at home. \n\nOne of the reasons I wanted to write the post is to try and express it doesn't have to be an either-or situation.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6rnlkg",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;This is supposed to be the easier solution, why is it so much harder?!&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I have 0% hate for ollama. They traded off configurability for ease of use. Who knows, maybe if I didn&amp;#39;t have that initial really fast setup and use of ollama, maybe I wouldn&amp;#39;t have made it to llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;I look at ollama like a quick prefab house - drop it in the lot and move in. You don&amp;#39;t expect it to exactly fit your needs. It&amp;#39;s like a quick pop-up tent in LLM land.&lt;/p&gt;\n\n&lt;p&gt;For those of us that want to move the dial all the way (or nearly all the way) to &amp;quot;total control&amp;quot; we can use llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;I think there is a time and place for both of them.&lt;/p&gt;\n\n&lt;p&gt;It looks like ollama might be trending towards closing up, well, it&amp;#39;s great we have an ecosystem with quite a few choices for running models at home. &lt;/p&gt;\n\n&lt;p&gt;One of the reasons I wanted to write the post is to try and express it doesn&amp;#39;t have to be an either-or situation.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgt5bx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rnlkg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754258451,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6tap6w",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "hainesk",
                      "can_mod_post": false,
                      "created_utc": 1754279994,
                      "send_replies": true,
                      "parent_id": "t1_n6rcrfs",
                      "score": 1,
                      "author_fullname": "t2_5rprd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I ended up just using API calls through OpenWebUI and defining the parameters, context and system prompts in custom models there, and then those are ran through Ollama. It was much easier, and more secure since OpenWebUI uses api keys.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6tap6w",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I ended up just using API calls through OpenWebUI and defining the parameters, context and system prompts in custom models there, and then those are ran through Ollama. It was much easier, and more secure since OpenWebUI uses api keys.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgt5bx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6tap6w/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754279994,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6rcrfs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1754254985,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 11,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;I had heard it said that ollama is \"just\" a llama.cpp wrapper, which isn't entirely fair, because they seem to do a lot of work to make it easy to install...\n\nRead your own post again. You're right, it's not just a llama.cpp wrapper. It's so much worse than that. You ran into trouble because instead of wrapping llama.cpp in a user friendly, sane way, they obfuscated it so much that you, a Linux master can't even figure it out. This is supposed to be the easier solution, why is it so much harder?!\n\nThe moment your task isn't using a front page Ollama approved, misnamed Deepseek distill 8b qwen model silently defaulted to q4 that entirely fits inside your VRAM, the wheels literally fall off. And so help you if your prompt isn't \"hi\" so you overflow out of the default context size. Then, if you come here to ask a local LLM expert how to fix it, nobody can help you here because nobody knows. \n\nIt's not experimenting with arguments at the cmd line, no now it's building custom models to define custom parameters on a per model basis and setting 20 environment OLLAMA variables. This is the easy option. \n\nDon't forget to try their new closed source app and buy a t shirt while you're there.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rcrfs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I had heard it said that ollama is &amp;quot;just&amp;quot; a llama.cpp wrapper, which isn&amp;#39;t entirely fair, because they seem to do a lot of work to make it easy to install...&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Read your own post again. You&amp;#39;re right, it&amp;#39;s not just a llama.cpp wrapper. It&amp;#39;s so much worse than that. You ran into trouble because instead of wrapping llama.cpp in a user friendly, sane way, they obfuscated it so much that you, a Linux master can&amp;#39;t even figure it out. This is supposed to be the easier solution, why is it so much harder?!&lt;/p&gt;\n\n&lt;p&gt;The moment your task isn&amp;#39;t using a front page Ollama approved, misnamed Deepseek distill 8b qwen model silently defaulted to q4 that entirely fits inside your VRAM, the wheels literally fall off. And so help you if your prompt isn&amp;#39;t &amp;quot;hi&amp;quot; so you overflow out of the default context size. Then, if you come here to ask a local LLM expert how to fix it, nobody can help you here because nobody knows. &lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s not experimenting with arguments at the cmd line, no now it&amp;#39;s building custom models to define custom parameters on a per model basis and setting 20 environment OLLAMA variables. This is the easy option. &lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t forget to try their new closed source app and buy a t shirt while you&amp;#39;re there.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rcrfs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754254985,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "03eba0e8-72f2-11ee-96eb-9a14648159ce",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "total_awards_received": 0,
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "ups": 1,
                      "removal_reason": null,
                      "link_id": "t3_1mgt5bx",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6u6d3b",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": "DELETED",
                      "no_follow": true,
                      "author": "[deleted]",
                      "can_mod_post": false,
                      "send_replies": true,
                      "parent_id": "t1_n6ruewz",
                      "score": 1,
                      "approved_by": null,
                      "report_reasons": null,
                      "all_awardings": [],
                      "subreddit_id": "t5_81eyvm",
                      "body": "[deleted]",
                      "edited": false,
                      "author_flair_css_class": null,
                      "collapsed": true,
                      "downs": 0,
                      "is_submitter": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "associated_award": null,
                      "stickied": false,
                      "subreddit_type": "public",
                      "can_gild": false,
                      "top_awarded_type": null,
                      "unrepliable_reason": null,
                      "author_flair_text_color": "dark",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6u6d3b/",
                      "num_reports": null,
                      "locked": false,
                      "name": "t1_n6u6d3b",
                      "created": 1754296790,
                      "subreddit": "LocalLLaMA",
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "created_utc": 1754296790,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "mod_note": null,
                      "distinguished": null
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ruewz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "LienniTa",
            "can_mod_post": false,
            "created_utc": 1754260748,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 5,
            "author_fullname": "t2_txs45qja",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "there are no pros of ollama and noone should ever use it, period",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ruewz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "koboldcpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;there are no pros of ollama and noone should ever use it, period&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6ruewz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754260748,
            "author_flair_text": "koboldcpp",
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6rfwjk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754255964,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 3,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; ollama / llama.cpp and when to use each\n\neasy-peasy, if you cannot into computers then use ollama, if you can into computers then use llama.cpp",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rfwjk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;ollama / llama.cpp and when to use each&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;easy-peasy, if you cannot into computers then use ollama, if you can into computers then use llama.cpp&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rfwjk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754255964,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6rnw85",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "UsualResult",
                      "can_mod_post": false,
                      "created_utc": 1754258548,
                      "send_replies": true,
                      "parent_id": "t1_n6rbjqe",
                      "score": 1,
                      "author_fullname": "t2_10iarzku",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "It's not very exotic. It's in a HP Z440 workstation with a 1KW power supply. There are cooling fans on the MI50s.\n\nI'm not sure I'd be able to fit a 3rd or 4th in the case, but for now this is fine. I have seen people using \"octominer\" motherboards and running dual or triple power supply.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6rnw85",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not very exotic. It&amp;#39;s in a HP Z440 workstation with a 1KW power supply. There are cooling fans on the MI50s.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure I&amp;#39;d be able to fit a 3rd or 4th in the case, but for now this is fine. I have seen people using &amp;quot;octominer&amp;quot; motherboards and running dual or triple power supply.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgt5bx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rnw85/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754258548,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ri46f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ttkciar",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6rgvq4",
                                "score": 1,
                                "author_fullname": "t2_cpegz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "&gt; Are we in bizarro world where power comes from motherboards?\n\nIt does on my Supermicro and Dell Xeon motherboards.  The power supplies are proprietary, and either redistribute power via a daughter-card or via 8-pin sockets in the motherboard.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ri46f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Are we in bizarro world where power comes from motherboards?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;It does on my Supermicro and Dell Xeon motherboards.  The power supplies are proprietary, and either redistribute power via a daughter-card or via 8-pin sockets in the motherboard.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgt5bx",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6ri46f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754256670,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754256670,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6rgvq4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Marksta",
                      "can_mod_post": false,
                      "created_utc": 1754256274,
                      "send_replies": true,
                      "parent_id": "t1_n6rbjqe",
                      "score": 0,
                      "author_fullname": "t2_559a1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Are we in bizarro world where power comes from motherboards? I know some ultra proprietary junk does that in consumer micro PCs but don't know about servers doing that. \n\nConfusion aside, you can just use power cable splitters like crazy. Quality PCIe 8 pins are specced way north of 150w at the connector. So just turn all the 2s into 1 PCIe connector only needed. Then the good 1300w+ have like, at least 5. Also can convert any free CPU EPS 8 pins into PCIe 8 pins too. And can double up on PSUs too.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6rgvq4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are we in bizarro world where power comes from motherboards? I know some ultra proprietary junk does that in consumer micro PCs but don&amp;#39;t know about servers doing that. &lt;/p&gt;\n\n&lt;p&gt;Confusion aside, you can just use power cable splitters like crazy. Quality PCIe 8 pins are specced way north of 150w at the connector. So just turn all the 2s into 1 PCIe connector only needed. Then the good 1300w+ have like, at least 5. Also can convert any free CPU EPS 8 pins into PCIe 8 pins too. And can double up on PSUs too.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgt5bx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rgvq4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754256274,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6rbjqe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754254612,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thanks for sharing your thoughts.  As a \"Linux goon\" who loves llama.cpp (for many reasons, including the low-level control you describe), I find myself in agreement.\n\nCan you say more about your dual MI50 setup?  Especially how you keep them powered.  Most motherboards I've found (even server motherboards) lack the four PCIe power rails (two rails per MI50).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rbjqe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for sharing your thoughts.  As a &amp;quot;Linux goon&amp;quot; who loves llama.cpp (for many reasons, including the low-level control you describe), I find myself in agreement.&lt;/p&gt;\n\n&lt;p&gt;Can you say more about your dual MI50 setup?  Especially how you keep them powered.  Most motherboards I&amp;#39;ve found (even server motherboards) lack the four PCIe power rails (two rails per MI50).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rbjqe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754254612,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6rpjik",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "DorphinPack",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6rokce",
                                "score": 1,
                                "author_fullname": "t2_zebuyjw9s",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Hmm sounds like the biggest gap out there might be a true frontend for (ik_)llama.cpp.\n\nOllama will continue to diverge in purpose, I think.\n\nBTW I cannot get specdec to work right but I only tried a few times. What’s your go-to main/draft model combo?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6rpjik",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hmm sounds like the biggest gap out there might be a true frontend for (ik_)llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;Ollama will continue to diverge in purpose, I think.&lt;/p&gt;\n\n&lt;p&gt;BTW I cannot get specdec to work right but I only tried a few times. What’s your go-to main/draft model combo?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgt5bx",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rpjik/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754259096,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754259096,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6rokce",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "UsualResult",
                      "can_mod_post": false,
                      "created_utc": 1754258770,
                      "send_replies": true,
                      "parent_id": "t1_n6rg0db",
                      "score": 1,
                      "author_fullname": "t2_10iarzku",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "One thing I forgot to mention is I also like the \"draft model\" feature for llama.cpp. That really helps me add some tokens/sec.\n\nI feel the same way as you about the layers and templates. I'd rather just have a gguf with a configuration on the side. It's way quicker for me to just edit one than crank out a model file, set up the command line, push it.\n\nOllama layers solve a problem I do not have.\n\nYou're totally right with the KV cache quantization too. I have it set exactly as I want, depending on the model.\n\nI find ollama a bit cagey with their environment variables. It seems to do anything semi-serious you need to use them, but their docs are a bit sparse about what each one does. \n\nI think on Windows, their auto-updating and dock app is a little over-eager as well.\n\nIf there is someone starting out, I'm like 100% sure I would tell them to start with ollama.Unless they are like a confirmed power-user, then it doesn't really matter what I tell them.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6rokce",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;One thing I forgot to mention is I also like the &amp;quot;draft model&amp;quot; feature for llama.cpp. That really helps me add some tokens/sec.&lt;/p&gt;\n\n&lt;p&gt;I feel the same way as you about the layers and templates. I&amp;#39;d rather just have a gguf with a configuration on the side. It&amp;#39;s way quicker for me to just edit one than crank out a model file, set up the command line, push it.&lt;/p&gt;\n\n&lt;p&gt;Ollama layers solve a problem I do not have.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re totally right with the KV cache quantization too. I have it set exactly as I want, depending on the model.&lt;/p&gt;\n\n&lt;p&gt;I find ollama a bit cagey with their environment variables. It seems to do anything semi-serious you need to use them, but their docs are a bit sparse about what each one does. &lt;/p&gt;\n\n&lt;p&gt;I think on Windows, their auto-updating and dock app is a little over-eager as well.&lt;/p&gt;\n\n&lt;p&gt;If there is someone starting out, I&amp;#39;m like 100% sure I would tell them to start with ollama.Unless they are like a confirmed power-user, then it doesn&amp;#39;t really matter what I tell them.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgt5bx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rokce/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754258770,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6rg0db",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DorphinPack",
            "can_mod_post": false,
            "created_utc": 1754255998,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 1,
            "author_fullname": "t2_zebuyjw9s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "100% agree with this.\n\nI hard recommend Ollama to people but tell them to watch out for when they want to start using hybrid inference. I think if you can fit your workload into VRAM then Ollama go brrrrr but the abstraction KILLS the possibility of squeezing extra performance out of modest hardware. Which feels unfortunately counterintuitive given the target audience. It’s a hard problem to solve and I think they’re doing pretty well.\n\nModelfiles having the layering clearly inspired by Docker but only exposing prediction settings and prompt templates is a good example. I get it, esp with them phasing in their in-house backend. But it’s a dealbreaker for me.\n\nEspecially KV cache quantization being global across all models and requiring a restart to change. Using it with certain model/context combos but not others is absurdly high friction. Esp given the high cost of realizing at token ~28K it’s going crazy because the cache was quantized for a problem that required high precision. I think it’s a BIG part of why you see the pushback against any KV quantization — suuuuper easy to get burned.\n\nMy scripts I use to launch my models (which are about to get dropped into llama-swap once they’ve settled a bit) have different context, offload and kv quant settings all aimed around maximizing utilization.\n\nThose presets + Modelfiles would be an AMAZING combo. I’m just mad I’d have to reimplement the layered config behavior lol. As it stands I just have to share prompts by variable and use f-strings.\n\nIMO there is space for a backend-agnostic solution like llama-swap where you plug in your specs, engine, model, settings and benchmark results. And then we can all share our strategies for maximizing utilization.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rg0db",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;100% agree with this.&lt;/p&gt;\n\n&lt;p&gt;I hard recommend Ollama to people but tell them to watch out for when they want to start using hybrid inference. I think if you can fit your workload into VRAM then Ollama go brrrrr but the abstraction KILLS the possibility of squeezing extra performance out of modest hardware. Which feels unfortunately counterintuitive given the target audience. It’s a hard problem to solve and I think they’re doing pretty well.&lt;/p&gt;\n\n&lt;p&gt;Modelfiles having the layering clearly inspired by Docker but only exposing prediction settings and prompt templates is a good example. I get it, esp with them phasing in their in-house backend. But it’s a dealbreaker for me.&lt;/p&gt;\n\n&lt;p&gt;Especially KV cache quantization being global across all models and requiring a restart to change. Using it with certain model/context combos but not others is absurdly high friction. Esp given the high cost of realizing at token ~28K it’s going crazy because the cache was quantized for a problem that required high precision. I think it’s a BIG part of why you see the pushback against any KV quantization — suuuuper easy to get burned.&lt;/p&gt;\n\n&lt;p&gt;My scripts I use to launch my models (which are about to get dropped into llama-swap once they’ve settled a bit) have different context, offload and kv quant settings all aimed around maximizing utilization.&lt;/p&gt;\n\n&lt;p&gt;Those presets + Modelfiles would be an AMAZING combo. I’m just mad I’d have to reimplement the layered config behavior lol. As it stands I just have to share prompts by variable and use f-strings.&lt;/p&gt;\n\n&lt;p&gt;IMO there is space for a backend-agnostic solution like llama-swap where you plug in your specs, engine, model, settings and benchmark results. And then we can all share our strategies for maximizing utilization.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rg0db/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754255998,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6rswsn",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "a_beautiful_rhind",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6roocf",
                                "score": 1,
                                "author_fullname": "t2_h5utwre7",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "There's also YALS, which is like tabbyapi and configured from yml file. No UI to speak of and no long commands.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6rswsn",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s also YALS, which is like tabbyapi and configured from yml file. No UI to speak of and no long commands.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgt5bx",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rswsn/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754260233,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754260233,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6roocf",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "UsualResult",
                      "can_mod_post": false,
                      "created_utc": 1754258807,
                      "send_replies": true,
                      "parent_id": "t1_n6rndij",
                      "score": 1,
                      "author_fullname": "t2_10iarzku",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I don't know. My use case is headless anyway, so I don't even have a use for it.\n\nI'm so sick of the bloated Electron UIs for every little thing.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6roocf",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t know. My use case is headless anyway, so I don&amp;#39;t even have a use for it.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m so sick of the bloated Electron UIs for every little thing.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgt5bx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6roocf/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754258807,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6rndij",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "a_beautiful_rhind",
            "can_mod_post": false,
            "created_utc": 1754258378,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 1,
            "author_fullname": "t2_h5utwre7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "didn't ollama just close source their UI?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rndij",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;didn&amp;#39;t ollama just close source their UI?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6rndij/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754258378,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ro4ok",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "celsowm",
            "can_mod_post": false,
            "created_utc": 1754258627,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 1,
            "author_fullname": "t2_dyvrh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The main problem of llama.cpp is the unified kv cache, but there is a PR on it recently",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ro4ok",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The main problem of llama.cpp is the unified kv cache, but there is a PR on it recently&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6ro4ok/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754258627,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6u49lp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "prusswan",
            "can_mod_post": false,
            "created_utc": 1754295545,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 1,
            "author_fullname": "t2_kegwk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Both llama-swap (with the group management added recently) and llama.cpp is needed to replace ollama functionality, just this alone already puts it out of reach of most ollama users who don't want/need to know how to host a LLM proxy via patchwork of clis.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6u49lp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Both llama-swap (with the group management added recently) and llama.cpp is needed to replace ollama functionality, just this alone already puts it out of reach of most ollama users who don&amp;#39;t want/need to know how to host a LLM proxy via patchwork of clis.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6u49lp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754295545,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6r3id5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bloomlike",
            "can_mod_post": false,
            "created_utc": 1754252200,
            "send_replies": true,
            "parent_id": "t3_1mgt5bx",
            "score": 0,
            "author_fullname": "t2_75qed647",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "f",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6r3id5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;f&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgt5bx/a_rambling_post_on_ollama_llamacpp_and_when_to/n6r3id5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754252200,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgt5bx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]