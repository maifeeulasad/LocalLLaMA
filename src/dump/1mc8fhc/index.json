[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey r/LocalLLaMA üëã!\n\nFor the past 18 months, my colleague and I have been working on **Ebiose**, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).\n\nEbiose aims to create a decentralized AI factory, a Darwin-style playground (√† la Google‚Äôs AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own \"forge,\" define a task, and watch AI agents compete until the fittest emerge.\n\nThis evolutionary approach demands massive inference resources. Currently, we're relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.\n\nThat's why we'd love input from the LocalLLaMA community!\n\n**The Big Idea: A Community-Powered P2P Inference Grid**\n\nWe‚Äôre dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here‚Äôs the plan:\n\n* **Lightweight Client:** A background app runs on your PC (and maybe phones later).\n* **Hardware Profiling:** The client auto-detects what LLMs your machine can handle.\n* **Orchestration Layer:** A system (centralized or decentralized?) assigns inference tasks to capable nodes.\n* **Dynamic LoRA Adapters:** Fine-tune models efficiently with lightweight, modular adapters.\n* **Batch &amp; Prompt Caching:** Optimize for high throughput by batching requests and reusing system prompts.\n\n**Technical Questions for the Community**\n\n1. **Inference Backend:** We‚Äôre leaning toward **llama.cpp** for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a distributed, high-throughput setup, would **vLLM**, **zml**, or another engine be better? Since we‚Äôre prioritizing batch processing over single-prompt speed, what‚Äôs your pick?\n2. **Task Orchestration:** How do we route inference jobs (e.g., ‚Äúrun this 13B model with this prompt‚Äù) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?\n3. **Existing Tools:** Are there open-source projects we could build on?\n\nWhat do you think? Got ideas, tools, or experiences to share?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Let's Build a \"Garage AI Supercomputer\": A P2P Compute Grid for Inference",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mc8fhc",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_6bm8s1wm",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753786996,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; üëã!&lt;/p&gt;\n\n&lt;p&gt;For the past 18 months, my colleague and I have been working on &lt;strong&gt;Ebiose&lt;/strong&gt;, an open-source initiative (MIT license) born at Inria (the French lab behind projects like scikit-learn).&lt;/p&gt;\n\n&lt;p&gt;Ebiose aims to create a decentralized AI factory, a Darwin-style playground (√† la Google‚Äôs AlphaEvolve) where AI agents design, test, and evolve other agents. Anyone can launch their own &amp;quot;forge,&amp;quot; define a task, and watch AI agents compete until the fittest emerge.&lt;/p&gt;\n\n&lt;p&gt;This evolutionary approach demands massive inference resources. Currently, we&amp;#39;re relying on cloud APIs, but our long-term vision is a fully decentralized, community-driven system.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s why we&amp;#39;d love input from the LocalLLaMA community!&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Big Idea: A Community-Powered P2P Inference Grid&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;We‚Äôre dreaming of a peer-to-peer compute grid that taps into the idle power of community-run machines, like Folding@home, but for local LLMs. Here‚Äôs the plan:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Lightweight Client:&lt;/strong&gt; A background app runs on your PC (and maybe phones later).&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Hardware Profiling:&lt;/strong&gt; The client auto-detects what LLMs your machine can handle.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Orchestration Layer:&lt;/strong&gt; A system (centralized or decentralized?) assigns inference tasks to capable nodes.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Dynamic LoRA Adapters:&lt;/strong&gt; Fine-tune models efficiently with lightweight, modular adapters.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Batch &amp;amp; Prompt Caching:&lt;/strong&gt; Optimize for high throughput by batching requests and reusing system prompts.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Technical Questions for the Community&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Inference Backend:&lt;/strong&gt; We‚Äôre leaning toward &lt;strong&gt;llama.cpp&lt;/strong&gt; for its lightweight design and broad hardware support (CPU, Metal, CUDA). But for a distributed, high-throughput setup, would &lt;strong&gt;vLLM&lt;/strong&gt;, &lt;strong&gt;zml&lt;/strong&gt;, or another engine be better? Since we‚Äôre prioritizing batch processing over single-prompt speed, what‚Äôs your pick?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Task Orchestration:&lt;/strong&gt; How do we route inference jobs (e.g., ‚Äúrun this 13B model with this prompt‚Äù) to nodes with the right model cached and enough VRAM/RAM? Has anyone tackled this kind of distributed task management?&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Existing Tools:&lt;/strong&gt; Are there open-source projects we could build on?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;What do you think? Got ideas, tools, or experiences to share?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mc8fhc",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ModeSquare8129",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/",
            "subreddit_subscribers": 506439,
            "created_utc": 1753786996,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5rxcg8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ModeSquare8129",
                      "can_mod_post": false,
                      "created_utc": 1753788920,
                      "send_replies": true,
                      "parent_id": "t1_n5rvtjq",
                      "score": 1,
                      "author_fullname": "t2_6bm8s1wm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for your feedback.  \n  \nThe short answer is yes!\n\nOur forges are designed not only to create agents, but also, in the long term, to generate models, perform fine-tuning, produce code, and create all the reusable building blocks needed to build new agents. More generally, they can be used to solve any type of problem that can benefit from an evolutionary approach.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5rxcg8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for your feedback.  &lt;/p&gt;\n\n&lt;p&gt;The short answer is yes!&lt;/p&gt;\n\n&lt;p&gt;Our forges are designed not only to create agents, but also, in the long term, to generate models, perform fine-tuning, produce code, and create all the reusable building blocks needed to build new agents. More generally, they can be used to solve any type of problem that can benefit from an evolutionary approach.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mc8fhc",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/n5rxcg8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753788920,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5rvtjq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "h3wro",
            "can_mod_post": false,
            "created_utc": 1753788250,
            "send_replies": true,
            "parent_id": "t3_1mc8fhc",
            "score": 2,
            "author_fullname": "t2_lmyx8qt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sounds super cool! Maybe it could be used not only to develop better agents, but generally to solve any kind of problems that require evolutionary approach?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5rvtjq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sounds super cool! Maybe it could be used not only to develop better agents, but generally to solve any kind of problems that require evolutionary approach?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/n5rvtjq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753788250,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mc8fhc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5s3xov",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ModeSquare8129",
                      "can_mod_post": false,
                      "created_utc": 1753791552,
                      "send_replies": true,
                      "parent_id": "t1_n5s15j3",
                      "score": 1,
                      "author_fullname": "t2_6bm8s1wm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's an excellent point, and thank you so much for the detailed and insightful suggestion.\n\nOur long-term vision is actually to support both approaches: running large, distributed models across the network, and running smaller, self-contained models on individual machines.\n\nFor the first part, running large models across many machines, I absolutely agree with your thinking. Our plan isn't to build that complex distributed inference logic from scratch. We'd rather integrate projects like Petals to handle that. Are you familiar with it?\n\nHowever, our immediate priority is to get the second part working: running smaller, complete models locally on user machines, using technologies like llama.cpp. There are two main reasons for this:\n\n1. It seems a more straightforward implementation challenge, which allows us to build and validate the core platform faster.\n2. Our evolutionary approach benefits from having a large population of smaller models running in parallel.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5s3xov",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s an excellent point, and thank you so much for the detailed and insightful suggestion.&lt;/p&gt;\n\n&lt;p&gt;Our long-term vision is actually to support both approaches: running large, distributed models across the network, and running smaller, self-contained models on individual machines.&lt;/p&gt;\n\n&lt;p&gt;For the first part, running large models across many machines, I absolutely agree with your thinking. Our plan isn&amp;#39;t to build that complex distributed inference logic from scratch. We&amp;#39;d rather integrate projects like Petals to handle that. Are you familiar with it?&lt;/p&gt;\n\n&lt;p&gt;However, our immediate priority is to get the second part working: running smaller, complete models locally on user machines, using technologies like llama.cpp. There are two main reasons for this:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;It seems a more straightforward implementation challenge, which allows us to build and validate the core platform faster.&lt;/li&gt;\n&lt;li&gt;Our evolutionary approach benefits from having a large population of smaller models running in parallel.&lt;/li&gt;\n&lt;/ol&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mc8fhc",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/n5s3xov/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753791552,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5s15j3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1753790479,
            "send_replies": true,
            "parent_id": "t3_1mc8fhc",
            "score": 2,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Orchestrating by requiring nodes to run entire models will greatly limit the amount of compute you can use and limit you in practice to small models, since most people have GPUs with 8GB or even less. \n\nA much better solution, but one that requires a lot more development work from your side is to follow the folding@home approach where you break the inference task into it's underlying operations and dispatch those individually to clients. This way, it won't even matter what GPU a client has, or even if the client has any dedicated GPU. You'd dispatch the matrices of individual layers, depending on what compute the client has and/or what they have cached along with the input vectors/matrices and collect the resulting vectors/matrices.\n\nWith this approach, clients would only need to be updated when there's a new type of operation that needs to be supported (ex: a new attention mechanism) instead of the software requiring full inference support for a given model. And while time to token output would be much much higher, I think you'd more than make up for this with the sheer amount of additional compute you'd have access to. You could even integrate some routing intelligence (since you plan this to be p2p) where one client sends their output directly to any of the nodes in the network that host/run the next layer. Clients would need to download much less data, and you can run much larger models over the network and most of the time even run newer models without requiring clients to update.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5s15j3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Orchestrating by requiring nodes to run entire models will greatly limit the amount of compute you can use and limit you in practice to small models, since most people have GPUs with 8GB or even less. &lt;/p&gt;\n\n&lt;p&gt;A much better solution, but one that requires a lot more development work from your side is to follow the folding@home approach where you break the inference task into it&amp;#39;s underlying operations and dispatch those individually to clients. This way, it won&amp;#39;t even matter what GPU a client has, or even if the client has any dedicated GPU. You&amp;#39;d dispatch the matrices of individual layers, depending on what compute the client has and/or what they have cached along with the input vectors/matrices and collect the resulting vectors/matrices.&lt;/p&gt;\n\n&lt;p&gt;With this approach, clients would only need to be updated when there&amp;#39;s a new type of operation that needs to be supported (ex: a new attention mechanism) instead of the software requiring full inference support for a given model. And while time to token output would be much much higher, I think you&amp;#39;d more than make up for this with the sheer amount of additional compute you&amp;#39;d have access to. You could even integrate some routing intelligence (since you plan this to be p2p) where one client sends their output directly to any of the nodes in the network that host/run the next layer. Clients would need to download much less data, and you can run much larger models over the network and most of the time even run newer models without requiring clients to update.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mc8fhc/lets_build_a_garage_ai_supercomputer_a_p2p/n5s15j3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753790479,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mc8fhc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]