[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "This is pretty much a catchall post for things people asked about in my first two posts about the Max+ 395. That being how/if it works for distributed LLM inference and image/video gen. It works for both those things.\n\nLet's start with distributed LLM inference. TBH, I'm pretty surprise the numbers hold up as well as they do. Since IME there's a pretty significant performance penalty for going multi-gpu. I ballpark it to be about 50%. In this case, though, it's better than that. That is probably because I'm using a dynamic quant of a MOE. Where the heavy lifting is done by the X2 and the leftovers are on the Mac. Anyways here are the numbers first for the X2 alone and then working with a M1 Max.\n\n    Max+\n    ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa | mmap |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           pp512 |        112.27 ± 0.38 |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           tg128 |         20.29 ± 0.02 |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  pp512 @ d10000 |         60.61 ± 0.34 |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  tg128 @ d10000 |         15.36 ± 0.03 |\n    \n    Max+ with M1 Max\n    ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n    | model                          |       size |     params | backend    | ngl | fa | mmap |            test |                  t/s |\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           pp512 |        101.53 ± 2.69 |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           tg128 |         13.90 ± 4.29 |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  pp512 @ d10000 |         56.71 ± 0.33 |\n    | glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  tg128 @ d10000 |          9.56 ± 0.12 |\n\nHere are the numbers for doing SD 1.5 image gens. Both at 512x512 and 1024x1024.\n\n**SD 1.5 512x512**\n\n    Max+\n    100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01&lt;00:00, 11.58it/s]\n    Prompt executed in 2.21 seconds\n    \n    7900xtx\n    100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01&lt;00:00, 18.54it/s]\n    Prompt executed in 1.24 seconds\n    \n    3060\n    100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02&lt;00:00,  8.86it/s]\n    Prompt executed in 2.60 seconds\n\n**SD 1.5 1024x1024**\n    \n    Max+\n    100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:11&lt;00:00,  1.69it/s]\n    Prompt executed in 13.70 seconds\n    \n    7900xtx\n    100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:07&lt;00:00,  2.58it/s]\n    Prompt executed in 8.69 seconds\n    \n    3060\n    100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:10&lt;00:00,  1.84it/s]\n    Prompt executed in 12.12 seconds\n\nLastly, here are some video gen numbers. This is for Wan 2.2. It's at 480x320 resolution since ROCm support for the Max+ 395 is still a work in progress. Under Windows it's fast but only works with about 32GB of RAM max before things go bad. Under Linux it doesn't seem to have that RAM limit but it's really really slow. Like 200 secs/iteration slow. Yes, I verified that it is using the GPU and not the CPU. So these results are from Windows. But because of the memory limit, I had to crank down the resolution. I'm using the Phr00t Wan 2.2 14B AIO.\n\n**Wan 2.2 480x320x41**\n    \n    Max+\n    100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:42&lt;00:00, 25.69s/it]\n    Prompt executed in 194.01 seconds\n    \n    7900xtx\n    100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:19&lt;00:00,  4.77s/it]\n    Prompt executed in 140.08 seconds\n    \n    3060\n    100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:01&lt;00:00, 15.44s/it]\n    Prompt executed in 133.89 seconds\n\nSo just like with the other two posts in this series, the Max+ 395 is basically a 128GB 3060.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "GMK X2(AMD Max+ 395 w/128GB) third impressions, RPC and Image/Video gen.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mkokj2",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 14,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_o65i6kx",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 14,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754637211,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is pretty much a catchall post for things people asked about in my first two posts about the Max+ 395. That being how/if it works for distributed LLM inference and image/video gen. It works for both those things.&lt;/p&gt;\n\n&lt;p&gt;Let&amp;#39;s start with distributed LLM inference. TBH, I&amp;#39;m pretty surprise the numbers hold up as well as they do. Since IME there&amp;#39;s a pretty significant performance penalty for going multi-gpu. I ballpark it to be about 50%. In this case, though, it&amp;#39;s better than that. That is probably because I&amp;#39;m using a dynamic quant of a MOE. Where the heavy lifting is done by the X2 and the leftovers are on the Mac. Anyways here are the numbers first for the X2 alone and then working with a M1 Max.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Max+\nggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n| model                          |       size |     params | backend    | ngl | fa | mmap |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           pp512 |        112.27 ± 0.38 |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           tg128 |         20.29 ± 0.02 |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  pp512 @ d10000 |         60.61 ± 0.34 |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  tg128 @ d10000 |         15.36 ± 0.03 |\n\nMax+ with M1 Max\nggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\n| model                          |       size |     params | backend    | ngl | fa | mmap |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           pp512 |        101.53 ± 2.69 |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |           tg128 |         13.90 ± 4.29 |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  pp512 @ d10000 |         56.71 ± 0.33 |\n| glm4moe 106B.A12B Q5_K - Medium |  77.75 GiB |   110.47 B | Vulkan,RPC | 9999 |  1 |    0 |  tg128 @ d10000 |          9.56 ± 0.12 |\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Here are the numbers for doing SD 1.5 image gens. Both at 512x512 and 1024x1024.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;SD 1.5 512x512&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Max+\n100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01&amp;lt;00:00, 11.58it/s]\nPrompt executed in 2.21 seconds\n\n7900xtx\n100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:01&amp;lt;00:00, 18.54it/s]\nPrompt executed in 1.24 seconds\n\n3060\n100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02&amp;lt;00:00,  8.86it/s]\nPrompt executed in 2.60 seconds\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;&lt;strong&gt;SD 1.5 1024x1024&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Max+\n100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:11&amp;lt;00:00,  1.69it/s]\nPrompt executed in 13.70 seconds\n\n7900xtx\n100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:07&amp;lt;00:00,  2.58it/s]\nPrompt executed in 8.69 seconds\n\n3060\n100%|███████████████████████████████████████████████████████████████████████████████████| 20/20 [00:10&amp;lt;00:00,  1.84it/s]\nPrompt executed in 12.12 seconds\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Lastly, here are some video gen numbers. This is for Wan 2.2. It&amp;#39;s at 480x320 resolution since ROCm support for the Max+ 395 is still a work in progress. Under Windows it&amp;#39;s fast but only works with about 32GB of RAM max before things go bad. Under Linux it doesn&amp;#39;t seem to have that RAM limit but it&amp;#39;s really really slow. Like 200 secs/iteration slow. Yes, I verified that it is using the GPU and not the CPU. So these results are from Windows. But because of the memory limit, I had to crank down the resolution. I&amp;#39;m using the Phr00t Wan 2.2 14B AIO.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Wan 2.2 480x320x41&lt;/strong&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Max+\n100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:42&amp;lt;00:00, 25.69s/it]\nPrompt executed in 194.01 seconds\n\n7900xtx\n100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:19&amp;lt;00:00,  4.77s/it]\nPrompt executed in 140.08 seconds\n\n3060\n100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:01&amp;lt;00:00, 15.44s/it]\nPrompt executed in 133.89 seconds\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So just like with the other two posts in this series, the Max+ 395 is basically a 128GB 3060.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mkokj2",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "fallingdowndizzyvr",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/",
            "subreddit_subscribers": 513814,
            "created_utc": 1754637211,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7l4bfl",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kaisurniwurer",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7krhbv",
                                "score": 1,
                                "author_fullname": "t2_qafso",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Isn't that quite comparable to high speed DDR5 though?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7l4bfl",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t that quite comparable to high speed DDR5 though?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mkokj2",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/n7l4bfl/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754654526,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754654526,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7krhbv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "uti24",
                      "can_mod_post": false,
                      "created_utc": 1754648739,
                      "send_replies": true,
                      "parent_id": "t1_n7kaikj",
                      "score": 3,
                      "author_fullname": "t2_13hbro",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "At this point we have mostly all information about AMD Max+ 395 and can safely-ish interpolate any model size speed.\n\nIt make 5t/s with 70B Q4 model (with like 2k context), so with 32B model Q4 it will have 10-11 t/s with tny context and less with anything more.\n\nI think it is a good reasonable stuff.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7krhbv",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;At this point we have mostly all information about AMD Max+ 395 and can safely-ish interpolate any model size speed.&lt;/p&gt;\n\n&lt;p&gt;It make 5t/s with 70B Q4 model (with like 2k context), so with 32B model Q4 it will have 10-11 t/s with tny context and less with anything more.&lt;/p&gt;\n\n&lt;p&gt;I think it is a good reasonable stuff.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkokj2",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/n7krhbv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754648739,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7kaikj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Czydera",
            "can_mod_post": false,
            "created_utc": 1754639176,
            "send_replies": true,
            "parent_id": "t3_1mkokj2",
            "score": 1,
            "author_fullname": "t2_qq6spcu23",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nice! Are you happy with this mini PC so far? What about dense models like 32B? Are they performing a bit better now?",
            "edited": 1754640261,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kaikj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice! Are you happy with this mini PC so far? What about dense models like 32B? Are they performing a bit better now?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/n7kaikj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754639176,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkokj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kqd6k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Desperate-Sir-5088",
            "can_mod_post": false,
            "created_utc": 1754648155,
            "send_replies": true,
            "parent_id": "t3_1mkokj2",
            "score": 1,
            "author_fullname": "t2_1dhesoqqtu",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The M1 and RPC results exceeded my expectations! If you can connect an Nvidia graphics card to AI 395+ via the M2 interface, you can expect a significant performance improvement.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kqd6k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The M1 and RPC results exceeded my expectations! If you can connect an Nvidia graphics card to AI 395+ via the M2 interface, you can expect a significant performance improvement.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkokj2/gmk_x2amd_max_395_w128gb_third_impressions_rpc/n7kqd6k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754648155,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkokj2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]