[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "The laptop has a previous-gen AMD processor series 7040U + radeon 780M igpu, with 128GB shared RAM, running with with llama.cpp + vulkan (you have to set dynamic igpu access to RAM high enough; 75% or 96GB is plenty. RAM is DDR5-5600). Laptop+RAM was in the $2200 range.\n\nResults from running one of my own tests:\n\n&gt; llama_perf_sampler_print:    sampling time =     167.01 ms /  1424 runs   (    0.12 ms per token,  8526.23 tokens per second)\n&gt; \n&gt; llama_perf_context_print:        load time =   61987.61 ms\n&gt; \n&gt; llama_perf_context_print: prompt eval time =    6335.35 ms /   159 tokens (   39.84 ms per token,    25.10 tokens per second)\n&gt; \n&gt; llama_perf_context_print:        eval time =   96710.35 ms /  1264 runs   (   76.51 ms per token,    13.07 tokens per second)\n&gt; \n&gt; llama_perf_context_print:       total time =  104775.76 ms /  1423 tokens\n&gt; \n&gt; llama_perf_context_print:    graphs reused =       1223\n\nThe prompt is a statistical programming problem that I've found often trips up thinking models quite badly. \n\ngpt-oss did very well, avoiding the common pitfalls that reasoning models hit here, and also avoided some of the logic errors in the code itself. The reasoning trace was also very terse versus many other models. I'd given up on reasoning models as a pair-programmer, at least on local laptop, but this one may actually be fine due to terseness. \n\nThis seems like a great MoE model for this RAM+processor setup, very pleased so far. Will try out with some other programming tasks.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "gpt-oss 120B runs ~13tps on laptop with igpu",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mj2q9j",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_m78cdz1nv",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754481681,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The laptop has a previous-gen AMD processor series 7040U + radeon 780M igpu, with 128GB shared RAM, running with with llama.cpp + vulkan (you have to set dynamic igpu access to RAM high enough; 75% or 96GB is plenty. RAM is DDR5-5600). Laptop+RAM was in the $2200 range.&lt;/p&gt;\n\n&lt;p&gt;Results from running one of my own tests:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;llama_perf_sampler_print:    sampling time =     167.01 ms /  1424 runs   (    0.12 ms per token,  8526.23 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:        load time =   61987.61 ms&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print: prompt eval time =    6335.35 ms /   159 tokens (   39.84 ms per token,    25.10 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:        eval time =   96710.35 ms /  1264 runs   (   76.51 ms per token,    13.07 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:       total time =  104775.76 ms /  1423 tokens&lt;/p&gt;\n\n&lt;p&gt;llama_perf_context_print:    graphs reused =       1223&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The prompt is a statistical programming problem that I&amp;#39;ve found often trips up thinking models quite badly. &lt;/p&gt;\n\n&lt;p&gt;gpt-oss did very well, avoiding the common pitfalls that reasoning models hit here, and also avoided some of the logic errors in the code itself. The reasoning trace was also very terse versus many other models. I&amp;#39;d given up on reasoning models as a pair-programmer, at least on local laptop, but this one may actually be fine due to terseness. &lt;/p&gt;\n\n&lt;p&gt;This seems like a great MoE model for this RAM+processor setup, very pleased so far. Will try out with some other programming tasks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mj2q9j",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "RobotRobotWhatDoUSee",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mj2q9j/gptoss_120b_runs_13tps_on_laptop_with_igpu/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj2q9j/gptoss_120b_runs_13tps_on_laptop_with_igpu/",
            "subreddit_subscribers": 511882,
            "created_utc": 1754481681,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n77vggm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1754482408,
            "send_replies": true,
            "parent_id": "t3_1mj2q9j",
            "score": 2,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It is a very strong model. It might not beat out the best Qwens and GLMs in terms of the performance/weight frontier but it is not far off them and is potentially less benchmaxxed",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n77vggm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is a very strong model. It might not beat out the best Qwens and GLMs in terms of the performance/weight frontier but it is not far off them and is potentially less benchmaxxed&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj2q9j/gptoss_120b_runs_13tps_on_laptop_with_igpu/n77vggm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754482408,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj2q9j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n77wj3r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Expensive-Apricot-25",
            "can_mod_post": false,
            "created_utc": 1754482805,
            "send_replies": true,
            "parent_id": "t3_1mj2q9j",
            "score": 2,
            "author_fullname": "t2_idqkwio0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I only get 3T/s for the 20b with a 1080ti…",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n77wj3r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I only get 3T/s for the 20b with a 1080ti…&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj2q9j/gptoss_120b_runs_13tps_on_laptop_with_igpu/n77wj3r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754482805,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj2q9j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n77ugvx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lly0571",
            "can_mod_post": false,
            "created_utc": 1754482038,
            "send_replies": true,
            "parent_id": "t3_1mj2q9j",
            "score": 1,
            "author_fullname": "t2_70vzcleel",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It would works fine as it only activates 5B for each token. You only need 96GB of RAM...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n77ugvx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It would works fine as it only activates 5B for each token. You only need 96GB of RAM...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj2q9j/gptoss_120b_runs_13tps_on_laptop_with_igpu/n77ugvx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754482038,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj2q9j",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]