[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey guys, \n\nI managed to get an RTX 5080 on my personal machine, so in total it will be 16GB VRAM plus 96 GB RAM.\n\nI am wondering what's the best general model for it? \n\nQwen 30B A3B or something else?\n\nIs it enough to run some bigger models with \\~100k context?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best Model for Single RTX 5080?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mijwom",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_lipuk",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754424591,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey guys, &lt;/p&gt;\n\n&lt;p&gt;I managed to get an RTX 5080 on my personal machine, so in total it will be 16GB VRAM plus 96 GB RAM.&lt;/p&gt;\n\n&lt;p&gt;I am wondering what&amp;#39;s the best general model for it? &lt;/p&gt;\n\n&lt;p&gt;Qwen 30B A3B or something else?&lt;/p&gt;\n\n&lt;p&gt;Is it enough to run some bigger models with ~100k context?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mijwom",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "SthMax",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mijwom/best_model_for_single_rtx_5080/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mijwom/best_model_for_single_rtx_5080/",
            "subreddit_subscribers": 511364,
            "created_utc": 1754424591,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n74cuzq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Wrong-Historian",
            "can_mod_post": false,
            "created_utc": 1754429335,
            "send_replies": true,
            "parent_id": "t3_1mijwom",
            "score": 1,
            "author_fullname": "t2_69r67vj3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "New OpenAI 120B. It's MOE and you can offload to CPU\n\n25T/s on my 3090.\n\n&gt;  \\--n-cpu-moe 24 \\\\\n\n&gt;  \\--n-gpu-layers 24 \\\\\n\n&gt;  \\--flash-attn \\\\\n\nYou might need to tune that for 16GB (these parameters use 22GB VRAM), but it really shouldn't matter too much for speed",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74cuzq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;New OpenAI 120B. It&amp;#39;s MOE and you can offload to CPU&lt;/p&gt;\n\n&lt;p&gt;25T/s on my 3090.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;--n-cpu-moe 24 \\&lt;/p&gt;\n\n&lt;p&gt;--n-gpu-layers 24 \\&lt;/p&gt;\n\n&lt;p&gt;--flash-attn \\&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You might need to tune that for 16GB (these parameters use 22GB VRAM), but it really shouldn&amp;#39;t matter too much for speed&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mijwom/best_model_for_single_rtx_5080/n74cuzq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754429335,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mijwom",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]