[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Maybe I missed it or something, but how are people running MOE models and getting decent speeds?\n\nI rented a 3090 on runpod since it also had like 124 gb of RAM. I compiled llama.cpp on it. Got my usual speeds for Qwen3 32B q4km completely offloaded to the 3090. Then tried qwen3 235b MOE at q3km and got prompt eval time of 1 t/s and token generation of 0.33 t/s. I made sure that I offloaded layers to fill up the 3090 but things did not speed up much. Maybe 1t/s better, but would have to revisit for the exact numbers. Not what I have been seeing posted by other people here.\n\nAny suggestions what to do differently for these large MOE and a single 3090? Or is this expected performance?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Setup for MOE",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdqlc6",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_40xsg56g",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753931203,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Maybe I missed it or something, but how are people running MOE models and getting decent speeds?&lt;/p&gt;\n\n&lt;p&gt;I rented a 3090 on runpod since it also had like 124 gb of RAM. I compiled llama.cpp on it. Got my usual speeds for Qwen3 32B q4km completely offloaded to the 3090. Then tried qwen3 235b MOE at q3km and got prompt eval time of 1 t/s and token generation of 0.33 t/s. I made sure that I offloaded layers to fill up the 3090 but things did not speed up much. Maybe 1t/s better, but would have to revisit for the exact numbers. Not what I have been seeing posted by other people here.&lt;/p&gt;\n\n&lt;p&gt;Any suggestions what to do differently for these large MOE and a single 3090? Or is this expected performance?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdqlc6",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "fgoricha",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/",
            "subreddit_subscribers": 507575,
            "created_utc": 1753931203,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n641wc7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "NoVibeCoding",
                      "can_mod_post": false,
                      "created_utc": 1753937972,
                      "send_replies": true,
                      "parent_id": "t1_n63p6ny",
                      "score": 1,
                      "author_fullname": "t2_1neapdttam",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I am not exactly sure how Runpod does it, but we allocate CPU for a VM cores with host passthrough, i.e. you get dedicated physical cores. The system specs are also defined and homogeneous within the provider. Typically, it is an AMD EPYC of the latest generation at the time the machine is built. Server motherboards usually have a lot of memory channels, so memory bandwidth should not be an issue, although I haven't performed real benchmarks.\n\n[https://www.cloudrift.ai/](https://www.cloudrift.ai/)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n641wc7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am not exactly sure how Runpod does it, but we allocate CPU for a VM cores with host passthrough, i.e. you get dedicated physical cores. The system specs are also defined and homogeneous within the provider. Typically, it is an AMD EPYC of the latest generation at the time the machine is built. Server motherboards usually have a lot of memory channels, so memory bandwidth should not be an issue, although I haven&amp;#39;t performed real benchmarks.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.cloudrift.ai/\"&gt;https://www.cloudrift.ai/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": true,
                      "can_gild": false,
                      "link_id": "t3_1mdqlc6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/n641wc7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753937972,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n63p6ny",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753932406,
            "send_replies": true,
            "parent_id": "t3_1mdqlc6",
            "score": 5,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You know the real tragedy of renting CPUs in the cloud?\n\nYou never know what you're getting. It's possible those CPU cores had poor AVX instruction sets, had other processes from other people competing for memory bandwidth, or possibly were spread across several CPU (in extreme cases).\n\nQwen 235B is a bit of an awkward one, though. It doesn't have a shared expert, meaning the experts, when tensor offloaded, are \\*all\\* on CPU, so if you offloaded \"correctly\" you probably had like, 6 GB of the model in VRAM and the rest on CPU.\n\nPersonally, with an optimized setup, I get around 3 T/s on Qwen 3 235B q6\\_k\n\nI have a consumer Ryzen 9950X and 192GB of system RAM (in dual channel).\n\nIt's also possible you did the offload command weirdly, so it's hard to give specific thoughts on the matter.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63p6ny",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You know the real tragedy of renting CPUs in the cloud?&lt;/p&gt;\n\n&lt;p&gt;You never know what you&amp;#39;re getting. It&amp;#39;s possible those CPU cores had poor AVX instruction sets, had other processes from other people competing for memory bandwidth, or possibly were spread across several CPU (in extreme cases).&lt;/p&gt;\n\n&lt;p&gt;Qwen 235B is a bit of an awkward one, though. It doesn&amp;#39;t have a shared expert, meaning the experts, when tensor offloaded, are *all* on CPU, so if you offloaded &amp;quot;correctly&amp;quot; you probably had like, 6 GB of the model in VRAM and the rest on CPU.&lt;/p&gt;\n\n&lt;p&gt;Personally, with an optimized setup, I get around 3 T/s on Qwen 3 235B q6_k&lt;/p&gt;\n\n&lt;p&gt;I have a consumer Ryzen 9950X and 192GB of system RAM (in dual channel).&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also possible you did the offload command weirdly, so it&amp;#39;s hard to give specific thoughts on the matter.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/n63p6ny/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753932406,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdqlc6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n63tyct",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1753934392,
            "send_replies": true,
            "parent_id": "t3_1mdqlc6",
            "score": 2,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That's the expected performance.  Unless almost all of your layers are in VRAM, overall performance is going to be close to pure-CPU.\n\nAFAIK almost everyone using large MoE locally is using pure-CPU inference with the fastest main system memory and as many memory channels as they can get (Apple M3/M4 is best at this).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63tyct",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s the expected performance.  Unless almost all of your layers are in VRAM, overall performance is going to be close to pure-CPU.&lt;/p&gt;\n\n&lt;p&gt;AFAIK almost everyone using large MoE locally is using pure-CPU inference with the fastest main system memory and as many memory channels as they can get (Apple M3/M4 is best at this).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/n63tyct/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753934392,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mdqlc6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64hfxr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Threatening-Silence-",
                      "can_mod_post": false,
                      "created_utc": 1753946111,
                      "send_replies": true,
                      "parent_id": "t1_n63v1lf",
                      "score": 1,
                      "author_fullname": "t2_15wqsifdjf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Just to piggyback on your comment, I have a Medium article on doing exactly that with Qwen3 235B:\n\nhttps://medium.com/@david.sanftenberg/gpu-poor-how-to-configure-offloading-for-the-qwen-3-235b-a22b-moe-model-using-llama-cpp-13dc15287bed\n\nAnd a draft PR on llama.cpp to enable numa memory locality for a big speedup on numa systems when CPU inferencing: \n\nhttps://github.com/ggml-org/llama.cpp/pull/14969",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64hfxr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just to piggyback on your comment, I have a Medium article on doing exactly that with Qwen3 235B:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://medium.com/@david.sanftenberg/gpu-poor-how-to-configure-offloading-for-the-qwen-3-235b-a22b-moe-model-using-llama-cpp-13dc15287bed\"&gt;https://medium.com/@david.sanftenberg/gpu-poor-how-to-configure-offloading-for-the-qwen-3-235b-a22b-moe-model-using-llama-cpp-13dc15287bed&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And a draft PR on llama.cpp to enable numa memory locality for a big speedup on numa systems when CPU inferencing: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/14969\"&gt;https://github.com/ggml-org/llama.cpp/pull/14969&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdqlc6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/n64hfxr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753946111,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n63v1lf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1753934856,
            "send_replies": true,
            "parent_id": "t3_1mdqlc6",
            "score": 1,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "First you need to learn how to use -ot option\nThere are multiple posts about it (including my own benchmarks)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63v1lf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;First you need to learn how to use -ot option\nThere are multiple posts about it (including my own benchmarks)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/n63v1lf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753934856,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mdqlc6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6465il",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1753940079,
            "send_replies": true,
            "parent_id": "t3_1mdqlc6",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Dense layers to gpu, experts to cpu. And your system ram needs to have high bandwidth, 12 channels ddr4 or 8/12 channels Ddr5. Anything else and you're not going to get over 1 t/s.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6465il",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Dense layers to gpu, experts to cpu. And your system ram needs to have high bandwidth, 12 channels ddr4 or 8/12 channels Ddr5. Anything else and you&amp;#39;re not going to get over 1 t/s.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdqlc6/setup_for_moe/n6465il/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753940079,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdqlc6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]