[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Small LLM in german",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfldxj",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.93,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 23,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1tcpn4d5tw",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 23,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754122940,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I’d like to start a small art project and I’m looking for a model that speaks German well. I’m currently using Gemma 3n:e4b and I’m quite satisfied with it. However, I’d like to know if there are any other models of a similar size that have even better German language capabilities. The whole thing should be run with Ollama on a PC with a maximum of 8GB of VRAM – ideally no more than 6GB.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mfldxj",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Ghulaschsuppe",
            "discussion_type": null,
            "num_comments": 17,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/",
            "subreddit_subscribers": 509054,
            "created_utc": 1754122940,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6i68ss",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Awwtifishal",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6i252b",
                                "score": 1,
                                "author_fullname": "t2_1d96a8k10t",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Let us know how the 12b works for you.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6i68ss",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Let us know how the 12b works for you.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfldxj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6i68ss/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754130110,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754130110,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6jfm5f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Evening_Ad6637",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6i252b",
                                "score": 1,
                                "author_fullname": "t2_p45er6oo",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Interesting! In my experience, Gemma-3-4b is much better than version 3n-e4b. Have you tried the Q8 version yet, ideally the XL version from unsloth? And if you've downloaded the model from ollama, you should definitely try a manually downloaded version.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6jfm5f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting! In my experience, Gemma-3-4b is much better than version 3n-e4b. Have you tried the Q8 version yet, ideally the XL version from unsloth? And if you&amp;#39;ve downloaded the model from ollama, you should definitely try a manually downloaded version.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfldxj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6jfm5f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754148114,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754148114,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6i252b",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Ghulaschsuppe",
                      "can_mod_post": false,
                      "created_utc": 1754127709,
                      "send_replies": true,
                      "parent_id": "t1_n6hwpay",
                      "score": 2,
                      "author_fullname": "t2_1tcpn4d5tw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thank you very much. Yeah the 3n:e4b is better than the normal 4b but i will try the 12b now",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6i252b",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you very much. Yeah the 3n:e4b is better than the normal 4b but i will try the 12b now&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfldxj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6i252b/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754127709,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6hwpay",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "1nicerBoye",
            "can_mod_post": false,
            "created_utc": 1754124354,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 10,
            "author_fullname": "t2_5hwlbj95",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I am currently using the much bigger Gemma 3 27b in a IQ4\\_XS variant and I must say, that it is really impressive. Apart from the Sauerkraut und Disco Leo finetuned models nothing of that size comes even close. But it is almost 15GB...\n\nQwen3 did also put out decent german but only the bigger ones starting at 14B, the smaller ones fell apart quickly and effectively only contain artifacts of german.\n\nThe aforementioned Sauerkraut and Leo finetunes can be found here:  \n[https://huggingface.co/DiscoResearch/Llama3-German-8B](https://huggingface.co/DiscoResearch/Llama3-German-8B)  \nAnd here:  \n[https://huggingface.co/VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct](https://huggingface.co/VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct)  \nBut in regards to the AI space those are old news and they are a big step below the bigger Gemma-3 honestly.\n\nI would recommend sticking to Gemma. Everything else performs worse in german and also often makes mistakes which breaks reading flow and straight up kills immersive TTS stuff.\n\nYou could try the IQ3\\_M of [https://huggingface.co/bartowski/mlabonne\\_gemma-3-12b-it-abliterated-GGUF/tree/main](https://huggingface.co/bartowski/mlabonne_gemma-3-12b-it-abliterated-GGUF/tree/main) but I think it may make grammatical errors, especially with higher temps.\n\nI would recommend the IQ4\\_XS Variant and make sure that the (quantized, i usually use q8\\_0) Context also fits into VRAM. That would leave the rest of your app to around 500 MB VRAM.\n\nYou could also try splitting the model into RAM and VRAM using LLama.cpp and see how that works for you. For this size a cpu with AVX2 or even better AVX512 might work decently. But then you need to use a Q4\\_K\\_M / Q5\\_K\\_S quant as the IQ requires the bandwidth of VRAM.\n\nUsing the normal Gemma-3-4b might be worth a try, I dont think the changes they made with the 3n stuff improved the model for non english use cases. But I havent tested it, saw that only this week myself that they exist.",
            "edited": 1754124913,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6hwpay",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am currently using the much bigger Gemma 3 27b in a IQ4_XS variant and I must say, that it is really impressive. Apart from the Sauerkraut und Disco Leo finetuned models nothing of that size comes even close. But it is almost 15GB...&lt;/p&gt;\n\n&lt;p&gt;Qwen3 did also put out decent german but only the bigger ones starting at 14B, the smaller ones fell apart quickly and effectively only contain artifacts of german.&lt;/p&gt;\n\n&lt;p&gt;The aforementioned Sauerkraut and Leo finetunes can be found here:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/DiscoResearch/Llama3-German-8B\"&gt;https://huggingface.co/DiscoResearch/Llama3-German-8B&lt;/a&gt;&lt;br/&gt;\nAnd here:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct\"&gt;https://huggingface.co/VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct&lt;/a&gt;&lt;br/&gt;\nBut in regards to the AI space those are old news and they are a big step below the bigger Gemma-3 honestly.&lt;/p&gt;\n\n&lt;p&gt;I would recommend sticking to Gemma. Everything else performs worse in german and also often makes mistakes which breaks reading flow and straight up kills immersive TTS stuff.&lt;/p&gt;\n\n&lt;p&gt;You could try the IQ3_M of &lt;a href=\"https://huggingface.co/bartowski/mlabonne_gemma-3-12b-it-abliterated-GGUF/tree/main\"&gt;https://huggingface.co/bartowski/mlabonne_gemma-3-12b-it-abliterated-GGUF/tree/main&lt;/a&gt; but I think it may make grammatical errors, especially with higher temps.&lt;/p&gt;\n\n&lt;p&gt;I would recommend the IQ4_XS Variant and make sure that the (quantized, i usually use q8_0) Context also fits into VRAM. That would leave the rest of your app to around 500 MB VRAM.&lt;/p&gt;\n\n&lt;p&gt;You could also try splitting the model into RAM and VRAM using LLama.cpp and see how that works for you. For this size a cpu with AVX2 or even better AVX512 might work decently. But then you need to use a Q4_K_M / Q5_K_S quant as the IQ requires the bandwidth of VRAM.&lt;/p&gt;\n\n&lt;p&gt;Using the normal Gemma-3-4b might be worth a try, I dont think the changes they made with the 3n stuff improved the model for non english use cases. But I havent tested it, saw that only this week myself that they exist.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6hwpay/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754124354,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 10
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6i2j5g",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Ghulaschsuppe",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6i2g38",
                                          "score": 2,
                                          "author_fullname": "t2_1tcpn4d5tw",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Alles gut, das hat Zeit 😃 ist ein Hobbyprojekt und darf ruhig länger dauern.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6i2j5g",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Alles gut, das hat Zeit 😃 ist ein Hobbyprojekt und darf ruhig länger dauern.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mfldxj",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6i2j5g/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754127943,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754127943,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6i2g38",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "zaschmaen",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6i211r",
                                "score": 1,
                                "author_fullname": "t2_ufk89ive",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Ja ich bin da noch hinter, habe deinen beitrag gespeichert und wenn ich weiter bin kann ich dir gerne mal bescheid geben. Aber bitte nicht gleich mit morgen rechnen xD",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6i2g38",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ja ich bin da noch hinter, habe deinen beitrag gespeichert und wenn ich weiter bin kann ich dir gerne mal bescheid geben. Aber bitte nicht gleich mit morgen rechnen xD&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfldxj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6i2g38/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754127892,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754127892,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6i6o9o",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Blizado",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6i211r",
                                "score": 1,
                                "author_fullname": "t2_j0e2r",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Muss es dafür schnell Antworten generieren können? Ansonsten könntest du neben Quants auch mit Offloading versuchen, sprich einen Teil in VRAM laden und einen Teil in den normalen RAM. Dadurch wird die Generierung zwar langsamer, aber größere Modelle können eben besser deutsch.\n\nAuch wichtig zu wissen: selbst wenn man bei einem größeren Modell nur ein Q4 Quant (4bit) nutzen kann, was ein LLM schon spürbar schlechter macht, ist es meist dennoch besser als ein kleineres Modell in Q8 (8bit), was ein LLM kaum schlechter macht. Also lieber ein Modell mit 4bit nutzen als ein kleineres mit 8bit.\n\nAuch wichtig: ein LLM braucht auch immer zusätzlich VRAM für den Context welchen du ihm sendest und für die Antwort die es generiert. Bei einem 7,5GB VRAM Modell wirst du also sehr wahrscheinlich out of Memory laufen, weil nicht genügend Platz für Context+Antwort vorhanden ist. 1+GB VRAM muss man dafür schon frei halten, je nach Context und Antwortlänge.\n\nDu schreibst ideal wäre, wenn es nicht mehr als 6GB VRAM wären, wenn das für die KI insgesamt gilt, musste du nach einer Download Größe von etwas 4-5GB suchen. Mit einem 12B Modell wird das dann nichts, da müsstest du runter bis auf 3 oder gar 2bit und da sind die Modelle kaum noch zu gebrauchen, 4bit ist so der Sweetspot. Bei einem 12B Modell müsstest du also schon Q4_K_S runter, vielleicht sogar auf IQ4_XS für mehr Platz für Context+Antwort um in 8GB VRAM zu passen. Bei einem 8B Modell könntest du noch Q4_K_M nutzen, was so der go to standard bei 4bit ist und unter 5GB VRAM käme. Alles GGUF Modelle.\n\nWie gesagt, wenn du Offloading betreiben könntest, weil Geschwindigkeit nicht so wichtig ist, dann wäre mehr möglich. Aber Offloading bremst sehr spürbar aus.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6i6o9o",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Muss es dafür schnell Antworten generieren können? Ansonsten könntest du neben Quants auch mit Offloading versuchen, sprich einen Teil in VRAM laden und einen Teil in den normalen RAM. Dadurch wird die Generierung zwar langsamer, aber größere Modelle können eben besser deutsch.&lt;/p&gt;\n\n&lt;p&gt;Auch wichtig zu wissen: selbst wenn man bei einem größeren Modell nur ein Q4 Quant (4bit) nutzen kann, was ein LLM schon spürbar schlechter macht, ist es meist dennoch besser als ein kleineres Modell in Q8 (8bit), was ein LLM kaum schlechter macht. Also lieber ein Modell mit 4bit nutzen als ein kleineres mit 8bit.&lt;/p&gt;\n\n&lt;p&gt;Auch wichtig: ein LLM braucht auch immer zusätzlich VRAM für den Context welchen du ihm sendest und für die Antwort die es generiert. Bei einem 7,5GB VRAM Modell wirst du also sehr wahrscheinlich out of Memory laufen, weil nicht genügend Platz für Context+Antwort vorhanden ist. 1+GB VRAM muss man dafür schon frei halten, je nach Context und Antwortlänge.&lt;/p&gt;\n\n&lt;p&gt;Du schreibst ideal wäre, wenn es nicht mehr als 6GB VRAM wären, wenn das für die KI insgesamt gilt, musste du nach einer Download Größe von etwas 4-5GB suchen. Mit einem 12B Modell wird das dann nichts, da müsstest du runter bis auf 3 oder gar 2bit und da sind die Modelle kaum noch zu gebrauchen, 4bit ist so der Sweetspot. Bei einem 12B Modell müsstest du also schon Q4_K_S runter, vielleicht sogar auf IQ4_XS für mehr Platz für Context+Antwort um in 8GB VRAM zu passen. Bei einem 8B Modell könntest du noch Q4_K_M nutzen, was so der go to standard bei 4bit ist und unter 5GB VRAM käme. Alles GGUF Modelle.&lt;/p&gt;\n\n&lt;p&gt;Wie gesagt, wenn du Offloading betreiben könntest, weil Geschwindigkeit nicht so wichtig ist, dann wäre mehr möglich. Aber Offloading bremst sehr spürbar aus.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfldxj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6i6o9o/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754130354,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754130354,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ik7an",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Mkengine",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6i211r",
                                "score": 1,
                                "author_fullname": "t2_9p2xe",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Mit Gemma 3 habe ich auch die besten Erfahrungen, die einzigen anderen Modelle die gut deutsch können sind die von Mistral, aber die sind zu groß für deinen Anwendungsfall. Ich teste gerade ob Qwen3-30B-A3B-2507-instruct noch gut deutsch kann aufgrund der hohen Parameteranzahl. Dadurch, dass es MoE ist, sollte es bei dir auch gut laufen.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ik7an",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mit Gemma 3 habe ich auch die besten Erfahrungen, die einzigen anderen Modelle die gut deutsch können sind die von Mistral, aber die sind zu groß für deinen Anwendungsfall. Ich teste gerade ob Qwen3-30B-A3B-2507-instruct noch gut deutsch kann aufgrund der hohen Parameteranzahl. Dadurch, dass es MoE ist, sollte es bei dir auch gut laufen.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfldxj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6ik7an/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754136942,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754136942,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6jgbpi",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Evening_Ad6637",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6i211r",
                                "score": 1,
                                "author_fullname": "t2_p45er6oo",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Kennst schon das hier? Das könnte vlt genau das richtige für dich sein (also zumindest der Ansatz, ein Modell „depri“ zu machen)\n\n\nhttps://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6jgbpi",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "llama.cpp"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Kennst schon das hier? Das könnte vlt genau das richtige für dich sein (also zumindest der Ansatz, ein Modell „depri“ zu machen)&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule\"&gt;https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfldxj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6jgbpi/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754148337,
                                "author_flair_text": "llama.cpp",
                                "treatment_tags": [],
                                "created_utc": 1754148337,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bbbdbf",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6i211r",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Ghulaschsuppe",
                      "can_mod_post": false,
                      "created_utc": 1754127641,
                      "send_replies": true,
                      "parent_id": "t1_n6i17bo",
                      "score": 0,
                      "author_fullname": "t2_1tcpn4d5tw",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Das klingt gut. Ich Versuche ein Kunstprojekt aufzuziehen in dem das Sprachmodell unglaublich \"leidet\" und über seine eigene Existenz nachdenkt, seine Ängste ausdrückt etc. und dafür wäre annähernd perfektes Deutsch natürlich viel besser 😂",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6i211r",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Das klingt gut. Ich Versuche ein Kunstprojekt aufzuziehen in dem das Sprachmodell unglaublich &amp;quot;leidet&amp;quot; und über seine eigene Existenz nachdenkt, seine Ängste ausdrückt etc. und dafür wäre annähernd perfektes Deutsch natürlich viel besser 😂&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfldxj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6i211r/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754127641,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6i17bo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zaschmaen",
            "can_mod_post": false,
            "created_utc": 1754127139,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 3,
            "author_fullname": "t2_ufk89ive",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Genau so ein Projekt baue ich auch gerade auf! Habe gerade erst gestern den Pc dafür zusammengebaut und getestet, den ich via Proxmox ins Netzwerk integrieren werde. Suche auch noch die Richtige LLM und habe schon einiges gelesen was am besten ist mit meiner Hardware. Würde ihn sogar am liebsten per Sprache steuern wollen können. Kann dir gerne bescheid geben falls ich was gefunden habe, ich habe eine rtx 2060 mit 6 gb.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6i17bo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Genau so ein Projekt baue ich auch gerade auf! Habe gerade erst gestern den Pc dafür zusammengebaut und getestet, den ich via Proxmox ins Netzwerk integrieren werde. Suche auch noch die Richtige LLM und habe schon einiges gelesen was am besten ist mit meiner Hardware. Würde ihn sogar am liebsten per Sprache steuern wollen können. Kann dir gerne bescheid geben falls ich was gefunden habe, ich habe eine rtx 2060 mit 6 gb.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6i17bo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754127139,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6hvhbq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "DunklerErpel",
            "can_mod_post": false,
            "created_utc": 1754123597,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 5,
            "author_fullname": "t2_alho5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You'll probably not get much better than Gemma, I tried as well. You might want to look at Phi4 4b, but don't get your hopes up. Alternatively Gemma2-9B-SimPo, I enjoyed that some time ago, but don't know whether it still compares.\n\nVAGOsolutions used to be dedicated to German LLMs, but as far as I know, they haven't released any text model for some time. EuroLLM is supposed to be good, but I wasn't satisfied.\n\nYou might want to look at the new model by Arcee, ALM-4.5 (or something in that direction).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6hvhbq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;ll probably not get much better than Gemma, I tried as well. You might want to look at Phi4 4b, but don&amp;#39;t get your hopes up. Alternatively Gemma2-9B-SimPo, I enjoyed that some time ago, but don&amp;#39;t know whether it still compares.&lt;/p&gt;\n\n&lt;p&gt;VAGOsolutions used to be dedicated to German LLMs, but as far as I know, they haven&amp;#39;t released any text model for some time. EuroLLM is supposed to be good, but I wasn&amp;#39;t satisfied.&lt;/p&gt;\n\n&lt;p&gt;You might want to look at the new model by Arcee, ALM-4.5 (or something in that direction).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6hvhbq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754123597,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6iqzda",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ayuei",
            "can_mod_post": false,
            "created_utc": 1754139671,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 2,
            "author_fullname": "t2_wtq77",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There's actually an LLM released recently trained exclusively for German:\n\nhttps://huggingface.co/LSX-UniWue/LLaMmlein_7B\n\nThere's also a 1B variant. \n\nThe paper for the model was recently accepted to a top AI/NLP conference as well!\n\nhttps://aclanthology.org/2025.acl-long.111/",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6iqzda",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There&amp;#39;s actually an LLM released recently trained exclusively for German:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/LSX-UniWue/LLaMmlein_7B\"&gt;https://huggingface.co/LSX-UniWue/LLaMmlein_7B&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s also a 1B variant. &lt;/p&gt;\n\n&lt;p&gt;The paper for the model was recently accepted to a top AI/NLP conference as well!&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://aclanthology.org/2025.acl-long.111/\"&gt;https://aclanthology.org/2025.acl-long.111/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6iqzda/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754139671,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6hvbql",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1754123499,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 2,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try gemma 3 4B",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6hvbql",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try gemma 3 4B&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6hvbql/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754123499,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6iq869",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1754139380,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 1,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "teuken",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6iq869",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;teuken&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6iq869/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754139380,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ixh0a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AvidCyclist250",
            "can_mod_post": false,
            "created_utc": 1754142093,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 1,
            "author_fullname": "t2_lmkezzo6j",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Mistral is worth a look, it's pretty good at German. Scrap that, just saw 8GB VRAM. Not sure how good it is once crammed into 8GB.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ixh0a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mistral is worth a look, it&amp;#39;s pretty good at German. Scrap that, just saw 8GB VRAM. Not sure how good it is once crammed into 8GB.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6ixh0a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754142093,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6k4ssq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zitr0y",
            "can_mod_post": false,
            "created_utc": 1754156109,
            "send_replies": true,
            "parent_id": "t3_1mfldxj",
            "score": 1,
            "author_fullname": "t2_kpqfh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Try Ministral 3b or 8b.\n\n\nIs it better? Not sure but worth a try",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6k4ssq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Try Ministral 3b or 8b.&lt;/p&gt;\n\n&lt;p&gt;Is it better? Not sure but worth a try&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfldxj/small_llm_in_german/n6k4ssq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754156109,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfldxj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]