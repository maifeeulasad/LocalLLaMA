[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have LLM studio installed on a server. And I did enable the feature to run as a server with Tailscale and on my Mac mini, I installed anything LLM . And when I set up anything LLM to use lm studio. It just says refreshing models and nothing else after that it does not pull any of the models I have installed. I’m just curious what I’m doing wrong. In my IP settings for anything LLM I have. http:// my up:1234/v1. But after letting it run 10 minutes, it does not pull any models at all. So to test to see if it was the server I installed ollama and that worked just fine. I’m just curious what am I doing wrong?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "New to LLM studio?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mfek6x",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_33j4dylg",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754099629,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have LLM studio installed on a server. And I did enable the feature to run as a server with Tailscale and on my Mac mini, I installed anything LLM . And when I set up anything LLM to use lm studio. It just says refreshing models and nothing else after that it does not pull any of the models I have installed. I’m just curious what I’m doing wrong. In my IP settings for anything LLM I have. http:// my up:1234/v1. But after letting it run 10 minutes, it does not pull any models at all. So to test to see if it was the server I installed ollama and that worked just fine. I’m just curious what am I doing wrong?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mfek6x",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "wbiggs205",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/",
            "subreddit_subscribers": 509054,
            "created_utc": 1754099629,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6gyvzj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Jattoe",
                      "can_mod_post": false,
                      "created_utc": 1754106396,
                      "send_replies": true,
                      "parent_id": "t1_n6gmvop",
                      "score": 1,
                      "author_fullname": "t2_d887zj2w0",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Na you don't, you can just place the models in whatever folder you chose as your model folder. But they have to be in a directory alone, and that directory has to be in another directory. It's silly but just look at the way it works when you DL one from the discover tab and copy that. It looks for   \nmodel\\_folder -&gt; model\\_group (name doesn't matter) -&gt; model name (directory) (name doesn't matter) -&gt; model  \nWithin the \"model group\" folder you can just add as many (directory+model) as you want",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6gyvzj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Na you don&amp;#39;t, you can just place the models in whatever folder you chose as your model folder. But they have to be in a directory alone, and that directory has to be in another directory. It&amp;#39;s silly but just look at the way it works when you DL one from the discover tab and copy that. It looks for&lt;br/&gt;\nmodel_folder -&amp;gt; model_group (name doesn&amp;#39;t matter) -&amp;gt; model name (directory) (name doesn&amp;#39;t matter) -&amp;gt; model&lt;br/&gt;\nWithin the &amp;quot;model group&amp;quot; folder you can just add as many (directory+model) as you want&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfek6x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6gyvzj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754106396,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6gmvop",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mysterious_Eye2249",
            "can_mod_post": false,
            "created_utc": 1754101505,
            "send_replies": true,
            "parent_id": "t3_1mfek6x",
            "score": 1,
            "author_fullname": "t2_7aqnz3id",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "i think you have to manually download the model in the discover tab",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6gmvop",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i think you have to manually download the model in the discover tab&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6gmvop/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754101505,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfek6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6hm5bb",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Current-Stop7806",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6gr3x0",
                                "score": 1,
                                "author_fullname": "t2_8c7clfk1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Could you solve your problem ? For what I saw on your screenshot and what ChatGPT said about it, the endpoint is wrong, because it shouldn't have the /v1 at the end. You should test it. I'll check it again and post it's message below.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6hm5bb",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Could you solve your problem ? For what I saw on your screenshot and what ChatGPT said about it, the endpoint is wrong, because it shouldn&amp;#39;t have the /v1 at the end. You should test it. I&amp;#39;ll check it again and post it&amp;#39;s message below.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mfek6x",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6hm5bb/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754118140,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754118140,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6gr3x0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "wbiggs205",
                      "can_mod_post": false,
                      "created_utc": 1754103177,
                      "send_replies": true,
                      "parent_id": "t1_n6gnjmd",
                      "score": 1,
                      "author_fullname": "t2_33j4dylg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I was mobile when I sent this. Now I'm at home here what I have anything llm set up\n\nhttps://preview.redd.it/f8457pr3tigf1.png?width=2954&amp;format=png&amp;auto=webp&amp;s=8160dca461dcef2f29454e9776de83d38a1be9ff",
                      "edited": 1754103753,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6gr3x0",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was mobile when I sent this. Now I&amp;#39;m at home here what I have anything llm set up&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/f8457pr3tigf1.png?width=2954&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8160dca461dcef2f29454e9776de83d38a1be9ff\"&gt;https://preview.redd.it/f8457pr3tigf1.png?width=2954&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8160dca461dcef2f29454e9776de83d38a1be9ff&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfek6x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6gr3x0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754103177,
                      "media_metadata": {
                        "f8457pr3tigf1": {
                          "status": "valid",
                          "e": "Image",
                          "m": "image/png",
                          "p": [
                            {
                              "y": 92,
                              "x": 108,
                              "u": "https://preview.redd.it/f8457pr3tigf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ca34a8d96610105cc3215bd60d07a58a3d3d3e57"
                            },
                            {
                              "y": 184,
                              "x": 216,
                              "u": "https://preview.redd.it/f8457pr3tigf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1f624ce60bbeb008ccb8e4df12b22e126b1a367f"
                            },
                            {
                              "y": 273,
                              "x": 320,
                              "u": "https://preview.redd.it/f8457pr3tigf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f59062d830d20518560534efab68d5622bd5649"
                            },
                            {
                              "y": 547,
                              "x": 640,
                              "u": "https://preview.redd.it/f8457pr3tigf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=75b0be9d805c4f90f6fa3ef17db6680ffb6e2502"
                            },
                            {
                              "y": 820,
                              "x": 960,
                              "u": "https://preview.redd.it/f8457pr3tigf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=132ce69014af980ff18fab65ba3403f529f854a8"
                            },
                            {
                              "y": 923,
                              "x": 1080,
                              "u": "https://preview.redd.it/f8457pr3tigf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bfb552c56ebbb1b86d9fda31abf927b451484192"
                            }
                          ],
                          "s": {
                            "y": 2526,
                            "x": 2954,
                            "u": "https://preview.redd.it/f8457pr3tigf1.png?width=2954&amp;format=png&amp;auto=webp&amp;s=8160dca461dcef2f29454e9776de83d38a1be9ff"
                          },
                          "id": "f8457pr3tigf1"
                        }
                      },
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6gnjmd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1754101768,
            "send_replies": true,
            "parent_id": "t3_1mfek6x",
            "score": 1,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here's what ChatGPT said about your problem. You can explain it better and send screenshots of the problem, so it will help you to fix it: \n\nSummary of the Issue\n\nLM Studio is installed and running as a server with Tailscale.\n\nAnythingLLM is trying to connect to LM Studio via http://&lt;IP&gt;:1234/v1.\n\nIt gets stuck on \"refreshing models\" and doesn’t show any installed models.\n\nWhen testing with Ollama, it works fine.\n\n\n\n---\n\nPossible Causes &amp; Fixes\n\n1. Wrong Endpoint\n\nLM Studio’s API does not use /v1 like OpenAI or Ollama.\nIf he’s using:\n\nhttp://&lt;IP&gt;:1234/v1\n\nHe should remove /v1, so it becomes:\n\nhttp://&lt;IP&gt;:1234\n\n&gt; Note: AnythingLLM assumes OpenAI-compatible endpoints, but LM Studio doesn’t respond properly with /v1, causing the \"refreshing models\" issue.\n\n\n\n\n---\n\n2. Missing Authentication Headers (if enabled)\n\nIf LM Studio has API authentication enabled:\n\nCheck if an API key is set in LM Studio.\n\nEnsure that key is also entered in AnythingLLM.\n\n\n\n---\n\n3. Model Must Be Loaded\n\nLM Studio needs a model actively loaded to respond correctly to API requests.\n\nOpen LM Studio in the browser.\n\nMake sure a model is running and works locally before testing the connection.\n\n\n\n---\n\n4. Check IP &amp; Firewall (even with Tailscale)\n\nConfirm that the IP is the actual Tailscale tunnel IP of the LM Studio machine.\n\nEnsure port 1234 is open and accessible.\n\nTest direct access from the Mac:\nOpen this in the browser:\n\nhttp://&lt;IP&gt;:1234\n\nIf nothing loads, it’s a network issue.\n\n\n\n---\n\n5. Check Logs &amp; Console\n\nLook for errors in the AnythingLLM console (e.g., ECONNREFUSED, CORS errors, timeout).\n\nThis will help pinpoint the connection problem.\n\n\n\n---\n\n6. Version Compatibility\n\nSome versions of AnythingLLM don’t natively support LM Studio’s API (they’re built for Ollama/OpenAI/Claude).\n\nAlternative: Use OpenWebUI as a bridge — it integrates easily with LM Studio and exposes a fully OpenAI-compatible /v1 API.\n\n\n\n---\n\nQuick Test\n\n1. Start LM Studio and load a model.\n\n\n2. From the Mac, run:\n\ncurl http://&lt;IP&gt;:1234/completion\n\nor\n\ncurl http://&lt;IP&gt;:1234\n\nIf there’s no response, there’s a network or endpoint issue.\n\n\n---\n\nIf Nothing Works\n\nUse Ollama as a bridge, or\n\nRun OpenWebUI with LM Studio, which gives you the /v1 endpoint AnythingLLM expects.\n\n\n\n---\n\nWould you like me to write him a short step-by-step “fix guide” that he can follow (with example settings and curl commands)?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6gnjmd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here&amp;#39;s what ChatGPT said about your problem. You can explain it better and send screenshots of the problem, so it will help you to fix it: &lt;/p&gt;\n\n&lt;p&gt;Summary of the Issue&lt;/p&gt;\n\n&lt;p&gt;LM Studio is installed and running as a server with Tailscale.&lt;/p&gt;\n\n&lt;p&gt;AnythingLLM is trying to connect to LM Studio via http://&amp;lt;IP&amp;gt;:1234/v1.&lt;/p&gt;\n\n&lt;p&gt;It gets stuck on &amp;quot;refreshing models&amp;quot; and doesn’t show any installed models.&lt;/p&gt;\n\n&lt;p&gt;When testing with Ollama, it works fine.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Possible Causes &amp;amp; Fixes&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Wrong Endpoint&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;LM Studio’s API does not use /v1 like OpenAI or Ollama.\nIf he’s using:&lt;/p&gt;\n\n&lt;p&gt;http://&amp;lt;IP&amp;gt;:1234/v1&lt;/p&gt;\n\n&lt;p&gt;He should remove /v1, so it becomes:&lt;/p&gt;\n\n&lt;p&gt;http://&amp;lt;IP&amp;gt;:1234&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Note: AnythingLLM assumes OpenAI-compatible endpoints, but LM Studio doesn’t respond properly with /v1, causing the &amp;quot;refreshing models&amp;quot; issue.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Missing Authentication Headers (if enabled)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;If LM Studio has API authentication enabled:&lt;/p&gt;\n\n&lt;p&gt;Check if an API key is set in LM Studio.&lt;/p&gt;\n\n&lt;p&gt;Ensure that key is also entered in AnythingLLM.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Model Must Be Loaded&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;LM Studio needs a model actively loaded to respond correctly to API requests.&lt;/p&gt;\n\n&lt;p&gt;Open LM Studio in the browser.&lt;/p&gt;\n\n&lt;p&gt;Make sure a model is running and works locally before testing the connection.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Check IP &amp;amp; Firewall (even with Tailscale)&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Confirm that the IP is the actual Tailscale tunnel IP of the LM Studio machine.&lt;/p&gt;\n\n&lt;p&gt;Ensure port 1234 is open and accessible.&lt;/p&gt;\n\n&lt;p&gt;Test direct access from the Mac:\nOpen this in the browser:&lt;/p&gt;\n\n&lt;p&gt;http://&amp;lt;IP&amp;gt;:1234&lt;/p&gt;\n\n&lt;p&gt;If nothing loads, it’s a network issue.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Check Logs &amp;amp; Console&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Look for errors in the AnythingLLM console (e.g., ECONNREFUSED, CORS errors, timeout).&lt;/p&gt;\n\n&lt;p&gt;This will help pinpoint the connection problem.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Version Compatibility&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Some versions of AnythingLLM don’t natively support LM Studio’s API (they’re built for Ollama/OpenAI/Claude).&lt;/p&gt;\n\n&lt;p&gt;Alternative: Use OpenWebUI as a bridge — it integrates easily with LM Studio and exposes a fully OpenAI-compatible /v1 API.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Quick Test&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Start LM Studio and load a model.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;From the Mac, run:&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;curl http://&amp;lt;IP&amp;gt;:1234/completion&lt;/p&gt;\n\n&lt;p&gt;or&lt;/p&gt;\n\n&lt;p&gt;curl http://&amp;lt;IP&amp;gt;:1234&lt;/p&gt;\n\n&lt;p&gt;If there’s no response, there’s a network or endpoint issue.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;If Nothing Works&lt;/p&gt;\n\n&lt;p&gt;Use Ollama as a bridge, or&lt;/p&gt;\n\n&lt;p&gt;Run OpenWebUI with LM Studio, which gives you the /v1 endpoint AnythingLLM expects.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Would you like me to write him a short step-by-step “fix guide” that he can follow (with example settings and curl commands)?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6gnjmd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754101768,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfek6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6hmfd0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1754118296,
            "send_replies": true,
            "parent_id": "t3_1mfek6x",
            "score": 1,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here is ChatGPT answer when it saw your screenshot: \n\nThanks for the screenshot — it's exactly what we needed. Here's the clear diagnosis and fix:\n\n\n---\n\n❗️Problem:\n\nIn the LM Studio Base URL, your friend is using:\n\nhttp://100.98.103.109:1234/v1\n\nBut LM Studio does NOT use the /v1 path like OpenAI or Ollama.\n\nThat’s why AnythingLLM is stuck on \"\" — it’s hitting a 404 or empty response, and doesn’t know how to proceed.\n\n\n---\n\n✅ The Fix:\n\nChange this:\n\nhttp://100.98.103.109:1234/v1\n\nTo this:\n\nhttp://100.98.103.109:1234\n\n\n---\n\n🔍 Optional Troubleshooting Steps:\n\nIf it still doesn’t load after fixing the URL:\n\n1. Open LM Studio in a browser: http://100.98.103.109:1234\n\nConfirm the model is running and available.\n\n\n\n2. On the Mac, try this terminal command:\n\ncurl http://100.98.103.109:1234\n\nIf you get a valid HTML/JSON response, then the connection is working.\n\n\n3. Restart AnythingLLM just to refresh its internal state after correcting the URL.\n\n---\n\n📝 Summary for Your Friend:\n\n&gt; You're using /v1 in the LM Studio URL, but LM Studio doesn't have that endpoint. Remove it and just use the base URL like this:\nhttp://100.98.103.109:1234\nAfter that, the models should load correctly in AnythingLLM.\n\n\n---\n\nLet me know if you'd like a short step-by-step walkthrough formatted as a guide to send him too.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6hmfd0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here is ChatGPT answer when it saw your screenshot: &lt;/p&gt;\n\n&lt;p&gt;Thanks for the screenshot — it&amp;#39;s exactly what we needed. Here&amp;#39;s the clear diagnosis and fix:&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;❗️Problem:&lt;/p&gt;\n\n&lt;p&gt;In the LM Studio Base URL, your friend is using:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://100.98.103.109:1234/v1\"&gt;http://100.98.103.109:1234/v1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;But LM Studio does NOT use the /v1 path like OpenAI or Ollama.&lt;/p&gt;\n\n&lt;p&gt;That’s why AnythingLLM is stuck on &amp;quot;&amp;quot; — it’s hitting a 404 or empty response, and doesn’t know how to proceed.&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;✅ The Fix:&lt;/p&gt;\n\n&lt;p&gt;Change this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://100.98.103.109:1234/v1\"&gt;http://100.98.103.109:1234/v1&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;To this:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://100.98.103.109:1234\"&gt;http://100.98.103.109:1234&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;🔍 Optional Troubleshooting Steps:&lt;/p&gt;\n\n&lt;p&gt;If it still doesn’t load after fixing the URL:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Open LM Studio in a browser: &lt;a href=\"http://100.98.103.109:1234\"&gt;http://100.98.103.109:1234&lt;/a&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Confirm the model is running and available.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;On the Mac, try this terminal command:&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;curl &lt;a href=\"http://100.98.103.109:1234\"&gt;http://100.98.103.109:1234&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you get a valid HTML/JSON response, then the connection is working.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Restart AnythingLLM just to refresh its internal state after correcting the URL.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;📝 Summary for Your Friend:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;You&amp;#39;re using /v1 in the LM Studio URL, but LM Studio doesn&amp;#39;t have that endpoint. Remove it and just use the base URL like this:\n&lt;a href=\"http://100.98.103.109:1234\"&gt;http://100.98.103.109:1234&lt;/a&gt;\nAfter that, the models should load correctly in AnythingLLM.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;hr/&gt;\n\n&lt;p&gt;Let me know if you&amp;#39;d like a short step-by-step walkthrough formatted as a guide to send him too.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6hmfd0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754118296,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfek6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6kea3y",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "wbiggs205",
                      "can_mod_post": false,
                      "created_utc": 1754159122,
                      "send_replies": true,
                      "parent_id": "t1_n6jtg0z",
                      "score": 1,
                      "author_fullname": "t2_33j4dylg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What I'm trying to do Is I have a server off site with 3 a4000 card 26 core 80g ram. I can get llm studio to use 2 of the card's So I'm trying to get anything llm on my mac to use LLM Studio to use it with tailscale. But when I set up anything it will not pull the list of models. I have download.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6kea3y",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What I&amp;#39;m trying to do Is I have a server off site with 3 a4000 card 26 core 80g ram. I can get llm studio to use 2 of the card&amp;#39;s So I&amp;#39;m trying to get anything llm on my mac to use LLM Studio to use it with tailscale. But when I set up anything it will not pull the list of models. I have download.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mfek6x",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6kea3y/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754159122,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6jtg0z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LocoLanguageModel",
            "can_mod_post": false,
            "created_utc": 1754152522,
            "send_replies": true,
            "parent_id": "t3_1mfek6x",
            "score": 1,
            "author_fullname": "t2_canyreqfh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm not sure exactly what you're doing but most issues on this topic arise from people trying to connect to lm studio directly from their browser as if it were koboldCPP etc, when instead you need to connect to it from an actual front end client with compatible API functionality rather than from your browser.  At least that's how it was when I last used it for that. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6jtg0z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not sure exactly what you&amp;#39;re doing but most issues on this topic arise from people trying to connect to lm studio directly from their browser as if it were koboldCPP etc, when instead you need to connect to it from an actual front end client with compatible API functionality rather than from your browser.  At least that&amp;#39;s how it was when I last used it for that. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6jtg0z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754152522,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfek6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6kemmi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wbiggs205",
            "can_mod_post": false,
            "created_utc": 1754159237,
            "send_replies": true,
            "parent_id": "t3_1mfek6x",
            "score": 1,
            "author_fullname": "t2_33j4dylg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "When I copy the server address it set to localhost not the ip do you think that would do that error",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6kemmi",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When I copy the server address it set to localhost not the ip do you think that would do that error&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mfek6x/new_to_llm_studio/n6kemmi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754159237,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mfek6x",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]