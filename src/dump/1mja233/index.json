[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "For some reason it failes to load half way and I cant figure out why?  \nAny of you have any success loading the module on LM Studio running on Ubuntu with a AMD RX 7900 XTX gpu?\n\nLM Studio 0.3.22 (build 1)  \nROCm llama.cpp (Linux) v1.43.1\n\n    [ModelLoadingProvider] Requested to load model openai/gpt-oss-20b with opts {\n      identifier: { desired: 'openai/gpt-oss-20b', conflictBehavior: 'bump' },\n      excludeUserModelDefaultConfigLayer: true,\n      instanceLoadTimeConfig: { fields: [] },\n      ttlMs: undefined\n    }\n    [CachedFileDataProvider] Watching file at /home/skaldudritti/.lmstudio/.internal/user-concrete-model-default-config/openai/gpt-oss-20b.json\n    [ModelLoadingProvider] Started loading model openai/gpt-oss-20b\n    [ModelProxyObject(id=openai/gpt-oss-20b)] Forking LLMWorker with custom envVars: {\"LD_LIBRARY_PATH\":\"/home/skaldudritti/.lmstudio/extensions/backends/vendor/linux-llama-rocm-vendor-v3\",\"HIP_VISIBLE_DEVICES\":\"0\"}\n    [ProcessForkingProvider][NodeProcessForker] Spawned process 215047\n    [ProcessForkingProvider][NodeProcessForker] Exited process 215047\n    18:51:54.347 › [LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in channel handler: Error: Error loading model.\n        at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n        at _0x4ec43c.emit (node:events:518:28)\n        at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n        at _0x66b5e7.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n        at _0x66b5e7.emit (node:events:530:35)\n        at ChildProcess.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n        at ChildProcess.emit (node:events:518:28)\n        at ChildProcess._handle.onexit (node:internal/child_process:293:12)\n    [LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in loadModel channel _0x179e10 [Error]: Error loading model.\n        at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n        at _0x4ec43c.emit (node:events:518:28)\n        at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n        at _0x66b5e7.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n        at _0x66b5e7.emit (node:events:530:35)\n        at ChildProcess.&lt;anonymous&gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n        at ChildProcess.emit (node:events:518:28)\n        at ChildProcess._handle.onexit (node:internal/child_process:293:12) {\n      cause: '(Exit code: null). Please check settings and try loading the model again. ',\n      suggestion: '',\n      errorData: undefined,\n      data: undefined,\n      displayData: undefined,\n      title: 'Error loading model.'\n    }\n    ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "gpt-oss-20b on LM Studio / Ubuntu / RX7900XTX",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mja233",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.4,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_j1fy2",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": true,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754499286,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;For some reason it failes to load half way and I cant figure out why?&lt;br/&gt;\nAny of you have any success loading the module on LM Studio running on Ubuntu with a AMD RX 7900 XTX gpu?&lt;/p&gt;\n\n&lt;p&gt;LM Studio 0.3.22 (build 1)&lt;br/&gt;\nROCm llama.cpp (Linux) v1.43.1&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;[ModelLoadingProvider] Requested to load model openai/gpt-oss-20b with opts {\n  identifier: { desired: &amp;#39;openai/gpt-oss-20b&amp;#39;, conflictBehavior: &amp;#39;bump&amp;#39; },\n  excludeUserModelDefaultConfigLayer: true,\n  instanceLoadTimeConfig: { fields: [] },\n  ttlMs: undefined\n}\n[CachedFileDataProvider] Watching file at /home/skaldudritti/.lmstudio/.internal/user-concrete-model-default-config/openai/gpt-oss-20b.json\n[ModelLoadingProvider] Started loading model openai/gpt-oss-20b\n[ModelProxyObject(id=openai/gpt-oss-20b)] Forking LLMWorker with custom envVars: {&amp;quot;LD_LIBRARY_PATH&amp;quot;:&amp;quot;/home/skaldudritti/.lmstudio/extensions/backends/vendor/linux-llama-rocm-vendor-v3&amp;quot;,&amp;quot;HIP_VISIBLE_DEVICES&amp;quot;:&amp;quot;0&amp;quot;}\n[ProcessForkingProvider][NodeProcessForker] Spawned process 215047\n[ProcessForkingProvider][NodeProcessForker] Exited process 215047\n18:51:54.347 › [LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in channel handler: Error: Error loading model.\n    at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n    at _0x4ec43c.emit (node:events:518:28)\n    at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n    at _0x66b5e7.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n    at _0x66b5e7.emit (node:events:530:35)\n    at ChildProcess.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n    at ChildProcess.emit (node:events:518:28)\n    at ChildProcess._handle.onexit (node:internal/child_process:293:12)\n[LMSInternal][Client=LM Studio][Endpoint=loadModel] Error in loadModel channel _0x179e10 [Error]: Error loading model.\n    at _0x4ec43c._0x534819 (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:101:7607)\n    at _0x4ec43c.emit (node:events:518:28)\n    at _0x4ec43c.onChildExit (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206794)\n    at _0x66b5e7.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:86:206108)\n    at _0x66b5e7.emit (node:events:530:35)\n    at ChildProcess.&amp;lt;anonymous&amp;gt; (/tmp/.mount_LM-StuqHz37P/resources/app/.webpack/main/index.js:461:22485)\n    at ChildProcess.emit (node:events:518:28)\n    at ChildProcess._handle.onexit (node:internal/child_process:293:12) {\n  cause: &amp;#39;(Exit code: null). Please check settings and try loading the model again. &amp;#39;,\n  suggestion: &amp;#39;&amp;#39;,\n  errorData: undefined,\n  data: undefined,\n  displayData: undefined,\n  title: &amp;#39;Error loading model.&amp;#39;\n}\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mja233",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "unkz0r",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mja233/gptoss20b_on_lm_studio_ubuntu_rx7900xtx/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mja233/gptoss20b_on_lm_studio_ubuntu_rx7900xtx/",
            "subreddit_subscribers": 512426,
            "created_utc": 1754499286,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7atxsb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "unkz0r",
                      "can_mod_post": false,
                      "created_utc": 1754513492,
                      "send_replies": true,
                      "parent_id": "t1_n79kdru",
                      "score": 1,
                      "author_fullname": "t2_j1fy2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "context lenght set to: 4096.  \nSetting it to 31000 did not help. same result\n\nAlso tried the update to 0.3.22 (build 2).  \nI have 32GB of memory on the machine if that info helps",
                      "edited": 1754513851,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7atxsb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;context lenght set to: 4096.&lt;br/&gt;\nSetting it to 31000 did not help. same result&lt;/p&gt;\n\n&lt;p&gt;Also tried the update to 0.3.22 (build 2).&lt;br/&gt;\nI have 32GB of memory on the machine if that info helps&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": true,
                      "can_gild": false,
                      "link_id": "t3_1mja233",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mja233/gptoss20b_on_lm_studio_ubuntu_rx7900xtx/n7atxsb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754513492,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n79kdru",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sleepingsysadmin",
            "can_mod_post": false,
            "created_utc": 1754500667,
            "send_replies": true,
            "parent_id": "t3_1mja233",
            "score": 1,
            "author_fullname": "t2_x73h4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What's your settings to the model? Context length?\n\nTry setting context length to 31,000.",
            "edited": 1754501995,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79kdru",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your settings to the model? Context length?&lt;/p&gt;\n\n&lt;p&gt;Try setting context length to 31,000.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mja233/gptoss20b_on_lm_studio_ubuntu_rx7900xtx/n79kdru/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754500667,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mja233",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7b1r4b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Exciting-Rub-8473",
            "can_mod_post": false,
            "created_utc": 1754515791,
            "send_replies": true,
            "parent_id": "t3_1mja233",
            "score": 1,
            "author_fullname": "t2_1u369o4ded",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I didn't get the same error as you, but I got this idea to switch my runtime from Rocm to Vulkan based off this issue comment ([https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/297#issuecomment-2604915772](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/297#issuecomment-2604915772)), and immediately the gpt-oss-20b model loaded for me in a new chat. I am on a 9070XT.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7b1r4b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I didn&amp;#39;t get the same error as you, but I got this idea to switch my runtime from Rocm to Vulkan based off this issue comment (&lt;a href=\"https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/297#issuecomment-2604915772\"&gt;https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/297#issuecomment-2604915772&lt;/a&gt;), and immediately the gpt-oss-20b model loaded for me in a new chat. I am on a 9070XT.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mja233/gptoss20b_on_lm_studio_ubuntu_rx7900xtx/n7b1r4b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754515791,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mja233",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]