[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I finally got the hardware I needed to run LLMs locally.\n\nAfter some testing, here’s a quick recap of my experience so far:\n\nJan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.\n\nLM Studio: While the performance was slightly worse compared to [Jan.ai](http://Jan.ai) on the same models, I had the exact same feature limitations.\n\nAnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn't work at all, the agent would say it was going to search online, then immediately say it couldn’t. Completely unusable for my needs. \n\nLocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned “connection failed”.\n\nSo here’s my question:\n\nIs there any tool out there that’s easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?\n\nAgent mode\n\nDeep research\n\nImage generation\n\nInternet search\n\nCanvas sharing\n\nReading images or files\n\nMemory\n\nI'm trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I'm starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.\n\nAny advice is welcome.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Most complete almost plug and play LLM Tool with features",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgvyyj",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.7,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 4,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_d0wbvrzc",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 4,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754258843,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754258586,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I finally got the hardware I needed to run LLMs locally.&lt;/p&gt;\n\n&lt;p&gt;After some testing, here’s a quick recap of my experience so far:&lt;/p&gt;\n\n&lt;p&gt;Jan.ai: Amazing performance. It runs my models incredibly well, and overall the experience was smooth. However, I really need features like image upload, web search, and deep research capabilities, which are still missing or limited.&lt;/p&gt;\n\n&lt;p&gt;LM Studio: While the performance was slightly worse compared to &lt;a href=\"http://Jan.ai\"&gt;Jan.ai&lt;/a&gt; on the same models, I had the exact same feature limitations.&lt;/p&gt;\n\n&lt;p&gt;AnythingLLM and OpenWebUI: This was... frustrating. I got long, irrelevant, looping replies using the same models that worked perfectly on Jan and LM Studio. Web browsing didn&amp;#39;t work at all, the agent would say it was going to search online, then immediately say it couldn’t. Completely unusable for my needs. &lt;/p&gt;\n\n&lt;p&gt;LocalAI: This one gave me a headache. YAML hell, GUFF model issues, and very unstable behavior, it might start once, and then fail ten times for no reason. Even when it said it was running, localhost just returned “connection failed”.&lt;/p&gt;\n\n&lt;p&gt;So here’s my question:&lt;/p&gt;\n\n&lt;p&gt;Is there any tool out there that’s easier to set up (semi plug-and-play), where I can use different models for different tasks, or maybe even one good multimodal model, that can support these features?&lt;/p&gt;\n\n&lt;p&gt;Agent mode&lt;/p&gt;\n\n&lt;p&gt;Deep research&lt;/p&gt;\n\n&lt;p&gt;Image generation&lt;/p&gt;\n\n&lt;p&gt;Internet search&lt;/p&gt;\n\n&lt;p&gt;Canvas sharing&lt;/p&gt;\n\n&lt;p&gt;Reading images or files&lt;/p&gt;\n\n&lt;p&gt;Memory&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m trying to build a mini ChatGPT-style assistant for work, but completely locally and private. I have average tech skills, but I&amp;#39;m starting to lose my mind over how fragmented and unreliable everything feels for a newb in this field.&lt;/p&gt;\n\n&lt;p&gt;Any advice is welcome.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?auto=webp&amp;s=b61c13003a78792af3c70dba491521f8befe780b",
                    "width": 2400,
                    "height": 1350
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=316ac2c235dbf757adc6d57077bbf14ff212c7fd",
                      "width": 108,
                      "height": 60
                    },
                    {
                      "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6ebed2d4cb10be2cddc86b49f0bc6f6f16178fef",
                      "width": 216,
                      "height": 121
                    },
                    {
                      "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7726b791c72a2145f757f5c6e90ddb14254a02e8",
                      "width": 320,
                      "height": 180
                    },
                    {
                      "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bbfa85537dbc57841336cb1db86b585484d97de4",
                      "width": 640,
                      "height": 360
                    },
                    {
                      "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=749aec5f85de17cb9987662552bbb052f4717125",
                      "width": 960,
                      "height": 540
                    },
                    {
                      "url": "https://external-preview.redd.it/-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=39a55dd433726ea925f43fa48706981f88afb2dc",
                      "width": 1080,
                      "height": 607
                    }
                  ],
                  "variants": {},
                  "id": "-ctwWkN6rHGc2V6GtsAmk-HLdFHSpEj4U0gSuMMDRmw"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgvyyj",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ExtensionAd182",
            "discussion_type": null,
            "num_comments": 14,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/",
            "subreddit_subscribers": 509913,
            "created_utc": 1754258586,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6rzukk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1754262604,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 3,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "After trying for several months and methods, I still couldn't make Open webUI search the internet the way it should be. Perhaps it can't, or perhaps I'm too used to ChatGPT search it the right way, flawlessly, even without asking, and joining the information with the conversation context on the most perfect way. Not too much information, nor too less.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rzukk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;After trying for several months and methods, I still couldn&amp;#39;t make Open webUI search the internet the way it should be. Perhaps it can&amp;#39;t, or perhaps I&amp;#39;m too used to ChatGPT search it the right way, flawlessly, even without asking, and joining the information with the conversation context on the most perfect way. Not too much information, nor too less.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6rzukk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754262604,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6sgy17",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArsNeph",
            "can_mod_post": false,
            "created_utc": 1754268624,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 3,
            "author_fullname": "t2_vt0xkv60d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "OpenWebUI is probably your best bet for most of this. For agents and deep research, you're going to need a separate application. Likewise, for image generation you're going to have to connect an API to either an online service, or ComfyUI/Forge webui in API mode. Internet search is a built-in feature, but you have to connect an API key for Google/Brave or SEARXNG if you want to self host. You also have to modify the amount of results. Sharing jobs as a built-in feature, memory is an experimental feature you have to enable in settings. Reading images is a multimodal task, so the model you're using has to be a VLM for that. Reading files is a RAG task, so you should switch out the embedding model to something like BGE-M3 and modify the confidence threshold. That will also improve the quality of search results.\n\nYour problem with repetition almost definitely has to do with Ollama, which has terrible defaults. Go to the OpenWebUI admin panel, and edit the models. Set context length to at least a bare minimum of 8192, if not 16384. For web search and RAG, this is crucial. Ollama default models are also all at Q4KM, which is severely degraded. Go to their model library, hit more, and copy paste the command to download a model at Q8, unless it's a larger model. I recommend Mistral Small 3.2 24B, which is generally a GPT 4o mini equivalent model, generally speaking.\n\nAlternatively, run a model through something like KoboldCPP and tweak the settings manually.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6sgy17",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;OpenWebUI is probably your best bet for most of this. For agents and deep research, you&amp;#39;re going to need a separate application. Likewise, for image generation you&amp;#39;re going to have to connect an API to either an online service, or ComfyUI/Forge webui in API mode. Internet search is a built-in feature, but you have to connect an API key for Google/Brave or SEARXNG if you want to self host. You also have to modify the amount of results. Sharing jobs as a built-in feature, memory is an experimental feature you have to enable in settings. Reading images is a multimodal task, so the model you&amp;#39;re using has to be a VLM for that. Reading files is a RAG task, so you should switch out the embedding model to something like BGE-M3 and modify the confidence threshold. That will also improve the quality of search results.&lt;/p&gt;\n\n&lt;p&gt;Your problem with repetition almost definitely has to do with Ollama, which has terrible defaults. Go to the OpenWebUI admin panel, and edit the models. Set context length to at least a bare minimum of 8192, if not 16384. For web search and RAG, this is crucial. Ollama default models are also all at Q4KM, which is severely degraded. Go to their model library, hit more, and copy paste the command to download a model at Q8, unless it&amp;#39;s a larger model. I recommend Mistral Small 3.2 24B, which is generally a GPT 4o mini equivalent model, generally speaking.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, run a model through something like KoboldCPP and tweak the settings manually.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6sgy17/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754268624,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6swfzh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "InevitableArea1",
            "can_mod_post": false,
            "created_utc": 1754274204,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 3,
            "author_fullname": "t2_giasjuouk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "All these dogcrap ai wrappers/apps and we're still stuck with featureless, buggy, or non-user friendly application ngl. I run an LMstudio local server and openweb-ui or anythingllm depending on use-case. Anythingllm does better web search, if limited still, LMStudio is where models just work, Openweb-ui has the features I want if I feel like rolling the dice.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6swfzh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;All these dogcrap ai wrappers/apps and we&amp;#39;re still stuck with featureless, buggy, or non-user friendly application ngl. I run an LMstudio local server and openweb-ui or anythingllm depending on use-case. Anythingllm does better web search, if limited still, LMStudio is where models just work, Openweb-ui has the features I want if I feel like rolling the dice.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6swfzh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754274204,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6u2syz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wasteofwillpower",
            "can_mod_post": false,
            "created_utc": 1754294682,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 2,
            "author_fullname": "t2_un7ry25uz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "SillyTavern",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6u2syz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;SillyTavern&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6u2syz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754294682,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6rt73b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zipperlein",
            "can_mod_post": false,
            "created_utc": 1754260331,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 1,
            "author_fullname": "t2_x3duw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Don't know much about [jan.ai](http://jan.ai) but it has an openai-comptabile api server.  \n[https://jan.ai/changelog/2024-01-29-local-api-server](https://jan.ai/changelog/2024-01-29-local-api-server)  \nJust install open-webui in a docker container and point it toward that in the settings. U can later swap it for a more granular controllable inference engine. Ollama is easy for using multiple models because it automatically unloads models after a while.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rt73b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Don&amp;#39;t know much about &lt;a href=\"http://jan.ai\"&gt;jan.ai&lt;/a&gt; but it has an openai-comptabile api server.&lt;br/&gt;\n&lt;a href=\"https://jan.ai/changelog/2024-01-29-local-api-server\"&gt;https://jan.ai/changelog/2024-01-29-local-api-server&lt;/a&gt;&lt;br/&gt;\nJust install open-webui in a docker container and point it toward that in the settings. U can later swap it for a more granular controllable inference engine. Ollama is easy for using multiple models because it automatically unloads models after a while.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6rt73b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754260331,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6tly4r",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "hedonihilistic",
            "can_mod_post": false,
            "created_utc": 1754285320,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 1,
            "author_fullname": "t2_281myw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I created an [app for deep research](https://github.com/murtaza-nasir/maestro) precisely for these reasons. It doesn't have memory, canvas, images etc., at the moment, but I think for deep-research with local RAG I haven't seen anything similar.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6tly4r",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I created an &lt;a href=\"https://github.com/murtaza-nasir/maestro\"&gt;app for deep research&lt;/a&gt; precisely for these reasons. It doesn&amp;#39;t have memory, canvas, images etc., at the moment, but I think for deep-research with local RAG I haven&amp;#39;t seen anything similar.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6tly4r/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754285320,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6upcem",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Evening_Ad6637",
            "can_mod_post": false,
            "created_utc": 1754306732,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 1,
            "author_fullname": "t2_p45er6oo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My recommendation for you is:\n\n- Cherry Studio as the frontend - it has a lot of meaningful features and it's evolving at a really fast pace.\n\n\n- pure llama.cpp as your local backend: run each model with llama-server -m &lt;model&gt; and in cherry studio you can add any provider you want. So here you add http://localhost:&lt;port&gt;\n\n\n- alternatively use lm-studio as the backend, which is much more convenient. It’s also possible to run lm-studio headless with thei cli binary \"lms\" which is completely opensource (other than the frontend)\n\n\nEverything else that you need like image generation or vision capability can be added through mcp servers, which is really convenient to setup in cherry studio",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6upcem",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My recommendation for you is:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Cherry Studio as the frontend - it has a lot of meaningful features and it&amp;#39;s evolving at a really fast pace.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;pure llama.cpp as your local backend: run each model with llama-server -m &amp;lt;model&amp;gt; and in cherry studio you can add any provider you want. So here you add http://localhost:&amp;lt;port&amp;gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;alternatively use lm-studio as the backend, which is much more convenient. It’s also possible to run lm-studio headless with thei cli binary &amp;quot;lms&amp;quot; which is completely opensource (other than the frontend)&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Everything else that you need like image generation or vision capability can be added through mcp servers, which is really convenient to setup in cherry studio&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6upcem/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754306732,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ropxa",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ExtensionAd182",
                      "can_mod_post": false,
                      "created_utc": 1754258822,
                      "send_replies": true,
                      "parent_id": "t1_n6rogtc",
                      "score": 1,
                      "author_fullname": "t2_d0wbvrzc",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I tried also this, i forgot to mention, same issue as anything llm, loop, nosense, out of context replies",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ropxa",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I tried also this, i forgot to mention, same issue as anything llm, loop, nosense, out of context replies&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgvyyj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6ropxa/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754258822,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6rogtc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "EdanStarfire",
            "can_mod_post": false,
            "created_utc": 1754258738,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": 1,
            "author_fullname": "t2_bb4i818",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Honestly, canvas sharing seems pointless locally, but OpenWebUI can do most all of this.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6rogtc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Honestly, canvas sharing seems pointless locally, but OpenWebUI can do most all of this.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6rogtc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754258738,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6uolu7",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LocoMod",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6u8s4t",
                                "score": 1,
                                "author_fullname": "t2_6uuoq",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I understand it because it’s my career. I’ve been an SRE/DevOps/Platform architect for the better part of a decade now (before that Sysadmin). I have a home lab with multiple servers and GPUs. I hate to be the bearer of bad news. Even if you did have the compute capacity to run the best of the best models at home, they still don’t match the closed source models. It is quite literally impossible today. Maybe not tomorrow, but today it is. The open source models are just not there yet.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6uolu7",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I understand it because it’s my career. I’ve been an SRE/DevOps/Platform architect for the better part of a decade now (before that Sysadmin). I have a home lab with multiple servers and GPUs. I hate to be the bearer of bad news. Even if you did have the compute capacity to run the best of the best models at home, they still don’t match the closed source models. It is quite literally impossible today. Maybe not tomorrow, but today it is. The open source models are just not there yet.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgvyyj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6uolu7/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754306410,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754306410,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6u8s4t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Lesser-than",
                      "can_mod_post": false,
                      "created_utc": 1754298192,
                      "send_replies": true,
                      "parent_id": "t1_n6sn4dm",
                      "score": 2,
                      "author_fullname": "t2_98d256k",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I mean trying to get an image gen llm and a good text gen llm and image reading llm is kind of asking for a small server worth of processing power and wanting the best of every domain loaded and running at once. I can see why there is some want for this but I just dont think poeple understand what a cloud model does and the tools api's they use to do it. You can in some cases get fairly close in one domain but to string them all together into one tidy application you will be sitting in a pile of network cables in a server room.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6u8s4t",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I mean trying to get an image gen llm and a good text gen llm and image reading llm is kind of asking for a small server worth of processing power and wanting the best of every domain loaded and running at once. I can see why there is some want for this but I just dont think poeple understand what a cloud model does and the tools api&amp;#39;s they use to do it. You can in some cases get fairly close in one domain but to string them all together into one tidy application you will be sitting in a pile of network cables in a server room.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgvyyj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6u8s4t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754298192,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6ulta3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Evening_Ad6637",
                      "can_mod_post": false,
                      "created_utc": 1754305167,
                      "send_replies": true,
                      "parent_id": "t1_n6sn4dm",
                      "score": 2,
                      "author_fullname": "t2_p45er6oo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Your mistake is that you encourage local AI enthusiasts to compare open models against closed models and claim that the difference between open and closed is so huge.\n\n\nThat is definitely not the case. The problem is that you are using misleading wording, because in fact your complaint is based on a mismatch of hardware resources - not on the models themselves. You want a mere mortal with their limited few thousand dollars hardware to reproduce results achieved by corporations that have the resources of an entire city at their disposal.\n\n\nAnyone interested in local AI should realize that they have to make compromises and to live with these limitations - if not, then they need to reevaluate their expectations.\n\n\nBut again: this challenge is not based on an alleged weakness of the (mostly Chinese) open-weight models, but on the available resources behind them.\nThe open-weight models are increasingly closing the gap that we had a few years ago.\n\n\nA more realistic objection would be, for example, a comparison between Gemma models and Chinese models. Here, it is very clear that a company like Google does not have a secret sauce for the superiority of its Gemini models, but that astronomically large infrastructures and resources are behind them (either the model itself is gigantic, or the supporting agentic stack is). It’s the same with OpenAI. You are talking about \"GPT-4.1-mini\", but we don’t know anything about  this thing. What does „mini“ mean? GPT-4 has probably ~ 1500 billions parameters. While double the size of Deepseek R1, gpt-4 can not hold a candle to Deepseek.\n\n\nAnd that’s only talking about the model. Let alone the entire ecosystems with agents, guards, classifiers etc that corporations have alongside their models.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6ulta3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your mistake is that you encourage local AI enthusiasts to compare open models against closed models and claim that the difference between open and closed is so huge.&lt;/p&gt;\n\n&lt;p&gt;That is definitely not the case. The problem is that you are using misleading wording, because in fact your complaint is based on a mismatch of hardware resources - not on the models themselves. You want a mere mortal with their limited few thousand dollars hardware to reproduce results achieved by corporations that have the resources of an entire city at their disposal.&lt;/p&gt;\n\n&lt;p&gt;Anyone interested in local AI should realize that they have to make compromises and to live with these limitations - if not, then they need to reevaluate their expectations.&lt;/p&gt;\n\n&lt;p&gt;But again: this challenge is not based on an alleged weakness of the (mostly Chinese) open-weight models, but on the available resources behind them.\nThe open-weight models are increasingly closing the gap that we had a few years ago.&lt;/p&gt;\n\n&lt;p&gt;A more realistic objection would be, for example, a comparison between Gemma models and Chinese models. Here, it is very clear that a company like Google does not have a secret sauce for the superiority of its Gemini models, but that astronomically large infrastructures and resources are behind them (either the model itself is gigantic, or the supporting agentic stack is). It’s the same with OpenAI. You are talking about &amp;quot;GPT-4.1-mini&amp;quot;, but we don’t know anything about  this thing. What does „mini“ mean? GPT-4 has probably ~ 1500 billions parameters. While double the size of Deepseek R1, gpt-4 can not hold a candle to Deepseek.&lt;/p&gt;\n\n&lt;p&gt;And that’s only talking about the model. Let alone the entire ecosystems with agents, guards, classifiers etc that corporations have alongside their models.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgvyyj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6ulta3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754305167,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6sn4dm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LocoMod",
            "can_mod_post": false,
            "created_utc": 1754270836,
            "send_replies": true,
            "parent_id": "t3_1mgvyyj",
            "score": -2,
            "author_fullname": "t2_6uuoq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The truth no one talks about is you're not going to get very far on local models due to context limitations. Sure, you can do some basic things. But if you send anything above 8192 tokens, no matter what the model was trained for, it wont work. For complex workflows you need to process tens of thousands, hundreds of thousands, or million of tokens every few minutes. There is no local model that can do this efficiently. You'd have to build a rig that has a ton of fast memory, and the cost of such rig would be way more expensive than just using a public API provider than can crunch through tokens no problem. Even if you did manage to build such a rig, the models themselves just arent capable.\n\nIf anyone disagrees, dont disagree with words, show us the video, with the configs and the setup and the thing you did that disproves what i've said. Do it please. I want to be wrong. I've spent thousands of hours on this issue by now and no matter what, you can't beat physics.\n\nYou will get more utility out of dumping $100 in OpenAI credits using gpt-4.1-mini than you will out of pretty much any local model.\n\nThis is the sad truth. No one talks about this because they dont like it. I hate it too. This is the truth.\n\nYou disagree? Show is the proof of a local model doing something that gpt-4.1-mini cannot do. Show don't tell.\n\nGo ahead, i'll wait.\n\nEDIT: I purposely chose a third tier model because thats how far ahead closed models are to pretty much all open models, benchmarks be damned. Benchmarks are irrelevant. Show us an actual video of a local model doing something impressive. Benchmarks are worthless. No open source chinese model even comes close to a third tier closed model like gpt-4.1-mini in real world usage. This is a fact. Dont be mad. Show us the proof. Make sure we can replicate it. You will fail. I promise.",
            "edited": 1754271028,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6sn4dm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The truth no one talks about is you&amp;#39;re not going to get very far on local models due to context limitations. Sure, you can do some basic things. But if you send anything above 8192 tokens, no matter what the model was trained for, it wont work. For complex workflows you need to process tens of thousands, hundreds of thousands, or million of tokens every few minutes. There is no local model that can do this efficiently. You&amp;#39;d have to build a rig that has a ton of fast memory, and the cost of such rig would be way more expensive than just using a public API provider than can crunch through tokens no problem. Even if you did manage to build such a rig, the models themselves just arent capable.&lt;/p&gt;\n\n&lt;p&gt;If anyone disagrees, dont disagree with words, show us the video, with the configs and the setup and the thing you did that disproves what i&amp;#39;ve said. Do it please. I want to be wrong. I&amp;#39;ve spent thousands of hours on this issue by now and no matter what, you can&amp;#39;t beat physics.&lt;/p&gt;\n\n&lt;p&gt;You will get more utility out of dumping $100 in OpenAI credits using gpt-4.1-mini than you will out of pretty much any local model.&lt;/p&gt;\n\n&lt;p&gt;This is the sad truth. No one talks about this because they dont like it. I hate it too. This is the truth.&lt;/p&gt;\n\n&lt;p&gt;You disagree? Show is the proof of a local model doing something that gpt-4.1-mini cannot do. Show don&amp;#39;t tell.&lt;/p&gt;\n\n&lt;p&gt;Go ahead, i&amp;#39;ll wait.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I purposely chose a third tier model because thats how far ahead closed models are to pretty much all open models, benchmarks be damned. Benchmarks are irrelevant. Show us an actual video of a local model doing something impressive. Benchmarks are worthless. No open source chinese model even comes close to a third tier closed model like gpt-4.1-mini in real world usage. This is a fact. Dont be mad. Show us the proof. Make sure we can replicate it. You will fail. I promise.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgvyyj/most_complete_almost_plug_and_play_llm_tool_with/n6sn4dm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754270836,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgvyyj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -2
          }
        }
      ],
      "before": null
    }
  }
]