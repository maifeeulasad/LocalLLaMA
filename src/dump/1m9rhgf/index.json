[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi LLM Folks,\n\nTL/DR: I'm seeking tips for improving my ollama setup with Qwen3, deepseek and nomic-embed for home sized LLM instance.\n\nI'm in the LLM game for a couple of weeks now and still learning something new every day. I have an ollama instance on my Ryzen workstation running Debian and control it with a Lenovo X1C laptop which is also running Debian. It's a home setup so nothing too fancy. You can find the technical details below.\n\nPurpose of this machine is to answer all kind of questions (qwen3-30B), analyze PDF files (nomic-embed-text:latest) and summarize mails (deepseek-r1:14b), websites (qwen3:14b) etc. I'm still discovering what I could do more with it. Overall it should act as a local AI assistant. I could use some of your wisdom how to improve the setup of that machine for those tasks.\n\n1. I found the Qwen3-30B-A3B-GGUF model running quite good (10-20 tk/s) for overall questions on this hardware but would like to squeeze a little bit more performance out of it. I'm running it with num\\_ctx=5120, temperature=0.6, top\\_K=20, top\\_P=0.95. What could be improved, to give me a better quality of the answers or improve speed of the model?\n2. I would also like to improve the quality of analyzing PDF files. I found that the quality can differ widely. Some PDFs are being analyzed properly for others barely anything is done right, eg. only the metadata is identified but not the content. I use nomic-embed-text:latest for this task. Do you have a suggestion how to improve that or know a better tool I could use?\n3. I'm also not perfectly satisfied with the summaries of (deepseek-r1:14b) and (qwen3:14b). Both fit into the VRAM but sometimes the language is poor if they have to translate summaries into German or the summaries are way too short and they seem to miss most of the context. I'm also not sure if I need thinking models for that task or if I should try something else?\n4. Do you have some overall tips for setting up ollama? I learned that I can play around with KV cache, GPU layers etc. Is it possible to make ollama use all of the 12GB VRAM of the RTX 3060? Somehow it seems that around 1GB is always left free. Are there already some best practices on this for setups like mine? You can find my current settings below. And, would it make a notable difference if I would change the storage location of the models to a fast 1TB nvme? The workstation has a bunch of disks and currently the models reside on an older 256GB SSD.\n\nAny help improving my setup is appreciated.\n\nThanks for reading so far!\n\nBelow are some technical information and some examples how the models fit into VRAM/RAM:\n\n    Environments settings for ollama:\n    \n    Environment=\"OLLAMA_DEBUG=0\"\n    Environment=\"CUDA_VISIBLE_DEVICES=0\"\n    Environment=\"OLLAMA_NEW_ENGINE=1\"\n    Environment=\"OLLAMA_LLM_LIBRARY=cuda\"\n    Environment=\"OLLAMA_FLASH_ATTENTION=1\"\n    Environment=\"OLLAMA_NUM_PARALLEL=1\"\n    Environment=\"OLLAMA_MAX_LOADED_MODELS=1\"\n    Environment=\"OLLAMA_KV_CACHE_TYPE=q8_0\"\n    Environment=\"OLLAMA_MODELS=/chroot/AI/share/ollama/.ollama/models/\"\n    Environment=\"OLLAMA_NUM_GPU_LAYERS=36\"\n    Environment=\"OLLAMA_ORIGINS=moz-extension://*\"\n    \n    \n    \n    $ ollama ps                                                                                            \n    NAME                                       ID              SIZE     PROCESSOR          UNTIL                \n    hf.co/unsloth/Qwen3-30B-A3B-GGUF:Q5_K_M    c8c7e4f7bc56    23 GB    46%/54% CPU/GPU    29 minutes from now \n    deepseek-r1:14b                            c333b7232bdb    10.0 GB  100% GPU           4 minutes from now \n    qwen3:14b                                  bdbd181c33f2    10 GB    100% GPU           29 minutes from now   \n    nomic-embed-text:latest                    0a109f422b47    849 MB    100% GPU          4 minutes from now   \n    \n    \n    \n    $ nvidia-smi \n    Sat Jul 26 11:30:56 2025                                                                              \n    +-----------------------------------------------------------------------------------------+\n    | NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n    |-----------------------------------------+------------------------+----------------------+\n    | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n    | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n    |                                         |                        |               MIG M. |\n    |=========================================+========================+======================|\n    |   0  NVIDIA GeForce RTX 3060        Off |   00000000:08:00.0  On |                  N/A |\n    | 68%   54C    P2             57W /  170W |   11074MiB /  12288MiB |     17%      Default |\n    |                                         |                        |                  N/A |\n    +-----------------------------------------+------------------------+----------------------+\n                                                                                              \n    +-----------------------------------------------------------------------------------------+\n    | Processes:                                                                              |\n    |  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n    |        ID   ID                                                               Usage      |\n    |=========================================================================================|\n    |    0   N/A  N/A      4296      C   /chroot/AI/bin/ollama                       11068MiB |\n    +-----------------------------------------------------------------------------------------+\n    \n    \n    \n    $ inxi -bB                                                                                            \n    System:                                                                                               \n      Host: morpheus Kernel: 6.15.8-1-liquorix-amd64 arch: x86_64 bits: 64                     \n      Console: pty pts/2 Distro: Debian GNU/Linux 13 (trixie)                                             \n    Machine:     \n      Type: Desktop Mobo: ASUSTeK model: TUF GAMING X570-PLUS (WI-FI) v: Rev X.0x                         \n        serial: &lt;superuser required&gt; UEFI: American Megatrends v: 5021 date: 09/29/2024        \n    Battery:                                                                                              \n      Message: No system battery data found. Is one present?                                   \n    CPU:                                                                                                  \n      Info: 6-core AMD Ryzen 5 3600 [MT MCP] speed (MHz): avg: 1724 min/max: 558/4208          \n    Graphics:                                                                                             \n      Device-1: NVIDIA GA106 [GeForce RTX 3060 Lite Hash Rate] driver: nvidia v: 550.163.01    \n      Display: server: X.org v: 1.21.1.16 with: Xwayland v: 24.1.6 driver: X: loaded: nvidia   \n        unloaded: modesetting gpu: nvidia,nvidia-nvswitch tty: 204x45                          \n      API: OpenGL v: 4.6.0 compat-v: 4.5 vendor: mesa v: 25.1.5-0siduction1                    \n        note: console (EGL sourced) renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2, llvmpipe (LLVM 19.1.7\n        256 bits)                                                                                         \n      Info: Tools: api: clinfo, eglinfo, glxinfo, vulkaninfo de: kscreen-console,kscreen-doctor\n        gpu: nvidia-settings,nvidia-smi wl: wayland-info x11: xdriinfo, xdpyinfo, xprop, xrandr\n    Network:                                                                                              \n      Device-1: Intel Wi-Fi 5 Wireless-AC 9x6x [Thunder Peak] driver: iwlwifi                  \n    Drives:                                                                                               \n      Local Storage: total: 6.6 TiB used: 2.61 TiB (39.6%)                                     \n    Info:                                                                                                 \n      Memory: total: N/A available: 62.71 GiB used: 12.78 GiB (20.4%)\n      Processes: 298 Uptime: 1h 15m Init: systemd Shell: Bash inxi: 3.3.38   ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Tips for improving my ollama setup? - Ryzen 5 3600/ RTX 3060 12GB VRAM / 64 GB RAM - Qwen3-30B-A3B",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9rhgf",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_9cfit5mp",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753531862,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753530946,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi LLM Folks,&lt;/p&gt;\n\n&lt;p&gt;TL/DR: I&amp;#39;m seeking tips for improving my ollama setup with Qwen3, deepseek and nomic-embed for home sized LLM instance.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m in the LLM game for a couple of weeks now and still learning something new every day. I have an ollama instance on my Ryzen workstation running Debian and control it with a Lenovo X1C laptop which is also running Debian. It&amp;#39;s a home setup so nothing too fancy. You can find the technical details below.&lt;/p&gt;\n\n&lt;p&gt;Purpose of this machine is to answer all kind of questions (qwen3-30B), analyze PDF files (nomic-embed-text:latest) and summarize mails (deepseek-r1:14b), websites (qwen3:14b) etc. I&amp;#39;m still discovering what I could do more with it. Overall it should act as a local AI assistant. I could use some of your wisdom how to improve the setup of that machine for those tasks.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;I found the Qwen3-30B-A3B-GGUF model running quite good (10-20 tk/s) for overall questions on this hardware but would like to squeeze a little bit more performance out of it. I&amp;#39;m running it with num_ctx=5120, temperature=0.6, top_K=20, top_P=0.95. What could be improved, to give me a better quality of the answers or improve speed of the model?&lt;/li&gt;\n&lt;li&gt;I would also like to improve the quality of analyzing PDF files. I found that the quality can differ widely. Some PDFs are being analyzed properly for others barely anything is done right, eg. only the metadata is identified but not the content. I use nomic-embed-text:latest for this task. Do you have a suggestion how to improve that or know a better tool I could use?&lt;/li&gt;\n&lt;li&gt;I&amp;#39;m also not perfectly satisfied with the summaries of (deepseek-r1:14b) and (qwen3:14b). Both fit into the VRAM but sometimes the language is poor if they have to translate summaries into German or the summaries are way too short and they seem to miss most of the context. I&amp;#39;m also not sure if I need thinking models for that task or if I should try something else?&lt;/li&gt;\n&lt;li&gt;Do you have some overall tips for setting up ollama? I learned that I can play around with KV cache, GPU layers etc. Is it possible to make ollama use all of the 12GB VRAM of the RTX 3060? Somehow it seems that around 1GB is always left free. Are there already some best practices on this for setups like mine? You can find my current settings below. And, would it make a notable difference if I would change the storage location of the models to a fast 1TB nvme? The workstation has a bunch of disks and currently the models reside on an older 256GB SSD.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Any help improving my setup is appreciated.&lt;/p&gt;\n\n&lt;p&gt;Thanks for reading so far!&lt;/p&gt;\n\n&lt;p&gt;Below are some technical information and some examples how the models fit into VRAM/RAM:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Environments settings for ollama:\n\nEnvironment=&amp;quot;OLLAMA_DEBUG=0&amp;quot;\nEnvironment=&amp;quot;CUDA_VISIBLE_DEVICES=0&amp;quot;\nEnvironment=&amp;quot;OLLAMA_NEW_ENGINE=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_LLM_LIBRARY=cuda&amp;quot;\nEnvironment=&amp;quot;OLLAMA_FLASH_ATTENTION=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_NUM_PARALLEL=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_MAX_LOADED_MODELS=1&amp;quot;\nEnvironment=&amp;quot;OLLAMA_KV_CACHE_TYPE=q8_0&amp;quot;\nEnvironment=&amp;quot;OLLAMA_MODELS=/chroot/AI/share/ollama/.ollama/models/&amp;quot;\nEnvironment=&amp;quot;OLLAMA_NUM_GPU_LAYERS=36&amp;quot;\nEnvironment=&amp;quot;OLLAMA_ORIGINS=moz-extension://*&amp;quot;\n\n\n\n$ ollama ps                                                                                            \nNAME                                       ID              SIZE     PROCESSOR          UNTIL                \nhf.co/unsloth/Qwen3-30B-A3B-GGUF:Q5_K_M    c8c7e4f7bc56    23 GB    46%/54% CPU/GPU    29 minutes from now \ndeepseek-r1:14b                            c333b7232bdb    10.0 GB  100% GPU           4 minutes from now \nqwen3:14b                                  bdbd181c33f2    10 GB    100% GPU           29 minutes from now   \nnomic-embed-text:latest                    0a109f422b47    849 MB    100% GPU          4 minutes from now   \n\n\n\n$ nvidia-smi \nSat Jul 26 11:30:56 2025                                                                              \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3060        Off |   00000000:08:00.0  On |                  N/A |\n| 68%   54C    P2             57W /  170W |   11074MiB /  12288MiB |     17%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A      4296      C   /chroot/AI/bin/ollama                       11068MiB |\n+-----------------------------------------------------------------------------------------+\n\n\n\n$ inxi -bB                                                                                            \nSystem:                                                                                               \n  Host: morpheus Kernel: 6.15.8-1-liquorix-amd64 arch: x86_64 bits: 64                     \n  Console: pty pts/2 Distro: Debian GNU/Linux 13 (trixie)                                             \nMachine:     \n  Type: Desktop Mobo: ASUSTeK model: TUF GAMING X570-PLUS (WI-FI) v: Rev X.0x                         \n    serial: &amp;lt;superuser required&amp;gt; UEFI: American Megatrends v: 5021 date: 09/29/2024        \nBattery:                                                                                              \n  Message: No system battery data found. Is one present?                                   \nCPU:                                                                                                  \n  Info: 6-core AMD Ryzen 5 3600 [MT MCP] speed (MHz): avg: 1724 min/max: 558/4208          \nGraphics:                                                                                             \n  Device-1: NVIDIA GA106 [GeForce RTX 3060 Lite Hash Rate] driver: nvidia v: 550.163.01    \n  Display: server: X.org v: 1.21.1.16 with: Xwayland v: 24.1.6 driver: X: loaded: nvidia   \n    unloaded: modesetting gpu: nvidia,nvidia-nvswitch tty: 204x45                          \n  API: OpenGL v: 4.6.0 compat-v: 4.5 vendor: mesa v: 25.1.5-0siduction1                    \n    note: console (EGL sourced) renderer: NVIDIA GeForce RTX 3060/PCIe/SSE2, llvmpipe (LLVM 19.1.7\n    256 bits)                                                                                         \n  Info: Tools: api: clinfo, eglinfo, glxinfo, vulkaninfo de: kscreen-console,kscreen-doctor\n    gpu: nvidia-settings,nvidia-smi wl: wayland-info x11: xdriinfo, xdpyinfo, xprop, xrandr\nNetwork:                                                                                              \n  Device-1: Intel Wi-Fi 5 Wireless-AC 9x6x [Thunder Peak] driver: iwlwifi                  \nDrives:                                                                                               \n  Local Storage: total: 6.6 TiB used: 2.61 TiB (39.6%)                                     \nInfo:                                                                                                 \n  Memory: total: N/A available: 62.71 GiB used: 12.78 GiB (20.4%)\n  Processes: 298 Uptime: 1h 15m Init: systemd Shell: Bash inxi: 3.3.38   \n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m9rhgf",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Speedy-Wonder",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/",
            "subreddit_subscribers": 504973,
            "created_utc": 1753530946,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n59eo7j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Speedy-Wonder",
                      "can_mod_post": false,
                      "created_utc": 1753536004,
                      "send_replies": true,
                      "parent_id": "t1_n599ats",
                      "score": 2,
                      "author_fullname": "t2_9cfit5mp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks, I will take a look at it. The description at github sounds promising.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n59eo7j",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, I will take a look at it. The description at github sounds promising.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9rhgf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n59eo7j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753536004,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n599ats",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AliNT77",
            "can_mod_post": false,
            "created_utc": 1753533966,
            "send_replies": true,
            "parent_id": "t3_1m9rhgf",
            "score": 5,
            "author_fullname": "t2_66tlmx2l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "use ik\\_llama.cpp \n\ni run with these settings and its literally twice as fast as vanilla llama.cpp: \n\n./llama-server -m \\~/dev/Qwen3-30B-A3B-Q4\\_K\\_M.gguf -ctk q8\\_0 -ctv q6\\_0 -ngl 999 -v -ot blk.3\\[0-9\\].ffn=CPU,blk.4\\[0-9\\].ffn=CPU,blk.2\\[0-9\\].ffn=CPU,blk.1\\[7-9\\].ffn=CPU -fa -c 40960 -p \"you are a helpful assistant. /no-think\" --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 -fmoe\n\nim running on rtx 3080 10gb and ryzen 5 5600g. with PP around 500tps and TG128 at 45tps",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n599ats",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;use ik_llama.cpp &lt;/p&gt;\n\n&lt;p&gt;i run with these settings and its literally twice as fast as vanilla llama.cpp: &lt;/p&gt;\n\n&lt;p&gt;./llama-server -m ~/dev/Qwen3-30B-A3B-Q4_K_M.gguf -ctk q8_0 -ctv q6_0 -ngl 999 -v -ot blk.3[0-9].ffn=CPU,blk.4[0-9].ffn=CPU,blk.2[0-9].ffn=CPU,blk.1[7-9].ffn=CPU -fa -c 40960 -p &amp;quot;you are a helpful assistant. /no-think&amp;quot; --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0 -fmoe&lt;/p&gt;\n\n&lt;p&gt;im running on rtx 3080 10gb and ryzen 5 5600g. with PP around 500tps and TG128 at 45tps&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n599ats/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753533966,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9rhgf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n59qs3x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "wooden-guy",
                      "can_mod_post": false,
                      "created_utc": 1753540214,
                      "send_replies": true,
                      "parent_id": "t1_n59606a",
                      "score": 1,
                      "author_fullname": "t2_16to413y2o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Tip 2?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n59qs3x",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tip 2?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9rhgf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n59qs3x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753540214,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n59606a",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Former-Ad-5757",
            "can_mod_post": false,
            "created_utc": 1753532638,
            "send_replies": true,
            "parent_id": "t3_1m9rhgf",
            "score": 3,
            "author_fullname": "t2_ihsdiwk6k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Tip 1 drop ollama",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n59606a",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tip 1 drop ollama&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n59606a/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753532638,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1m9rhgf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n59gbx5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Speedy-Wonder",
                      "can_mod_post": false,
                      "created_utc": 1753536616,
                      "send_replies": true,
                      "parent_id": "t1_n59709x",
                      "score": 1,
                      "author_fullname": "t2_9cfit5mp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the tips on OCR. Regarding Qwen3-30B, the speed is overall ok for my regular questions, so I first would try to improve that for me. I've already tried the smaller ones (qwen 14b and gemma 12b, deepseek 14b, phi4-reasononing) but the responses of the bigger models are better. I made some tests on topics I'm familiar with and gemma had good pronunciation in German but the answers were not always correct. phii4-reasoning was really good but thinks a long time, deepseek 14b is good overall and qwen3 14b was way worse than the 30b variant.  I also use the smaller ones but mainly when I care about the response speed.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n59gbx5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the tips on OCR. Regarding Qwen3-30B, the speed is overall ok for my regular questions, so I first would try to improve that for me. I&amp;#39;ve already tried the smaller ones (qwen 14b and gemma 12b, deepseek 14b, phi4-reasononing) but the responses of the bigger models are better. I made some tests on topics I&amp;#39;m familiar with and gemma had good pronunciation in German but the answers were not always correct. phii4-reasoning was really good but thinks a long time, deepseek 14b is good overall and qwen3 14b was way worse than the 30b variant.  I also use the smaller ones but mainly when I care about the response speed.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9rhgf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n59gbx5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753536616,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n59709x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lly0571",
            "can_mod_post": false,
            "created_utc": 1753533051,
            "send_replies": true,
            "parent_id": "t3_1m9rhgf",
            "score": 1,
            "author_fullname": "t2_70vzcleel",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think PDF analysis are largely affected by your OCR config. Maybe you can try docling, markitdown or mineru.\n\nFor LLMs, I think you should use Qwen3-14B or Gemma3-12B on your GPU only. Gemma3 could be slightly better in German(idk, I know little about German) while Qwen is better in Chinese or Japanese.\n\nIf you want to use Qwen3-30B-A3B, you can try to offload MoE Tensor to CPU rather than offload layers to CPU using llama.cpp like this:\n\n    ./build/bin/llama-server --model /data/huggingface/qwen3-gguf/Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-UD-Q6_K_XL.gguf -ngl 99 -ot \"(1[6-9]|[2-9][0-9]).ffn_.*_exps.=CPU\"  --port 8000 -fa -a Qwen3-30B-A3B-Q6 --temp 0.6 --top_p 0.95 --top_k 20 --min_p 0 --prio 3 --ctx_size 16384 --jinja",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n59709x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think PDF analysis are largely affected by your OCR config. Maybe you can try docling, markitdown or mineru.&lt;/p&gt;\n\n&lt;p&gt;For LLMs, I think you should use Qwen3-14B or Gemma3-12B on your GPU only. Gemma3 could be slightly better in German(idk, I know little about German) while Qwen is better in Chinese or Japanese.&lt;/p&gt;\n\n&lt;p&gt;If you want to use Qwen3-30B-A3B, you can try to offload MoE Tensor to CPU rather than offload layers to CPU using llama.cpp like this:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;./build/bin/llama-server --model /data/huggingface/qwen3-gguf/Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-UD-Q6_K_XL.gguf -ngl 99 -ot &amp;quot;(1[6-9]|[2-9][0-9]).ffn_.*_exps.=CPU&amp;quot;  --port 8000 -fa -a Qwen3-30B-A3B-Q6 --temp 0.6 --top_p 0.95 --top_k 20 --min_p 0 --prio 3 --ctx_size 16384 --jinja\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n59709x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753533051,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9rhgf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5bfq3g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "steezy13312",
                      "can_mod_post": false,
                      "created_utc": 1753559471,
                      "send_replies": true,
                      "parent_id": "t1_n593lay",
                      "score": 1,
                      "author_fullname": "t2_rfjj2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Geez. Thanks for letting me know not to purchase from you. ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5bfq3g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Geez. Thanks for letting me know not to purchase from you. &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9rhgf",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n5bfq3g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753559471,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n593lay",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTshop_ai",
            "can_mod_post": false,
            "created_utc": 1753531629,
            "send_replies": true,
            "parent_id": "t3_1m9rhgf",
            "score": -4,
            "author_fullname": "t2_rkmud0isr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Pro tip: give to some kid and get something real.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n593lay",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Pro tip: give to some kid and get something real.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9rhgf/tips_for_improving_my_ollama_setup_ryzen_5_3600/n593lay/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753531629,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9rhgf",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -4
          }
        }
      ],
      "before": null
    }
  }
]