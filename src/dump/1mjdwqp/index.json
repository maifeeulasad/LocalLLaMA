[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Everyone is fine-tuning LLMs could be more better.\nI thought a method that lets your llm learn a new programming language (like Zig) with 500 examples instead of 10,000.\nIt even strengthens the base language in the process.\nGitHub link:https://github.com/Intro0siddiqui/Cross-Structural-Alignment-for-Efficient-Code-Language-Fine-Tuning",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Cross-Structural Alignment for Efficient Code Language Fine-Tuning",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "News"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjdwqp",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1814na85l6",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "News",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754507925,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Everyone is fine-tuning LLMs could be more better.\nI thought a method that lets your llm learn a new programming language (like Zig) with 500 examples instead of 10,000.\nIt even strengthens the base language in the process.\nGitHub link:&lt;a href=\"https://github.com/Intro0siddiqui/Cross-Structural-Alignment-for-Efficient-Code-Language-Fine-Tuning\"&gt;https://github.com/Intro0siddiqui/Cross-Structural-Alignment-for-Efficient-Code-Language-Fine-Tuning&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?auto=webp&amp;s=37358af54dbda8886cd4fc99512cbcc367b29572",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=038431aec3a98d39bf09caa6c4528da7b16af86e",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ccb1389746c864f477d3dab9a48f8e5ca726e98",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=268f71640457e68a0a5bf7822fe593968693af2d",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c4115ade58eacbf6dd3b1c76803a3adabac664c8",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=697afc51362246adfe30d9329009efd47d71a56b",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=117a2ba62619b7df7d8a2742bb12d85fa0cfc6e9",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "Fh4kwpsP6ncFlWtVmWh1fD_Y4sFOwCEjfOesaIiXhVU"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#cc3600",
            "id": "1mjdwqp",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Ok_Horror_8567",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjdwqp/crossstructural_alignment_for_efficient_code/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjdwqp/crossstructural_alignment_for_efficient_code/",
            "subreddit_subscribers": 512426,
            "created_utc": 1754507925,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7abng0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "i-exist-man",
            "can_mod_post": false,
            "created_utc": 1754508312,
            "send_replies": true,
            "parent_id": "t3_1mjdwqp",
            "score": 1,
            "author_fullname": "t2_1o76rg3law",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sounds interesting. \n\nI have always thought that we need the best 3-30B coding models which are just the master of one langauge whether it be rust, python or typescript as these are the best known\n\nSo one could be a python-30B which beats everything at python\n\nand so on.., so basically specialisation of the models into different langauges\n\nand then maybe as you propose, we can fine tune those models to even a much further degree into more niche like zig etc. I find this idea to be really interesting. \n\n\n\nThis might seem like a rookie question but it mentions lora and so I want to ask how much difference would there be between models..\n\nSo like lets say I have a python model trained and then I want to have a typescript model instead..\n\nWhat if I can just pull in git diff from hugging face instead of downloading the WHOLE model again or just download the lora part of this, pardon me but I don't know much about lora.\n\nI imagine an agentic capability where it can automatically download and update itself with restrictions ofc and so it is the master of one and jack of all trades.\n\n  \nReally curious, you did a great job!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7abng0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sounds interesting. &lt;/p&gt;\n\n&lt;p&gt;I have always thought that we need the best 3-30B coding models which are just the master of one langauge whether it be rust, python or typescript as these are the best known&lt;/p&gt;\n\n&lt;p&gt;So one could be a python-30B which beats everything at python&lt;/p&gt;\n\n&lt;p&gt;and so on.., so basically specialisation of the models into different langauges&lt;/p&gt;\n\n&lt;p&gt;and then maybe as you propose, we can fine tune those models to even a much further degree into more niche like zig etc. I find this idea to be really interesting. &lt;/p&gt;\n\n&lt;p&gt;This might seem like a rookie question but it mentions lora and so I want to ask how much difference would there be between models..&lt;/p&gt;\n\n&lt;p&gt;So like lets say I have a python model trained and then I want to have a typescript model instead..&lt;/p&gt;\n\n&lt;p&gt;What if I can just pull in git diff from hugging face instead of downloading the WHOLE model again or just download the lora part of this, pardon me but I don&amp;#39;t know much about lora.&lt;/p&gt;\n\n&lt;p&gt;I imagine an agentic capability where it can automatically download and update itself with restrictions ofc and so it is the master of one and jack of all trades.&lt;/p&gt;\n\n&lt;p&gt;Really curious, you did a great job!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdwqp/crossstructural_alignment_for_efficient_code/n7abng0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754508312,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdwqp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ad6x7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ok_Horror_8567",
            "can_mod_post": false,
            "created_utc": 1754508754,
            "send_replies": true,
            "parent_id": "t3_1mjdwqp",
            "score": 1,
            "author_fullname": "t2_1814na85l6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes, LoRA is the diff. The base model stays static. You just download the small LoRA file (50MB–300MB typically), inject it, and boom — new behavior.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ad6x7",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, LoRA is the diff. The base model stays static. You just download the small LoRA file (50MB–300MB typically), inject it, and boom — new behavior.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdwqp/crossstructural_alignment_for_efficient_code/n7ad6x7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754508754,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdwqp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7at7hg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Accomplished_Ad9530",
            "can_mod_post": false,
            "created_utc": 1754513288,
            "send_replies": true,
            "parent_id": "t3_1mjdwqp",
            "score": 1,
            "author_fullname": "t2_88fma001",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Only a readme, no implementation or experiments?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7at7hg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Only a readme, no implementation or experiments?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjdwqp/crossstructural_alignment_for_efficient_code/n7at7hg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754513288,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjdwqp",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]