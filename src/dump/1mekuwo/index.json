[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone,\n\nI want to run Qwen3-Coder-30B-A3B-Instruct locally and get fast code suggestions similar to Cursor AI. Here is my current system:\n\n* CPU: 8-core, 16-thread Intel i7-12700K\n* GPU: NVIDIA RTX 3070 or 4070 with 12 to 16 GB VRAM\n* RAM: 64 GB DDR4 or DDR5\n* Storage: 1 TB NVMe SSD\n* Operating System: Windows 10 or 11 64-bit or Linux\n\nI am wondering if this setup is enough to run the model smoothly with tools like LM Studio or llama.cpp. Will I get good speed or will it feel slow? What kind of performance can I expect when doing agentic coding tasks or handling large contexts like full repositories?\n\nAlso, would upgrading to a 3090 or 4090 GPU make a big difference for running this model?\n\n**Note:** I am pretty new to this stuff, so please go easy on me.\n\nAny advice or real experience would be really helpful. Thanks!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What kind of system do I need to run Qwen3-Coder locally like Cursor AI? Is my setup enough?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mekuwo",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_q1yzxk7k1",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754015664,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I want to run Qwen3-Coder-30B-A3B-Instruct locally and get fast code suggestions similar to Cursor AI. Here is my current system:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;CPU: 8-core, 16-thread Intel i7-12700K&lt;/li&gt;\n&lt;li&gt;GPU: NVIDIA RTX 3070 or 4070 with 12 to 16 GB VRAM&lt;/li&gt;\n&lt;li&gt;RAM: 64 GB DDR4 or DDR5&lt;/li&gt;\n&lt;li&gt;Storage: 1 TB NVMe SSD&lt;/li&gt;\n&lt;li&gt;Operating System: Windows 10 or 11 64-bit or Linux&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I am wondering if this setup is enough to run the model smoothly with tools like LM Studio or llama.cpp. Will I get good speed or will it feel slow? What kind of performance can I expect when doing agentic coding tasks or handling large contexts like full repositories?&lt;/p&gt;\n\n&lt;p&gt;Also, would upgrading to a 3090 or 4090 GPU make a big difference for running this model?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I am pretty new to this stuff, so please go easy on me.&lt;/p&gt;\n\n&lt;p&gt;Any advice or real experience would be really helpful. Thanks!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mekuwo",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Medical_Path2953",
            "discussion_type": null,
            "num_comments": 17,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/",
            "subreddit_subscribers": 508191,
            "created_utc": 1754015664,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6agj5u",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6aalny",
                                "score": 1,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Ah, I was wondering.  In that case definitely go with the DDR5 and make sure it's dual channel (2 32GB sticks not 1 64GB stick).  The GPU is a bit of a tossup, maybe get the 3070 to save for the 3090?\n\nI haven't used Cursor so I can't say.  20 is a little slow, 160 is super fast.  A token is about 3/4 of an English word or a group of spaces or a symbol, etc.  For example:\n\n    background-color: #f4f4f4;\n\nis 15 tokens. `background-color` is 3, and then pretty much every character after that is its own token (` #` is the only double character one).  So it can definitely be a little slow for coding, but for writing it's faster than you can read.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6agj5u",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah, I was wondering.  In that case definitely go with the DDR5 and make sure it&amp;#39;s dual channel (2 32GB sticks not 1 64GB stick).  The GPU is a bit of a tossup, maybe get the 3070 to save for the 3090?&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t used Cursor so I can&amp;#39;t say.  20 is a little slow, 160 is super fast.  A token is about 3/4 of an English word or a group of spaces or a symbol, etc.  For example:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;background-color: #f4f4f4;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;is 15 tokens. &lt;code&gt;background-color&lt;/code&gt; is 3, and then pretty much every character after that is its own token (&lt;code&gt;#&lt;/code&gt; is the only double character one).  So it can definitely be a little slow for coding, but for writing it&amp;#39;s faster than you can read.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mekuwo",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6agj5u/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754020100,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754020100,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6aalny",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Medical_Path2953",
                      "can_mod_post": false,
                      "created_utc": 1754017650,
                      "send_replies": true,
                      "parent_id": "t1_n6a80yo",
                      "score": 1,
                      "author_fullname": "t2_q1yzxk7k1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the detailed breakdown, really helpful. \n\nI was planning to get this build mentioned in the description. So yeah, that’s why I had \"or\" in the description. I’m still finalizing parts. Sounds like I’ll be mostly running on CPU with this setup, and 20 to 25 tokens per second is totally acceptable for my use case, especially if I can get a bit more with hybrid GPU and CPU.\n\nGood to know that 3090 gives a big boost. I’ll consider it down the line.\n\nAlso, just curious about what token per second range would you consider smooth or close to Cursor-like speed?\n\nThanks again for your input.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6aalny",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the detailed breakdown, really helpful. &lt;/p&gt;\n\n&lt;p&gt;I was planning to get this build mentioned in the description. So yeah, that’s why I had &amp;quot;or&amp;quot; in the description. I’m still finalizing parts. Sounds like I’ll be mostly running on CPU with this setup, and 20 to 25 tokens per second is totally acceptable for my use case, especially if I can get a bit more with hybrid GPU and CPU.&lt;/p&gt;\n\n&lt;p&gt;Good to know that 3090 gives a big boost. I’ll consider it down the line.&lt;/p&gt;\n\n&lt;p&gt;Also, just curious about what token per second range would you consider smooth or close to Cursor-like speed?&lt;/p&gt;\n\n&lt;p&gt;Thanks again for your input.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mekuwo",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6aalny/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754017650,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ao2be",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6al6l6",
                                "score": 1,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It's honestly hard to say... unsloth's Q4_K_S is probably fine, it's just that after Q4 things do tend to get worse fast, and for smaller models even Q4 can show noticeable degradation.  Also some people swear for coding anything less that Q6 is bad, but I'm not super convinced myself.\n\nHaving messed with the original Qwen3-30B-A3B, I do think Q6 helped stabilize it somewhat and seemed to give less, let's say, \"3B moments\".  How the new ones fare though, it's too early for me to really say.  If you do have problems, though, I'd definitely say try Q6 before giving up on the model.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ao2be",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s honestly hard to say... unsloth&amp;#39;s Q4_K_S is probably fine, it&amp;#39;s just that after Q4 things do tend to get worse fast, and for smaller models even Q4 can show noticeable degradation.  Also some people swear for coding anything less that Q6 is bad, but I&amp;#39;m not super convinced myself.&lt;/p&gt;\n\n&lt;p&gt;Having messed with the original Qwen3-30B-A3B, I do think Q6 helped stabilize it somewhat and seemed to give less, let&amp;#39;s say, &amp;quot;3B moments&amp;quot;.  How the new ones fare though, it&amp;#39;s too early for me to really say.  If you do have problems, though, I&amp;#39;d definitely say try Q6 before giving up on the model.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mekuwo",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6ao2be/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754023433,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754023433,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6al6l6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Independent-Desk5910",
                      "can_mod_post": false,
                      "created_utc": 1754022115,
                      "send_replies": true,
                      "parent_id": "t1_n6a80yo",
                      "score": 1,
                      "author_fullname": "t2_1t8nko6ocb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Are the middling quants really that bad? I've been using unsloth's Q4\\_K\\_S and I haven't really been disappointed, though my usecases aren't particularly intensive, and I haven't tried the lower quants due to internet cap concerns. (Thanks, comcast.) Should I be using a lower quant?",
                      "edited": 1754022610,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6al6l6",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are the middling quants really that bad? I&amp;#39;ve been using unsloth&amp;#39;s Q4_K_S and I haven&amp;#39;t really been disappointed, though my usecases aren&amp;#39;t particularly intensive, and I haven&amp;#39;t tried the lower quants due to internet cap concerns. (Thanks, comcast.) Should I be using a lower quant?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mekuwo",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6al6l6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754022115,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6amc9q",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Gridhub",
                      "can_mod_post": false,
                      "created_utc": 1754022638,
                      "send_replies": true,
                      "parent_id": "t1_n6a80yo",
                      "score": 1,
                      "author_fullname": "t2_m3i96ufh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "for a 7900XTX could i try q5",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6amc9q",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;for a 7900XTX could i try q5&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mekuwo",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6amc9q/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754022638,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6a80yo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754016636,
            "send_replies": true,
            "parent_id": "t3_1mekuwo",
            "score": 5,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why do you have \"or\" in your current system description?\n\nAt Q4, the model is roughly 18GB, which means it won't fit on either GPU.  You could go with a higher quant, but I think Q4 is already pushing it a little for that one.  So that means you be running it mostly on CPU.  There the \"DDR4 or DDR5\" makes a fairly large difference.  If you are running purely on CPU I'd expect you to get something like 20-25 t/s which should be pretty alright.  It you put part of it on the GPU, maybe bump that to 40 or so.\n\nIf you upgrade to a 3090 you'll get maybe about 160t/s but you will be a little more limited on the total context you can hold, since getting that speed is conditional on fitting the entire model + context in the 24GB.\n\n&gt; What kind of performance can I expect when doing agentic coding tasks or handling large contexts like full repositories?\n\nI think that's a but TBD, but initial reports seem good?  Predicting the performance at larger contexts is a bit more difficult, so you'll need to benchmark, but I would say that it's worth the time to do so.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6a80yo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why do you have &amp;quot;or&amp;quot; in your current system description?&lt;/p&gt;\n\n&lt;p&gt;At Q4, the model is roughly 18GB, which means it won&amp;#39;t fit on either GPU.  You could go with a higher quant, but I think Q4 is already pushing it a little for that one.  So that means you be running it mostly on CPU.  There the &amp;quot;DDR4 or DDR5&amp;quot; makes a fairly large difference.  If you are running purely on CPU I&amp;#39;d expect you to get something like 20-25 t/s which should be pretty alright.  It you put part of it on the GPU, maybe bump that to 40 or so.&lt;/p&gt;\n\n&lt;p&gt;If you upgrade to a 3090 you&amp;#39;ll get maybe about 160t/s but you will be a little more limited on the total context you can hold, since getting that speed is conditional on fitting the entire model + context in the 24GB.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;What kind of performance can I expect when doing agentic coding tasks or handling large contexts like full repositories?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I think that&amp;#39;s a but TBD, but initial reports seem good?  Predicting the performance at larger contexts is a bit more difficult, so you&amp;#39;ll need to benchmark, but I would say that it&amp;#39;s worth the time to do so.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6a80yo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754016636,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mekuwo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6advur",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "jwpbe",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6a9obs",
                                "score": 1,
                                "author_fullname": "t2_1uqfjcqyh3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I would say that it's probably the best you're going to get on that setup and more than sufficient for hobbyist tasks. you may have to \"quantize the kv cache\" on qwen3 or \"offload experts to system ram\" but it's still really performant.\n\nYou can always use kimi.ai as well to get some questions answered too, it's agentic web search so you can toss in a printout of an error you're getting or just a simple \"what are the most popular firefox forks for arch\" or the technical terms I threw at you in the first paragraph.\n\nYou're better off switching to Linux. The only time I use windows anymore is for games that don't have anti cheat support under linux. the proton library for linux covers so many games now i don't really worry about it for most things. It's not 2007 anymore, Linux use is very simplified and straightforward for most things and very pretty and nice to look at.\n\nThe speedup is really noticable because all of the telemetry and junk is stripped out. You can customize everything about KDE and the built in \"search -&gt; install\" feature for themes, etc, makes it really simple.\n\nif you use arch via endevourOS i recommend installing \"paru\" immediately and \"lsparu\" to get access to the arch user repository. Everyone will have a personally preferred terminal setup, software library manager, etc. something like those two terminal software managers combined with the \"kitty\" terminal emulator will get you off to a solid, powerful start. get the cuda toolkit from paru and git clone the ik_llama repo and download some ggufs. you can search up a paru tutorial but just typing \"paru (program)\" will get you a list of programs to install. lsparu allows you to search in a text user interface to find programs, it's a little more verbose and simpler to work with.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6advur",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would say that it&amp;#39;s probably the best you&amp;#39;re going to get on that setup and more than sufficient for hobbyist tasks. you may have to &amp;quot;quantize the kv cache&amp;quot; on qwen3 or &amp;quot;offload experts to system ram&amp;quot; but it&amp;#39;s still really performant.&lt;/p&gt;\n\n&lt;p&gt;You can always use kimi.ai as well to get some questions answered too, it&amp;#39;s agentic web search so you can toss in a printout of an error you&amp;#39;re getting or just a simple &amp;quot;what are the most popular firefox forks for arch&amp;quot; or the technical terms I threw at you in the first paragraph.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;re better off switching to Linux. The only time I use windows anymore is for games that don&amp;#39;t have anti cheat support under linux. the proton library for linux covers so many games now i don&amp;#39;t really worry about it for most things. It&amp;#39;s not 2007 anymore, Linux use is very simplified and straightforward for most things and very pretty and nice to look at.&lt;/p&gt;\n\n&lt;p&gt;The speedup is really noticable because all of the telemetry and junk is stripped out. You can customize everything about KDE and the built in &amp;quot;search -&amp;gt; install&amp;quot; feature for themes, etc, makes it really simple.&lt;/p&gt;\n\n&lt;p&gt;if you use arch via endevourOS i recommend installing &amp;quot;paru&amp;quot; immediately and &amp;quot;lsparu&amp;quot; to get access to the arch user repository. Everyone will have a personally preferred terminal setup, software library manager, etc. something like those two terminal software managers combined with the &amp;quot;kitty&amp;quot; terminal emulator will get you off to a solid, powerful start. get the cuda toolkit from paru and git clone the ik_llama repo and download some ggufs. you can search up a paru tutorial but just typing &amp;quot;paru (program)&amp;quot; will get you a list of programs to install. lsparu allows you to search in a text user interface to find programs, it&amp;#39;s a little more verbose and simpler to work with.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mekuwo",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6advur/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754018987,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754018987,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6a9obs",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Medical_Path2953",
                      "can_mod_post": false,
                      "created_utc": 1754017280,
                      "send_replies": true,
                      "parent_id": "t1_n6a6r0q",
                      "score": 1,
                      "author_fullname": "t2_q1yzxk7k1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks so much for sharing all this detailed info and the helpful links! I really appreciate it.\n\nIf I follow your advice and use ik\\_llama with ubergarm’s quant models on my setup (RTX 3070 or similar), do you think I can expect smooth and fast performance for coding tasks, especially for PHP and MERN stack development?\n\nAlso, I’m new to Linux and SSH setups, do you think it’s worth switching from Windows to a simple Linux distro like EndeavourOS just for better performance, or can I still get decent results on Windows?\n\nThanks again for your help!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6a9obs",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks so much for sharing all this detailed info and the helpful links! I really appreciate it.&lt;/p&gt;\n\n&lt;p&gt;If I follow your advice and use ik_llama with ubergarm’s quant models on my setup (RTX 3070 or similar), do you think I can expect smooth and fast performance for coding tasks, especially for PHP and MERN stack development?&lt;/p&gt;\n\n&lt;p&gt;Also, I’m new to Linux and SSH setups, do you think it’s worth switching from Windows to a simple Linux distro like EndeavourOS just for better performance, or can I still get decent results on Windows?&lt;/p&gt;\n\n&lt;p&gt;Thanks again for your help!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mekuwo",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6a9obs/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754017280,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6a6r0q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jwpbe",
            "can_mod_post": false,
            "created_utc": 1754016150,
            "send_replies": true,
            "parent_id": "t3_1mekuwo",
            "score": 1,
            "author_fullname": "t2_1uqfjcqyh3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "yeah, i know i just signed up to reddit and all of my posts are about this so far (lmao) but I was running the original qwen3 30b a3b on a 3070 and was getting 30 to 60 tokens a second with ik_llama and ubergarm's quant.\n\ni would recommend a simple arch setup like endevourOS with kde plasma, or ideally don't run a desktop environment and ssh into it from another machine to save yourself the vram.\n\nyou can find instructions on how to use ik_llama here:\nhttps://github.com/ikawrakow/ik_llama.cpp/discussions/258\n\nubergarm's quants are here:\nhttps://huggingface.co/ubergarm\n\nmy current command i'm running on a 3090 running another qwen3 a3b:\n\nik-llama-server --model ~/ai/models/Qwen3-30B-A3B-Thinking-2507-IQ4_K.gguf --port xxxx --host 0.0.0.0 -fmoe -fa -ngl 99 --threads 1 --alias Qwen3-A3B-30B --temp 0.6 --top_p 0.95 --min_p 0 --top_k 20 -c 65536\n\nmy output with a fat chunk of context (havent optimized this yet):\n\ngeneration eval time =     750.83 ms /    70 runs   (   10.73 ms per token,    93.23 tokens per second)\n\nit's a lot simpler than it looks. with all of the tutorials available you can get your hand held all the way up to the end when you're inferring. you can use https://chat.chutes.ai to talk to one of the bigger models too if you want free help with getting it set up.",
            "edited": 1754016377,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6a6r0q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah, i know i just signed up to reddit and all of my posts are about this so far (lmao) but I was running the original qwen3 30b a3b on a 3070 and was getting 30 to 60 tokens a second with ik_llama and ubergarm&amp;#39;s quant.&lt;/p&gt;\n\n&lt;p&gt;i would recommend a simple arch setup like endevourOS with kde plasma, or ideally don&amp;#39;t run a desktop environment and ssh into it from another machine to save yourself the vram.&lt;/p&gt;\n\n&lt;p&gt;you can find instructions on how to use ik_llama here:\n&lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp/discussions/258\"&gt;https://github.com/ikawrakow/ik_llama.cpp/discussions/258&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;ubergarm&amp;#39;s quants are here:\n&lt;a href=\"https://huggingface.co/ubergarm\"&gt;https://huggingface.co/ubergarm&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;my current command i&amp;#39;m running on a 3090 running another qwen3 a3b:&lt;/p&gt;\n\n&lt;p&gt;ik-llama-server --model ~/ai/models/Qwen3-30B-A3B-Thinking-2507-IQ4_K.gguf --port xxxx --host 0.0.0.0 -fmoe -fa -ngl 99 --threads 1 --alias Qwen3-A3B-30B --temp 0.6 --top_p 0.95 --min_p 0 --top_k 20 -c 65536&lt;/p&gt;\n\n&lt;p&gt;my output with a fat chunk of context (havent optimized this yet):&lt;/p&gt;\n\n&lt;p&gt;generation eval time =     750.83 ms /    70 runs   (   10.73 ms per token,    93.23 tokens per second)&lt;/p&gt;\n\n&lt;p&gt;it&amp;#39;s a lot simpler than it looks. with all of the tutorials available you can get your hand held all the way up to the end when you&amp;#39;re inferring. you can use &lt;a href=\"https://chat.chutes.ai\"&gt;https://chat.chutes.ai&lt;/a&gt; to talk to one of the bigger models too if you want free help with getting it set up.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6a6r0q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754016150,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mekuwo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6a9u11",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Medical_Path2953",
                      "can_mod_post": false,
                      "created_utc": 1754017344,
                      "send_replies": true,
                      "parent_id": "t1_n6a6t2b",
                      "score": 1,
                      "author_fullname": "t2_q1yzxk7k1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the insight! It’s good to know a 3090 can handle Q4 quantization comfortably with enough room for context.\n\nI’ll wait to hear from you once you’ve had a chance to try it out on your rig. I’ll ping you tomorrow if I don’t hear anything by then.\n\nReally appreciate your help!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6a9u11",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the insight! It’s good to know a 3090 can handle Q4 quantization comfortably with enough room for context.&lt;/p&gt;\n\n&lt;p&gt;I’ll wait to hear from you once you’ve had a chance to try it out on your rig. I’ll ping you tomorrow if I don’t hear anything by then.&lt;/p&gt;\n\n&lt;p&gt;Really appreciate your help!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mekuwo",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6a9u11/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754017344,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6a6t2b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1754016172,
            "send_replies": true,
            "parent_id": "t3_1mekuwo",
            "score": 1,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "A 3090 would definitely make a very big difference. You can comfortably fit Q4 with plenty left for context. \n\nHaven't had time to download and fiddle with it yet. Ping me again tomorrow if you haven't heard any numbers. I'll be downloading it on my 3090s rig.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6a6t2b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;A 3090 would definitely make a very big difference. You can comfortably fit Q4 with plenty left for context. &lt;/p&gt;\n\n&lt;p&gt;Haven&amp;#39;t had time to download and fiddle with it yet. Ping me again tomorrow if you haven&amp;#39;t heard any numbers. I&amp;#39;ll be downloading it on my 3090s rig.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6a6t2b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754016172,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mekuwo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bq5c6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "amokerajvosa",
            "can_mod_post": false,
            "created_utc": 1754043965,
            "send_replies": true,
            "parent_id": "t3_1mekuwo",
            "score": 1,
            "author_fullname": "t2_5965u12q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have 7950X, 64 GB DDR5, RTX 5070ti 16GB and I have 10-15 tokens withh Qwen Coder Q4. GPU is fully used and about 10 GB of RAM.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bq5c6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have 7950X, 64 GB DDR5, RTX 5070ti 16GB and I have 10-15 tokens withh Qwen Coder Q4. GPU is fully used and about 10 GB of RAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6bq5c6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754043965,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mekuwo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6bu069",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Easy_Kitchen7819",
            "can_mod_post": false,
            "created_utc": 1754045850,
            "send_replies": true,
            "parent_id": "t3_1mekuwo",
            "score": 1,
            "author_fullname": "t2_b51tl28l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "7900xtx + ryzen 9900x. Unsolth k4xl with 16 experts. About 50-65 t/s. Kv cache 8q in vram",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bu069",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;7900xtx + ryzen 9900x. Unsolth k4xl with 16 experts. About 50-65 t/s. Kv cache 8q in vram&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6bu069/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754045850,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mekuwo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6c79ht",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "getmevodka",
            "can_mod_post": false,
            "created_utc": 1754051250,
            "send_replies": true,
            "parent_id": "t3_1mekuwo",
            "score": 1,
            "author_fullname": "t2_7uoa6r1b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "possibly dual 3090 cause 48gb vram can run q8 k xl plus context. model is 34.23gb and with about 128k you come out at 45-46gb of vram use and get 80tok/s in speed",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c79ht",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;possibly dual 3090 cause 48gb vram can run q8 k xl plus context. model is 34.23gb and with about 128k you come out at 45-46gb of vram use and get 80tok/s in speed&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6c79ht/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754051250,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mekuwo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6a96kd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Medical_Path2953",
                      "can_mod_post": false,
                      "created_utc": 1754017085,
                      "send_replies": true,
                      "parent_id": "t1_n6a6eof",
                      "score": 1,
                      "author_fullname": "t2_q1yzxk7k1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks so much for the help! If I use this setup (Q8, Q4 or Q6), how good can the performance be for me? Do you think I can get Cursor-like speed? I’ll mainly use it for coding and programming tasks, mostly PHP and MERN stack.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6a96kd",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks so much for the help! If I use this setup (Q8, Q4 or Q6), how good can the performance be for me? Do you think I can get Cursor-like speed? I’ll mainly use it for coding and programming tasks, mostly PHP and MERN stack.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mekuwo",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6a96kd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754017085,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6b8bc3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Linkpharm2",
                      "can_mod_post": false,
                      "created_utc": 1754034010,
                      "send_replies": true,
                      "parent_id": "t1_n6a6eof",
                      "score": 1,
                      "author_fullname": "t2_9oid4hi0",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not at all. Any GPU is always faster than just CPU. Unless you have something like a GT 1030 (48gbps) and ddr5 (80gbps).\n\nFor comparison, a 3090 runs at 1000GBps.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6b8bc3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not at all. Any GPU is always faster than just CPU. Unless you have something like a GT 1030 (48gbps) and ddr5 (80gbps).&lt;/p&gt;\n\n&lt;p&gt;For comparison, a 3090 runs at 1000GBps.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mekuwo",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6b8bc3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754034010,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6a6eof",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "International_Air700",
            "can_mod_post": false,
            "created_utc": 1754016019,
            "send_replies": true,
            "parent_id": "t3_1mekuwo",
            "score": 0,
            "author_fullname": "t2_aml42hwr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just download lmstudio and use Q8 version, the ram does fit.\nTry to use only cpu for inferencing, I think it will be faster than partially loaded on GPU.\nFor GPUs only, I think Q4 or Q6 would fit in 24g of vram, depends on context window size.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6a6eof",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just download lmstudio and use Q8 version, the ram does fit.\nTry to use only cpu for inferencing, I think it will be faster than partially loaded on GPU.\nFor GPUs only, I think Q4 or Q6 would fit in 24g of vram, depends on context window size.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mekuwo/what_kind_of_system_do_i_need_to_run_qwen3coder/n6a6eof/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754016019,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mekuwo",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]