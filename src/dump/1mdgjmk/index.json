[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have a mac and whenever a new model launches, I see MLX quants available in a day or two. However GGUF takes more time due to llama.cpp support.\nRecent example is GLM 4.5\n\nI’m just genuinely curious to know, what makes it easy or faster to add support in MLX.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "I’m curious to know how does MLX adds support for models faster than llama.cpp",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdgjmk",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.92,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 20,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_jqxb4pte",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 20,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753904976,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a mac and whenever a new model launches, I see MLX quants available in a day or two. However GGUF takes more time due to llama.cpp support.\nRecent example is GLM 4.5&lt;/p&gt;\n\n&lt;p&gt;I’m just genuinely curious to know, what makes it easy or faster to add support in MLX.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mdgjmk",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "No_Conversation9561",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/",
            "subreddit_subscribers": 507574,
            "created_utc": 1753904976,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n61ycli",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "AlwaysInconsistant",
                      "can_mod_post": false,
                      "created_utc": 1753910977,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 6,
                      "author_fullname": "t2_65e1spw7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is a great write up, I’d also add that so long as it isn’t an architecture change, the community can generate their own quant files from safetensors fairly easily - it’s way easier than creating a gguf, I think, I haven’t created my own gguf but I have uploaded quants to mlx community.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n61ycli",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a great write up, I’d also add that so long as it isn’t an architecture change, the community can generate their own quant files from safetensors fairly easily - it’s way easier than creating a gguf, I think, I haven’t created my own gguf but I have uploaded quants to mlx community.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n61ycli/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753910977,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n62wo7r",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Accomplished_Ad9530",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62nqme",
                                          "score": 3,
                                          "author_fullname": "t2_88fma001",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Yep, here's a colab notebook: [https://colab.research.google.com/drive/1orolX420Esc3Gcz\\_vOrYoR1V2k-yJTdP](https://colab.research.google.com/drive/1orolX420Esc3Gcz_vOrYoR1V2k-yJTdP)\n\nNot sure how it performs compared to llama.cpp and vllm and such, but it's still early days and optimizations are ongoing.",
                                          "edited": 1753948759,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n62wo7r",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep, here&amp;#39;s a colab notebook: &lt;a href=\"https://colab.research.google.com/drive/1orolX420Esc3Gcz_vOrYoR1V2k-yJTdP\"&gt;https://colab.research.google.com/drive/1orolX420Esc3Gcz_vOrYoR1V2k-yJTdP&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Not sure how it performs compared to llama.cpp and vllm and such, but it&amp;#39;s still early days and optimizations are ongoing.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdgjmk",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62wo7r/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753922057,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753922057,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n62nqme",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "Few-Yam9901",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n620gxc",
                                "score": 4,
                                "author_fullname": "t2_1rhlf3bcfk",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Wait can I use mlx on Linux with Nvidia gpu?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62nqme",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Wait can I use mlx on Linux with Nvidia gpu?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgjmk",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62nqme/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753918999,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753918999,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n620gxc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Accomplished_Ad9530",
                      "can_mod_post": false,
                      "created_utc": 1753911609,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 6,
                      "author_fullname": "t2_88fma001",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's a pretty good rundown!\n\nMy only correction is the last part about ASi-- MLX supports CUDA on linux now (also whatever CPUs on linux and windows). AMD ROCm support should also not be too difficult to add now that CUDA is in.\n\nThink of MLX as what Modular Mojo has been trying to achieve, but openly developed and fully available from the start.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n620gxc",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s a pretty good rundown!&lt;/p&gt;\n\n&lt;p&gt;My only correction is the last part about ASi-- MLX supports CUDA on linux now (also whatever CPUs on linux and windows). AMD ROCm support should also not be too difficult to add now that CUDA is in.&lt;/p&gt;\n\n&lt;p&gt;Think of MLX as what Modular Mojo has been trying to achieve, but openly developed and fully available from the start.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n620gxc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753911609,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n62sb9g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Pristine-Woodpecker",
                      "can_mod_post": false,
                      "created_utc": 1753920538,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 3,
                      "author_fullname": "t2_5b972ieo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt;But then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.\n\nUh, if you look at the GLM 4.5 patch for MLX, it's doing exactly this. It's slightly less verbose than the same thing in the llama.cpp patch, but they're really doing the exact same thing. In fact, defining this is the *only* thing the patch does. Most of the extras llama.cpp needs are shuffling everything in and out of GGUF format and especially mapping GGUF's naming convention, whereas MLX shares the original Python one.\n\n[https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46](https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46)",
                      "edited": 1753920993,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62sb9g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;But then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Uh, if you look at the GLM 4.5 patch for MLX, it&amp;#39;s doing exactly this. It&amp;#39;s slightly less verbose than the same thing in the llama.cpp patch, but they&amp;#39;re really doing the exact same thing. In fact, defining this is the &lt;em&gt;only&lt;/em&gt; thing the patch does. Most of the extras llama.cpp needs are shuffling everything in and out of GGUF format and especially mapping GGUF&amp;#39;s naming convention, whereas MLX shares the original Python one.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46\"&gt;https://github.com/ml-explore/mlx-lm/pull/333/files#diff-d6a56738db419ae83f569243994d3cb2ff6ca7cd5b0b1651700925c8c83837edR46&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62sb9g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920538,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n62sc9z",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "No-Refrigerator-1672",
                      "can_mod_post": false,
                      "created_utc": 1753920547,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 3,
                      "author_fullname": "t2_baavelp5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Fun fact: you actually don't need a llama.cpp support to make a gguf - i.e. ComfyUI community [uses GGUFs](https://github.com/city96/ComfyUI-GGUF) for image generation models which are, needless to say, completely unsupported in llama.cpp. And they are easily pulling this off because, just as you said, Python libraries handle every computational corcern for them. So you can generate a correct GGUF the moment the model is released; it's just that you have nothing to feed it into until days or weeks later.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62sc9z",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fun fact: you actually don&amp;#39;t need a llama.cpp support to make a gguf - i.e. ComfyUI community &lt;a href=\"https://github.com/city96/ComfyUI-GGUF\"&gt;uses GGUFs&lt;/a&gt; for image generation models which are, needless to say, completely unsupported in llama.cpp. And they are easily pulling this off because, just as you said, Python libraries handle every computational corcern for them. So you can generate a correct GGUF the moment the model is released; it&amp;#39;s just that you have nothing to feed it into until days or weeks later.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62sc9z/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753920547,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64qyyo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Creative-Size2658",
                      "can_mod_post": false,
                      "created_utc": 1753951539,
                      "send_replies": true,
                      "parent_id": "t1_n61jxix",
                      "score": 1,
                      "author_fullname": "t2_1f3xb4r4ae",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Great write up\n\n&gt; MLX only needs to care about apple silicon.\n\nMLX is open source and Apple is working with Nvidia to bring CUDA support to MLX.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64qyyo",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great write up&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;MLX only needs to care about apple silicon.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;MLX is open source and Apple is working with Nvidia to bring CUDA support to MLX.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n64qyyo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753951539,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61jxix",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "lothariusdark",
            "can_mod_post": false,
            "created_utc": 1753906917,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": 32,
            "author_fullname": "t2_idhb522c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "As far as I know MLX LM is abstracted far higher than llama.cpp, this means they dont really have to worry about low level stuff like memory management for example.\n\nIts also python and closely related to the way models are run using transformers with pytorch, which every model supports when it comes out.\n\nAs such they have far less work to do to adapt it.\n\nIn llama.cpp the developers need to define everything, the model first needs to be convertable to gguf properly, then you need to define the model, like model parameters, memory for tensors, mapping names etc.\n\nBut then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.\n\nIn the end it also needs to be tested in all the different backends llama.cpp supports, vulkan/cuda/hip/cpu/etc.\n\nMLX only needs to care about apple silicon.\n\n*If I wrote something really wrong, feel free to correct me.*",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61jxix",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As far as I know MLX LM is abstracted far higher than llama.cpp, this means they dont really have to worry about low level stuff like memory management for example.&lt;/p&gt;\n\n&lt;p&gt;Its also python and closely related to the way models are run using transformers with pytorch, which every model supports when it comes out.&lt;/p&gt;\n\n&lt;p&gt;As such they have far less work to do to adapt it.&lt;/p&gt;\n\n&lt;p&gt;In llama.cpp the developers need to define everything, the model first needs to be convertable to gguf properly, then you need to define the model, like model parameters, memory for tensors, mapping names etc.&lt;/p&gt;\n\n&lt;p&gt;But then comes the hard part, the computation graph, that is built automatically with MLX, but in llama.cpp this has to be figured out by hand. This requires explicitly defining every matrix multiplication, activation function, and normalization step in the correct sequence. Its very error prone and requires lots of manual corrections.&lt;/p&gt;\n\n&lt;p&gt;In the end it also needs to be tested in all the different backends llama.cpp supports, vulkan/cuda/hip/cpu/etc.&lt;/p&gt;\n\n&lt;p&gt;MLX only needs to care about apple silicon.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;If I wrote something really wrong, feel free to correct me.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n61jxix/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753906917,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 32
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n63xojg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Tiny_Judge_2119",
            "can_mod_post": false,
            "created_utc": 1753936011,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": 5,
            "author_fullname": "t2_aqcxxu50",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The MLX team is focusing on simplicity and maintainability from day one, so it makes it extremely easy for people to contribute and collaborate!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63xojg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The MLX team is focusing on simplicity and maintainability from day one, so it makes it extremely easy for people to contribute and collaborate!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n63xojg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753936011,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n63nz0v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Thrumpwart",
            "can_mod_post": false,
            "created_utc": 1753931925,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": 3,
            "author_fullname": "t2_iol3buybk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MLX also supports the liquid models - I don’t know if llama.cop does yet. I was surprised to see an LM Studio staff pick of an LFM yesterday.\n\nHaven’t tried it yet, but that was cool to see.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63nz0v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MLX also supports the liquid models - I don’t know if llama.cop does yet. I was surprised to see an LM Studio staff pick of an LFM yesterday.&lt;/p&gt;\n\n&lt;p&gt;Haven’t tried it yet, but that was cool to see.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n63nz0v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753931925,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n63hc2d",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "thedarthsider",
                      "can_mod_post": false,
                      "created_utc": 1753929415,
                      "send_replies": true,
                      "parent_id": "t1_n62pj25",
                      "score": 3,
                      "author_fullname": "t2_1t6uy6ghzc",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "On Mac specifically, MLX is faster than using GGUF. In terms of quality, you can say 4-bit MLX may be comparable to not Q4 but Q3_K_M/Q3_K_L which lies between Q3 and Q4.\n\nBut these days quality is getting better  in MLX using dynamic or DWQ quants.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63hc2d",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;On Mac specifically, MLX is faster than using GGUF. In terms of quality, you can say 4-bit MLX may be comparable to not Q4 but Q3_K_M/Q3_K_L which lies between Q3 and Q4.&lt;/p&gt;\n\n&lt;p&gt;But these days quality is getting better  in MLX using dynamic or DWQ quants.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n63hc2d/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753929415,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64a67w",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "taimusrs",
                      "can_mod_post": false,
                      "created_utc": 1753942157,
                      "send_replies": true,
                      "parent_id": "t1_n62pj25",
                      "score": 1,
                      "author_fullname": "t2_nrjel",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, for some models. The results are sometimes wildly different compared to GGUFs (or other MLX converts). The conversion script doesn't really do anything out of the ordinary either. But in most cases the results are pretty much the same",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64a67w",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, for some models. The results are sometimes wildly different compared to GGUFs (or other MLX converts). The conversion script doesn&amp;#39;t really do anything out of the ordinary either. But in most cases the results are pretty much the same&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n64a67w/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753942157,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62pj25",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Gold_Scholar1111",
            "can_mod_post": false,
            "created_utc": 1753919592,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": 1,
            "author_fullname": "t2_ccc788to",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are they deliver results very differently? in terms like resource using efficiency or output quality for equivalent hardwares.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62pj25",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are they deliver results very differently? in terms like resource using efficiency or output quality for equivalent hardwares.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62pj25/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753919592,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n64dyus",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pristine-Woodpecker",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n63gkzs",
                                "score": 1,
                                "author_fullname": "t2_5b972ieo",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "This baffled me because mlx-lm definitely doesn't support it, but [https://lmstudio.ai/blog/unified-mlx-engine](https://lmstudio.ai/blog/unified-mlx-engine) kind of explains the situation. You need to manually cobble together the 2 different MLX engines to get this to work, and LM Studio did just that.\n\nAt least this helps me to understand where to file some bugs related to this :-)",
                                "edited": 1753944485,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n64dyus",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This baffled me because mlx-lm definitely doesn&amp;#39;t support it, but &lt;a href=\"https://lmstudio.ai/blog/unified-mlx-engine\"&gt;https://lmstudio.ai/blog/unified-mlx-engine&lt;/a&gt; kind of explains the situation. You need to manually cobble together the 2 different MLX engines to get this to work, and LM Studio did just that.&lt;/p&gt;\n\n&lt;p&gt;At least this helps me to understand where to file some bugs related to this :-)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgjmk",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n64dyus/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753944208,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753944208,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n63gkzs",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "thedarthsider",
                      "can_mod_post": false,
                      "created_utc": 1753929148,
                      "send_replies": true,
                      "parent_id": "t1_n62s06w",
                      "score": 6,
                      "author_fullname": "t2_1t6uy6ghzc",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I use MLX versions of Gemma 3 and Qwen 2.5 VL and vision works fine.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63gkzs",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use MLX versions of Gemma 3 and Qwen 2.5 VL and vision works fine.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgjmk",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n63gkzs/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753929148,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62s06w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pristine-Woodpecker",
            "can_mod_post": false,
            "created_utc": 1753920433,
            "send_replies": true,
            "parent_id": "t3_1mdgjmk",
            "score": -3,
            "author_fullname": "t2_5b972ieo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For one, GLM 4.5 support in MLX was added by Apple. They have \"a few\" more resources than the random hobbyist that's trying to get it into llama.cpp.\n\nllama.cpp support is marginally more messy because of the need to translate to GGUF. But it's not fundamentally different.\n\nMLX still doesn't have any vision support for \\*any\\* of these models, so I'm not sure they really \"add support faster\".",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62s06w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For one, GLM 4.5 support in MLX was added by Apple. They have &amp;quot;a few&amp;quot; more resources than the random hobbyist that&amp;#39;s trying to get it into llama.cpp.&lt;/p&gt;\n\n&lt;p&gt;llama.cpp support is marginally more messy because of the need to translate to GGUF. But it&amp;#39;s not fundamentally different.&lt;/p&gt;\n\n&lt;p&gt;MLX still doesn&amp;#39;t have any vision support for *any* of these models, so I&amp;#39;m not sure they really &amp;quot;add support faster&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgjmk/im_curious_to_know_how_does_mlx_adds_support_for/n62s06w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753920433,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgjmk",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -3
          }
        }
      ],
      "before": null
    }
  }
]