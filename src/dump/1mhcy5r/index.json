[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Just recently updated text generation web ui and am running Deepseek Distill Llama 3.3 70b with llama.cpp. Its one of those imatrix quants or whatever but it’s labeled Q4\\_KM I think. I am utilizing the full 131k context length and making it possible with K\\_V cache quantization. The weird part is the same exact configuration in LM Studio with the same exact gguf file was giving me like 15-16 t/s. Running the weights split between a 3090TI and a 3090, k\\_v quantized to q4 and ran on cpu and full context. In text generation web ui, i average at 3-4 t/s which is painfully slower. Anyone have any advice or insight? I apologize if this post wasn’t super coherent and detailed I just woke up but this issue is really bothering me. Thanks all.\n\nEdit: when loading a 70b model with the described setup, this is what the console reports:\n\n`13:03:04-177711 INFO     Loading \"Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf\"`\n\n`13:03:04-226187 INFO     Using gpu_layers=81 | ctx_size=131072 | cache_type=q4_0`\n\n`ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no`\n\n`ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no`\n\n`ggml_cuda_init: found 2 CUDA devices:`\n\n  `Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes`\n\n  `Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes`\n\n`build: 1 (9008328) with MSVC 19.44.35211.0 for x64`\n\n`system info: n_threads = 16, n_threads_batch = 16, total_threads = 32`\n\n\n\n`system_info: n_threads = 16 (n_threads_batch = 16) / 32 | CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |`\n\n\n\n`Web UI is disabled`\n\n`main: binding port with default address family`\n\n`main: HTTP server is listening, hostname:` [`127.0.0.1`](http://127.0.0.1)`, port: 55856, http threads: 31`\n\n`main: loading model`\n\n`llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090 Ti) - 23287 MiB free`\n\n`llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3090) - 23306 MiB free`\n\n`llama_model_loader: loaded meta data with 39 key-value pairs and 724 tensors from user_data\\models\\Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf (version GGUF V3 (latest))`\n\n`llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.`\n\n`llama_model_loader: - kv   0:                       general.architecture str              = llama`\n\n`llama_model_loader: - kv   1:                               general.type str              = model`\n\n`llama_model_loader: - kv   2:`                               [`general.name`](http://general.name) `str              = Llama 3.3 70B Vulpecula R1`\n\n`llama_model_loader: - kv   3:                       general.organization str              = Sao10K`\n\n`llama_model_loader: - kv   4:                           general.finetune str              = Vulpecula-r1`\n\n`llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3`\n\n`llama_model_loader: - kv   6:                         general.size_label str              = 70B`\n\n`llama_model_loader: - kv   7:                            general.license str              = llama3.3`\n\n`llama_model_loader: - kv   8:                   general.base_model.count u32              = 1`\n\n`llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.3 70B Instruct`\n\n`llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama`\n\n`llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...`\n\n`llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [\"en\"]`\n\n`llama_model_loader: - kv  13:                          llama.block_count u32              = 80`\n\n`llama_model_loader: - kv  14:                       llama.context_length u32              = 131072`\n\n`llama_model_loader: - kv  15:                     llama.embedding_length u32              = 8192`\n\n`llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 28672`\n\n`llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 64`\n\n`llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8`\n\n`llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 500000.000000`\n\n`llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010`\n\n`llama_model_loader: - kv  21:                 llama.attention.key_length u32              = 128`\n\n`llama_model_loader: - kv  22:               llama.attention.value_length u32              = 128`\n\n`llama_model_loader: - kv  23:                           llama.vocab_size u32              = 128256`\n\n`llama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 128`\n\n`llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2`\n\n`llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = llama-bpe`\n\n`llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&amp;\", \"'\", ...`\n\n`llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...`\n\n`llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ä  Ä \", \"Ä  Ä Ä Ä \", \"Ä Ä  Ä Ä \", \"...`\n\n`llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 128000`\n\n`llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 128009`\n\n`llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...`\n\n`llama_model_loader: - kv  33:               general.quantization_version u32              = 2`\n\n`llama_model_loader: - kv  34:                          general.file_type u32              = 15`\n\n`llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/Llama-3.3-70B-Vulpecula-r...`\n\n`llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt`\n\n`llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 560`\n\n`llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 125`\n\n`llama_model_loader: - type  f32:  162 tensors`\n\n`llama_model_loader: - type q4_K:  441 tensors`\n\n`llama_model_loader: - type q5_K:   40 tensors`\n\n`llama_model_loader: - type q6_K:   81 tensors`\n\n`print_info: file format = GGUF V3 (latest)`\n\n`print_info: file type   = Q4_K - Medium`\n\n`print_info: file size   = 39.59 GiB (4.82 BPW)`\n\n`load: special tokens cache size = 256`\n\n`load: token to piece cache size = 0.7999 MB`\n\n`print_info: arch             = llama`\n\n`print_info: vocab_only       = 0`\n\n`print_info: n_ctx_train      = 131072`\n\n`print_info: n_embd           = 8192`\n\n`print_info: n_layer          = 80`\n\n`print_info: n_head           = 64`\n\n`print_info: n_head_kv        = 8`\n\n`print_info: n_rot            = 128`\n\n`print_info: n_swa            = 0`\n\n`print_info: is_swa_any       = 0`\n\n`print_info: n_embd_head_k    = 128`\n\n`print_info: n_embd_head_v    = 128`\n\n`print_info: n_gqa            = 8`\n\n`print_info: n_embd_k_gqa     = 1024`\n\n`print_info: n_embd_v_gqa     = 1024`\n\n`print_info: f_norm_eps       = 0.0e+00`\n\n`print_info: f_norm_rms_eps   = 1.0e-05`\n\n`print_info: f_clamp_kqv      = 0.0e+00`\n\n`print_info: f_max_alibi_bias = 0.0e+00`\n\n`print_info: f_logit_scale    = 0.0e+00`\n\n`print_info: f_attn_scale     = 0.0e+00`\n\n`print_info: n_ff             = 28672`\n\n`print_info: n_expert         = 0`\n\n`print_info: n_expert_used    = 0`\n\n`print_info: causal attn      = 1`\n\n`print_info: pooling type     = 0`\n\n`print_info: rope type        = 0`\n\n`print_info: rope scaling     = linear`\n\n`print_info: freq_base_train  = 500000.0`\n\n`print_info: freq_scale_train = 1`\n\n`print_info: n_ctx_orig_yarn  = 131072`\n\n`print_info: rope_finetuned   = unknown`\n\n`print_info: model type       = 70B`\n\n`print_info: model params     = 70.55 B`\n\n`print_info:` [`general.name`](http://general.name)`= Llama 3.3 70B Vulpecula R1`\n\n`print_info: vocab type       = BPE`\n\n`print_info: n_vocab          = 128256`\n\n`print_info: n_merges         = 280147`\n\n`print_info: BOS token        = 128000 '&lt;|begin_of_text|&gt;'`\n\n`print_info: EOS token        = 128009 '&lt;|eot_id|&gt;'`\n\n`print_info: EOT token        = 128009 '&lt;|eot_id|&gt;'`\n\n`print_info: EOM token        = 128008 '&lt;|eom_id|&gt;'`\n\n`print_info: LF token         = 198 'ÄŠ'`\n\n`print_info: EOG token        = 128001 '&lt;|end_of_text|&gt;'`\n\n`print_info: EOG token        = 128008 '&lt;|eom_id|&gt;'`\n\n`print_info: EOG token        = 128009 '&lt;|eot_id|&gt;'`\n\n`print_info: max token length = 256`\n\n`load_tensors: loading model tensors, this can take a while... (mmap = true)`\n\n`load_tensors: offloading 80 repeating layers to GPU`\n\n`load_tensors: offloading output layer to GPU`\n\n`load_tensors: offloaded 81/81 layers to GPU`\n\n`load_tensors:  CUDA0_Split model buffer size = 20036.25 MiB`\n\n`load_tensors:  CUDA1_Split model buffer size = 19938.20 MiB`\n\n`load_tensors:        CUDA0 model buffer size =     2.56 MiB`\n\n`load_tensors:        CUDA1 model buffer size =     2.47 MiB`\n\n`load_tensors:   CPU_Mapped model buffer size =   563.62 MiB`\n\n`......`\n\n`llama_context: constructing llama_context`\n\n`llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache`\n\n`llama_context: n_seq_max     = 1`\n\n`llama_context: n_ctx         = 131072`\n\n`llama_context: n_ctx_per_seq = 131072`\n\n`llama_context: n_batch       = 256`\n\n`llama_context: n_ubatch      = 256`\n\n`llama_context: causal_attn   = 1`\n\n`llama_context: flash_attn    = 1`\n\n`llama_context: kv_unified    = true`\n\n`llama_context: freq_base     = 500000.0`\n\n`llama_context: freq_scale    = 1`\n\n`llama_context:  CUDA_Host  output buffer size =     0.49 MiB`\n\n`llama_kv_cache_unified:        CPU KV buffer size = 11520.00 MiB`\n\n`llama_kv_cache_unified: size = 11520.00 MiB (131072 cells,  80 layers,  1/ 1 seqs), K (q4_0): 5760.00 MiB, V (q4_0): 5760.00 MiB`\n\n`llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility`\n\n`llama_context:      CUDA0 compute buffer size =   368.00 MiB`\n\n`llama_context:      CUDA1 compute buffer size =   240.00 MiB`\n\n`llama_context:  CUDA_Host compute buffer size =   136.00 MiB`\n\n`llama_context: graph nodes  = 2647`\n\n`llama_context: graph splits = 163`\n\n`common_init_from_params: added &lt;|end_of_text|&gt; logit bias = -inf`\n\n`common_init_from_params: added &lt;|eom_id|&gt; logit bias = -inf`\n\n`common_init_from_params: added &lt;|eot_id|&gt; logit bias = -inf`\n\n`common_init_from_params: setting dry_penalty_last_n to ctx_size = 131072`\n\n`common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)`\n\n`main: model loaded`\n\n`main: chat template, chat_template: {{- bos_token }}`\n\n`{%- if custom_tools is defined %}`\n\n`{%- set tools = custom_tools %}`\n\n`{%- endif %}`\n\n`{%- if not tools_in_user_message is defined %}`\n\n`{%- set tools_in_user_message = true %}`\n\n`{%- endif %}`\n\n`{%- if not date_string is defined %}`\n\n`{%- set date_string = \"26 Jul 2024\" %}`\n\n`{%- endif %}`\n\n`{%- if not tools is defined %}`\n\n`{%- set tools = none %}`\n\n`{%- endif %}`\n\n\n\n`{#- This block extracts the system message, so we can slot it into the right place. #}`\n\n`{%- if messages[0]['role'] == 'system' %}`\n\n`{%- set system_message = messages[0]['content']|trim %}`\n\n`{%- set messages = messages[1:] %}`\n\n`{%- else %}`\n\n`{%- set system_message = \"\" %}`\n\n`{%- endif %}`\n\n\n\n`{#- System message + builtin tools #}`\n\n`{{- \"&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n\" }}`\n\n`{%- if builtin_tools is defined or tools is not none %}`\n\n`{{- \"Environment: ipython\\n\" }}`\n\n`{%- endif %}`\n\n`{%- if builtin_tools is defined %}`\n\n`{{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}`\n\n`{%- endif %}`\n\n\n\n`{%- if tools is not none and not tools_in_user_message %}`\n\n`{{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}`\n\n`{{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}`\n\n`{{- \"Do not use variables.\\n\\n\" }}`\n\n`{%- for t in tools %}`\n\n`{{- t | tojson(indent=4) }}`\n\n`{{- \"\\n\\n\" }}`\n\n`{%- endfor %}`\n\n`{%- endif %}`\n\n`{{- system_message }}`\n\n`{{- \"&lt;|eot_id|&gt;\" }}`\n\n\n\n`{#- Custom tools are passed in a user message with some extra guidance #}`\n\n`{%- if tools_in_user_message and not tools is none %}`\n\n`{#- Extract the first user message so we can plug it in here #}`\n\n`{%- if messages | length != 0 %}`\n\n`{%- set first_user_message = messages[0]['content']|trim %}`\n\n`{%- set messages = messages[1:] %}`\n\n`{%- else %}`\n\n`{{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}`\n\n`{%- endif %}`\n\n`{{- '&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n' -}}`\n\n`{{- \"Given the following functions, please respond with a JSON for a function call \" }}`\n\n`{{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}`\n\n`{{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}`\n\n`{{- \"Do not use variables.\\n\\n\" }}`\n\n`{%- for t in tools %}`\n\n`{{- t | tojson(indent=4) }}`\n\n`{{- \"\\n\\n\" }}`\n\n`{%- endfor %}`\n\n`{{- first_user_message + \"&lt;|eot_id|&gt;\"}}`\n\n`{%- endif %}`\n\n\n\n`{%- for message in messages %}`\n\n`{%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}`\n\n`{{- '&lt;|start_header_id|&gt;' + message['role'] + '&lt;|end_header_id|&gt;\\n\\n'+ message['content'] | trim + '&lt;|eot_id|&gt;' }}`\n\n`{%- elif 'tool_calls' in message %}`\n\n`{%- if not message.tool_calls|length == 1 %}`\n\n`{{- raise_exception(\"This model only supports single tool-calls at once!\") }}`\n\n`{%- endif %}`\n\n`{%- set tool_call = message.tool_calls[0].function %}`\n\n`{%- if builtin_tools is defined and tool_call.name in builtin_tools %}`\n\n`{{- '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n' -}}`\n\n`{{- \"&lt;|python_tag|&gt;\" + tool_call.name + \".call(\" }}`\n\n`{%- for arg_name, arg_val in tool_call.arguments | items %}`\n\n`{{- arg_name + '=\"' + arg_val + '\"' }}`\n\n`{%- if not loop.last %}`\n\n`{{- \", \" }}`\n\n`{%- endif %}`\n\n`{%- endfor %}`\n\n`{{- \")\" }}`\n\n`{%- else  %}`\n\n`{{- '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n' -}}`\n\n`{{- '{\"name\": \"' + tool_call.name + '\", ' }}`\n\n`{{- '\"parameters\": ' }}`\n\n`{{- tool_call.arguments | tojson }}`\n\n`{{- \"}\" }}`\n\n`{%- endif %}`\n\n`{%- if builtin_tools is defined %}`\n\n`{#- This means we're in ipython mode #}`\n\n`{{- \"&lt;|eom_id|&gt;\" }}`\n\n`{%- else %}`\n\n`{{- \"&lt;|eot_id|&gt;\" }}`\n\n`{%- endif %}`\n\n`{%- elif message.role == \"tool\" or message.role == \"ipython\" %}`\n\n`{{- \"&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt;\\n\\n\" }}`\n\n`{%- if message.content is mapping or message.content is iterable %}`\n\n`{{- message.content | tojson }}`\n\n`{%- else %}`\n\n`{{- message.content }}`\n\n`{%- endif %}`\n\n`{{- \"&lt;|eot_id|&gt;\" }}`\n\n`{%- endif %}`\n\n`{%- endfor %}`\n\n`{%- if add_generation_prompt %}`\n\n`{{- '&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n' }}`\n\n`{%- endif %}`\n\n`, example_format: '&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;`\n\n\n\n`You are a helpful assistant&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;`\n\n\n\n`Hello&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;`\n\n\n\n`Hi there&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;`\n\n\n\n`How are you?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;`\n\n\n\n`'`\n\n`main: server is listening on` [`http://127.0.0.1:55856`](http://127.0.0.1:55856) `- starting the main loop`\n\n`13:05:54-605320 INFO     Loaded \"Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf\" in 170.43 seconds.`\n\n`13:05:54-606803 INFO     LOADER: \"llama.cpp\"`\n\n`13:05:54-607751 INFO     TRUNCATION LENGTH: 131072`\n\n`13:05:54-608751 INFO     INSTRUCTION TEMPLATE: \"Custom (obtained from model metadata)\"`\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Poor performance from llama.cpp in text-generation-webui?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mhcy5r",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.63,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1flwpwd3",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754327191,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754312832,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Just recently updated text generation web ui and am running Deepseek Distill Llama 3.3 70b with llama.cpp. Its one of those imatrix quants or whatever but it’s labeled Q4_KM I think. I am utilizing the full 131k context length and making it possible with K_V cache quantization. The weird part is the same exact configuration in LM Studio with the same exact gguf file was giving me like 15-16 t/s. Running the weights split between a 3090TI and a 3090, k_v quantized to q4 and ran on cpu and full context. In text generation web ui, i average at 3-4 t/s which is painfully slower. Anyone have any advice or insight? I apologize if this post wasn’t super coherent and detailed I just woke up but this issue is really bothering me. Thanks all.&lt;/p&gt;\n\n&lt;p&gt;Edit: when loading a 70b model with the described setup, this is what the console reports:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:03:04-177711 INFO     Loading &amp;quot;Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:03:04-226187 INFO     Using gpu_layers=81 | ctx_size=131072 | cache_type=q4_0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;ggml_cuda_init: found 2 CUDA devices:&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Device 0: NVIDIA GeForce RTX 3090 Ti, compute capability 8.6, VMM: yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;build: 1 (9008328) with MSVC 19.44.35211.0 for x64&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;system info: n_threads = 16, n_threads_batch = 16, total_threads = 32&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;system_info: n_threads = 16 (n_threads_batch = 16) / 32 | CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 |&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Web UI is disabled&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: binding port with default address family&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: HTTP server is listening, hostname:&lt;/code&gt; &lt;a href=\"http://127.0.0.1\"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt;&lt;code&gt;, port: 55856, http threads: 31&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: loading model&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090 Ti) - 23287 MiB free&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 3090) - 23306 MiB free&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: loaded meta data with 39 key-value pairs and 724 tensors from user_data\\models\\Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf (version GGUF V3 (latest))&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   0:                       general.architecture str              = llama&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   1:                               general.type str              = model&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   2:&lt;/code&gt;                               &lt;a href=\"http://general.name\"&gt;&lt;code&gt;general.name&lt;/code&gt;&lt;/a&gt; &lt;code&gt;str              = Llama 3.3 70B Vulpecula R1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   3:                       general.organization str              = Sao10K&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   4:                           general.finetune str              = Vulpecula-r1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   5:                           general.basename str              = Llama-3.3&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   6:                         general.size_label str              = 70B&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   7:                            general.license str              = llama3.3&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   8:                   general.base_model.count u32              = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv   9:                  general.base_model.0.name str              = Llama 3.3 70B Instruct&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  10:          general.base_model.0.organization str              = Meta Llama&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Lla...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  12:                          general.languages arr[str,1]       = [&amp;quot;en&amp;quot;]&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  13:                          llama.block_count u32              = 80&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  14:                       llama.context_length u32              = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  15:                     llama.embedding_length u32              = 8192&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 28672&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 64&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 500000.000000&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  21:                 llama.attention.key_length u32              = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  22:               llama.attention.value_length u32              = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  23:                           llama.vocab_size u32              = 128256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = llama-bpe&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,128256]  = [&amp;quot;!&amp;quot;, &amp;quot;\\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;&amp;#39;&amp;quot;, ...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,280147]  = [&amp;quot;Ä  Ä &amp;quot;, &amp;quot;Ä  Ä Ä Ä &amp;quot;, &amp;quot;Ä Ä  Ä Ä &amp;quot;, &amp;quot;...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 128000&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 128009&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  33:               general.quantization_version u32              = 2&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  34:                          general.file_type u32              = 15&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/Llama-3.3-70B-Vulpecula-r...&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 560&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 125&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type  f32:  162 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type q4_K:  441 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type q5_K:   40 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_model_loader: - type q6_K:   81 tensors&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: file format = GGUF V3 (latest)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: file type   = Q4_K - Medium&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: file size   = 39.59 GiB (4.82 BPW)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load: special tokens cache size = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load: token to piece cache size = 0.7999 MB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: arch             = llama&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: vocab_only       = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_ctx_train      = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd           = 8192&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_layer          = 80&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_head           = 64&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_head_kv        = 8&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_rot            = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_swa            = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: is_swa_any       = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_head_k    = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_head_v    = 128&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_gqa            = 8&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_k_gqa     = 1024&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_embd_v_gqa     = 1024&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_norm_eps       = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_norm_rms_eps   = 1.0e-05&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_clamp_kqv      = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_max_alibi_bias = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_logit_scale    = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: f_attn_scale     = 0.0e+00&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_ff             = 28672&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_expert         = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_expert_used    = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: causal attn      = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: pooling type     = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: rope type        = 0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: rope scaling     = linear&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: freq_base_train  = 500000.0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: freq_scale_train = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_ctx_orig_yarn  = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: rope_finetuned   = unknown&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: model type       = 70B&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: model params     = 70.55 B&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info:&lt;/code&gt; &lt;a href=\"http://general.name\"&gt;&lt;code&gt;general.name&lt;/code&gt;&lt;/a&gt;&lt;code&gt;= Llama 3.3 70B Vulpecula R1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: vocab type       = BPE&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_vocab          = 128256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: n_merges         = 280147&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: BOS token        = 128000 &amp;#39;&amp;lt;|begin_of_text|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOS token        = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOT token        = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOM token        = 128008 &amp;#39;&amp;lt;|eom_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: LF token         = 198 &amp;#39;ÄŠ&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOG token        = 128001 &amp;#39;&amp;lt;|end_of_text|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOG token        = 128008 &amp;#39;&amp;lt;|eom_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: EOG token        = 128009 &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;print_info: max token length = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: loading model tensors, this can take a while... (mmap = true)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: offloading 80 repeating layers to GPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: offloading output layer to GPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors: offloaded 81/81 layers to GPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:  CUDA0_Split model buffer size = 20036.25 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:  CUDA1_Split model buffer size = 19938.20 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:        CUDA0 model buffer size =     2.56 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:        CUDA1 model buffer size =     2.47 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;load_tensors:   CPU_Mapped model buffer size =   563.62 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;......&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: constructing llama_context&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: non-unified KV cache requires ggml_set_rows() - forcing unified KV cache&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_seq_max     = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_ctx         = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_ctx_per_seq = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_batch       = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: n_ubatch      = 256&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: causal_attn   = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: flash_attn    = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: kv_unified    = true&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: freq_base     = 500000.0&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: freq_scale    = 1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:  CUDA_Host  output buffer size =     0.49 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_kv_cache_unified:        CPU KV buffer size = 11520.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_kv_cache_unified: size = 11520.00 MiB (131072 cells,  80 layers,  1/ 1 seqs), K (q4_0): 5760.00 MiB, V (q4_0): 5760.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:      CUDA0 compute buffer size =   368.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:      CUDA1 compute buffer size =   240.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context:  CUDA_Host compute buffer size =   136.00 MiB&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: graph nodes  = 2647&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama_context: graph splits = 163&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: added &amp;lt;|end_of_text|&amp;gt; logit bias = -inf&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: added &amp;lt;|eom_id|&amp;gt; logit bias = -inf&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: added &amp;lt;|eot_id|&amp;gt; logit bias = -inf&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: setting dry_penalty_last_n to ctx_size = 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: model loaded&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: chat template, chat_template: {{- bos_token }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if custom_tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tools = custom_tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not tools_in_user_message is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tools_in_user_message = true %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not date_string is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set date_string = &amp;quot;26 Jul 2024&amp;quot; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tools = none %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- This block extracts the system message, so we can slot it into the right place. #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if messages[0][&amp;#39;role&amp;#39;] == &amp;#39;system&amp;#39; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set system_message = messages[0][&amp;#39;content&amp;#39;]|trim %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set messages = messages[1:] %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set system_message = &amp;quot;&amp;quot; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- System message + builtin tools #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined or tools is not none %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Environment: ipython\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Tools: &amp;quot; + builtin_tools | reject(&amp;#39;equalto&amp;#39;, &amp;#39;code_interpreter&amp;#39;) | join(&amp;quot;, &amp;quot;) + &amp;quot;\\n\\n&amp;quot;}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if tools is not none and not tools_in_user_message %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;You have access to the following functions. To call a function, please respond with JSON for a function call.&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Do not use variables.\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for t in tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- t | tojson(indent=4) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- system_message }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- Custom tools are passed in a user message with some extra guidance #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if tools_in_user_message and not tools is none %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- Extract the first user message so we can plug it in here #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if messages | length != 0 %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set first_user_message = messages[0][&amp;#39;content&amp;#39;]|trim %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set messages = messages[1:] %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- raise_exception(&amp;quot;Cannot put tools in the first user message when there&amp;#39;s no first user message!&amp;quot;) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Given the following functions, please respond with a JSON for a function call &amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;with its proper arguments that best answers the given prompt.\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;Respond in the format {&amp;quot;name&amp;quot;: function name, &amp;quot;parameters&amp;quot;: dictionary of argument name and its value}.&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;Do not use variables.\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for t in tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- t | tojson(indent=4) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- first_user_message + &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot;}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for message in messages %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not (message.role == &amp;#39;ipython&amp;#39; or message.role == &amp;#39;tool&amp;#39; or &amp;#39;tool_calls&amp;#39; in message) %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;&amp;#39; + message[&amp;#39;role&amp;#39;] + &amp;#39;&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39;+ message[&amp;#39;content&amp;#39;] | trim + &amp;#39;&amp;lt;|eot_id|&amp;gt;&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- elif &amp;#39;tool_calls&amp;#39; in message %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not message.tool_calls|length == 1 %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- raise_exception(&amp;quot;This model only supports single tool-calls at once!&amp;quot;) }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- set tool_call = message.tool_calls[0].function %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined and tool_call.name in builtin_tools %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|python_tag|&amp;gt;&amp;quot; + tool_call.name + &amp;quot;.call(&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- for arg_name, arg_val in tool_call.arguments | items %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- arg_name + &amp;#39;=&amp;quot;&amp;#39; + arg_val + &amp;#39;&amp;quot;&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if not loop.last %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;, &amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;)&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else  %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; -}}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;{&amp;quot;name&amp;quot;: &amp;quot;&amp;#39; + tool_call.name + &amp;#39;&amp;quot;, &amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;quot;parameters&amp;quot;: &amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- tool_call.arguments | tojson }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;}&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if builtin_tools is defined %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{#- This means we&amp;#39;re in ipython mode #}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eom_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- elif message.role == &amp;quot;tool&amp;quot; or message.role == &amp;quot;ipython&amp;quot; %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|start_header_id|&amp;gt;ipython&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if message.content is mapping or message.content is iterable %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- message.content | tojson }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- else %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- message.content }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;quot;&amp;lt;|eot_id|&amp;gt;&amp;quot; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endfor %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- if add_generation_prompt %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{{- &amp;#39;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;\\n\\n&amp;#39; }}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;, example_format: &amp;#39;&amp;lt;|start_header_id|&amp;gt;system&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;You are a helpful assistant&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Hello&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;Hi there&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;user&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;How are you?&amp;lt;|eot_id|&amp;gt;&amp;lt;|start_header_id|&amp;gt;assistant&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;&amp;#39;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;main: server is listening on&lt;/code&gt; &lt;a href=\"http://127.0.0.1:55856\"&gt;&lt;code&gt;http://127.0.0.1:55856&lt;/code&gt;&lt;/a&gt; &lt;code&gt;- starting the main loop&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-605320 INFO     Loaded &amp;quot;Sao10K_Llama-3.3-70B-Vulpecula-r1-Q4_K_M.gguf&amp;quot; in 170.43 seconds.&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-606803 INFO     LOADER: &amp;quot;llama.cpp&amp;quot;&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-607751 INFO     TRUNCATION LENGTH: 131072&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;13:05:54-608751 INFO     INSTRUCTION TEMPLATE: &amp;quot;Custom (obtained from model metadata)&amp;quot;&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mhcy5r",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "WyattTheSkid",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/",
            "subreddit_subscribers": 510259,
            "created_utc": 1754312832,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6v8dcb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "WyattTheSkid",
                      "can_mod_post": false,
                      "created_utc": 1754313935,
                      "send_replies": true,
                      "parent_id": "t1_n6v66dj",
                      "score": 2,
                      "author_fullname": "t2_1flwpwd3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I will get on my pc and do that in a sec. My bad for not thinking of that",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6v8dcb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I will get on my pc and do that in a sec. My bad for not thinking of that&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mhcy5r",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/n6v8dcb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754313935,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6v66dj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pristine-Woodpecker",
            "can_mod_post": false,
            "created_utc": 1754313197,
            "send_replies": true,
            "parent_id": "t3_1mhcy5r",
            "score": 2,
            "author_fullname": "t2_5b972ieo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why not post the most important part to diagnose this: the actual command line you are using.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v66dj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why not post the most important part to diagnose this: the actual command line you are using.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/n6v66dj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754313197,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcy5r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6v87yc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kulchacop",
            "can_mod_post": false,
            "created_utc": 1754313886,
            "send_replies": true,
            "parent_id": "t3_1mhcy5r",
            "score": 2,
            "author_fullname": "t2_mctkn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Post in r/oobabooga",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v87yc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Post in &lt;a href=\"/r/oobabooga\"&gt;r/oobabooga&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/n6v87yc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754313886,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcy5r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6v91hb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1754314159,
            "send_replies": true,
            "parent_id": "t3_1mhcy5r",
            "score": 1,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Probably different VRAM use",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6v91hb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Probably different VRAM use&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/n6v91hb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754314159,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhcy5r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6vg1ih",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jeffwadsworth",
            "can_mod_post": false,
            "created_utc": 1754316424,
            "send_replies": true,
            "parent_id": "t3_1mhcy5r",
            "score": 1,
            "author_fullname": "t2_11m4x2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just use the plain-jane Llama.cpp from the github.  Run the command line llama-cli and you will be much happier.  The other llama web tool also gets lackluster output in my experience, so stick with the CLI.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6vg1ih",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just use the plain-jane Llama.cpp from the github.  Run the command line llama-cli and you will be much happier.  The other llama web tool also gets lackluster output in my experience, so stick with the CLI.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/n6vg1ih/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754316424,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mhcy5r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6wdlrq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Evening_Ad6637",
            "can_mod_post": false,
            "created_utc": 1754326202,
            "send_replies": true,
            "parent_id": "t3_1mhcy5r",
            "score": 1,
            "author_fullname": "t2_p45er6oo",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you on Mac?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6wdlrq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you on Mac?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mhcy5r/poor_performance_from_llamacpp_in/n6wdlrq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754326202,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mhcy5r",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]