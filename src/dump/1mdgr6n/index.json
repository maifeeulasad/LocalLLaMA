[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Thanks to the recent price surge on crypto I have rougly 10k I can spend on equipments. I have always wanted to run sota models like deepseek R1 or GLM 4.5 locally, and also fine tuning them. So far the mac studio 256gb model looks good, but I wanted to ask if there are any better alternatives.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Best way to spend 7k on local model",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdgr6n",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.79,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 8,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ekrnmt5z",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 8,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753905468,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Thanks to the recent price surge on crypto I have rougly 10k I can spend on equipments. I have always wanted to run sota models like deepseek R1 or GLM 4.5 locally, and also fine tuning them. So far the mac studio 256gb model looks good, but I wanted to ask if there are any better alternatives.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdgr6n",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "monoidconcat",
            "discussion_type": null,
            "num_comments": 22,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/",
            "subreddit_subscribers": 507574,
            "created_utc": 1753905468,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n64fora",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "Agitated_Camel1886",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n64a60r",
                                                              "score": 1,
                                                              "author_fullname": "t2_1jf10fah7i",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "I see, that makes a lot of sense now. Thanks for sharing!",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n64fora",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I see, that makes a lot of sense now. Thanks for sharing!&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mdgr6n",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n64fora/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753945140,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753945140,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n64a60r",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "eloquentemu",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n647afv",
                                                    "score": 2,
                                                    "author_fullname": "t2_lpdsy",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "It's not _frequently_ used but _always_ used.  They are things like the attention tensors, shared experts (if the model uses them), etc.  [This image](https://substackcdn.com/image/fetch/$s_!7Tla!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb97a8ac7-db97-497f-866d-10400729d51e_1248x764.png) might help - see how most of the block diagram is the same and only the FFN block gets split up?  That means all those other blocks are still always used, like in a dense model.  And if the model uses shared experts those parameters will also always be used.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n64a60r",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s not &lt;em&gt;frequently&lt;/em&gt; used but &lt;em&gt;always&lt;/em&gt; used.  They are things like the attention tensors, shared experts (if the model uses them), etc.  &lt;a href=\"https://substackcdn.com/image/fetch/$s_!7Tla!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb97a8ac7-db97-497f-866d-10400729d51e_1248x764.png\"&gt;This image&lt;/a&gt; might help - see how most of the block diagram is the same and only the FFN block gets split up?  That means all those other blocks are still always used, like in a dense model.  And if the model uses shared experts those parameters will also always be used.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mdgr6n",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n64a60r/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753942155,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753942155,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n647afv",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Agitated_Camel1886",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62j0mz",
                                          "score": 3,
                                          "author_fullname": "t2_1jf10fah7i",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Ok so essentially we can put some frequently used layers into the GPU for a speed boost... How do we know which layers will be often used? And we can't harness the speedup if some rarely used experts are being run right?\n\nAlso, thank you for the detailed explanation!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n647afv",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ok so essentially we can put some frequently used layers into the GPU for a speed boost... How do we know which layers will be often used? And we can&amp;#39;t harness the speedup if some rarely used experts are being run right?&lt;/p&gt;\n\n&lt;p&gt;Also, thank you for the detailed explanation!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdgr6n",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n647afv/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753940664,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753940664,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 3
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n64wj1f",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "kaisurniwurer",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n62j0mz",
                                          "score": 1,
                                          "author_fullname": "t2_qafso",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "No, that's not the point of GPU in CPU inference server.\n\nGPU here acts as a KVcache storage, basically it's where calculation heavy prompt processing takes place.\n\nMoE expert usage is roughly balanced trough a router. Sure your specific use case might lean more on one \"expert\", but it will still most likely be a small difference between them.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n64wj1f",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, that&amp;#39;s not the point of GPU in CPU inference server.&lt;/p&gt;\n\n&lt;p&gt;GPU here acts as a KVcache storage, basically it&amp;#39;s where calculation heavy prompt processing takes place.&lt;/p&gt;\n\n&lt;p&gt;MoE expert usage is roughly balanced trough a router. Sure your specific use case might lean more on one &amp;quot;expert&amp;quot;, but it will still most likely be a small difference between them.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdgr6n",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n64wj1f/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753954708,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753954708,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n62j0mz",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62eyz1",
                                "score": 6,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "These big models use the MoE architecture where only a fraction of the model is active for any given token, e.g. Kimi K2 is 1000B parameters but only 32B will be active for a given token generation.  Most of those 32B are effectively random, but some aren't.  So really you could view it as a ~10B model with a random ~2% of a 990B model... Kind of.  But basically if you offload the 10B part you only need 10B of VRAM and you can run that 1/3 of the model very quickly.  The CPU then runs the remaining large part but now only needs to process 22B parameters instead of 32B for a ~50% speedup.\n\nAlso, it would just be silly to not have a GPU if you're building an AI rig :).  A 5090 will run a 32B model super fast if you need speed for some tasks.  If you didn't have it a 32B model would be as slow as Kimi K2.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62j0mz",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These big models use the MoE architecture where only a fraction of the model is active for any given token, e.g. Kimi K2 is 1000B parameters but only 32B will be active for a given token generation.  Most of those 32B are effectively random, but some aren&amp;#39;t.  So really you could view it as a ~10B model with a random ~2% of a 990B model... Kind of.  But basically if you offload the 10B part you only need 10B of VRAM and you can run that 1/3 of the model very quickly.  The CPU then runs the remaining large part but now only needs to process 22B parameters instead of 32B for a ~50% speedup.&lt;/p&gt;\n\n&lt;p&gt;Also, it would just be silly to not have a GPU if you&amp;#39;re building an AI rig :).  A 5090 will run a 32B model super fast if you need speed for some tasks.  If you didn&amp;#39;t have it a 32B model would be as slow as Kimi K2.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgr6n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62j0mz/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753917460,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753917460,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 6
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62guy4",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Willing_Landscape_61",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62eyz1",
                                "score": 2,
                                "author_fullname": "t2_8lvrytgw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For MoE , you can get a good speed up by specifying what you offload to the GPU.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62guy4",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For MoE , you can get a good speed up by specifying what you offload to the GPU.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgr6n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62guy4/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753916768,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753916768,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n647l8e",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Agitated_Camel1886",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n637luv",
                                          "score": 1,
                                          "author_fullname": "t2_1jf10fah7i",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks for the explanation!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n647l8e",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the explanation!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdgr6n",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n647l8e/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753940820,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753940820,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n637luv",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Marksta",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62eyz1",
                                "score": 2,
                                "author_fullname": "t2_559a1",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It's a very good combo offloading heavy computation KVcache to the GPU and mostly bandwidth limited tensor computation onto the CPU.\n\nIt's the reason the Apple and other unified memory systems are pretty bad, they have the bandwidth and capacity but are lacking the powerful GPU compute in the mix so it takes a year waiting prompt processing. And 4x the bandwidth to feed the fast GPU compute too.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n637luv",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s a very good combo offloading heavy computation KVcache to the GPU and mostly bandwidth limited tensor computation onto the CPU.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s the reason the Apple and other unified memory systems are pretty bad, they have the bandwidth and capacity but are lacking the powerful GPU compute in the mix so it takes a year waiting prompt processing. And 4x the bandwidth to feed the fast GPU compute too.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgr6n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n637luv/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753925960,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753925960,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62eyz1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Agitated_Camel1886",
                      "can_mod_post": false,
                      "created_utc": 1753916171,
                      "send_replies": true,
                      "parent_id": "t1_n61qvrg",
                      "score": 2,
                      "author_fullname": "t2_1jf10fah7i",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "(A newbie here)\nWhat's the reason behind a GPU plus many RAM? Won't the inference speed be limited by RAM speed anyway?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62eyz1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;(A newbie here)\nWhat&amp;#39;s the reason behind a GPU plus many RAM? Won&amp;#39;t the inference speed be limited by RAM speed anyway?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgr6n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62eyz1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753916171,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n64xvzc",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "kaisurniwurer",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n64izrj",
                                                              "score": 1,
                                                              "author_fullname": "t2_qafso",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Would you be willing to make similar comparison for memory channels?",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n64xvzc",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Would you be willing to make similar comparison for memory channels?&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mdgr6n",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n64xvzc/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753955451,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753955451,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n64izrj",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "eloquentemu",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n64hl2z",
                                                    "score": 2,
                                                    "author_fullname": "t2_lpdsy",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Funny enough, I was literally just benchmarking this:\n\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      12 |           pp512 |         13.56 ± 0.14 |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      24 |           pp512 |         27.08 ± 0.00 |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      36 |           pp512 |         39.40 ± 0.00 |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      48 |           pp512 |         50.88 ± 0.02 |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      60 |           pp512 |         58.89 ± 0.03 |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      72 |           pp512 |         64.75 ± 0.07 |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      84 |           pp512 |         69.79 ± 0.09 |\n| qwen3 32B Q4_K - Medium        |  18.40 GiB |    32.76 B | CPU        |      95 |           pp512 |         72.88 ± 0.25 |\n\nYou'll note I get fairly linear scaling to ~48 cores and then it drops off.  This is almost certainly due to power limits of the processor which will run ~48c at full boost but then starts throttling.  I suspect your problem is indeed NUMA as support for it is still rather poor but people are working on it these days at least.",
                                                    "edited": 1753948772,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n64izrj",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Funny enough, I was literally just benchmarking this:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;model&lt;/th&gt;\n&lt;th align=\"right\"&gt;size&lt;/th&gt;\n&lt;th align=\"right\"&gt;params&lt;/th&gt;\n&lt;th&gt;backend&lt;/th&gt;\n&lt;th align=\"right\"&gt;threads&lt;/th&gt;\n&lt;th align=\"right\"&gt;test&lt;/th&gt;\n&lt;th align=\"right\"&gt;t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;12&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;13.56 ± 0.14&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;24&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;27.08 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;36&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;39.40 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;48&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;50.88 ± 0.02&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;60&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;58.89 ± 0.03&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;72&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;64.75 ± 0.07&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;84&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;69.79 ± 0.09&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3 32B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.40 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;32.76 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;95&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;72.88 ± 0.25&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;You&amp;#39;ll note I get fairly linear scaling to ~48 cores and then it drops off.  This is almost certainly due to power limits of the processor which will run ~48c at full boost but then starts throttling.  I suspect your problem is indeed NUMA as support for it is still rather poor but people are working on it these days at least.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mdgr6n",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n64izrj/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753946984,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753946984,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 2
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n64hl2z",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "DmitryOksenchuk",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n644woi",
                                          "score": 1,
                                          "author_fullname": "t2_6ogr65md",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks for sharing! PP in llama.cpp does not scale beyond about 30 cores in my tests. Maybe NUMA kills the speedup or the code relies on some synchronization.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n64hl2z",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for sharing! PP in llama.cpp does not scale beyond about 30 cores in my tests. Maybe NUMA kills the speedup or the code relies on some synchronization.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdgr6n",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n64hl2z/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753946192,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753946192,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n644woi",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n63jqlo",
                                "score": 3,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "My build is similar but with a 4090, all 12 channels of DDR5, the Epyc 9B14 (only using 48 of 96 cores).  The RAM is the most meaningful thing, because 12 channels is 1.5x faster than 8 channels.  For Deepseek 671B I get:\n\n| model                          |       size |     params | backend    | ngl | ot                    |            test |                  t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | --: | --------------------- | --------------: | -------------------: |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |           pp512 |         27.51 ± 0.06 |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |           tg128 |         14.49 ± 0.02 |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |   pp512 @ d2000 |         27.24 ± 0.01 |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |   tg128 @ d2000 |         11.35 ± 0.01 |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |   pp512 @ d4000 |         26.87 ± 0.00 |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |   tg128 @ d4000 |          9.28 ± 0.15 |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |   pp512 @ d8000 |         26.47 ± 0.04 |\n| deepseek2 671B Q4_K - Medium   | 378.02 GiB |   671.03 B | CUDA       |  99 | exps=CPU              |   tg128 @ d8000 |          6.81 ± 0.16 |\n\nMind that PP will scale depend on the number of cores you have (and maybe a little with 4090 vs 5090).  The TG of the 8channel build will be about 2/3 of the numbers here.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n644woi",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My build is similar but with a 4090, all 12 channels of DDR5, the Epyc 9B14 (only using 48 of 96 cores).  The RAM is the most meaningful thing, because 12 channels is 1.5x faster than 8 channels.  For Deepseek 671B I get:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;model&lt;/th&gt;\n&lt;th align=\"right\"&gt;size&lt;/th&gt;\n&lt;th align=\"right\"&gt;params&lt;/th&gt;\n&lt;th&gt;backend&lt;/th&gt;\n&lt;th align=\"right\"&gt;ngl&lt;/th&gt;\n&lt;th&gt;ot&lt;/th&gt;\n&lt;th align=\"right\"&gt;test&lt;/th&gt;\n&lt;th align=\"right\"&gt;t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512&lt;/td&gt;\n&lt;td align=\"right\"&gt;27.51 ± 0.06&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"right\"&gt;14.49 ± 0.02&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512 @ d2000&lt;/td&gt;\n&lt;td align=\"right\"&gt;27.24 ± 0.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128 @ d2000&lt;/td&gt;\n&lt;td align=\"right\"&gt;11.35 ± 0.01&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512 @ d4000&lt;/td&gt;\n&lt;td align=\"right\"&gt;26.87 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128 @ d4000&lt;/td&gt;\n&lt;td align=\"right\"&gt;9.28 ± 0.15&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;pp512 @ d8000&lt;/td&gt;\n&lt;td align=\"right\"&gt;26.47 ± 0.04&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;deepseek2 671B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;378.02 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;671.03 B&lt;/td&gt;\n&lt;td&gt;CUDA&lt;/td&gt;\n&lt;td align=\"right\"&gt;99&lt;/td&gt;\n&lt;td&gt;exps=CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128 @ d8000&lt;/td&gt;\n&lt;td align=\"right\"&gt;6.81 ± 0.16&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;Mind that PP will scale depend on the number of cores you have (and maybe a little with 4090 vs 5090).  The TG of the 8channel build will be about 2/3 of the numbers here.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdgr6n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n644woi/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753939456,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753939456,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n63jqlo",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "_extruded",
                      "can_mod_post": false,
                      "created_utc": 1753930291,
                      "send_replies": true,
                      "parent_id": "t1_n61qvrg",
                      "score": 2,
                      "author_fullname": "t2_okj220w34",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "About how many tokens would you get with this configuration?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n63jqlo",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;About how many tokens would you get with this configuration?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgr6n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n63jqlo/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753930291,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61qvrg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753908826,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 11,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would advise against the 256GB Studio as that's too small for large models at decent quants.  At Q4: GLM-4.5-358B is ~202GB, Qwen3-Coder-480B is 271GB, Deepseek is 379GB, Kimi-K2 is 578GB.\n\nIf you aren't comfortable building something, the Mac Studio 512GB is a decent option (since the body of your post says $10k).  Enough memory for most models and good speed \n\nIf you're comfortable building something:\n\n- 5090: $2500\n- 8x64GB DDR5-5600: $2400\n- Epyc 9004: Price depends on part and market, but say $1500?\n- Motherboard: $700 (begrudgingly recc H13SSL)\n\nThat hits your $7k and you have room to expand with more memory down the line.  You could spend a bit more to get DDR5-6400 to be ready to upgrade to a Epyc 9005 when those drop in price (the cheap ones are mostly bad but the 9255 is ~okay).  The 5090 is a little overkill and you could get a 3090 without losing a lot of capabilities.  For the Epyc 9004, the 9B14 is a good deal right now IMHO.  Watch out for QS/ES chips since compatibility is spotty with those.\n\nNote you will _not_ be fine tuning them for less than, say, $100k?  Probably more :).  You'll want to rent hardware for that.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61qvrg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would advise against the 256GB Studio as that&amp;#39;s too small for large models at decent quants.  At Q4: GLM-4.5-358B is ~202GB, Qwen3-Coder-480B is 271GB, Deepseek is 379GB, Kimi-K2 is 578GB.&lt;/p&gt;\n\n&lt;p&gt;If you aren&amp;#39;t comfortable building something, the Mac Studio 512GB is a decent option (since the body of your post says $10k).  Enough memory for most models and good speed &lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re comfortable building something:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;5090: $2500&lt;/li&gt;\n&lt;li&gt;8x64GB DDR5-5600: $2400&lt;/li&gt;\n&lt;li&gt;Epyc 9004: Price depends on part and market, but say $1500?&lt;/li&gt;\n&lt;li&gt;Motherboard: $700 (begrudgingly recc H13SSL)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;That hits your $7k and you have room to expand with more memory down the line.  You could spend a bit more to get DDR5-6400 to be ready to upgrade to a Epyc 9005 when those drop in price (the cheap ones are mostly bad but the 9255 is ~okay).  The 5090 is a little overkill and you could get a 3090 without losing a lot of capabilities.  For the Epyc 9004, the 9B14 is a good deal right now IMHO.  Watch out for QS/ES chips since compatibility is spotty with those.&lt;/p&gt;\n\n&lt;p&gt;Note you will &lt;em&gt;not&lt;/em&gt; be fine tuning them for less than, say, $100k?  Probably more :).  You&amp;#39;ll want to rent hardware for that.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n61qvrg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753908826,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61j54y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1753906702,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 2,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm not an expert on such configs (for such big models I prefer to for cloud providers, since my use case do not require total privacy... stil, there are providers with quite honest ToS) but I don't think you can not expect to fine tune a 600+B MoE locally with this budget (until the next magic trick from unsloth).\nabout inference it... yeh, probably someone here can suggest some quite effective configurations (still, I assume you will have to offload at least portion of the weights to ram)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61j54y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not an expert on such configs (for such big models I prefer to for cloud providers, since my use case do not require total privacy... stil, there are providers with quite honest ToS) but I don&amp;#39;t think you can not expect to fine tune a 600+B MoE locally with this budget (until the next magic trick from unsloth).\nabout inference it... yeh, probably someone here can suggest some quite effective configurations (still, I assume you will have to offload at least portion of the weights to ram)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n61j54y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753906702,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61s91b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "perelmanych",
            "can_mod_post": false,
            "created_utc": 1753909206,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 2,
            "author_fullname": "t2_63q8kong",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It is either Mac Studio or EPYC server. No other alternatives in your budget to run big models like DeepSeek-R1 at a somehow acceptable speeds. Personally I would go with EPYC server + RTX 3090, since if there soon will be 2T models you still will be able to fit their low quants into RAM or alternatively you can use higher quants than Mac Studio of the same models. As a downside Mac Studio probably will be a bit faster, of course a lot depends on the specific model of EPYC server that you choose.\n\nFinetuning of such big models is out of reach even for moderate labs, unless you have spare several years.",
            "edited": 1753909390,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61s91b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It is either Mac Studio or EPYC server. No other alternatives in your budget to run big models like DeepSeek-R1 at a somehow acceptable speeds. Personally I would go with EPYC server + RTX 3090, since if there soon will be 2T models you still will be able to fit their low quants into RAM or alternatively you can use higher quants than Mac Studio of the same models. As a downside Mac Studio probably will be a bit faster, of course a lot depends on the specific model of EPYC server that you choose.&lt;/p&gt;\n\n&lt;p&gt;Finetuning of such big models is out of reach even for moderate labs, unless you have spare several years.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n61s91b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753909206,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n638izl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CMDR-Bugsbunny",
                      "can_mod_post": false,
                      "created_utc": 1753926294,
                      "send_replies": true,
                      "parent_id": "t1_n6204b8",
                      "score": 2,
                      "author_fullname": "t2_lj6an",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I can run GLM 4.5 Air (4-bit) on my MacBook M2 Max with 96GB and get 20-30 T/s, so I agree with the above. I got the MacBook used for under $3k USD! \n\nI'm waiting for the M5/M6 or other AI optimized hardware later.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n638izl",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I can run GLM 4.5 Air (4-bit) on my MacBook M2 Max with 96GB and get 20-30 T/s, so I agree with the above. I got the MacBook used for under $3k USD! &lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m waiting for the M5/M6 or other AI optimized hardware later.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdgr6n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n638izl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753926294,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6204b8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "-dysangel-",
            "can_mod_post": false,
            "created_utc": 1753911506,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 1,
            "author_fullname": "t2_12ggykute6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I also bought a Mac Studio 512GB after selling some crypto - but if that's your whole bag rather than like 10% of it or whatever, I'd say hold off for now and wait. Hardware prices are going to come down, and models are going to keep getting better for the same size of RAM. Right now you only need a Mac with 128GB of RAM to run GLM 4.5 Air, which is a f\\*\\*\\*ing amazing model. I've been testing it in chat and agent apps, and it feels as smart and useful as Claude Sonnet. It only takes up 70-80GB of VRAM with 128k context",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6204b8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I also bought a Mac Studio 512GB after selling some crypto - but if that&amp;#39;s your whole bag rather than like 10% of it or whatever, I&amp;#39;d say hold off for now and wait. Hardware prices are going to come down, and models are going to keep getting better for the same size of RAM. Right now you only need a Mac with 128GB of RAM to run GLM 4.5 Air, which is a f***ing amazing model. I&amp;#39;ve been testing it in chat and agent apps, and it feels as smart and useful as Claude Sonnet. It only takes up 70-80GB of VRAM with 128k context&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n6204b8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753911506,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62nos6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753918982,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Probably not your best option but worth mentioning for your research: dual RTX 8000. These are coming down in price and are about 2k each. This will give you 96 GB of fast vram.\n\nI have one rtx 8000 it works great! I can run 70b with 6k context just under 10 tokens a second. Two would be great for 100b models like CommandA, Glm 4.5 air, mistral large.\n\n\nI have not fine tuned across two gpus before so I’m not sure if you will be able to fine tune the larger models. \n\nIf your budget is 10,000, you might be able full send a RTX pro 6000. There is a 300 watt version called Max Q which might be more readily available at a lower price than the non-Max Q version.\n\n\nAnd as others have said, it will be a challenge to run the largest open source models like deepseek and kimi with good tps.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62nos6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Probably not your best option but worth mentioning for your research: dual RTX 8000. These are coming down in price and are about 2k each. This will give you 96 GB of fast vram.&lt;/p&gt;\n\n&lt;p&gt;I have one rtx 8000 it works great! I can run 70b with 6k context just under 10 tokens a second. Two would be great for 100b models like CommandA, Glm 4.5 air, mistral large.&lt;/p&gt;\n\n&lt;p&gt;I have not fine tuned across two gpus before so I’m not sure if you will be able to fine tune the larger models. &lt;/p&gt;\n\n&lt;p&gt;If your budget is 10,000, you might be able full send a RTX pro 6000. There is a 300 watt version called Max Q which might be more readily available at a lower price than the non-Max Q version.&lt;/p&gt;\n\n&lt;p&gt;And as others have said, it will be a challenge to run the largest open source models like deepseek and kimi with good tps.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n62nos6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753918982,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n63w507",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tomz17",
            "can_mod_post": false,
            "created_utc": 1753935331,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 1,
            "author_fullname": "t2_1mhx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; also fine tuning them\n\nYeah, that's not going to happen for R1, GLM-scale models @ $7k",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63w507",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;also fine tuning them&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Yeah, that&amp;#39;s not going to happen for R1, GLM-scale models @ $7k&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n63w507/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753935331,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "1c60b73a-72f2-11ee-bdcc-8e2a7b94d6a0",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n642a61",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Equivalent-Bet-8771",
            "can_mod_post": false,
            "created_utc": 1753938157,
            "send_replies": true,
            "parent_id": "t3_1mdgr6n",
            "score": 1,
            "author_fullname": "t2_l16sej0pt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Put the money in a savings account and wait 6  months for proper inference gear. Project Digits just launched and I'm sure there will be a response from AMD and Qualcomm with proper NPUs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n642a61",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "textgen web UI"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Put the money in a savings account and wait 6  months for proper inference gear. Project Digits just launched and I&amp;#39;m sure there will be a response from AMD and Qualcomm with proper NPUs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdgr6n/best_way_to_spend_7k_on_local_model/n642a61/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753938157,
            "author_flair_text": "textgen web UI",
            "treatment_tags": [],
            "link_id": "t3_1mdgr6n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]