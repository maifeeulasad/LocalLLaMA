[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey r/LocalLLaMA community! I'm planning a local AI implementation for a local company in my country and need some reality checks on my hardware choices before pulling the trigger on this investment.\n\n**TL;DR:** Dual RTX 5090 setup to run Qwen 3 30B (RAG) + Llama 3.1 8B (chatbot) concurrently. Good idea or terrible mistake?\n\n**The Setup:**\n\n* **Heavy Model:** Qwen 3 30B (Q6 quantization, 19GB) for enterprise RAG/GraphRAG\n* **Light Model:** Llama 3.1 8B Instruct (Q8 quantization, 8.5GB) for customer chatbot\n* **Both models need to run simultaneously** during business hours\n\n**Expected Workload:**\n\n* RAG: \\~150 queries/day, peaks of 7-10 concurrent users (business hours only)\n* Chatbot: 700-1000 conversations/day, peaks of 7-10 concurrent users (24/7)\n* Monthly fine-tuning of the 8B model (overnight, while keeping production chatbot running)\n\n**Proposed Hardware:**\n\n* 2√ó NVIDIA RTX 5090 (32GB VRAM each = 64GB total)\n* AMD Threadripper 7970X (32C/64T) or 7965WX (24C/48T)\n* 128GB DDR5 RAM\n* ASRock Pro WS TRX50-SAGE WIFI mobo\n* 2√ó 2TB NVMe in RAID 1\n* 1600W PSU\n\n**Infrastructure:** Everything local - PostgreSQL, vector DBs, embeddings, rerankers. No cloud dependencies.\n\n**My Concerns:**\n\n1. Is 64GB VRAM enough for concurrent inference + occasional fine-tuning?\n2. Will the Qwen 30B + Llama 8B fit comfortably with room for batching?\n3. Am I bottlenecking somewhere else (CPU, RAM, storage)?\n4. Is the Threadripper overkill, or should I go Intel for better inference?\n\n**Extra questions:**\n\n* Anyone running similar concurrent setups? How's your experience?\n* Should I consider 4090s instead to save costs, or go all-in on 5090s?\n* Any red flags in this configuration?\n* Better alternatives for this use case?\n\nI've been researching for weeks but nothing beats real-world experience. This is a significant investment for the company, so I want to get it right the first time.\n\nThanks in advance for any insights! üôè\n\n**Edit:** Budget isn't unlimited, but we're committed to going local for data privacy reasons. Open to alternative approaches if there's a smarter way to achieve these requirements.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Dual RTX 5090 setup for enterprise RAG + fine-tuned chatbot - is this overkill or underpowered?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdfi5e",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.56,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1amqummgqy",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753902578,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey &lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; community! I&amp;#39;m planning a local AI implementation for a local company in my country and need some reality checks on my hardware choices before pulling the trigger on this investment.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Dual RTX 5090 setup to run Qwen 3 30B (RAG) + Llama 3.1 8B (chatbot) concurrently. Good idea or terrible mistake?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Setup:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Heavy Model:&lt;/strong&gt; Qwen 3 30B (Q6 quantization, 19GB) for enterprise RAG/GraphRAG&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Light Model:&lt;/strong&gt; Llama 3.1 8B Instruct (Q8 quantization, 8.5GB) for customer chatbot&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Both models need to run simultaneously&lt;/strong&gt; during business hours&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Expected Workload:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;RAG: ~150 queries/day, peaks of 7-10 concurrent users (business hours only)&lt;/li&gt;\n&lt;li&gt;Chatbot: 700-1000 conversations/day, peaks of 7-10 concurrent users (24/7)&lt;/li&gt;\n&lt;li&gt;Monthly fine-tuning of the 8B model (overnight, while keeping production chatbot running)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Proposed Hardware:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;2√ó NVIDIA RTX 5090 (32GB VRAM each = 64GB total)&lt;/li&gt;\n&lt;li&gt;AMD Threadripper 7970X (32C/64T) or 7965WX (24C/48T)&lt;/li&gt;\n&lt;li&gt;128GB DDR5 RAM&lt;/li&gt;\n&lt;li&gt;ASRock Pro WS TRX50-SAGE WIFI mobo&lt;/li&gt;\n&lt;li&gt;2√ó 2TB NVMe in RAID 1&lt;/li&gt;\n&lt;li&gt;1600W PSU&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Infrastructure:&lt;/strong&gt; Everything local - PostgreSQL, vector DBs, embeddings, rerankers. No cloud dependencies.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My Concerns:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Is 64GB VRAM enough for concurrent inference + occasional fine-tuning?&lt;/li&gt;\n&lt;li&gt;Will the Qwen 30B + Llama 8B fit comfortably with room for batching?&lt;/li&gt;\n&lt;li&gt;Am I bottlenecking somewhere else (CPU, RAM, storage)?&lt;/li&gt;\n&lt;li&gt;Is the Threadripper overkill, or should I go Intel for better inference?&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;&lt;strong&gt;Extra questions:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Anyone running similar concurrent setups? How&amp;#39;s your experience?&lt;/li&gt;\n&lt;li&gt;Should I consider 4090s instead to save costs, or go all-in on 5090s?&lt;/li&gt;\n&lt;li&gt;Any red flags in this configuration?&lt;/li&gt;\n&lt;li&gt;Better alternatives for this use case?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve been researching for weeks but nothing beats real-world experience. This is a significant investment for the company, so I want to get it right the first time.&lt;/p&gt;\n\n&lt;p&gt;Thanks in advance for any insights! üôè&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Budget isn&amp;#39;t unlimited, but we&amp;#39;re committed to going local for data privacy reasons. Open to alternative approaches if there&amp;#39;s a smarter way to achieve these requirements.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdfi5e",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "HuascarSuarez",
            "discussion_type": null,
            "num_comments": 15,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/",
            "subreddit_subscribers": 507574,
            "created_utc": 1753902578,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n617z97",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "HuascarSuarez",
                      "can_mod_post": false,
                      "created_utc": 1753903512,
                      "send_replies": true,
                      "parent_id": "t1_n615qnu",
                      "score": 4,
                      "author_fullname": "t2_1amqummgqy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the response. Is there any online calculator for this? How can you know how much concurrency the hardware can handle? Sorry if this seems like a noob question - I'm actually a bit of a beginner with this stuff.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n617z97",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the response. Is there any online calculator for this? How can you know how much concurrency the hardware can handle? Sorry if this seems like a noob question - I&amp;#39;m actually a bit of a beginner with this stuff.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdfi5e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n617z97/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753903512,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n615qnu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "intellidumb",
            "can_mod_post": false,
            "created_utc": 1753902865,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 7,
            "author_fullname": "t2_73sd7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You‚Äôre going to want more VRAM for any concurrency or useful context window usage",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n615qnu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You‚Äôre going to want more VRAM for any concurrency or useful context window usage&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n615qnu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753902865,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6182ch",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HuascarSuarez",
                      "can_mod_post": false,
                      "created_utc": 1753903537,
                      "send_replies": true,
                      "parent_id": "t1_n615ayw",
                      "score": 2,
                      "author_fullname": "t2_1amqummgqy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6182ch",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdfi5e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n6182ch/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753903537,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n615ayw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mayo551",
            "can_mod_post": false,
            "created_utc": 1753902739,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 5,
            "author_fullname": "t2_vsz5kd9o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt; Is 64GB VRAM enough for concurrent inference + occasional fine-tuning?\n\nhttps://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n615ayw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Is 64GB VRAM enough for concurrent inference + occasional fine-tuning?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator\"&gt;https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n615ayw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753902739,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61wznw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "KernQ",
            "can_mod_post": false,
            "created_utc": 1753910571,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 3,
            "author_fullname": "t2_1pozn81kn1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Underpowered. Spend the money on a single RTX Pro 6000 and treat this project as experimental. That is your best bang for buck. It's 1 component and you can sell it if this goes to shit. It also gives you an upgrade pathway (more RTX pros).\n\nIf it's a new rig, plan for a 4 GPU scenario. Get enough CPU/mobo with enough gen5 PCIE lanes and 16x slots with spacing for the double-slot cards. Consider the MaxQ for simplicity in cooling and power.\n\nValidate the business case with 1 card. Scale it with more cards if needed üëç",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61wznw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Underpowered. Spend the money on a single RTX Pro 6000 and treat this project as experimental. That is your best bang for buck. It&amp;#39;s 1 component and you can sell it if this goes to shit. It also gives you an upgrade pathway (more RTX pros).&lt;/p&gt;\n\n&lt;p&gt;If it&amp;#39;s a new rig, plan for a 4 GPU scenario. Get enough CPU/mobo with enough gen5 PCIE lanes and 16x slots with spacing for the double-slot cards. Consider the MaxQ for simplicity in cooling and power.&lt;/p&gt;\n\n&lt;p&gt;Validate the business case with 1 card. Scale it with more cards if needed üëç&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n61wznw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753910571,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61u98n",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Popular_Brief335",
            "can_mod_post": false,
            "created_utc": 1753909781,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 2,
            "author_fullname": "t2_1j9oxxzd6c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For enterprise not getting a a6000 pro is a massive failure. 64GB is not enough to run medium models¬†",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61u98n",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For enterprise not getting a a6000 pro is a massive failure. 64GB is not enough to run medium models¬†&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n61u98n/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753909781,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62e686",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DepthHour1669",
            "can_mod_post": false,
            "created_utc": 1753915910,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 2,
            "author_fullname": "t2_t6glzswk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Q6 of Qwen3 30b won't fit on a 4090, that's 25-26GB right there. \n\nYou can't do 7-10 users concurrently with a 2x 5090 build. You'll run out of VRAM for context. You can do 1-2 users just fine with this, probably a few more, but 7-10 is definitely over the limit for what this machine can do. \n\nQwen3 30b uses 98304 bytes per token for context, so at max context you need 25.7GB more vram. I doubt you'll have all 10 users at max context, but assume that each user is at ~1/10th context... then you'll need 98,304 bytes/token√ó262,144tokens= 25,769,803,776 bytes = 25.7 GB of vram for all 10 users. Note that the kv cache for each layer must be on the same GPU as the 25-26GB of model weights.\n\n\nYou need a Nvidia RTX Pro 6000 Blackwell 96GB.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62e686",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Q6 of Qwen3 30b won&amp;#39;t fit on a 4090, that&amp;#39;s 25-26GB right there. &lt;/p&gt;\n\n&lt;p&gt;You can&amp;#39;t do 7-10 users concurrently with a 2x 5090 build. You&amp;#39;ll run out of VRAM for context. You can do 1-2 users just fine with this, probably a few more, but 7-10 is definitely over the limit for what this machine can do. &lt;/p&gt;\n\n&lt;p&gt;Qwen3 30b uses 98304 bytes per token for context, so at max context you need 25.7GB more vram. I doubt you&amp;#39;ll have all 10 users at max context, but assume that each user is at ~1/10th context... then you&amp;#39;ll need 98,304 bytes/token√ó262,144tokens= 25,769,803,776 bytes = 25.7 GB of vram for all 10 users. Note that the kv cache for each layer must be on the same GPU as the 25-26GB of model weights.&lt;/p&gt;\n\n&lt;p&gt;You need a Nvidia RTX Pro 6000 Blackwell 96GB.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n62e686/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753915910,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n61ujzq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Popular_Brief335",
                      "can_mod_post": false,
                      "created_utc": 1753909867,
                      "send_replies": true,
                      "parent_id": "t1_n61a7ir",
                      "score": 2,
                      "author_fullname": "t2_1j9oxxzd6c",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Even with a 3090 I got qwen3 0.6B to go from 2% Yara rule complication over 10k tests to 40% off 438 training samples¬†",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n61ujzq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Even with a 3090 I got qwen3 0.6B to go from 2% Yara rule complication over 10k tests to 40% off 438 training samples¬†&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdfi5e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n61ujzq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753909867,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n61a7ir",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Agreeable-Market-692",
            "can_mod_post": false,
            "created_utc": 1753904160,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 1,
            "author_fullname": "t2_ajuhoi00",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "* Monthly fine-tuning of the 8B model (overnight, while keeping production chatbot running)\n\nWhy though? \n\nAs far as hardware requirements you can build a great RAG for 1-2 concurrent users (maybe more) on an off lease office computer. Take a look at IBM's Granite model family. Depending on the niche you could probably get away with Qwen3 0.6B on a phone, there's probably even a branch of llamacpp available for your phone. \n\nBuy an off lease Xeon, slap some 4070s in there and set up RAGFlow and wireguard and call it a day.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61a7ir",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;ul&gt;\n&lt;li&gt;Monthly fine-tuning of the 8B model (overnight, while keeping production chatbot running)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Why though? &lt;/p&gt;\n\n&lt;p&gt;As far as hardware requirements you can build a great RAG for 1-2 concurrent users (maybe more) on an off lease office computer. Take a look at IBM&amp;#39;s Granite model family. Depending on the niche you could probably get away with Qwen3 0.6B on a phone, there&amp;#39;s probably even a branch of llamacpp available for your phone. &lt;/p&gt;\n\n&lt;p&gt;Buy an off lease Xeon, slap some 4070s in there and set up RAGFlow and wireguard and call it a day.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n61a7ir/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753904160,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61sbpw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Armym",
            "can_mod_post": false,
            "created_utc": 1753909227,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 1,
            "author_fullname": "t2_cnt0748",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hi, I would actually recommend the new RTX 6000 blackwell instead. Or two if you have the money. That would suit your needs well for concurrent users. You could easily run fp4 quants to use bigger models but still with fast inference. Fine-tuning is pretty annoying with multiple cards. But I don't think you really need to finetune. Make sure to design your rag well and use good LLM inference engines though! Let me know if you want to know more",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61sbpw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi, I would actually recommend the new RTX 6000 blackwell instead. Or two if you have the money. That would suit your needs well for concurrent users. You could easily run fp4 quants to use bigger models but still with fast inference. Fine-tuning is pretty annoying with multiple cards. But I don&amp;#39;t think you really need to finetune. Make sure to design your rag well and use good LLM inference engines though! Let me know if you want to know more&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n61sbpw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753909227,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6226d7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "cchung261",
            "can_mod_post": false,
            "created_utc": 1753912118,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 1,
            "author_fullname": "t2_gqynovrj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is power cheap in your country? It‚Äôs 1150 watts vs 600 watts you‚Äôre looking at.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6226d7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is power cheap in your country? It‚Äôs 1150 watts vs 600 watts you‚Äôre looking at.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n6226d7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753912118,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n62kp20",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CryptoCryst828282",
            "can_mod_post": false,
            "created_utc": 1753918007,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 1,
            "author_fullname": "t2_b8e4vw6kg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Maybe if the usage is very light, but there are so many red flags here to me.   I wouldn't even consider having more than one person accessing a setup like this for each model at a time.   If you haven't done it a lot tuning on the 30b is not nearly as easy as the 8b due to MOE and will likely take days or weeks to complete for any meaningful amount of data on those.   I dont know what types of speeds you are looking to serve them at, but 5090s are great for large models that you want to run locally for you, but once you start doing multiple clients on it you are actually better off with multiple smaller ones running.    I wouldn't be shocked if you got 2 separate x99 setup with 4x 5060ti 16gb which would cost you less than a single 5090 if it wouldnt smoke the 2x 5090 on a threadripper setup once you have 3-4+ people asking for stuff from it and you would have the ability to fine tune models by taking some of those down while running the rest at a slower rate. \n\n  \nObviously, this isnt an ideal setup either, but for low cost i think i would rather go with that.     I guess if power isnt a concer,n you could grab a supermicro gpu server like **AS-4124GS-TNR**¬† with room for 8x 3090s which which still would be less than 5090s and smoke them all day long in your use case.     Could even go with 16 MI50s and have a real strong setup.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62kp20",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Maybe if the usage is very light, but there are so many red flags here to me.   I wouldn&amp;#39;t even consider having more than one person accessing a setup like this for each model at a time.   If you haven&amp;#39;t done it a lot tuning on the 30b is not nearly as easy as the 8b due to MOE and will likely take days or weeks to complete for any meaningful amount of data on those.   I dont know what types of speeds you are looking to serve them at, but 5090s are great for large models that you want to run locally for you, but once you start doing multiple clients on it you are actually better off with multiple smaller ones running.    I wouldn&amp;#39;t be shocked if you got 2 separate x99 setup with 4x 5060ti 16gb which would cost you less than a single 5090 if it wouldnt smoke the 2x 5090 on a threadripper setup once you have 3-4+ people asking for stuff from it and you would have the ability to fine tune models by taking some of those down while running the rest at a slower rate. &lt;/p&gt;\n\n&lt;p&gt;Obviously, this isnt an ideal setup either, but for low cost i think i would rather go with that.     I guess if power isnt a concer,n you could grab a supermicro gpu server like &lt;strong&gt;AS-4124GS-TNR&lt;/strong&gt;¬† with room for 8x 3090s which which still would be less than 5090s and smoke them all day long in your use case.     Could even go with 16 MI50s and have a real strong setup.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n62kp20/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753918007,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n64mhc8",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Awkward-Candle-4977",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6486fq",
                                "score": 2,
                                "author_fullname": "t2_77uz74xj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "the gddr chips might be different because the pro uses ecc gddr.  \nand pro card goes to higher qa checks\n\n[https://www.youtube.com/watch?v=bd1L3h9kLq8](https://www.youtube.com/watch?v=bd1L3h9kLq8)",
                                "edited": 1753949269,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n64mhc8",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the gddr chips might be different because the pro uses ecc gddr.&lt;br/&gt;\nand pro card goes to higher qa checks&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.youtube.com/watch?v=bd1L3h9kLq8\"&gt;https://www.youtube.com/watch?v=bd1L3h9kLq8&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdfi5e",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n64mhc8/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753948962,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753948962,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6486fq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Current-Rabbit-620",
                      "can_mod_post": false,
                      "created_utc": 1753941122,
                      "send_replies": true,
                      "parent_id": "t1_n63r234",
                      "score": 1,
                      "author_fullname": "t2_9mvs9oc9",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Does this run on usual gaming MB?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6486fq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does this run on usual gaming MB?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdfi5e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n6486fq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753941122,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n63r234",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Awkward-Candle-4977",
            "can_mod_post": false,
            "created_utc": 1753933179,
            "send_replies": true,
            "parent_id": "t3_1mdfi5e",
            "score": 1,
            "author_fullname": "t2_77uz74xj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://www.cdw.com/product/pny-nvidia-rtx-pro-6000-graphic-card-96-gb-gddr7/8326705\n\nhttps://www.cdw.com/product/pny-nvidia-rtx-pro-6000-graphic-card-96-gb-gddr7-full-height/8326706\n\nRTX pro 6000 has 96GB at 8400 usd.\nOn vram aspect, I think it's more economical than 2 5090.\nAnd the vram on those pro card has ecc",
            "edited": 1753933371,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n63r234",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.cdw.com/product/pny-nvidia-rtx-pro-6000-graphic-card-96-gb-gddr7/8326705\"&gt;https://www.cdw.com/product/pny-nvidia-rtx-pro-6000-graphic-card-96-gb-gddr7/8326705&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.cdw.com/product/pny-nvidia-rtx-pro-6000-graphic-card-96-gb-gddr7-full-height/8326706\"&gt;https://www.cdw.com/product/pny-nvidia-rtx-pro-6000-graphic-card-96-gb-gddr7-full-height/8326706&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;RTX pro 6000 has 96GB at 8400 usd.\nOn vram aspect, I think it&amp;#39;s more economical than 2 5090.\nAnd the vram on those pro card has ecc&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdfi5e/dual_rtx_5090_setup_for_enterprise_rag_finetuned/n63r234/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753933179,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdfi5e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]