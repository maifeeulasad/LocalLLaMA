[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "So MXFP4 just entered the zeitgeist out of nowhere.\n\nWhat are the implications for this newfangled technology on our GPUs and Macs with Apple Silicon?\n\nRDNA3/4? Various Nvidia generations? Apple silicon?\n\nI asked Gemini for a quick summary on my phone and the result was unintelligible (everything is awesome!!!1!! was pretty much the answer).\n\nAny insight is appreciated.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "MXFP4 and various hardware",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mij7ki",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_iol3buybk",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754423032,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;So MXFP4 just entered the zeitgeist out of nowhere.&lt;/p&gt;\n\n&lt;p&gt;What are the implications for this newfangled technology on our GPUs and Macs with Apple Silicon?&lt;/p&gt;\n\n&lt;p&gt;RDNA3/4? Various Nvidia generations? Apple silicon?&lt;/p&gt;\n\n&lt;p&gt;I asked Gemini for a quick summary on my phone and the result was unintelligible (everything is awesome!!!1!! was pretty much the answer).&lt;/p&gt;\n\n&lt;p&gt;Any insight is appreciated.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mij7ki",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Thrumpwart",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/",
            "subreddit_subscribers": 511364,
            "created_utc": 1754423032,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n73u6qo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "snowyuser",
            "can_mod_post": false,
            "created_utc": 1754423331,
            "send_replies": true,
            "parent_id": "t3_1mij7ki",
            "score": 0,
            "author_fullname": "t2_25knm7js",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Same! Also curious about implications for further quantization, especially 3-4 bit range needed to run on 64 GB hardware.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n73u6qo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Same! Also curious about implications for further quantization, especially 3-4 bit range needed to run on 64 GB hardware.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/n73u6qo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754423331,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mij7ki",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n74gq12",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "entsnack",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n745m6g",
                                "score": 1,
                                "author_fullname": "t2_1a48h7vf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Some sources that make me think otherwise below, but I suspect you are right about the performance gap between Blackwell and Hopper support for MXFP4:\n\n&gt;Both gpt-oss-20b and gpt-oss-120b are flexible, open-weight reasoning models with chain-of-thought capabilities and adjustable reasoning effort levels using the popular mixture-of-experts architecture. The models are designed to support features like instruction-following and tool use, and were trained on [NVIDIA H100 GPUs](https://www.nvidia.com/en-us/data-center/h100/). \n\n[https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/](https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/)\n\n&gt;Both are **MXFP4 quantized** by default. Please, note that MXFP4 is supported in Hopper or later architectures. This includes data center GPUs such as H100 or GB200, as well as the latest RTX 50xx family of consumer cards.\n\n[https://cookbook.openai.com/articles/gpt-oss/run-transformers](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n&gt;To efficiently run MXFP4 MoE, vLLM has integrated two specialized GPU kernels via collaboration with OpenAI and NVIDIA:\n\n&gt;**Blackwell GPUs (e.g., B200):** A new MoE kernel from [FlashInfer](https://github.com/flashinfer-ai/flashinfer). This kernel is implemented by NVIDIA and uses Blackwell’s native MXFP4 tensor cores for maximum performance.\n\n&gt;**Hopper GPUs (e.g., H100, H200):** Triton [`matmul_ogs` kernel](https://github.com/triton-lang/triton/tree/main/python/triton_kernels), officially implemented by the OpenAI Triton team. This kernel is optimized specifically for Hopper architectures, includes the [swizzling](https://en.wikipedia.org/wiki/Swizzling_/(computer_graphics/)) optimization and built-in heuristics, removing the need for manual tuning.\n\n[https://blog.vllm.ai/2025/08/05/gpt-oss.html](https://blog.vllm.ai/2025/08/05/gpt-oss.html)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n74gq12",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "a": ":X:",
                                    "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                                    "e": "emoji"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Some sources that make me think otherwise below, but I suspect you are right about the performance gap between Blackwell and Hopper support for MXFP4:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Both gpt-oss-20b and gpt-oss-120b are flexible, open-weight reasoning models with chain-of-thought capabilities and adjustable reasoning effort levels using the popular mixture-of-experts architecture. The models are designed to support features like instruction-following and tool use, and were trained on &lt;a href=\"https://www.nvidia.com/en-us/data-center/h100/\"&gt;NVIDIA H100 GPUs&lt;/a&gt;. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/\"&gt;https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Both are &lt;strong&gt;MXFP4 quantized&lt;/strong&gt; by default. Please, note that MXFP4 is supported in Hopper or later architectures. This includes data center GPUs such as H100 or GB200, as well as the latest RTX 50xx family of consumer cards.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://cookbook.openai.com/articles/gpt-oss/run-transformers\"&gt;https://cookbook.openai.com/articles/gpt-oss/run-transformers&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;To efficiently run MXFP4 MoE, vLLM has integrated two specialized GPU kernels via collaboration with OpenAI and NVIDIA:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Blackwell GPUs (e.g., B200):&lt;/strong&gt; A new MoE kernel from &lt;a href=\"https://github.com/flashinfer-ai/flashinfer\"&gt;FlashInfer&lt;/a&gt;. This kernel is implemented by NVIDIA and uses Blackwell’s native MXFP4 tensor cores for maximum performance.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Hopper GPUs (e.g., H100, H200):&lt;/strong&gt; Triton &lt;a href=\"https://github.com/triton-lang/triton/tree/main/python/triton_kernels\"&gt;&lt;code&gt;matmul_ogs&lt;/code&gt; kernel&lt;/a&gt;, officially implemented by the OpenAI Triton team. This kernel is optimized specifically for Hopper architectures, includes the &lt;a href=\"https://en.wikipedia.org/wiki/Swizzling_/(computer_graphics/\"&gt;swizzling&lt;/a&gt;) optimization and built-in heuristics, removing the need for manual tuning.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;&lt;a href=\"https://blog.vllm.ai/2025/08/05/gpt-oss.html\"&gt;https://blog.vllm.ai/2025/08/05/gpt-oss.html&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mij7ki",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "dark",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/n74gq12/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754430578,
                                "author_flair_text": ":X:",
                                "treatment_tags": [],
                                "created_utc": 1754430578,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "transparent",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n745m6g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "BobbyL2k",
                      "can_mod_post": false,
                      "created_utc": 1754427019,
                      "send_replies": true,
                      "parent_id": "t1_n73ztg1",
                      "score": 0,
                      "author_fullname": "t2_ghoyg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I’m pretty sure FP4 is introduced in Blackwell, not Hopper. Hopper might be able to run FP4 at FP8 speeds. But not twice the performance like Blackwell.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n745m6g",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m pretty sure FP4 is introduced in Blackwell, not Hopper. Hopper might be able to run FP4 at FP8 speeds. But not twice the performance like Blackwell.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mij7ki",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/n745m6g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754427019,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n73ztg1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "entsnack",
            "can_mod_post": false,
            "created_utc": 1754425181,
            "send_replies": true,
            "parent_id": "t3_1mij7ki",
            "score": 1,
            "author_fullname": "t2_1a48h7vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The spec was released [back in 2023](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf) and implemented in the Nvidia Hopper and Blackwell architectures, so it's fairly new.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n73ztg1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The spec was released &lt;a href=\"https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf\"&gt;back in 2023&lt;/a&gt; and implemented in the Nvidia Hopper and Blackwell architectures, so it&amp;#39;s fairly new.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/n73ztg1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754425181,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "link_id": "t3_1mij7ki",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "transparent",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n74c5zb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "skindoom",
            "can_mod_post": false,
            "created_utc": 1754429113,
            "send_replies": true,
            "parent_id": "t3_1mij7ki",
            "score": 1,
            "author_fullname": "t2_6uxpi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For apple the ggml appears to be placing the MXFP4 structure into 32bit floats (I've never written metal this is just my take from looking at the commit). \n\nhttps://github.com/ggml-org/llama.cpp/commit/fd1234cb468935ea087d6929b2487926c3afff4b#diff-a20569b2657181025d81f20db1e999f51e21f93029128f4e3b5bd66a5e6a8c40\n\nOn my 128G MBP I'm getting 30-45tks (depending on reasoning effort), and I am able to fit a full context of 131k along with the model (gpt-oss-120b) into memory with no issue.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74c5zb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For apple the ggml appears to be placing the MXFP4 structure into 32bit floats (I&amp;#39;ve never written metal this is just my take from looking at the commit). &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/ggml-org/llama.cpp/commit/fd1234cb468935ea087d6929b2487926c3afff4b#diff-a20569b2657181025d81f20db1e999f51e21f93029128f4e3b5bd66a5e6a8c40\"&gt;https://github.com/ggml-org/llama.cpp/commit/fd1234cb468935ea087d6929b2487926c3afff4b#diff-a20569b2657181025d81f20db1e999f51e21f93029128f4e3b5bd66a5e6a8c40&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;On my 128G MBP I&amp;#39;m getting 30-45tks (depending on reasoning effort), and I am able to fit a full context of 131k along with the model (gpt-oss-120b) into memory with no issue.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/n74c5zb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754429113,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mij7ki",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n73umn8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DinoAmino",
            "can_mod_post": false,
            "created_utc": 1754423483,
            "send_replies": true,
            "parent_id": "t3_1mij7ki",
            "score": 0,
            "author_fullname": "t2_j1v7f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "To hell with asking an LLM. Got any links?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n73umn8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;To hell with asking an LLM. Got any links?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mij7ki/mxfp4_and_various_hardware/n73umn8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754423483,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mij7ki",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]