[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I tried to create a little intuitive explanation of what's happening \"under the hood\" of the transformer architecture without any math... it glosses over a lot but I think starting to talk about it in this way at least dispels some of the myths of how they work.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "An attempt to explain LLM Transformers without math",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 105,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mearht",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {
              "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/VlbBgj2lBls?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Explaining the internals of LLM Transformers -- without math\"&gt;&lt;/iframe&gt;",
              "width": 356,
              "scrolling": false,
              "height": 200
            },
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_yhuwl",
            "secure_media": {
              "oembed": {
                "provider_url": "https://www.youtube.com/",
                "title": "Explaining the internals of LLM Transformers -- without math",
                "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/VlbBgj2lBls?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Explaining the internals of LLM Transformers -- without math\"&gt;&lt;/iframe&gt;",
                "thumbnail_width": 480,
                "height": 200,
                "width": 356,
                "version": "1.0",
                "author_name": "Nimish Gåtam",
                "provider_name": "YouTube",
                "thumbnail_url": "https://i.ytimg.com/vi/VlbBgj2lBls/hqdefault.jpg",
                "type": "video",
                "thumbnail_height": 360,
                "author_url": "https://www.youtube.com/@nimishgatam8901"
              },
              "type": "youtube.com"
            },
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {
              "content": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/VlbBgj2lBls?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Explaining the internals of LLM Transformers -- without math\"&gt;&lt;/iframe&gt;",
              "width": 356,
              "scrolling": false,
              "media_domain_url": "https://www.redditmedia.com/mediaembed/1mearht",
              "height": 200
            },
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/mK7kA7DOTqxkSxj8ISMxYvYyNAvwQ_CTmMB_afvnSp8.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=6105862a8ec459d05fde21560b0c01f00edbc02b",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "rich:video",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1753989634,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "youtu.be",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I tried to create a little intuitive explanation of what&amp;#39;s happening &amp;quot;under the hood&amp;quot; of the transformer architecture without any math... it glosses over a lot but I think starting to talk about it in this way at least dispels some of the myths of how they work.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://youtu.be/VlbBgj2lBls",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/mK7kA7DOTqxkSxj8ISMxYvYyNAvwQ_CTmMB_afvnSp8.jpeg?auto=webp&amp;s=7888ea3f23c563526b9dae12cf1db525255c95ac",
                    "width": 480,
                    "height": 360
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/mK7kA7DOTqxkSxj8ISMxYvYyNAvwQ_CTmMB_afvnSp8.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=09ff60921ab0b3dfbcd14be0956687fb43b50ffc",
                      "width": 108,
                      "height": 81
                    },
                    {
                      "url": "https://external-preview.redd.it/mK7kA7DOTqxkSxj8ISMxYvYyNAvwQ_CTmMB_afvnSp8.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c920b443c86b06f64b414411eee0ec9160ca809b",
                      "width": 216,
                      "height": 162
                    },
                    {
                      "url": "https://external-preview.redd.it/mK7kA7DOTqxkSxj8ISMxYvYyNAvwQ_CTmMB_afvnSp8.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=44949ff326b188e136513d04eb4d4b3b961cd8f1",
                      "width": 320,
                      "height": 240
                    }
                  ],
                  "variants": {},
                  "id": "mK7kA7DOTqxkSxj8ISMxYvYyNAvwQ_CTmMB_afvnSp8"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mearht",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "nimishg",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": {
              "oembed": {
                "provider_url": "https://www.youtube.com/",
                "title": "Explaining the internals of LLM Transformers -- without math",
                "html": "&lt;iframe width=\"356\" height=\"200\" src=\"https://www.youtube.com/embed/VlbBgj2lBls?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Explaining the internals of LLM Transformers -- without math\"&gt;&lt;/iframe&gt;",
                "thumbnail_width": 480,
                "height": 200,
                "width": 356,
                "version": "1.0",
                "author_name": "Nimish Gåtam",
                "provider_name": "YouTube",
                "thumbnail_url": "https://i.ytimg.com/vi/VlbBgj2lBls/hqdefault.jpg",
                "type": "video",
                "thumbnail_height": 360,
                "author_url": "https://www.youtube.com/@nimishgatam8901"
              },
              "type": "youtube.com"
            },
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/",
            "stickied": false,
            "url": "https://youtu.be/VlbBgj2lBls",
            "subreddit_subscribers": 508191,
            "created_utc": 1753989634,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n68djow",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "nimishg",
                      "can_mod_post": false,
                      "created_utc": 1753994438,
                      "send_replies": true,
                      "parent_id": "t1_n68363g",
                      "score": 2,
                      "author_fullname": "t2_yhuwl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks! I hope it makes some kind of sense :)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n68djow",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks! I hope it makes some kind of sense :)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mearht",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n68djow/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753994438,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n68363g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Suspicious_Young8152",
            "can_mod_post": false,
            "created_utc": 1753991506,
            "send_replies": true,
            "parent_id": "t3_1mearht",
            "score": 2,
            "author_fullname": "t2_1gzoposi1r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Congrats dude this is really well done. Really clever way to break it down.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n68363g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Congrats dude this is really well done. Really clever way to break it down.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n68363g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753991506,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mearht",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n682qn3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "RemindMeBot",
                      "can_mod_post": false,
                      "created_utc": 1753991383,
                      "send_replies": true,
                      "parent_id": "t1_n682lf1",
                      "score": 1,
                      "author_fullname": "t2_gbm4p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I will be messaging you in 1 day on [**2025-08-01 19:49:02 UTC**](http://www.wolframalpha.com/input/?i=2025-08-01%2019:49:02%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n682lf1/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1mearht%2Fan_attempt_to_explain_llm_transformers_without%2Fn682lf1%2F%5D%0A%0ARemindMe%21%202025-08-01%2019%3A49%3A02%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%201mearht)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n682qn3",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I will be messaging you in 1 day on &lt;a href=\"http://www.wolframalpha.com/input/?i=2025-08-01%2019:49:02%20UTC%20To%20Local%20Time\"&gt;&lt;strong&gt;2025-08-01 19:49:02 UTC&lt;/strong&gt;&lt;/a&gt; to remind you of &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n682lf1/?context=3\"&gt;&lt;strong&gt;this link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1mearht%2Fan_attempt_to_explain_llm_transformers_without%2Fn682lf1%2F%5D%0A%0ARemindMe%21%202025-08-01%2019%3A49%3A02%20UTC\"&gt;&lt;strong&gt;CLICK THIS LINK&lt;/strong&gt;&lt;/a&gt; to send a PM to also be reminded and to reduce spam.&lt;/p&gt;\n\n&lt;p&gt;&lt;sup&gt;Parent commenter can &lt;/sup&gt; &lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Delete%20Comment&amp;amp;message=Delete%21%201mearht\"&gt;&lt;sup&gt;delete this message to hide from others.&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/\"&gt;&lt;sup&gt;Info&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here\"&gt;&lt;sup&gt;Custom&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=List%20Of%20Reminders&amp;amp;message=MyReminders%21\"&gt;&lt;sup&gt;Your Reminders&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"https://www.reddit.com/message/compose/?to=Watchful1&amp;amp;subject=RemindMeBot%20Feedback\"&gt;&lt;sup&gt;Feedback&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mearht",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n682qn3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753991383,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n682lf1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1753991342,
            "send_replies": true,
            "parent_id": "t3_1mearht",
            "score": 1,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "!RemindMe 1 day",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n682lf1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;!RemindMe 1 day&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n682lf1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753991342,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mearht",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6bf865",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "nimishg",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6b75bl",
                                          "score": 2,
                                          "author_fullname": "t2_yhuwl",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thanks. I think this is also why an 'intuitive' explanation is needed since it's so easy to misunderstand techniques and conflate them with their intended outcomes. Cross-attention (part of the flow when you're building a translator vs a chat bot) is slightly different and even though the techniques and the math is almost the same, the interpretations of what's happening end up being quite different... for me, it's hard to figure out when it's helpful to dive deeper into the math and when it confuses the goal of understanding what the system is 'basically' doing. I'm also thinking about whether I should just walk through or comment on some of the explainability papers that already exist...",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6bf865",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks. I think this is also why an &amp;#39;intuitive&amp;#39; explanation is needed since it&amp;#39;s so easy to misunderstand techniques and conflate them with their intended outcomes. Cross-attention (part of the flow when you&amp;#39;re building a translator vs a chat bot) is slightly different and even though the techniques and the math is almost the same, the interpretations of what&amp;#39;s happening end up being quite different... for me, it&amp;#39;s hard to figure out when it&amp;#39;s helpful to dive deeper into the math and when it confuses the goal of understanding what the system is &amp;#39;basically&amp;#39; doing. I&amp;#39;m also thinking about whether I should just walk through or comment on some of the explainability papers that already exist...&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mearht",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n6bf865/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754037917,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754037917,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6b75bl",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "jackdareel",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6apmoj",
                                "score": 1,
                                "author_fullname": "t2_1puly589vf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Thanks a lot, but that's clear as mud, I'm afraid. Your explanation here is similar to all the explanations I've had from SOTA LLMs. It's not good enough, doesn't do it.\n\nIt may be helpful to note that I have read the 2014 paper that introduced attention to the RNN, for translation tasks. That paper had some images in the test section, and togeher with help of LLMs I got to the point where I concluded that I understood the technique: it's a remapping. So you take \"European Economic Community\" in English, and remap or trasform to the French equivalant, which has a different word order (I forget the French version, might be \"zone economique europeane\"). So that's a good start. But the attention in the 2017 paper is further developed and more difficult to explain. I have yet to see an explantion that clears it up.\n\nThe key error that LLMs make is in quoting too much of the math. You're on a better track. But you do need to connect to the math. More importantly, show at every step what the calculations are doing, and what they are not doing.\n\nOne further insight. A learner like me will think of Query as a search query. So we think of attention as matching the search query to the text being searched. It would help if the teacher acknowledged this and showed how attention is different, why it must be different, and then how it works.\n\nThanks again and good luck!",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6b75bl",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks a lot, but that&amp;#39;s clear as mud, I&amp;#39;m afraid. Your explanation here is similar to all the explanations I&amp;#39;ve had from SOTA LLMs. It&amp;#39;s not good enough, doesn&amp;#39;t do it.&lt;/p&gt;\n\n&lt;p&gt;It may be helpful to note that I have read the 2014 paper that introduced attention to the RNN, for translation tasks. That paper had some images in the test section, and togeher with help of LLMs I got to the point where I concluded that I understood the technique: it&amp;#39;s a remapping. So you take &amp;quot;European Economic Community&amp;quot; in English, and remap or trasform to the French equivalant, which has a different word order (I forget the French version, might be &amp;quot;zone economique europeane&amp;quot;). So that&amp;#39;s a good start. But the attention in the 2017 paper is further developed and more difficult to explain. I have yet to see an explantion that clears it up.&lt;/p&gt;\n\n&lt;p&gt;The key error that LLMs make is in quoting too much of the math. You&amp;#39;re on a better track. But you do need to connect to the math. More importantly, show at every step what the calculations are doing, and what they are not doing.&lt;/p&gt;\n\n&lt;p&gt;One further insight. A learner like me will think of Query as a search query. So we think of attention as matching the search query to the text being searched. It would help if the teacher acknowledged this and showed how attention is different, why it must be different, and then how it works.&lt;/p&gt;\n\n&lt;p&gt;Thanks again and good luck!&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mearht",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n6b75bl/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754033351,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754033351,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6apmoj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "nimishg",
                      "can_mod_post": false,
                      "created_utc": 1754024178,
                      "send_replies": true,
                      "parent_id": "t1_n68xs7e",
                      "score": 3,
                      "author_fullname": "t2_yhuwl",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks... yes it's my first attempt.   \n  \nThe QKV calculations are exactly what I'm calling \"looking something up in the context dictionary\". On a conceptual level, the query and key vectors (QK of the QK V) multiplied together give you the self-attention weights for all the input you've seen so far, and they \"select\" values (the V part) that are retained/accentuated/accumulated for further processing.\n\nI wasn't sure what the right level of abstraction would be for explaining it, but you're right it might be worthwhile to show the result of self-attention as an intermediate step to clarify that's what's selecting the values.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6apmoj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks... yes it&amp;#39;s my first attempt.   &lt;/p&gt;\n\n&lt;p&gt;The QKV calculations are exactly what I&amp;#39;m calling &amp;quot;looking something up in the context dictionary&amp;quot;. On a conceptual level, the query and key vectors (QK of the QK V) multiplied together give you the self-attention weights for all the input you&amp;#39;ve seen so far, and they &amp;quot;select&amp;quot; values (the V part) that are retained/accentuated/accumulated for further processing.&lt;/p&gt;\n\n&lt;p&gt;I wasn&amp;#39;t sure what the right level of abstraction would be for explaining it, but you&amp;#39;re right it might be worthwhile to show the result of self-attention as an intermediate step to clarify that&amp;#39;s what&amp;#39;s selecting the values.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mearht",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n6apmoj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754024178,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n68xs7e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jackdareel",
            "can_mod_post": false,
            "created_utc": 1754000473,
            "send_replies": true,
            "parent_id": "t3_1mearht",
            "score": 1,
            "author_fullname": "t2_1puly589vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I appreciate the effort you put into this. The explanation helps and gets close, but I would benefit from an updated version. Connect the sliders and dictionaries more closely to the concepts and terminology in the LLM. I haven't got a great sense of how the dictionaries connect, why all are needed. And most importantly, I didn't get the sense that QKV calculations, the core of the attention mechanism, are explained here. If this is your first attempt, well done, but I hope for an improved version. Thank you!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n68xs7e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I appreciate the effort you put into this. The explanation helps and gets close, but I would benefit from an updated version. Connect the sliders and dictionaries more closely to the concepts and terminology in the LLM. I haven&amp;#39;t got a great sense of how the dictionaries connect, why all are needed. And most importantly, I didn&amp;#39;t get the sense that QKV calculations, the core of the attention mechanism, are explained here. If this is your first attempt, well done, but I hope for an improved version. Thank you!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mearht/an_attempt_to_explain_llm_transformers_without/n68xs7e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754000473,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mearht",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]