[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don't mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I'd appreciate it if anybody to tell me why it isn't, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Local LLMs I have been using, through different two backends, seem to hardly use GPU",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1m9etng",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_49abw3rv",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753487567,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3060 RTX for my i7 PC. I check the task manager it is has been using about 75% CPU, 55% RAM, and GPU 1% (although it will jump up to 48% and then plummet back to 1% after about a second. I have used Ooba and Kobold.ccp which use the llama.ccp server and kobold.ccp (of course) respectively. I have tried playing around with offloading different number of layers. I have noticed this with Gemma 3 27G, Mistral Small 22B, Mistral Nemo, and Qwen 14B. I don&amp;#39;t mind waiting for a response so I realize that the models are probably too big to give me real time t/s. So, what am I doing wrong? I am still basically a newb when it comes to AI tech. I&amp;#39;d appreciate it if anybody to tell me why it isn&amp;#39;t, at least the the Windows 10 task manager, utilizing the GPU much. My laptop which has only a 2040 RTX seems to run the models better and the settings are basically the same except I use 7 out of 8 cores on the laptop and 3 of 4 of the cores on my desktop CPU. I use Silly Tavern as my frontend so, it could be a setting in there such as the tokenizer I use (I usually just stick with the auto option).&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m9etng",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "theshadowraven",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9etng/local_llms_i_have_been_using_through_different/",
            "subreddit_subscribers": 504486,
            "created_utc": 1753487567,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]