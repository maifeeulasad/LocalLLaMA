[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey y'all, i'm looking for a new model that would fit those requirements:\n\n\\- Great prose with quality writing, not much slop, repetitions etc  \n\\- Very long context (Over 10k minimum to 100k tokens)  \n\\- Fits under 24GB of VRAM for my 3090  \n\\- Uncensored, not necessarily something trained for RP/NSFW stuff, but something that won't constantly complain about \"muh i can't do that because blablabla\". I decide what i want it to do.  \n\\- Not overly obedient to the point where it's not capable of counterargumenting with you and constantly agrees, but still able to follow instructions well enough.  \n\\- Preferably, something trained to be pretty factual. I don't want the AI outright lying to me nonetheless.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What's the best uncensored (but not necessarily NSFW) model under 24GB to use as a creative writing assistant?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mj7c6d",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.82,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_12e33e",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754493196,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey y&amp;#39;all, i&amp;#39;m looking for a new model that would fit those requirements:&lt;/p&gt;\n\n&lt;p&gt;- Great prose with quality writing, not much slop, repetitions etc&lt;br/&gt;\n- Very long context (Over 10k minimum to 100k tokens)&lt;br/&gt;\n- Fits under 24GB of VRAM for my 3090&lt;br/&gt;\n- Uncensored, not necessarily something trained for RP/NSFW stuff, but something that won&amp;#39;t constantly complain about &amp;quot;muh i can&amp;#39;t do that because blablabla&amp;quot;. I decide what i want it to do.&lt;br/&gt;\n- Not overly obedient to the point where it&amp;#39;s not capable of counterargumenting with you and constantly agrees, but still able to follow instructions well enough.&lt;br/&gt;\n- Preferably, something trained to be pretty factual. I don&amp;#39;t want the AI outright lying to me nonetheless.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mj7c6d",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "HRudy94",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/",
            "subreddit_subscribers": 512427,
            "created_utc": 1754493196,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n79gmnu",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LagOps91",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7943xc",
                                "score": 1,
                                "author_fullname": "t2_3wi6j7vwh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Qwen 3 30b doesn't really need abliteration, it's not censored. i have tried it a bit and the instruct version was very repetitive/slop heavy. the thinking version was much better, but still worse than most dense 32b models. the postive is that it's very fast and can easily fit 32k+ context. i know 40k still fits, but much more is going to be tight at Q4.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79gmnu",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen 3 30b doesn&amp;#39;t really need abliteration, it&amp;#39;s not censored. i have tried it a bit and the instruct version was very repetitive/slop heavy. the thinking version was much better, but still worse than most dense 32b models. the postive is that it&amp;#39;s very fast and can easily fit 32k+ context. i know 40k still fits, but much more is going to be tight at Q4.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj7c6d",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n79gmnu/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754499641,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754499641,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7943xc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "QFGTrialByFire",
                      "can_mod_post": false,
                      "created_utc": 1754496114,
                      "send_replies": true,
                      "parent_id": "t1_n78vdpn",
                      "score": 2,
                      "author_fullname": "t2_1h4o7f23eh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Those look like good choices ... if you're going to go with qwen might as well use the latest version, bet tho - qwen 3 https://huggingface.co/huihui-ai/Qwen3-30B-A3B-abliterated. In your settings if you want to mess around with creativity try adjusting temperature and if you want to reduce repetition try adjusting repetition penalty.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7943xc",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Those look like good choices ... if you&amp;#39;re going to go with qwen might as well use the latest version, bet tho - qwen 3 &lt;a href=\"https://huggingface.co/huihui-ai/Qwen3-30B-A3B-abliterated\"&gt;https://huggingface.co/huihui-ai/Qwen3-30B-A3B-abliterated&lt;/a&gt;. In your settings if you want to mess around with creativity try adjusting temperature and if you want to reduce repetition try adjusting repetition penalty.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj7c6d",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n7943xc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754496114,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n78vdpn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "balianone",
            "can_mod_post": false,
            "created_utc": 1754493674,
            "send_replies": true,
            "parent_id": "t3_1mj7c6d",
            "score": 5,
            "author_fullname": "t2_8pgou3uq9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For your 3090, check out Dolphin 2.9.3 Mistral Nemo 12B for its huge 128k context window. Alternatively, look at quantized versions of Qwen2.5-32B-Instruct-abliterated or WizardLM-30B for great, uncensored creative writing.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78vdpn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For your 3090, check out Dolphin 2.9.3 Mistral Nemo 12B for its huge 128k context window. Alternatively, look at quantized versions of Qwen2.5-32B-Instruct-abliterated or WizardLM-30B for great, uncensored creative writing.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n78vdpn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754493674,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj7c6d",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n78wgjz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fp4guru",
            "can_mod_post": false,
            "created_utc": 1754493981,
            "send_replies": true,
            "parent_id": "t3_1mj7c6d",
            "score": 3,
            "author_fullname": "t2_1tp8zldw5g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Mistral small if your target is not NSFW content.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78wgjz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mistral small if your target is not NSFW content.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n78wgjz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754493981,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj7c6d",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n79htyb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "LagOps91",
                      "can_mod_post": false,
                      "created_utc": 1754499972,
                      "send_replies": true,
                      "parent_id": "t1_n79g2wb",
                      "score": 1,
                      "author_fullname": "t2_3wi6j7vwh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "for GLM-4 there is also a finetune that improves long context performance through continual pre-training. i felt it was a bit better at least and worth checking out:  \n[https://huggingface.co/bartowski/Delta-Vector\\_Austral-32B-GLM4-Winton-GGUF](https://huggingface.co/bartowski/Delta-Vector_Austral-32B-GLM4-Winton-GGUF)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n79htyb",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;for GLM-4 there is also a finetune that improves long context performance through continual pre-training. i felt it was a bit better at least and worth checking out:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/bartowski/Delta-Vector_Austral-32B-GLM4-Winton-GGUF\"&gt;https://huggingface.co/bartowski/Delta-Vector_Austral-32B-GLM4-Winton-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj7c6d",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n79htyb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754499972,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n79g2wb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1754499491,
            "send_replies": true,
            "parent_id": "t3_1mj7c6d",
            "score": 3,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GLM 4 32b Q4 instruct (thinking version is meh) is great out of the box with 32k context. nemotron super IQ3XXS works very well despite the quant - can fit 16k context.\n\nif you have even 32gb ram, Q2 GLM-4.5 air with partial offloading (about 30 layers worth of expert on cpu, the rest on gpu) is incredible. Make sure the quant you use quants the shared experts with Q4 to preserved quality (i used unsloth's Q2 XL quant). you can fit 16k context with a 32gb ram setup. if you happen to have 64gb ram, then you can just run Q4 with much higher context, 32k+. if you want a lot of context, this is your best option imo. keep in mind that models tend to degrade past 16k. 32k is still fine for most models but beyond that your milage may vary.\n\nall of the above are uncensored and pretty damed great models on top of it if you want something for regular use. the writing is solid with no repetition or noticable slop issues for all of those models (some slop is unavoidable sadly, feel free to edit it out or retry the response).",
            "edited": 1754499716,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n79g2wb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GLM 4 32b Q4 instruct (thinking version is meh) is great out of the box with 32k context. nemotron super IQ3XXS works very well despite the quant - can fit 16k context.&lt;/p&gt;\n\n&lt;p&gt;if you have even 32gb ram, Q2 GLM-4.5 air with partial offloading (about 30 layers worth of expert on cpu, the rest on gpu) is incredible. Make sure the quant you use quants the shared experts with Q4 to preserved quality (i used unsloth&amp;#39;s Q2 XL quant). you can fit 16k context with a 32gb ram setup. if you happen to have 64gb ram, then you can just run Q4 with much higher context, 32k+. if you want a lot of context, this is your best option imo. keep in mind that models tend to degrade past 16k. 32k is still fine for most models but beyond that your milage may vary.&lt;/p&gt;\n\n&lt;p&gt;all of the above are uncensored and pretty damed great models on top of it if you want something for regular use. the writing is solid with no repetition or noticable slop issues for all of those models (some slop is unavoidable sadly, feel free to edit it out or retry the response).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n79g2wb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754499491,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj7c6d",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n79wsw1",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Sicarius_The_First",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n78y9tr",
                                "score": 1,
                                "author_fullname": "t2_ik8czvp65",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "for the 12b gemma tested up to 16k, for the 24b tested up to 20k.  \nin general for rp, i think probably no longer than 32k is possible.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n79wsw1",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;for the 12b gemma tested up to 16k, for the 24b tested up to 20k.&lt;br/&gt;\nin general for rp, i think probably no longer than 32k is possible.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mj7c6d",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n79wsw1/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754504081,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754504081,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n78y9tr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "HRudy94",
                      "can_mod_post": false,
                      "created_utc": 1754494497,
                      "send_replies": true,
                      "parent_id": "t1_n78vt0g",
                      "score": 1,
                      "author_fullname": "t2_12e33e",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Nice yeah i'll check them out :D  \nWhat are the context windows like for your 12B-24B models?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n78y9tr",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nice yeah i&amp;#39;ll check them out :D&lt;br/&gt;\nWhat are the context windows like for your 12B-24B models?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mj7c6d",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n78y9tr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754494497,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n78vt0g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Sicarius_The_First",
            "can_mod_post": false,
            "created_utc": 1754493793,
            "send_replies": true,
            "parent_id": "t3_1mj7c6d",
            "score": 1,
            "author_fullname": "t2_ik8czvp65",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can try some of mine, various sizes available, some are especially fit for creative writing:  \n[https://huggingface.co/collections/SicariusSicariiStuff/all-my-models-in-order-66c046f1cb6aab0774007a1f](https://huggingface.co/collections/SicariusSicariiStuff/all-my-models-in-order-66c046f1cb6aab0774007a1f)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n78vt0g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can try some of mine, various sizes available, some are especially fit for creative writing:&lt;br/&gt;\n&lt;a href=\"https://huggingface.co/collections/SicariusSicariiStuff/all-my-models-in-order-66c046f1cb6aab0774007a1f\"&gt;https://huggingface.co/collections/SicariusSicariiStuff/all-my-models-in-order-66c046f1cb6aab0774007a1f&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mj7c6d/whats_the_best_uncensored_but_not_necessarily/n78vt0g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754493793,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mj7c6d",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]