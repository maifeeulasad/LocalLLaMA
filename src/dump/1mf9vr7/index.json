[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Im no way programmer nor IT guy. Just history teacher trying to make myself companion for job. For whatever reason, my laptop doesnt let me run openwebUI by terminal commands (cant even pip it), so I cant use instructions from herehttps://www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm\\_studio\\_over\\_a\\_lan/\n\nRn, Im trying to do same stuff with docker but for whatever reason I always get error 500 in my openwebui then trying to reach my running model(by LM studio) on PC.  \nCan someone give me guide/step-by-step instruction/what to read on subject in order to be able to use model which is running on another my device in same network?  \nHope this isn't off topic post",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Want to run models on PC and use them via same wifi with my laptop",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mf9vr7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2l9kaflr",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754086388,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Im no way programmer nor IT guy. Just history teacher trying to make myself companion for job. For whatever reason, my laptop doesnt let me run openwebUI by terminal commands (cant even pip it), so I cant use instructions from herehttps://&lt;a href=\"http://www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm%5C_studio%5C_over%5C_a%5C_lan/\"&gt;www.reddit.com/r/LocalLLaMA/comments/1iqngrb/lm\\_studio\\_over\\_a\\_lan/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Rn, Im trying to do same stuff with docker but for whatever reason I always get error 500 in my openwebui then trying to reach my running model(by LM studio) on PC.&lt;br/&gt;\nCan someone give me guide/step-by-step instruction/what to read on subject in order to be able to use model which is running on another my device in same network?&lt;br/&gt;\nHope this isn&amp;#39;t off topic post&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mf9vr7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "RussianNewbie",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/",
            "subreddit_subscribers": 508541,
            "created_utc": 1754086388,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6fjz3s",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ShengrenR",
            "can_mod_post": false,
            "created_utc": 1754087510,
            "send_replies": true,
            "parent_id": "t3_1mf9vr7",
            "score": 1,
            "author_fullname": "t2_ji4n4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Lot of ways that can go wrong - networking is most likely the main culprit.. but also maybe not. My 2c is try first to go bug Claude over at anthropic - give as much technical detail as you can manage with exactly what you've tried.. write it up in a document and dump that sucker into the chat window - ask it to help step you through trouble-shooting. Reasonable chance it goes well .. offchance it blows things up even worse lol.\n\nI'd recommend you save the details of what goes wrong along the way, so if it doesn't work out you can update this post with more technical information - this is the equivalent of telling a car repair guy your car broke, but no he can't look under the hood and your car is still in your garage.. can you fix it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6fjz3s",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Lot of ways that can go wrong - networking is most likely the main culprit.. but also maybe not. My 2c is try first to go bug Claude over at anthropic - give as much technical detail as you can manage with exactly what you&amp;#39;ve tried.. write it up in a document and dump that sucker into the chat window - ask it to help step you through trouble-shooting. Reasonable chance it goes well .. offchance it blows things up even worse lol.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d recommend you save the details of what goes wrong along the way, so if it doesn&amp;#39;t work out you can update this post with more technical information - this is the equivalent of telling a car repair guy your car broke, but no he can&amp;#39;t look under the hood and your car is still in your garage.. can you fix it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/n6fjz3s/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754087510,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf9vr7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6g1h2p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1754093565,
            "send_replies": true,
            "parent_id": "t3_1mf9vr7",
            "score": 1,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I use Open webUI for doing this. But not LM Studio. Open webUI uses ollama or external APIs, but can connect to your local LM Studio also. I use it with Ollama models. I struggled a lot to run it on my LAN and Windows 10 machines but I could solve the problem. That being said, you should begin by disabling all Firewalls, all antivirus you may have installed, and in Windows, all security services. If after that, you can communicate using your browser with the server machine, then the problem is with something blocking the network. So, step by step, you begin to enable all security services and define firewall exceptions so that all your machines can keep communicating through the defined port ( usually 3000 for Open webUI ). I hope it helps.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6g1h2p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I use Open webUI for doing this. But not LM Studio. Open webUI uses ollama or external APIs, but can connect to your local LM Studio also. I use it with Ollama models. I struggled a lot to run it on my LAN and Windows 10 machines but I could solve the problem. That being said, you should begin by disabling all Firewalls, all antivirus you may have installed, and in Windows, all security services. If after that, you can communicate using your browser with the server machine, then the problem is with something blocking the network. So, step by step, you begin to enable all security services and define firewall exceptions so that all your machines can keep communicating through the defined port ( usually 3000 for Open webUI ). I hope it helps.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/n6g1h2p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754093565,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf9vr7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6g94tq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "isbrowser",
            "can_mod_post": false,
            "created_utc": 1754096390,
            "send_replies": true,
            "parent_id": "t3_1mf9vr7",
            "score": 1,
            "author_fullname": "t2_b2r28249",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Download Tailscale from [https://tailscale.com/download](https://tailscale.com/download) and install it on both computers. Once it's running, you'll have a secure local network over the internet, completely free. Tailscale assigns a unique IP address to each of your devices, allowing them to connect to each other directly.\n\nSo, the only thing you need to do is:\n\n\n\nhttps://preview.redd.it/epot0qce8igf1.jpeg?width=1358&amp;format=pjpg&amp;auto=webp&amp;s=41d5d862452024245ef52631ac6840b84e4c86c0\n\nEnable Local Network and CORS settings. Once that's done, you'll be able to connect to LM Studio from anywhere.\n\nfor example\n\n  \ntailscaleipforhost:1234",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6g94tq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Download Tailscale from &lt;a href=\"https://tailscale.com/download\"&gt;https://tailscale.com/download&lt;/a&gt; and install it on both computers. Once it&amp;#39;s running, you&amp;#39;ll have a secure local network over the internet, completely free. Tailscale assigns a unique IP address to each of your devices, allowing them to connect to each other directly.&lt;/p&gt;\n\n&lt;p&gt;So, the only thing you need to do is:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/epot0qce8igf1.jpeg?width=1358&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=41d5d862452024245ef52631ac6840b84e4c86c0\"&gt;https://preview.redd.it/epot0qce8igf1.jpeg?width=1358&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=41d5d862452024245ef52631ac6840b84e4c86c0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Enable Local Network and CORS settings. Once that&amp;#39;s done, you&amp;#39;ll be able to connect to LM Studio from anywhere.&lt;/p&gt;\n\n&lt;p&gt;for example&lt;/p&gt;\n\n&lt;p&gt;tailscaleipforhost:1234&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf9vr7/want_to_run_models_on_pc_and_use_them_via_same/n6g94tq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754096390,
            "media_metadata": {
              "epot0qce8igf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/jpeg",
                "p": [
                  {
                    "y": 55,
                    "x": 108,
                    "u": "https://preview.redd.it/epot0qce8igf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=89f536f88b0a8bbf60fd679604b34434377dda43"
                  },
                  {
                    "y": 111,
                    "x": 216,
                    "u": "https://preview.redd.it/epot0qce8igf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=619664702d3f800be90ad8b35a345a234e1b2a8c"
                  },
                  {
                    "y": 165,
                    "x": 320,
                    "u": "https://preview.redd.it/epot0qce8igf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=1d23646e7e0a8b67903a3c07ca6951c1a60830db"
                  },
                  {
                    "y": 331,
                    "x": 640,
                    "u": "https://preview.redd.it/epot0qce8igf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=85ef42e123666bc07d4b0cd29638b6873d5090e2"
                  },
                  {
                    "y": 497,
                    "x": 960,
                    "u": "https://preview.redd.it/epot0qce8igf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=8d4cb448c186e599745c78bf5cea813113961f06"
                  },
                  {
                    "y": 559,
                    "x": 1080,
                    "u": "https://preview.redd.it/epot0qce8igf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d28ed48ee0b86548e876e9d89e46d5eeaf15c679"
                  }
                ],
                "s": {
                  "y": 704,
                  "x": 1358,
                  "u": "https://preview.redd.it/epot0qce8igf1.jpeg?width=1358&amp;format=pjpg&amp;auto=webp&amp;s=41d5d862452024245ef52631ac6840b84e4c86c0"
                },
                "id": "epot0qce8igf1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf9vr7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]