[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I got my hands on M1 Max MacBook Pro 64Gb RAM 1Tb SSD. \nCan someone suggest me how should i proceed?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Suggest models for local computer use agent",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1meadtx",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_12609wntbf",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753988766,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I got my hands on M1 Max MacBook Pro 64Gb RAM 1Tb SSD. \nCan someone suggest me how should i proceed?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1meadtx",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Haunting_Stomach8967",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1meadtx/suggest_models_for_local_computer_use_agent/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meadtx/suggest_models_for_local_computer_use_agent/",
            "subreddit_subscribers": 508191,
            "created_utc": 1753988766,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n67xunb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mapppo",
            "can_mod_post": false,
            "created_utc": 1753989949,
            "send_replies": true,
            "parent_id": "t3_1meadtx",
            "score": 1,
            "author_fullname": "t2_16qoui",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://github.com/openai/openai-cua-sample-app/tree/main \nThis is the closest im aware of, but the actual model is closed. You would want something similar controlling docker, I guess, running a virtual desktop. Tool calling + vision models something like qwen, llama3.1, gemma 3.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n67xunb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/openai/openai-cua-sample-app/tree/main\"&gt;https://github.com/openai/openai-cua-sample-app/tree/main&lt;/a&gt; \nThis is the closest im aware of, but the actual model is closed. You would want something similar controlling docker, I guess, running a virtual desktop. Tool calling + vision models something like qwen, llama3.1, gemma 3.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meadtx/suggest_models_for_local_computer_use_agent/n67xunb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753989949,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meadtx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n693p77",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "this-just_in",
            "can_mod_post": false,
            "created_utc": 1754002381,
            "send_replies": true,
            "parent_id": "t3_1meadtx",
            "score": 1,
            "author_fullname": "t2_kdmu4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I’m not really up to date in this space but a few months ago UITARS was about the pinnacle of local llama computer use with a companion app and model family (7B and 70B variants).  That still might be true today, even if it isn’t maybe the most robust modern option (such as CUA, which probably works well with frontier models).  If you just needed web, Playwight MCP server + good tool calling models that are fast (such as Qwen3 30BA3B) would work very well and be less error prone.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n693p77",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m not really up to date in this space but a few months ago UITARS was about the pinnacle of local llama computer use with a companion app and model family (7B and 70B variants).  That still might be true today, even if it isn’t maybe the most robust modern option (such as CUA, which probably works well with frontier models).  If you just needed web, Playwight MCP server + good tool calling models that are fast (such as Qwen3 30BA3B) would work very well and be less error prone.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meadtx/suggest_models_for_local_computer_use_agent/n693p77/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754002381,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meadtx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n698mrm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "rpiguy9907",
            "can_mod_post": false,
            "created_utc": 1754004016,
            "send_replies": true,
            "parent_id": "t3_1meadtx",
            "score": 2,
            "author_fullname": "t2_7l87g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Download LM Studio. \n\nGet a model that fits in your memory and is still performant. \n\nYou can download them directly from the LM Studio UX.\n\nGet the MLX model impossible.\n\nMagistral. Qwen3 32B. Gemma3 27B should all run well enough.\n\nIf you need more speed go for a smaller model.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n698mrm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Download LM Studio. &lt;/p&gt;\n\n&lt;p&gt;Get a model that fits in your memory and is still performant. &lt;/p&gt;\n\n&lt;p&gt;You can download them directly from the LM Studio UX.&lt;/p&gt;\n\n&lt;p&gt;Get the MLX model impossible.&lt;/p&gt;\n\n&lt;p&gt;Magistral. Qwen3 32B. Gemma3 27B should all run well enough.&lt;/p&gt;\n\n&lt;p&gt;If you need more speed go for a smaller model.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meadtx/suggest_models_for_local_computer_use_agent/n698mrm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754004016,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meadtx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n67wguf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "iolairemcfadden",
            "can_mod_post": false,
            "created_utc": 1753989544,
            "send_replies": true,
            "parent_id": "t3_1meadtx",
            "score": 0,
            "author_fullname": "t2_aysy686e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ollama is easy to run on the mac, use it to download models, see [https://ollama.com/search](https://ollama.com/search), you can connect to it via the various VSCode spinoffs like RooCode for coding, or also install Open WebUI via Docker to have a chat like window.  Obviously you can just communicate with it via the command line.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n67wguf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama is easy to run on the mac, use it to download models, see &lt;a href=\"https://ollama.com/search\"&gt;https://ollama.com/search&lt;/a&gt;, you can connect to it via the various VSCode spinoffs like RooCode for coding, or also install Open WebUI via Docker to have a chat like window.  Obviously you can just communicate with it via the command line.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meadtx/suggest_models_for_local_computer_use_agent/n67wguf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753989544,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meadtx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]