[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "After reading that ik\\_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp; testing until it worked on both of Windows machines:\n\n||Desktop|Notebook|\n|:-|:-|:-|\n|OS|Windows 11|Windows 10|\n|CPU|AMD Ryzen 5 7600|Intel i7 8750H|\n|RAM|32GB DDR5 5600|32GB DDR4 2667|\n|GPU|NVIDIA RTX 4070 Ti 12GB|NVIDIA GTX 1070 8GB|\n|Tokens/s|35|9.5|\n\n\n\nFor my desktop PC that works out great and I get super nice results.\n\nOn my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!\n\nAlthough this might not be the perfect solution I thought I'd share it here, maybe someone finds it useful:\n\n[https://github.com/Danmoreng/local-qwen3-coder-env](https://github.com/Danmoreng/local-qwen3-coder-env)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Installscript for Qwen3-Coder running on ik_llama.cpp for high performance",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1metf4h",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_7z26p",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754045970,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading that ik_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp;amp; testing until it worked on both of Windows machines:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;Desktop&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notebook&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OS&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 11&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;CPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 5 7600&lt;/td&gt;\n&lt;td align=\"left\"&gt;Intel i7 8750H&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;RAM&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR5 5600&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR4 2667&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA RTX 4070 Ti 12GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA GTX 1070 8GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tokens/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;35&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;For my desktop PC that works out great and I get super nice results.&lt;/p&gt;\n\n&lt;p&gt;On my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Although this might not be the perfect solution I thought I&amp;#39;d share it here, maybe someone finds it useful:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?auto=webp&amp;s=696008496e1bd6e0694f2b9886836ed38e83c28c",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ff28b544d610ccf62e98f3feddd075959ce926b",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72c1589c6e0b5c99b15e4303d1779e5eec8f9a80",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3fcaf94729c1bcd29118e071f6822ad5c896bec0",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e9afa83dad53bd5ba85b1ab44cba7113663f13b",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6c818c201ba8089a89a84387df82adfd28220d6",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b492d3d79ff10edeacd4e012c9aa3e160eaf88db",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1metf4h",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Danmoreng",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
            "subreddit_subscribers": 508192,
            "created_utc": 1754045970,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6by7u4",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ArchdukeofHyperbole",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6bx13l",
                                "score": 1,
                                "author_fullname": "t2_1p41v97q5d",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Oh, idk about the responses. My experience is it's usually  an issue with the chat template when the responses are non-related.\n\nEdit: this ik_llama.cpp seems interesting though. I have a 1660 ti on my notebook and I guess that's why I was focusing on the speed part. I currently get about 9 tps running qwen 3AB on lm studio which is based on llama.cpp. So was thinking of the possibility since my gpu has 7.5 compute it would run somewhat faster on ik_llama",
                                "edited": 1754048136,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6by7u4",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh, idk about the responses. My experience is it&amp;#39;s usually  an issue with the chat template when the responses are non-related.&lt;/p&gt;\n\n&lt;p&gt;Edit: this ik_llama.cpp seems interesting though. I have a 1660 ti on my notebook and I guess that&amp;#39;s why I was focusing on the speed part. I currently get about 9 tps running qwen 3AB on lm studio which is based on llama.cpp. So was thinking of the possibility since my gpu has 7.5 compute it would run somewhat faster on ik_llama&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1metf4h",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6by7u4/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754047721,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754047721,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6bx13l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754047212,
                      "send_replies": true,
                      "parent_id": "t1_n6bwadq",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That its slower is normal, that’s not the problem. The problem is, that I get random responses as soon as I type anything longer than „test“. And that’s weird. To the „test“ prompt the answer was actually related, to any other prompt I get random output.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6bx13l",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That its slower is normal, that’s not the problem. The problem is, that I get random responses as soon as I type anything longer than „test“. And that’s weird. To the „test“ prompt the answer was actually related, to any other prompt I get random output.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1metf4h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6bx13l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754047212,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6bwadq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArchdukeofHyperbole",
            "can_mod_post": false,
            "created_utc": 1754046886,
            "send_replies": true,
            "parent_id": "t3_1metf4h",
            "score": 1,
            "author_fullname": "t2_1p41v97q5d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "From that GitHub, it recommends compute ≥ 7.0\n\nYour gpu would be the cause on the slower machine\n\nEdit: the 1070 has a compute of 6.1\n\nhttps://developer.nvidia.com/cuda-legacy-gpus",
            "edited": 1754047585,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bwadq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;From that GitHub, it recommends compute ≥ 7.0&lt;/p&gt;\n\n&lt;p&gt;Your gpu would be the cause on the slower machine&lt;/p&gt;\n\n&lt;p&gt;Edit: the 1070 has a compute of 6.1&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://developer.nvidia.com/cuda-legacy-gpus\"&gt;https://developer.nvidia.com/cuda-legacy-gpus&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6bwadq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754046886,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1metf4h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6c6woc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1754051120,
            "send_replies": true,
            "parent_id": "t3_1metf4h",
            "score": 1,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The random text issue could be because of flash attention, try disabling it. I had the same issue last week with Qwen 235b on my dual-GPU setup. My second GPU is also compute 6.1 (Quadro P5000).",
            "edited": 1754051366,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c6woc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The random text issue could be because of flash attention, try disabling it. I had the same issue last week with Qwen 235b on my dual-GPU setup. My second GPU is also compute 6.1 (Quadro P5000).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6c6woc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754051120,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1metf4h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]