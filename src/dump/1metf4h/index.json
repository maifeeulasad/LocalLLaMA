[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "After reading that ik\\_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp; testing until it worked on both of Windows machines:\n\n||Desktop|Notebook|\n|:-|:-|:-|\n|OS|Windows 11|Windows 10|\n|CPU|AMD Ryzen 5 7600|Intel i7 8750H|\n|RAM|32GB DDR5 5600|32GB DDR4 2667|\n|GPU|NVIDIA RTX 4070 Ti 12GB|NVIDIA GTX 1070 8GB|\n|Tokens/s|35|9.5|\n\n\n\nFor my desktop PC that works out great and I get super nice results.\n\nOn my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!\n\nAlthough this might not be the perfect solution I thought I'd share it here, maybe someone finds it useful:\n\n[https://github.com/Danmoreng/local-qwen3-coder-env](https://github.com/Danmoreng/local-qwen3-coder-env)",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Installscript for Qwen3-Coder running on ik_llama.cpp for high performance",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1metf4h",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 8,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_7z26p",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 8,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754045970,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;After reading that ik_llama.cpp gives way higher performance than LMStudio, I wanted to have a simple method of installing and running the Qwen3 Coder model under Windows. I chose to install everything needed and build from source within one single script - written mainly by ChatGPT with experimenting &amp;amp; testing until it worked on both of Windows machines:&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;&lt;/th&gt;\n&lt;th align=\"left\"&gt;Desktop&lt;/th&gt;\n&lt;th align=\"left\"&gt;Notebook&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;OS&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 11&lt;/td&gt;\n&lt;td align=\"left\"&gt;Windows 10&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;CPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;AMD Ryzen 5 7600&lt;/td&gt;\n&lt;td align=\"left\"&gt;Intel i7 8750H&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;RAM&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR5 5600&lt;/td&gt;\n&lt;td align=\"left\"&gt;32GB DDR4 2667&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;GPU&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA RTX 4070 Ti 12GB&lt;/td&gt;\n&lt;td align=\"left\"&gt;NVIDIA GTX 1070 8GB&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Tokens/s&lt;/td&gt;\n&lt;td align=\"left\"&gt;35&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.5&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;For my desktop PC that works out great and I get super nice results.&lt;/p&gt;\n\n&lt;p&gt;On my notebook however there seems to be a problem with context: the model mostly outputs random text instead of referencing my questions. If anyone has any idea help would be greatly appreciated!&lt;/p&gt;\n\n&lt;p&gt;Although this might not be the perfect solution I thought I&amp;#39;d share it here, maybe someone finds it useful:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/Danmoreng/local-qwen3-coder-env\"&gt;https://github.com/Danmoreng/local-qwen3-coder-env&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?auto=webp&amp;s=696008496e1bd6e0694f2b9886836ed38e83c28c",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ff28b544d610ccf62e98f3feddd075959ce926b",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=72c1589c6e0b5c99b15e4303d1779e5eec8f9a80",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3fcaf94729c1bcd29118e071f6822ad5c896bec0",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6e9afa83dad53bd5ba85b1ab44cba7113663f13b",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c6c818c201ba8089a89a84387df82adfd28220d6",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b492d3d79ff10edeacd4e012c9aa3e160eaf88db",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "VUxQEsGlvwLQLt8I6vmES5t3eMsC-GHbmpsKUyFalso"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1metf4h",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Danmoreng",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/",
            "subreddit_subscribers": 508541,
            "created_utc": 1754045970,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6cjoc8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754055429,
                      "send_replies": true,
                      "parent_id": "t1_n6c6woc",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks, will try this out.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6cjoc8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks, will try this out.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1metf4h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6cjoc8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754055429,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6dft6s",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AdamDhahabi",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6cuyme",
                                "score": 1,
                                "author_fullname": "t2_x5lnbc2",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I had it only with ik\\_llama.cpp tough, vanilla llama.cpp was fine with -fa",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6dft6s",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I had it only with ik_llama.cpp tough, vanilla llama.cpp was fine with -fa&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1metf4h",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6dft6s/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754064782,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754064782,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6cuyme",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FullstackSensei",
                      "can_mod_post": false,
                      "created_utc": 1754058809,
                      "send_replies": true,
                      "parent_id": "t1_n6c6woc",
                      "score": 1,
                      "author_fullname": "t2_17n3nqtj56",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yep, can confirm the random text issue on both ik_llama.cpp and vanilla llama.cpp occur when -fa is enabled.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6cuyme",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep, can confirm the random text issue on both ik_llama.cpp and vanilla llama.cpp occur when -fa is enabled.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1metf4h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6cuyme/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754058809,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6denv6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754064449,
                      "send_replies": true,
                      "parent_id": "t1_n6c6woc",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Awesome that was the problem. Had to remove the kv cache params well, I also reduced the context size and now I get 12.5 t/s on the notebook. With these parameters:\n\n&gt;.\\llama-server.exe --model \".\\models\\Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_XS.gguf\" -c 32000 -fmoe -rtr -ot exps=CPU -ngl 99 --threads 8 --temp 0.6 --min-p 0.0 --top-p 0.8 --top-k 20",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6denv6",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Awesome that was the problem. Had to remove the kv cache params well, I also reduced the context size and now I get 12.5 t/s on the notebook. With these parameters:&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;.\\llama-server.exe --model &amp;quot;.\\models\\Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_XS.gguf&amp;quot; -c 32000 -fmoe -rtr -ot exps=CPU -ngl 99 --threads 8 --temp 0.6 --min-p 0.0 --top-p 0.8 --top-k 20&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1metf4h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6denv6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754064449,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6c6woc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1754051120,
            "send_replies": true,
            "parent_id": "t3_1metf4h",
            "score": 3,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The random text issue could be because of flash attention, try disabling it. I had the same issue last week with Qwen 235b on my dual-GPU setup. My second GPU is also compute 6.1 (Quadro P5000).",
            "edited": 1754051366,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c6woc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The random text issue could be because of flash attention, try disabling it. I had the same issue last week with Qwen 235b on my dual-GPU setup. My second GPU is also compute 6.1 (Quadro P5000).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6c6woc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754051120,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1metf4h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6by7u4",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ArchdukeofHyperbole",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6bx13l",
                                "score": 1,
                                "author_fullname": "t2_1p41v97q5d",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Oh, idk about the responses. My experience is it's usually  an issue with the chat template when the responses are non-related.\n\nEdit: this ik_llama.cpp seems interesting though. I have a 1660 ti on my notebook and I guess that's why I was focusing on the speed part. I currently get about 9 tps running qwen 3AB on lm studio which is based on llama.cpp. So was thinking of the possibility since my gpu has 7.5 compute it would run somewhat faster on ik_llama",
                                "edited": 1754048136,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6by7u4",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh, idk about the responses. My experience is it&amp;#39;s usually  an issue with the chat template when the responses are non-related.&lt;/p&gt;\n\n&lt;p&gt;Edit: this ik_llama.cpp seems interesting though. I have a 1660 ti on my notebook and I guess that&amp;#39;s why I was focusing on the speed part. I currently get about 9 tps running qwen 3AB on lm studio which is based on llama.cpp. So was thinking of the possibility since my gpu has 7.5 compute it would run somewhat faster on ik_llama&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1metf4h",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6by7u4/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754047721,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754047721,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6bx13l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754047212,
                      "send_replies": true,
                      "parent_id": "t1_n6bwadq",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That its slower is normal, that’s not the problem. The problem is, that I get random responses as soon as I type anything longer than „test“. And that’s weird. To the „test“ prompt the answer was actually related, to any other prompt I get random output.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6bx13l",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That its slower is normal, that’s not the problem. The problem is, that I get random responses as soon as I type anything longer than „test“. And that’s weird. To the „test“ prompt the answer was actually related, to any other prompt I get random output.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1metf4h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6bx13l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754047212,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6bwadq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArchdukeofHyperbole",
            "can_mod_post": false,
            "created_utc": 1754046886,
            "send_replies": true,
            "parent_id": "t3_1metf4h",
            "score": 2,
            "author_fullname": "t2_1p41v97q5d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "From that GitHub, it recommends compute ≥ 7.0\n\nYour gpu would be the cause on the slower machine\n\nEdit: the 1070 has a compute of 6.1\n\nhttps://developer.nvidia.com/cuda-legacy-gpus",
            "edited": 1754047585,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bwadq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;From that GitHub, it recommends compute ≥ 7.0&lt;/p&gt;\n\n&lt;p&gt;Your gpu would be the cause on the slower machine&lt;/p&gt;\n\n&lt;p&gt;Edit: the 1070 has a compute of 6.1&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://developer.nvidia.com/cuda-legacy-gpus\"&gt;https://developer.nvidia.com/cuda-legacy-gpus&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6bwadq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754046886,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1metf4h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6e4222",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "wooden-guy",
            "can_mod_post": false,
            "created_utc": 1754071691,
            "send_replies": true,
            "parent_id": "t3_1metf4h",
            "score": 2,
            "author_fullname": "t2_16to413y2o",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My brain can't understand why lm studio doesn't implement ik llama or give us an option to run it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6e4222",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My brain can&amp;#39;t understand why lm studio doesn&amp;#39;t implement ik llama or give us an option to run it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6e4222/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754071691,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1metf4h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6e0grp",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Danmoreng",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6dzf7f",
                                          "score": 1,
                                          "author_fullname": "t2_7z26p",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "yea then you might want to try ik_llama.cpp. For me its ~80% faster than base llama.cpp (20 t/s/ vs 35-38 t/s)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6e0grp",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yea then you might want to try ik_llama.cpp. For me its ~80% faster than base llama.cpp (20 t/s/ vs 35-38 t/s)&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1metf4h",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6e0grp/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754070662,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754070662,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6dzf7f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "QFGTrialByFire",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6df0o8",
                                "score": 1,
                                "author_fullname": "t2_1h4o7f23eh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "oops sorry meant to say 4-5x faster than tensorflow and about 25% faster than llama.cpp. The main benefit for me at least on my setup is llama.cpp does sampling during forward pass back on the cpu. I have an old cpu and motherboard (old pcie) so every transfer during forward pass causes it to slow down a lot on llama.cpp. Try it yourself its not harder to setup/use vllm than llama.cpp. Even on a faster cpu/mb/pcie that hop back for sampling has got to be slower. I'm not sure about benchmarks most I could see seem to focus on large setups.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6dzf7f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;oops sorry meant to say 4-5x faster than tensorflow and about 25% faster than llama.cpp. The main benefit for me at least on my setup is llama.cpp does sampling during forward pass back on the cpu. I have an old cpu and motherboard (old pcie) so every transfer during forward pass causes it to slow down a lot on llama.cpp. Try it yourself its not harder to setup/use vllm than llama.cpp. Even on a faster cpu/mb/pcie that hop back for sampling has got to be slower. I&amp;#39;m not sure about benchmarks most I could see seem to focus on large setups.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1metf4h",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6dzf7f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754070369,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754070369,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6df0o8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754064553,
                      "send_replies": true,
                      "parent_id": "t1_n6cs1l5",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I know vllm is another fast inference engine, but I highly doubt the 5-6x claim. Do you have any benchmarks that show this?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6df0o8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I know vllm is another fast inference engine, but I highly doubt the 5-6x claim. Do you have any benchmarks that show this?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1metf4h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6df0o8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754064553,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6cs1l5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "QFGTrialByFire",
            "can_mod_post": false,
            "created_utc": 1754057962,
            "send_replies": true,
            "parent_id": "t3_1metf4h",
            "score": 1,
            "author_fullname": "t2_1h4o7f23eh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hi just FYI If you want faster especially with that nvdia 4070ti ...load the model with vllm on WSL in windows it will be a lot faster than llama.cpp/lmstudio probably around 5-6x faster for generation of tokens.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6cs1l5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi just FYI If you want faster especially with that nvdia 4070ti ...load the model with vllm on WSL in windows it will be a lot faster than llama.cpp/lmstudio probably around 5-6x faster for generation of tokens.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6cs1l5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754057962,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1metf4h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6fdeqk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Danmoreng",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6f4uze",
                                "score": 1,
                                "author_fullname": "t2_7z26p",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Hm, doesn't seem to change anything regarding performance, at least not with a quick test on my notebook without flash attention. Seems to be even slower, although that might be due to the longer output it gave me for a simple Todo app.\n\n&gt; **Qwen3-Coder-30B-A3B-Instruct-IQ4_KSS.gguf**\n\n&gt; Prompt\n\n&gt; - Tokens: 22\n\n&gt; - Time: 1006.776 ms\n\n&gt; - Speed: 21.9 t/s\n\n&gt; Generation\n\n&gt; - Tokens: 1760\n\n&gt; - Time: 189053.671 ms\n\n&gt; - Speed: 9.3 t/s\n\n\n&gt; **Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_XS.gguf**\n\n&gt; Prompt\n\n&gt; - Tokens: 22\n\n&gt; - Time: 998.047 ms\n\n&gt; - Speed: 22.0 t/s\n\n&gt; Generation\n\n&gt; - Tokens: 1269\n\n&gt; - Time: 106599.278 ms\n\n&gt; - Speed: 11.9 t/s",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6fdeqk",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hm, doesn&amp;#39;t seem to change anything regarding performance, at least not with a quick test on my notebook without flash attention. Seems to be even slower, although that might be due to the longer output it gave me for a simple Todo app.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;&lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct-IQ4_KSS.gguf&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Prompt&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Tokens: 22&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Time: 1006.776 ms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Speed: 21.9 t/s&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Generation&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Tokens: 1760&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Time: 189053.671 ms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Speed: 9.3 t/s&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_XS.gguf&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;Prompt&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Tokens: 22&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Time: 998.047 ms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Speed: 22.0 t/s&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Generation&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Tokens: 1269&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Time: 106599.278 ms&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Speed: 11.9 t/s&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1metf4h",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6fdeqk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754085321,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754085321,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6f4uze",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Danmoreng",
                      "can_mod_post": false,
                      "created_utc": 1754082611,
                      "send_replies": true,
                      "parent_id": "t1_n6ewb2f",
                      "score": 1,
                      "author_fullname": "t2_7z26p",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Tbh I just took that ik_llama.cpp is faster for MoE from another [reddit comment](https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/n5zziaz/) and made an install script for it.\n\nI actually thought IQ quants cannot be run in llama.cpp and already are better? What's the difference with IQ4 KSS?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6f4uze",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Tbh I just took that ik_llama.cpp is faster for MoE from another &lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/n5zziaz/\"&gt;reddit comment&lt;/a&gt; and made an install script for it.&lt;/p&gt;\n\n&lt;p&gt;I actually thought IQ quants cannot be run in llama.cpp and already are better? What&amp;#39;s the difference with IQ4 KSS?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1metf4h",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6f4uze/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754082611,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ewb2f",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mkengine",
            "can_mod_post": false,
            "created_utc": 1754080054,
            "send_replies": true,
            "parent_id": "t3_1metf4h",
            "score": 1,
            "author_fullname": "t2_9p2xe",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Am I seeing this right on your repo, that you recommend ik_llama with normal IQ4_XS quants? Why not the ik_llama specific quants by ubergarm, like IQ4_KSS?\n https://huggingface.co/ubergarm/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/Qwen3-Coder-30B-A3B-Instruct-IQ4_KSS.gguf ?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ewb2f",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Am I seeing this right on your repo, that you recommend ik_llama with normal IQ4_XS quants? Why not the ik_llama specific quants by ubergarm, like IQ4_KSS?\n &lt;a href=\"https://huggingface.co/ubergarm/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/Qwen3-Coder-30B-A3B-Instruct-IQ4_KSS.gguf\"&gt;https://huggingface.co/ubergarm/Qwen3-Coder-30B-A3B-Instruct-GGUF/blob/main/Qwen3-Coder-30B-A3B-Instruct-IQ4_KSS.gguf&lt;/a&gt; ?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/n6ewb2f/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754080054,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1metf4h",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]