[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi. Coming from a \"classic ML\" background, I know what do training data look like for a typical supervised learning problem. But when thinking of an instruction-tuned LLM,things get confusing.\n\nWhat does one sample look like, e.g. if I want to fine tune a small LLM for text summarisation in a particular domain? And how many samples does it take to meaningfully change the model weights, given that we are talking about hundreds of millions of parameters?\n\nThank you\n\nBest",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What do the samples look like for fine tuning?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1milr4q",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_127kho",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754428811,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi. Coming from a &amp;quot;classic ML&amp;quot; background, I know what do training data look like for a typical supervised learning problem. But when thinking of an instruction-tuned LLM,things get confusing.&lt;/p&gt;\n\n&lt;p&gt;What does one sample look like, e.g. if I want to fine tune a small LLM for text summarisation in a particular domain? And how many samples does it take to meaningfully change the model weights, given that we are talking about hundreds of millions of parameters?&lt;/p&gt;\n\n&lt;p&gt;Thank you&lt;/p&gt;\n\n&lt;p&gt;Best&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1milr4q",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ihatebeinganonymous",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1milr4q/what_do_the_samples_look_like_for_fine_tuning/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1milr4q/what_do_the_samples_look_like_for_fine_tuning/",
            "subreddit_subscribers": 511364,
            "created_utc": 1754428811,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "c07aa42e-51fe-11f0-afcc-462aad931709",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n74j3sv",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "entsnack",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n74egkc",
                                "score": 1,
                                "author_fullname": "t2_1a48h7vf",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It's very similar.\n\nThe loss function is cross-entropy loss over the sequence of output tokens. The loss for a set of tokens \"INPUT: Today is a OUTPUT: good day\" is -log P(good | today is a ) - log P(day | today is a good). This is average over all examples in a minibatch.\n\nSo yes we have iterations and the loss goes down in each iteration. When fine-tuning an LLM, people often use the Huggingface Transformers or TRL libraries, which abstracts away a lot of the gruntwork like padding to make all examples the same size in a batch.\n\nI highly recommend \"LLMs from Scratch\" at Stanford and Sebastian Raschka's content to learn more.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n74j3sv",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "a": ":X:",
                                    "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                                    "e": "emoji"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It&amp;#39;s very similar.&lt;/p&gt;\n\n&lt;p&gt;The loss function is cross-entropy loss over the sequence of output tokens. The loss for a set of tokens &amp;quot;INPUT: Today is a OUTPUT: good day&amp;quot; is -log P(good | today is a ) - log P(day | today is a good). This is average over all examples in a minibatch.&lt;/p&gt;\n\n&lt;p&gt;So yes we have iterations and the loss goes down in each iteration. When fine-tuning an LLM, people often use the Huggingface Transformers or TRL libraries, which abstracts away a lot of the gruntwork like padding to make all examples the same size in a batch.&lt;/p&gt;\n\n&lt;p&gt;I highly recommend &amp;quot;LLMs from Scratch&amp;quot; at Stanford and Sebastian Raschka&amp;#39;s content to learn more.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1milr4q",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "dark",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1milr4q/what_do_the_samples_look_like_for_fine_tuning/n74j3sv/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754431349,
                                "author_flair_text": ":X:",
                                "treatment_tags": [],
                                "created_utc": 1754431349,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "transparent",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n74egkc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ihatebeinganonymous",
                      "can_mod_post": false,
                      "created_utc": 1754429852,
                      "send_replies": true,
                      "parent_id": "t1_n74csk1",
                      "score": 2,
                      "author_fullname": "t2_127kho",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Many thanks. Do we have iterations here, like we did when training \"older\" deep networks by optimising the loss function etc? Do we get to that level of details in fine-tuning an LLM too?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74egkc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Many thanks. Do we have iterations here, like we did when training &amp;quot;older&amp;quot; deep networks by optimising the loss function etc? Do we get to that level of details in fine-tuning an LLM too?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1milr4q",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1milr4q/what_do_the_samples_look_like_for_fine_tuning/n74egkc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754429852,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n74hoiz",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Different741",
                      "can_mod_post": false,
                      "created_utc": 1754430888,
                      "send_replies": true,
                      "parent_id": "t1_n74csk1",
                      "score": 2,
                      "author_fullname": "t2_1v2au0ln9m",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "thanks as well. very helpful.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n74hoiz",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;thanks as well. very helpful.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1milr4q",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1milr4q/what_do_the_samples_look_like_for_fine_tuning/n74hoiz/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754430888,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n74csk1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "entsnack",
            "can_mod_post": false,
            "created_utc": 1754429314,
            "send_replies": true,
            "parent_id": "t3_1milr4q",
            "score": 1,
            "author_fullname": "t2_1a48h7vf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The samples are just \"prompt\" and \"answer\" pairs or \"user\" and \"assistant\" pairs.\n\nThis page has a nice example: [https://huggingface.co/docs/trl/main/en/sft\\_trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)\n\n        # Standard language modeling\n        {\"text\": \"The sky is blue.\"}\n        \n        # Conversational language modeling\n        {\"messages\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"},\n                      {\"role\": \"assistant\", \"content\": \"It is blue.\"}]}\n        \n        # Standard prompt-completion\n        {\"prompt\": \"The sky is\",\n         \"completion\": \" blue.\"}\n        \n        # Conversational prompt-completion\n        {\"prompt\": [{\"role\": \"user\", \"content\": \"What color is the sky?\"}],\n         \"completion\": [{\"role\": \"assistant\", \"content\": \"It is blue.\"}]}\n\nYou will start seeing meaningful results with as few as 10 examples (depending on how many steps you fine-tune for), but at that scale you might as well use few-shot prompting.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74csk1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "a": ":X:",
                "u": "https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X",
                "e": "emoji"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The samples are just &amp;quot;prompt&amp;quot; and &amp;quot;answer&amp;quot; pairs or &amp;quot;user&amp;quot; and &amp;quot;assistant&amp;quot; pairs.&lt;/p&gt;\n\n&lt;p&gt;This page has a nice example: &lt;a href=\"https://huggingface.co/docs/trl/main/en/sft_trainer\"&gt;https://huggingface.co/docs/trl/main/en/sft_trainer&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;    # Standard language modeling\n    {&amp;quot;text&amp;quot;: &amp;quot;The sky is blue.&amp;quot;}\n\n    # Conversational language modeling\n    {&amp;quot;messages&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What color is the sky?&amp;quot;},\n                  {&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;It is blue.&amp;quot;}]}\n\n    # Standard prompt-completion\n    {&amp;quot;prompt&amp;quot;: &amp;quot;The sky is&amp;quot;,\n     &amp;quot;completion&amp;quot;: &amp;quot; blue.&amp;quot;}\n\n    # Conversational prompt-completion\n    {&amp;quot;prompt&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What color is the sky?&amp;quot;}],\n     &amp;quot;completion&amp;quot;: [{&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;It is blue.&amp;quot;}]}\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;You will start seeing meaningful results with as few as 10 examples (depending on how many steps you fine-tune for), but at that scale you might as well use few-shot prompting.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1milr4q/what_do_the_samples_look_like_for_fine_tuning/n74csk1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754429314,
            "author_flair_text": ":X:",
            "treatment_tags": [],
            "link_id": "t3_1milr4q",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "transparent",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]