[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm trying to test out iklcpp and the new Qwen3 235B non thinking. I'm using Unsloth UD-Q4_K_XL quant. My system is 64Gb DDR4 ram and 2x 16Gb GPUs. I have previously tested this split gguf with latest release of koboldcpp. But with iklcpp, I'm getting memory allocation failure.\n\nBasically I'm using mmap as I don't have enough Ram+Vram. \n\nFor kcpp, I use the following settings:\n```\nkobold --model AI/LLM/Qwen3/Qwen3-235B-A228-Instruct-2507-UD-Q4_KXL-00001-of00003.gguf \\ --contextsize 65536 \\\n--blasbatchsize 2048 \\\n--tensor_split 0.5 0.5 \\\n--usecuda nommq \\\n--gpulayers 999 \\\n--flashattention \\\n--overridetensors \"([0-9]+).ffn_.*_exps.weight=CPU\"  \\\n--usemmap \\\n--threads 24\n```\n\nWith this, I get about 10+10Gib vram usage on my two GPUs. Model loads and works, however slow it might be. \n\nI compiled iklcpp using the following instructions:\n\n```\n# Install build dependencies and cuda toolkit as needed\n\n# Clone\ngit clone https://github.com/ikawrakow/ik_llama.cpp\ncd ik_llama.cpp\n\n# Configure CUDA+CPU Backend (I used this) \ncmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF\n\n# *or* Configure CPU Only Backend\ncmake -B ./build -DGGML_CUDA=OFF -DGGML_BLAS=OFF\n\n# Build\ncmake --build ./build --config Release -j $(nproc)\n\n# Confirm\n./build/bin/llama-server --version\nversion: 3597 (68a5b604)\n```\n\nNow if I try to use the gguf with iklcpp with the following command:\n```\n./AI/ik_llama.cpp/build/bin/llama-server \\\n-m AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf \\\n-t 20 \\\n-c 65536 \\\n-b 4096 \\\n-ub 4096 \\\n-fa \\\n-ot \"([0-9]+).ffn_.*_exps.weight=CPU\" \\\n-ngl 95 \\\n-sm layer \\\n-ts 1,1 \\\n-amb 512 \\\n-fmoe 1\n```\n\nI get the following error:\n```\nllama_new_context_with_model: n_ctx      = 65536\nllama_new_context_with_model: n_batch    = 4096\nllama_new_context_with_model: n_ubatch   = 4096\nllama_new_context_with_model: flash_attn = 1\nllama_new_context_with_model: mla_attn   = 0\nllama_new_context_with_model: attn_max_b = 512\nllama_new_context_with_model: fused_moe  = 1\nllama_new_context_with_model: ser        = -1, 0\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  6144.00 MiB\nllama_kv_cache_init:      CUDA1 KV buffer size =  5888.00 MiB\nllama_new_context_with_model: KV self size  = 12032.00 MiB, K (f16): 6016.00 MiB, V (f16): 6016.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     1.16 MiB\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 523616.00 MiB on device 0: cudaMalloc failed: out of memory\nggml_gallocr_reserve_n: failed to allocate CUDA0 buffer of size 549051165696\nllama_new_context_with_model: failed to allocate compute buffers\nllama_init_from_gpt_params: error: failed to create context with model 'AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-0000\n1-of-00003.gguf'\nERR [              load_model] unable to load model | tid=\"140606057730048\" timestamp=1753561505 model=\"AI/LLM/Qwen3/Qwen3-235B-A\n22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf\"\nfish: Job 1, './AI/ik_llama.cpp/build/bin/lla…' terminated by signal -m AI/LLM/Qwen3/Qwen3-235B-A22B… (-t 20 \\)\nfish: Job -c 65536 \\, '-b 4096 \\' terminated by signal -ub 4096 \\ (-fa \\)\nfish: Job -ot \"([0-9]+).ffn_.*_exps.weigh…, '-ngl 95 \\' terminated by signal -sm layer \\ (-ts 1,1 \\)\nfish: Job -amb 512 \\, '-fmoe' terminated by signal SIGSEGV (Address boundary error)\n```\n\nI'm guessing the issue is with the pipeline parallelism n_copies = 4. But I couldn't find any flag to turn it off.\n\nI would appreciate any explanation of the issue and advice regarding getting this working. Thank you. \n\n\nEdit: solved, needed `DGGML_SCHED_MAX_COPIES=1` as build option. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "ik_llama.cpp help!",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1ma41wu",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_9nex5np2",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753567668,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753563267,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m trying to test out iklcpp and the new Qwen3 235B non thinking. I&amp;#39;m using Unsloth UD-Q4_K_XL quant. My system is 64Gb DDR4 ram and 2x 16Gb GPUs. I have previously tested this split gguf with latest release of koboldcpp. But with iklcpp, I&amp;#39;m getting memory allocation failure.&lt;/p&gt;\n\n&lt;p&gt;Basically I&amp;#39;m using mmap as I don&amp;#39;t have enough Ram+Vram. &lt;/p&gt;\n\n&lt;p&gt;For kcpp, I use the following settings:\n&lt;code&gt;\nkobold --model AI/LLM/Qwen3/Qwen3-235B-A228-Instruct-2507-UD-Q4_KXL-00001-of00003.gguf \\ --contextsize 65536 \\\n--blasbatchsize 2048 \\\n--tensor_split 0.5 0.5 \\\n--usecuda nommq \\\n--gpulayers 999 \\\n--flashattention \\\n--overridetensors &amp;quot;([0-9]+).ffn_.*_exps.weight=CPU&amp;quot;  \\\n--usemmap \\\n--threads 24\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;With this, I get about 10+10Gib vram usage on my two GPUs. Model loads and works, however slow it might be. &lt;/p&gt;\n\n&lt;p&gt;I compiled iklcpp using the following instructions:&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;h1&gt;Install build dependencies and cuda toolkit as needed&lt;/h1&gt;\n\n&lt;h1&gt;Clone&lt;/h1&gt;\n\n&lt;p&gt;git clone &lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp\"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt;\ncd ik_llama.cpp&lt;/p&gt;\n\n&lt;h1&gt;Configure CUDA+CPU Backend (I used this)&lt;/h1&gt;\n\n&lt;p&gt;cmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF&lt;/p&gt;\n\n&lt;h1&gt;&lt;em&gt;or&lt;/em&gt; Configure CPU Only Backend&lt;/h1&gt;\n\n&lt;p&gt;cmake -B ./build -DGGML_CUDA=OFF -DGGML_BLAS=OFF&lt;/p&gt;\n\n&lt;h1&gt;Build&lt;/h1&gt;\n\n&lt;p&gt;cmake --build ./build --config Release -j $(nproc)&lt;/p&gt;\n\n&lt;h1&gt;Confirm&lt;/h1&gt;\n\n&lt;p&gt;./build/bin/llama-server --version\nversion: 3597 (68a5b604)\n```&lt;/p&gt;\n\n&lt;p&gt;Now if I try to use the gguf with iklcpp with the following command:\n&lt;code&gt;\n./AI/ik_llama.cpp/build/bin/llama-server \\\n-m AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf \\\n-t 20 \\\n-c 65536 \\\n-b 4096 \\\n-ub 4096 \\\n-fa \\\n-ot &amp;quot;([0-9]+).ffn_.*_exps.weight=CPU&amp;quot; \\\n-ngl 95 \\\n-sm layer \\\n-ts 1,1 \\\n-amb 512 \\\n-fmoe 1\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I get the following error:\n&lt;code&gt;\nllama_new_context_with_model: n_ctx      = 65536\nllama_new_context_with_model: n_batch    = 4096\nllama_new_context_with_model: n_ubatch   = 4096\nllama_new_context_with_model: flash_attn = 1\nllama_new_context_with_model: mla_attn   = 0\nllama_new_context_with_model: attn_max_b = 512\nllama_new_context_with_model: fused_moe  = 1\nllama_new_context_with_model: ser        = -1, 0\nllama_new_context_with_model: freq_base  = 5000000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  6144.00 MiB\nllama_kv_cache_init:      CUDA1 KV buffer size =  5888.00 MiB\nllama_new_context_with_model: KV self size  = 12032.00 MiB, K (f16): 6016.00 MiB, V (f16): 6016.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     1.16 MiB\nllama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\nggml_backend_cuda_buffer_type_alloc_buffer: allocating 523616.00 MiB on device 0: cudaMalloc failed: out of memory\nggml_gallocr_reserve_n: failed to allocate CUDA0 buffer of size 549051165696\nllama_new_context_with_model: failed to allocate compute buffers\nllama_init_from_gpt_params: error: failed to create context with model &amp;#39;AI/LLM/Qwen3/Qwen3-235B-A22B-Instruct-2507-UD-Q4_K_XL-0000\n1-of-00003.gguf&amp;#39;\nERR [              load_model] unable to load model | tid=&amp;quot;140606057730048&amp;quot; timestamp=1753561505 model=&amp;quot;AI/LLM/Qwen3/Qwen3-235B-A\n22B-Instruct-2507-UD-Q4_K_XL-00001-of-00003.gguf&amp;quot;\nfish: Job 1, &amp;#39;./AI/ik_llama.cpp/build/bin/lla…&amp;#39; terminated by signal -m AI/LLM/Qwen3/Qwen3-235B-A22B… (-t 20 \\)\nfish: Job -c 65536 \\, &amp;#39;-b 4096 \\&amp;#39; terminated by signal -ub 4096 \\ (-fa \\)\nfish: Job -ot &amp;quot;([0-9]+).ffn_.*_exps.weigh…, &amp;#39;-ngl 95 \\&amp;#39; terminated by signal -sm layer \\ (-ts 1,1 \\)\nfish: Job -amb 512 \\, &amp;#39;-fmoe&amp;#39; terminated by signal SIGSEGV (Address boundary error)\n&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m guessing the issue is with the pipeline parallelism n_copies = 4. But I couldn&amp;#39;t find any flag to turn it off.&lt;/p&gt;\n\n&lt;p&gt;I would appreciate any explanation of the issue and advice regarding getting this working. Thank you. &lt;/p&gt;\n\n&lt;p&gt;Edit: solved, needed &lt;code&gt;DGGML_SCHED_MAX_COPIES=1&lt;/code&gt; as build option. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?auto=webp&amp;s=6e0a61fe992b8503db41032a1bd82d39cfd4f716",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f9eee7355afb465bfdb476b246b097f0a0153b4",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=39c878657e2261a4938ec3ba26bd53b57f5a89c9",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eea7730ed14db82624c81e1d19b49d3d315fc245",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0c6e108cbe3e0d3ed12257fd8b9943594383239",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca14e51a272d2532a144d6b2e4453d5222de5f88",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a2dc4fb0d9648fe2bc8b7af0ae5e80923d6c4129",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "d-Wkvq34ZRzACVPwHO_DAPDTtuOd0WoROIS_xIxGbqQ"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1ma41wu",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "lacerating_aura",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/",
            "subreddit_subscribers": 504973,
            "created_utc": 1753563267,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": {
                                                                "kind": "Listing",
                                                                "data": {
                                                                  "after": null,
                                                                  "dist": null,
                                                                  "modhash": "",
                                                                  "geo_filter": "",
                                                                  "children": [
                                                                    {
                                                                      "kind": "t1",
                                                                      "data": {
                                                                        "subreddit_id": "t5_81eyvm",
                                                                        "approved_at_utc": null,
                                                                        "author_is_blocked": false,
                                                                        "comment_type": null,
                                                                        "awarders": [],
                                                                        "mod_reason_by": null,
                                                                        "banned_by": null,
                                                                        "author_flair_type": "text",
                                                                        "total_awards_received": 0,
                                                                        "subreddit": "LocalLLaMA",
                                                                        "author_flair_template_id": null,
                                                                        "distinguished": null,
                                                                        "likes": null,
                                                                        "replies": "",
                                                                        "user_reports": [],
                                                                        "saved": false,
                                                                        "id": "n5c1tqz",
                                                                        "banned_at_utc": null,
                                                                        "mod_reason_title": null,
                                                                        "gilded": 0,
                                                                        "archived": false,
                                                                        "collapsed_reason_code": null,
                                                                        "no_follow": true,
                                                                        "author": "mxmumtuna",
                                                                        "can_mod_post": false,
                                                                        "send_replies": true,
                                                                        "parent_id": "t1_n5c136a",
                                                                        "score": 2,
                                                                        "author_fullname": "t2_igbly",
                                                                        "approved_by": null,
                                                                        "mod_note": null,
                                                                        "all_awardings": [],
                                                                        "collapsed": false,
                                                                        "body": "Great! Share an update of your command when you’re happy with it and we can give more feedback. \n\nThere’s a huge thread of running ik with these thicccboi models at [L1T](https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826?u=mxmumtuna). Lots of good info, command references, and general thoughts on why some switches work better than others over there.",
                                                                        "edited": false,
                                                                        "gildings": {},
                                                                        "author_flair_css_class": null,
                                                                        "name": "t1_n5c1tqz",
                                                                        "is_submitter": false,
                                                                        "downs": 0,
                                                                        "author_flair_richtext": [],
                                                                        "author_patreon_flair": false,
                                                                        "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Great! Share an update of your command when you’re happy with it and we can give more feedback. &lt;/p&gt;\n\n&lt;p&gt;There’s a huge thread of running ik with these thicccboi models at &lt;a href=\"https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826?u=mxmumtuna\"&gt;L1T&lt;/a&gt;. Lots of good info, command references, and general thoughts on why some switches work better than others over there.&lt;/p&gt;\n&lt;/div&gt;",
                                                                        "removal_reason": null,
                                                                        "collapsed_reason": null,
                                                                        "link_id": "t3_1ma41wu",
                                                                        "associated_award": null,
                                                                        "stickied": false,
                                                                        "author_premium": true,
                                                                        "can_gild": false,
                                                                        "top_awarded_type": null,
                                                                        "unrepliable_reason": null,
                                                                        "author_flair_text_color": null,
                                                                        "score_hidden": false,
                                                                        "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5c1tqz/",
                                                                        "subreddit_type": "public",
                                                                        "locked": false,
                                                                        "report_reasons": null,
                                                                        "created": 1753566801,
                                                                        "author_flair_text": null,
                                                                        "treatment_tags": [],
                                                                        "created_utc": 1753566801,
                                                                        "subreddit_name_prefixed": "r/LocalLLaMA",
                                                                        "controversiality": 0,
                                                                        "depth": 6,
                                                                        "author_flair_background_color": null,
                                                                        "collapsed_because_crowd_control": null,
                                                                        "mod_reports": [],
                                                                        "num_reports": null,
                                                                        "ups": 2
                                                                      }
                                                                    }
                                                                  ],
                                                                  "before": null
                                                                }
                                                              },
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n5c136a",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "lacerating_aura",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n5bzaw5",
                                                              "score": 1,
                                                              "author_fullname": "t2_9nex5np2",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Thank you very much. I'll try tweaking this combo.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n5c136a",
                                                              "is_submitter": true,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you very much. I&amp;#39;ll try tweaking this combo.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1ma41wu",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5c136a/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1753566549,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1753566549,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n5bzaw5",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "mxmumtuna",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n5by7c4",
                                                    "score": 1,
                                                    "author_fullname": "t2_igbly",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "The example has it pretty well covered\n\nFor single GPU:\n\n-ot \"blk\\.[0-9]\\.ffn.*=CUDA0\"\n\n-ot \"blk.*\\.ffn.*=CPU”\n\nFor multi GPU:\n\n-ot \"blk\\.[0-9]\\.ffn.*=CUDA0\" # offload 0-9 to the first GPU \n\n-ot \"blk\\.1[0-9]\\.ffn.*=CUDA1\" # offload 10-19 to the second GPU \n\n-ot \"blk.*\\.ffn.*=CPU” # rest to cpu\n\nTweak as needed for your setup until you can load with the context you desire and can fit it all without OOM errors",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n5bzaw5",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The example has it pretty well covered&lt;/p&gt;\n\n&lt;p&gt;For single GPU:&lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;blk.[0-9].ffn.*=CUDA0&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;blk.&lt;em&gt;.ffn.&lt;/em&gt;=CPU”&lt;/p&gt;\n\n&lt;p&gt;For multi GPU:&lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;blk.[0-9].ffn.*=CUDA0&amp;quot; # offload 0-9 to the first GPU &lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;blk.1[0-9].ffn.*=CUDA1&amp;quot; # offload 10-19 to the second GPU &lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;blk.&lt;em&gt;.ffn.&lt;/em&gt;=CPU” # rest to cpu&lt;/p&gt;\n\n&lt;p&gt;Tweak as needed for your setup until you can load with the context you desire and can fit it all without OOM errors&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1ma41wu",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": true,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5bzaw5/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753565937,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753565937,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5by7c4",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "lacerating_aura",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5bwt88",
                                          "score": 2,
                                          "author_fullname": "t2_9nex5np2",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Thank you for your help.\nI've got the unsloth gguf loaded now. I already planned to test ik specific quants. \n\nAbout the -ot flag, from the example, I could use:\n`-ot exps=CPU`\n\nWouldn't -ngl take care of offloading rest to GPU? Could you please give an example of what you mean?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5by7c4",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for your help.\nI&amp;#39;ve got the unsloth gguf loaded now. I already planned to test ik specific quants. &lt;/p&gt;\n\n&lt;p&gt;About the -ot flag, from the example, I could use:\n&lt;code&gt;-ot exps=CPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Wouldn&amp;#39;t -ngl take care of offloading rest to GPU? Could you please give an example of what you mean?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1ma41wu",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5by7c4/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753565565,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753565565,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5bwt88",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "mxmumtuna",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5btsy4",
                                "score": 2,
                                "author_fullname": "t2_igbly",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "This is correct but your ot flags are also not correct. You’ll need to explicitly offload tensors to your gpu(s) with a catch all for your CPU.\n\nAlso, if you’re using ik, you’ll want to use one of the ik quants. Check [here](https://huggingface.co/ubergarm/Qwen3-235B-A22B-Instruct-2507-GGUF). \n\nAlso it’s -DGGML_SCHED_MAX_COPIES=1",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5bwt88",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is correct but your ot flags are also not correct. You’ll need to explicitly offload tensors to your gpu(s) with a catch all for your CPU.&lt;/p&gt;\n\n&lt;p&gt;Also, if you’re using ik, you’ll want to use one of the ik quants. Check &lt;a href=\"https://huggingface.co/ubergarm/Qwen3-235B-A22B-Instruct-2507-GGUF\"&gt;here&lt;/a&gt;. &lt;/p&gt;\n\n&lt;p&gt;Also it’s -DGGML_SCHED_MAX_COPIES=1&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": true,
                                "can_gild": false,
                                "link_id": "t3_1ma41wu",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5bwt88/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753565096,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753565096,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5btsy4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "lacerating_aura",
                      "can_mod_post": false,
                      "created_utc": 1753564095,
                      "send_replies": true,
                      "parent_id": "t1_n5bsqoq",
                      "score": 1,
                      "author_fullname": "t2_9nex5np2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thank you, I'll give this a shot.\n\nEdit: it doesn't recoganise the new option:\n`cmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF -GGML_SCHED_MAX_COPIES=1`\n\nEdit 2: fixed, my bad, needed DGGML.",
                      "edited": 1753564868,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5btsy4",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you, I&amp;#39;ll give this a shot.&lt;/p&gt;\n\n&lt;p&gt;Edit: it doesn&amp;#39;t recoganise the new option:\n&lt;code&gt;cmake -B ./build -DGGML_CUDA=ON -DGGML_BLAS=OFF -GGML_SCHED_MAX_COPIES=1&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;Edit 2: fixed, my bad, needed DGGML.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ma41wu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5btsy4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753564095,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5bsqoq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Kooshi_Govno",
            "can_mod_post": false,
            "created_utc": 1753563740,
            "send_replies": true,
            "parent_id": "t3_1ma41wu",
            "score": 3,
            "author_fullname": "t2_7kg5p",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You are correct about the 4 copies being the issue.\n\nYou'll need to recompile with `GGML_SCHED_MAX_COPIES=1`. I have no idea why the default is 4. It's a ridiculous waste of space.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5bsqoq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You are correct about the 4 copies being the issue.&lt;/p&gt;\n\n&lt;p&gt;You&amp;#39;ll need to recompile with &lt;code&gt;GGML_SCHED_MAX_COPIES=1&lt;/code&gt;. I have no idea why the default is 4. It&amp;#39;s a ridiculous waste of space.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5bsqoq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753563740,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ma41wu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5btqs9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "lacerating_aura",
                      "can_mod_post": false,
                      "created_utc": 1753564074,
                      "send_replies": true,
                      "parent_id": "t1_n5btbud",
                      "score": 1,
                      "author_fullname": "t2_9nex5np2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "mmap\n\nEdit: it allows using SSD as virtual memory of sorts. Kinda like swap space. It's REALLY slow, but still allows for proof of concepting. Could be sped up a bit by using raid0 I guess.",
                      "edited": 1753565748,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5btqs9",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;mmap&lt;/p&gt;\n\n&lt;p&gt;Edit: it allows using SSD as virtual memory of sorts. Kinda like swap space. It&amp;#39;s REALLY slow, but still allows for proof of concepting. Could be sped up a bit by using raid0 I guess.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ma41wu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5btqs9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753564074,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5btbud",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fp4guru",
            "can_mod_post": false,
            "created_utc": 1753563935,
            "send_replies": true,
            "parent_id": "t3_1ma41wu",
            "score": 3,
            "author_fullname": "t2_1tp8zldw5g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Model weight is 140gb and you have 64gb + 32gb. How did you test this? Something is off.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5btbud",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Model weight is 140gb and you have 64gb + 32gb. How did you test this? Something is off.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5btbud/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753563935,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ma41wu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5c43vj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "lacerating_aura",
                      "can_mod_post": false,
                      "created_utc": 1753567581,
                      "send_replies": true,
                      "parent_id": "t1_n5c2edr",
                      "score": 1,
                      "author_fullname": "t2_9nex5np2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thank you. For now I have just built with cuda on, blas off and max copies 1. I'll do some testing and then rebuild with suggested options.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5c43vj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you. For now I have just built with cuda on, blas off and max copies 1. I&amp;#39;ll do some testing and then rebuild with suggested options.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1ma41wu",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5c43vj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753567581,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5c2edr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "panchovix",
            "can_mod_post": false,
            "created_utc": 1753566998,
            "send_replies": true,
            "parent_id": "t3_1ma41wu",
            "score": 2,
            "author_fullname": "t2_j1kqr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I suggest building it with these flags, which I use without issues. It will take a while though.\n\n    cmake -B build \\\n        -DGGML_CUDA=ON \\\n        -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n        -DGGML_BLAS=OFF \\\n        -DGGML_IQK_FA_ALL_QUANTS=1 \\\n        -DGGML_SCHED_MAX_COPIES=1 \\\n        -DGGML_CUDA_IQK_FORCE_BF16=1 \\\n        -DGGML_MAX_CONTEXTS=2048 \\\n    \n    cmake --build build --config Release -j 7\n\nThis makes only have 1 copy instead of 4. You can remove the BF16 line if you have an older GPU.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5c2edr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 405B"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I suggest building it with these flags, which I use without issues. It will take a while though.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cmake -B build \\\n    -DGGML_CUDA=ON \\\n    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n    -DGGML_BLAS=OFF \\\n    -DGGML_IQK_FA_ALL_QUANTS=1 \\\n    -DGGML_SCHED_MAX_COPIES=1 \\\n    -DGGML_CUDA_IQK_FORCE_BF16=1 \\\n    -DGGML_MAX_CONTEXTS=2048 \\\n\ncmake --build build --config Release -j 7\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This makes only have 1 copy instead of 4. You can remove the BF16 line if you have an older GPU.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ma41wu/ik_llamacpp_help/n5c2edr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753566998,
            "author_flair_text": "Llama 405B",
            "treatment_tags": [],
            "link_id": "t3_1ma41wu",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]