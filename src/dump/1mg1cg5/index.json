[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).\n\nAiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?\n\n\n16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).\n\n24 GB (go for size - Radeon 7900 - can get new for 800-1000)\n\n24 GB (3090 - refurbished around 1000, new closet to 1500)\n\nPrimary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?\n\nWhat would you do, and why? Or - where can I read up to better understand my options? Threads like this (https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/) make me think offloading to CPU/RAM doesn't need to absolutely throttle throughput when running locally...",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Guide for GPU Purchase for Local LLM?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mg1cg5",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.89,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2orcezep",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754168901,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Trying to balance cost, model size, and token throughput on a PC build (Linux Mint).&lt;/p&gt;\n\n&lt;p&gt;Aiming to keep my gpu cost as close to 1k (or lower) as possible - which would you recommend?&lt;/p&gt;\n\n&lt;p&gt;16 GB (go for fast enough, low power, cheaper, Nvidia 5060 series, runs 500).\n16 GB (go for speed, Nvidia 5070 series - new runs around 700-900).&lt;/p&gt;\n\n&lt;p&gt;24 GB (go for size - Radeon 7900 - can get new for 800-1000)&lt;/p&gt;\n\n&lt;p&gt;24 GB (3090 - refurbished around 1000, new closet to 1500)&lt;/p&gt;\n\n&lt;p&gt;Primary use case is local agent driven (think Claude Code) work. So Qwen 2.5 Coder 32B seems to have an advantage over 14B. Or should I just go with cloud at this point?&lt;/p&gt;\n\n&lt;p&gt;What would you do, and why? Or - where can I read up to better understand my options? Threads like this (&lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1metf4h/installscript_for_qwen3coder_running_on_ik/&lt;/a&gt;) make me think offloading to CPU/RAM doesn&amp;#39;t need to absolutely throttle throughput when running locally...&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mg1cg5",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "DanManPanther",
            "discussion_type": null,
            "num_comments": 20,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/",
            "subreddit_subscribers": 509291,
            "created_utc": 1754168901,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6lankq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754169885,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 19,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "No matter what you get you will want more VRAM inside a month, I promise. So get the most VRAM you can conceivably afford and make sure you‚Äôve got room in your computer for the next time you‚Äôve got GPU money.\n\nPinkie swear this is the truth.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lankq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No matter what you get you will want more VRAM inside a month, I promise. So get the most VRAM you can conceivably afford and make sure you‚Äôve got room in your computer for the next time you‚Äôve got GPU money.&lt;/p&gt;\n\n&lt;p&gt;Pinkie swear this is the truth.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lankq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754169885,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 19
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6lu4j1",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Tyme4Trouble",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6lgtat",
                                "score": 3,
                                "author_fullname": "t2_973amyap",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I'm running vLLM with LMCache.\n\n    export LMCACHE_USE_EXPERIMENTAL=True\n    export LMCACHE_CHUNK_SIZE=256\n    export LMCACHE_LOCAL_CPU=True\n    export LMCACHE_MAX_LOCAL_CPU_SIZE=5.0\n    vllm serve ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W8A8 \\\n      --host 0.0.0.0 \\\n      --port 8000 \\\n      --tensor-parallel-size 2 \\\n      --gpu-memory-utilization 0.9 \\\n      --max-model-len 131072 \\\n      --max-num-seqs 8 \\\n      --trust-remote-code \\\n      --disable-log-requests \\\n      --enable-chunked-prefill \\\n      --max-num-batched-tokens 2048 \\\n      --cuda-graph-sizes 8 \\\n      --enable-prefix-caching \\\n      --enable-auto-tool-choice \\\n      --tool-call-parser hermes \\\n      --kv-transfer-config '{\"kv_connector\":\"LMCacheConnectorV1\", \"kv_role\":\"kv_both\"}' \\\n      --enable-expert-parallel\n\nNote that in my testing an NVLink bridge was necessary to get past about 110 tok/s. Something to do with the number of active parameters. NVLink only benefited Batch &gt; 1 on Qwen3-32B\n\nEdit: Fix the codeblock so it looks right.",
                                "edited": 1754176869,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6lu4j1",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running vLLM with LMCache.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;export LMCACHE_USE_EXPERIMENTAL=True\nexport LMCACHE_CHUNK_SIZE=256\nexport LMCACHE_LOCAL_CPU=True\nexport LMCACHE_MAX_LOCAL_CPU_SIZE=5.0\nvllm serve ramblingpolymath/Qwen3-Coder-30B-A3B-Instruct-W8A8 \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --tensor-parallel-size 2 \\\n  --gpu-memory-utilization 0.9 \\\n  --max-model-len 131072 \\\n  --max-num-seqs 8 \\\n  --trust-remote-code \\\n  --disable-log-requests \\\n  --enable-chunked-prefill \\\n  --max-num-batched-tokens 2048 \\\n  --cuda-graph-sizes 8 \\\n  --enable-prefix-caching \\\n  --enable-auto-tool-choice \\\n  --tool-call-parser hermes \\\n  --kv-transfer-config &amp;#39;{&amp;quot;kv_connector&amp;quot;:&amp;quot;LMCacheConnectorV1&amp;quot;, &amp;quot;kv_role&amp;quot;:&amp;quot;kv_both&amp;quot;}&amp;#39; \\\n  --enable-expert-parallel\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Note that in my testing an NVLink bridge was necessary to get past about 110 tok/s. Something to do with the number of active parameters. NVLink only benefited Batch &amp;gt; 1 on Qwen3-32B&lt;/p&gt;\n\n&lt;p&gt;Edit: Fix the codeblock so it looks right.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mg1cg5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lu4j1/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754176671,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754176671,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6lgtat",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Filo0104",
                      "can_mod_post": false,
                      "created_utc": 1754171972,
                      "send_replies": true,
                      "parent_id": "t1_n6lc2dv",
                      "score": 2,
                      "author_fullname": "t2_1qq10p5wjo",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "How do you run qwen30b with 128k ctx? i have the same dual 3090s with vllm i Can only fit 107k ctx with ~60tk/s",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6lgtat",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do you run qwen30b with 128k ctx? i have the same dual 3090s with vllm i Can only fit 107k ctx with ~60tk/s&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg1cg5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lgtat/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754171972,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6n7b38",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "horeaper",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6mf12l",
                                "score": 3,
                                "author_fullname": "t2_jr0nz87",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That looks expensive üòê",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6n7b38",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That looks expensive üòê&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mg1cg5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6n7b38/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754196137,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754196137,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6mf12l",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DanManPanther",
                      "can_mod_post": false,
                      "created_utc": 1754184399,
                      "send_replies": true,
                      "parent_id": "t1_n6lc2dv",
                      "score": 1,
                      "author_fullname": "t2_2orcezep",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Radeon AI Pro R9700\n\nThat looks promising.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6mf12l",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Radeon AI Pro R9700&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That looks promising.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg1cg5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6mf12l/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754184399,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6lc2dv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Tyme4Trouble",
            "can_mod_post": false,
            "created_utc": 1754170365,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 6,
            "author_fullname": "t2_973amyap",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sorry to say, but for a 30-32B parameter model you‚Äôre going to need ~20GB of vRAM just to hold the weights at and 10-20 GB of addition vRAM for a large enough context window for it to be useful for coding. \n\nFor reference I‚Äôm able to run Qwen3-Coder-30B-A3B at Q8 at 150tok/s with a context window of 128k (FP16 KV) on dual 3090s. \n\nMight want to wait and see what the Radeon AI Pro R9700 ends up retailing for. It‚Äôll have 32GB of vRAM and proper FP16 and FP8 support. \n\nEven then, Qwen3-Coder-30B is great for its size, but cannot compete with Claude.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lc2dv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry to say, but for a 30-32B parameter model you‚Äôre going to need ~20GB of vRAM just to hold the weights at and 10-20 GB of addition vRAM for a large enough context window for it to be useful for coding. &lt;/p&gt;\n\n&lt;p&gt;For reference I‚Äôm able to run Qwen3-Coder-30B-A3B at Q8 at 150tok/s with a context window of 128k (FP16 KV) on dual 3090s. &lt;/p&gt;\n\n&lt;p&gt;Might want to wait and see what the Radeon AI Pro R9700 ends up retailing for. It‚Äôll have 32GB of vRAM and proper FP16 and FP8 support. &lt;/p&gt;\n\n&lt;p&gt;Even then, Qwen3-Coder-30B is great for its size, but cannot compete with Claude.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lc2dv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754170365,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6n3kr5",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AppearanceHeavy6724",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6mf3qd",
                                "score": 0,
                                "author_fullname": "t2_uz37qfx5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "My point stays same.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6n3kr5",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My point stays same.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mg1cg5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6n3kr5/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754194415,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754194415,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 0
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6mf3qd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DanManPanther",
                      "can_mod_post": false,
                      "created_utc": 1754184426,
                      "send_replies": true,
                      "parent_id": "t1_n6liyyt",
                      "score": 2,
                      "author_fullname": "t2_2orcezep",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I've found together.ai to be much cheaper.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6mf3qd",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve found together.ai to be much cheaper.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg1cg5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6mf3qd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754184426,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6liyyt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1754172724,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 3,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Unless you are not allergic to cloud due to nonexistent privacy use openrouter. I cannot tolerate privacy loss so I run local.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6liyyt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unless you are not allergic to cloud due to nonexistent privacy use openrouter. I cannot tolerate privacy loss so I run local.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6liyyt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754172724,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6naei0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Willing_Landscape_61",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6mezja",
                                "score": 1,
                                "author_fullname": "t2_8lvrytgw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "It seems that you could get 4x32GB for around $1k if you can hook all of them to your mobo.\nLower perf than with a 3090 if your model and context fit in 24GB but should become much faster soon after especially for dense models .",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6naei0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It seems that you could get 4x32GB for around $1k if you can hook all of them to your mobo.\nLower perf than with a 3090 if your model and context fit in 24GB but should become much faster soon after especially for dense models .&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mg1cg5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6naei0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754197646,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754197646,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6mezja",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DanManPanther",
                      "can_mod_post": false,
                      "created_utc": 1754184383,
                      "send_replies": true,
                      "parent_id": "t1_n6lgr17",
                      "score": 1,
                      "author_fullname": "t2_2orcezep",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Will look into this.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6mezja",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Will look into this.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg1cg5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6mezja/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754184383,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6lgr17",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1754171951,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Isn't the 32GB version of MI50 the best bang for the buck for inference on Linux?\nDepending on how many you can fit, it's hard to beat if you can go with the recommended Ubuntu version.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6lgr17",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t the 32GB version of MI50 the best bang for the buck for inference on Linux?\nDepending on how many you can fit, it&amp;#39;s hard to beat if you can go with the recommended Ubuntu version.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6lgr17/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754171951,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6n5jrl",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Current-Stop7806",
            "can_mod_post": false,
            "created_utc": 1754195323,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_8c7clfk1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Beware of used cards if you don't know they were used for mining ! - There are many old RTX 3090 used for mining.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6n5jrl",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Beware of used cards if you don&amp;#39;t know they were used for mining ! - There are many old RTX 3090 used for mining.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6n5jrl/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754195323,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6ng78d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Shouldhaveknown2015",
            "can_mod_post": false,
            "created_utc": 1754200667,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_rkkc3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "M1 Studio Max 64gb for 1,200ish? That what I did and it's still available. Wish I had the 128gb though with these ~100b models dropping.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ng78d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;M1 Studio Max 64gb for 1,200ish? That what I did and it&amp;#39;s still available. Wish I had the 128gb though with these ~100b models dropping.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6ng78d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754200667,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6nmyuw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1754204390,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Your answer for your budget is going to be some combination of multiple MI50 32GBs or a 3090, and something between an X99 DRR4 2400Mhz x4 to an EPYC 7002 3200Mhz x8 system. 256GB or 512 GB IMO. The Epyc 7002 does roughly 10 t/s TG on R1/K2 to start and dips towards 5 t/s near the back end of 128K ctx.\n\nIf you just want a normie system then a 3090 or 2 5070ti might keep you in the game of 32B Q6-ish. I guess ideally wait for 5080ti to drop if you refuse to deal with MI50 jank. But it's going to really under perform quality wise compared to the top local models and cloud obviously.\n\nI feel like your budget is constrained really, if you can't go over the $1000 but want the biggest bang then it's time for you to get the X99 board and as much MI50 32GB as you can afford, 6-8? (8 is ideal-er for TP if you run vLLM)\n\nI'd scrounge up some cash and try to swing the Epyc 7002. But you can hold off on that and migrate to one later with the MI50s in hand. The Epyc alone isn't really going to do much for you without like, a 16GB card at least for KV. So you need at least $1000 just for the Epyc and then a GPU too.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6nmyuw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Your answer for your budget is going to be some combination of multiple MI50 32GBs or a 3090, and something between an X99 DRR4 2400Mhz x4 to an EPYC 7002 3200Mhz x8 system. 256GB or 512 GB IMO. The Epyc 7002 does roughly 10 t/s TG on R1/K2 to start and dips towards 5 t/s near the back end of 128K ctx.&lt;/p&gt;\n\n&lt;p&gt;If you just want a normie system then a 3090 or 2 5070ti might keep you in the game of 32B Q6-ish. I guess ideally wait for 5080ti to drop if you refuse to deal with MI50 jank. But it&amp;#39;s going to really under perform quality wise compared to the top local models and cloud obviously.&lt;/p&gt;\n\n&lt;p&gt;I feel like your budget is constrained really, if you can&amp;#39;t go over the $1000 but want the biggest bang then it&amp;#39;s time for you to get the X99 board and as much MI50 32GB as you can afford, 6-8? (8 is ideal-er for TP if you run vLLM)&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;d scrounge up some cash and try to swing the Epyc 7002. But you can hold off on that and migrate to one later with the MI50s in hand. The Epyc alone isn&amp;#39;t really going to do much for you without like, a 16GB card at least for KV. So you need at least $1000 just for the Epyc and then a GPU too.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6nmyuw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754204390,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6noy84",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "forgotmyolduserinfo",
            "can_mod_post": false,
            "created_utc": 1754205510,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_y6gp2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "2x mi50 32gb from alibaba. It will be about 300 dollar total for 64gb vram",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6noy84",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;2x mi50 32gb from alibaba. It will be about 300 dollar total for 64gb vram&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6noy84/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754205510,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6n3gxk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AppearanceHeavy6724",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6meyjy",
                                "score": 1,
                                "author_fullname": "t2_uz37qfx5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yes,l on local analog of Craigslist.  not sure, but I recently bought mining gpu from 2017 and it works fine.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6n3gxk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes,l on local analog of Craigslist.  not sure, but I recently bought mining gpu from 2017 and it works fine.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mg1cg5",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6n3gxk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754194367,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754194367,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6meyjy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DanManPanther",
                      "can_mod_post": false,
                      "created_utc": 1754184373,
                      "send_replies": true,
                      "parent_id": "t1_n6limvo",
                      "score": 1,
                      "author_fullname": "t2_2orcezep",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I haven't seen it that low anywhere, is this at some local store? How reliable is a used GPU?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6meyjy",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I haven&amp;#39;t seen it that low anywhere, is this at some local store? How reliable is a used GPU?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mg1cg5",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6meyjy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754184373,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6limvo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1754172608,
            "send_replies": true,
            "parent_id": "t3_1mg1cg5",
            "score": 1,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "3090 where I live is 650 used.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6limvo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;3090 where I live is 650 used.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mg1cg5/guide_for_gpu_purchase_for_local_llm/n6limvo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754172608,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mg1cg5",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]