[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "If you read most of the technical release papers, they sample plenty.  5, 8, 10, 25, 100times!   Some of those scores we are seeing are after so many sampling.   Fair enough, I don't think an LLM should be judged by one sample, but definitely a few.   Yet it seems folks are not sampling plenty of times when doing one shot.   Why is that?     IMO, seems if you are not chatting, you should be sampling 3 or 5 times at least.   It certainly makes for a slow down, but isn't quality better?    Furthermore those of us local are often running quantized models, seems we will also need sampling more.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How many times do you sample, and why not more?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mei5ya",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": "#bbbdbf",
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "is_original_content": false,
            "author_fullname": "t2_ah13x",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754007886,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;If you read most of the technical release papers, they sample plenty.  5, 8, 10, 25, 100times!   Some of those scores we are seeing are after so many sampling.   Fair enough, I don&amp;#39;t think an LLM should be judged by one sample, but definitely a few.   Yet it seems folks are not sampling plenty of times when doing one shot.   Why is that?     IMO, seems if you are not chatting, you should be sampling 3 or 5 times at least.   It certainly makes for a slow down, but isn&amp;#39;t quality better?    Furthermore those of us local are often running quantized models, seems we will also need sampling more.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mei5ya",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "segmond",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": "light",
            "permalink": "/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/",
            "subreddit_subscribers": 507935,
            "created_utc": 1754007886,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n69pmbe",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "segmond",
                      "can_mod_post": false,
                      "created_utc": 1754009886,
                      "send_replies": true,
                      "parent_id": "t1_n69m6ef",
                      "score": 1,
                      "author_fullname": "t2_ah13x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm not talking about evaluation but while doing work.   If you are having to generate any data, based on how complicated your output seems multiple sampling should be the norm.  Doesn't make sense for chat, and an agentic framework will handle the repeated sampling upon tool failure, but",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n69pmbe",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m not talking about evaluation but while doing work.   If you are having to generate any data, based on how complicated your output seems multiple sampling should be the norm.  Doesn&amp;#39;t make sense for chat, and an agentic framework will handle the repeated sampling upon tool failure, but&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mei5ya",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/n69pmbe/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754009886,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n69m6ef",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754008677,
            "send_replies": true,
            "parent_id": "t3_1mei5ya",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My evaluation script iterates on each of its 44 test prompts five times.  That's a trade-off between a few factors:\n\n* Evaluation duration.  Evals don't take long when a model fits in VRAM, but larger models have to infer on pure CPU, which means evaluation can take days.\n\n* Outlier detection.  Five iterations has been barely enough to detect and quantify outlying behavior -- sufficient, so far, but I'd really like to up this to ten iterations.\n\n* My own time and energy.  My evaluation runs have to be assessed manually, and reading 220 replies with a critical eye is already quite a chore.\n\nRecently I've seen evidence that models might be including my eval's tests in their training, so I've been meaning to overhaul the entire harness anyway.  I'd like to switch it to a format which can be assessed automatically, but I'm still figuring out how to do that for all of the skills currently tested for.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n69m6ef",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My evaluation script iterates on each of its 44 test prompts five times.  That&amp;#39;s a trade-off between a few factors:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;p&gt;Evaluation duration.  Evals don&amp;#39;t take long when a model fits in VRAM, but larger models have to infer on pure CPU, which means evaluation can take days.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Outlier detection.  Five iterations has been barely enough to detect and quantify outlying behavior -- sufficient, so far, but I&amp;#39;d really like to up this to ten iterations.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;My own time and energy.  My evaluation runs have to be assessed manually, and reading 220 replies with a critical eye is already quite a chore.&lt;/p&gt;&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Recently I&amp;#39;ve seen evidence that models might be including my eval&amp;#39;s tests in their training, so I&amp;#39;ve been meaning to overhaul the entire harness anyway.  I&amp;#39;d like to switch it to a format which can be assessed automatically, but I&amp;#39;m still figuring out how to do that for all of the skills currently tested for.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/n69m6ef/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754008677,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mei5ya",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n69rhqk",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "mz_gt",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n69q2ea",
                                "score": 1,
                                "author_fullname": "t2_4b700mxj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Temperature of 0 will not generate different results if youâ€™re using the same method. It is greedy decoding. The reason why you donâ€™t want to always rely on temperature 0 is because models often perform better at slightly higher temperatures",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n69rhqk",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Temperature of 0 will not generate different results if youâ€™re using the same method. It is greedy decoding. The reason why you donâ€™t want to always rely on temperature 0 is because models often perform better at slightly higher temperatures&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mei5ya",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/n69rhqk/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754010558,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754010558,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n69q2ea",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "segmond",
                      "can_mod_post": false,
                      "created_utc": 1754010044,
                      "send_replies": true,
                      "parent_id": "t1_n69pgso",
                      "score": 1,
                      "author_fullname": "t2_ah13x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "yes, a temp of 0 is different from a seed as I understand it.  a temp of 0 will still produce different samples.  If you are running an experiment and want a reproducible output then a fixed seed (provided you are using the same hardware)  You can still get different outputs.   If you have it generating a story or code, then why not multiple samples?  I imagine that outside of the time it takes most people don't want to play human as judge.   We are inherently lazy and just want to get the result and move on with minimal work.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n69q2ea",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes, a temp of 0 is different from a seed as I understand it.  a temp of 0 will still produce different samples.  If you are running an experiment and want a reproducible output then a fixed seed (provided you are using the same hardware)  You can still get different outputs.   If you have it generating a story or code, then why not multiple samples?  I imagine that outside of the time it takes most people don&amp;#39;t want to play human as judge.   We are inherently lazy and just want to get the result and move on with minimal work.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mei5ya",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/n69q2ea/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754010044,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n69pgso",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "HistorianPotential48",
            "can_mod_post": false,
            "created_utc": 1754009831,
            "send_replies": true,
            "parent_id": "t3_1mei5ya",
            "score": 1,
            "author_fullname": "t2_4dzthia7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "if we use temp 0 and prompt never changes, are multiple samples still needed?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n69pgso",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;if we use temp 0 and prompt never changes, are multiple samples still needed?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/n69pgso/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754009831,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mei5ya",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n69qv4k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DinoAmino",
            "can_mod_post": false,
            "created_utc": 1754010332,
            "send_replies": true,
            "parent_id": "t3_1mei5ya",
            "score": 1,
            "author_fullname": "t2_j1v7f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think you're expecting too much from the recent crowd here ðŸ˜„ Too many judge reasoning models by how they respond to \"Hello\".\n\nTo your point, adding speculative decoding and quantized caching to your quantized model and then multiple samples becomes even more important for accuracy. I've been using OptiLLM and the 'mixture of agents' strategy with 5 samples and it works quite well - better than 'best of n' and just as 'fast'.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n69qv4k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think you&amp;#39;re expecting too much from the recent crowd here ðŸ˜„ Too many judge reasoning models by how they respond to &amp;quot;Hello&amp;quot;.&lt;/p&gt;\n\n&lt;p&gt;To your point, adding speculative decoding and quantized caching to your quantized model and then multiple samples becomes even more important for accuracy. I&amp;#39;ve been using OptiLLM and the &amp;#39;mixture of agents&amp;#39; strategy with 5 samples and it works quite well - better than &amp;#39;best of n&amp;#39; and just as &amp;#39;fast&amp;#39;.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mei5ya/how_many_times_do_you_sample_and_why_not_more/n69qv4k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754010332,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mei5ya",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]