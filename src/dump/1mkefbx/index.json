[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;format=pjpg&amp;auto=webp&amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616\n\n# Benchmarks\n\n     python3 benchmark_serving.py --backend openai --base-url \"http://127.0.0.1:11345\" --endpoint='/v1/completions' --model 'openai/gpt-oss-120b' --dataset-name random --num-prompts 20 --max-concurrency 3 --request-rate inf --random-input-len 2048 --random-output-len 4096\n\n# Results\n\n|Metric|Concurrency: 1|Concurrency: 3|Concurrency: 5|Concurrency: 8|\n|:-|:-|:-|:-|:-|\n|**Request Statistics**|||||\n|Successful requests|10|20|40|40|\n|Maximum request concurrency|1|3|5|8|\n|Benchmark duration (s)|83.21|89.46|160.30|126.58|\n|**Token Metrics**|||||\n|Total input tokens|20,325|40,805|81,603|81,603|\n|Total generated tokens|8,442|16,928|46,046|49,813|\n|**Throughput**|||||\n|Request throughput (req/s)|0.12|0.22|0.25|0.32|\n|Output token throughput (tok/s)|101.45|189.23|287.25|393.53|\n|Total token throughput (tok/s)|345.71|645.38|796.32|1,038.21|\n|**Time to First Token (TTFT)**|||||\n|Mean TTFT (ms)|787.62|51.83|59.78|881.60|\n|Median TTFT (ms)|614.22|51.08|58.83|655.81|\n|P99 TTFT (ms)|2,726.43|70.12|78.94|1,912.05|\n|**Time per Output Token (TPOT)**|||||\n|Mean TPOT (ms)|8.83|12.95|15.47|66.61|\n|Median TPOT (ms)|8.92|13.19|15.59|62.21|\n|P99 TPOT (ms)|9.33|13.59|17.61|191.42|\n|**Inter-token Latency (ITL)**|||||\n|Mean ITL (ms)|8.93|11.72|14.24|15.68|\n|Median ITL (ms)|8.80|12.29|14.58|12.92|\n|P99 ITL (ms)|11.42|13.73|16.26|16.50|\n\n# Dockerfile\n\nThis builds [https://github.com/zyongye/vllm/tree/rc1](https://github.com/zyongye/vllm/tree/rc1) .  \nWhich is behind this pull request [https://github.com/vllm-project/vllm/pull/22259](https://github.com/vllm-project/vllm/pull/22259)\n\n    FROM nvidia/cuda:12.8.1-devel-ubuntu24.04\n    \n    RUN apt update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt install -y python3.12 python3-pip git-core curl build-essential cmake &amp;&amp; apt clean &amp;&amp; rm -rf /var/lib/apt/lists/*\n    \n    RUN pip install uv --break-system-packages\n    \n    RUN uv venv --python 3.12 --seed --directory / --prompt workspace workspace-lib\n    RUN echo \"source /workspace-lib/bin/activate\" &gt;&gt; /root/.bash_profile\n    \n    SHELL [ \"/bin/bash\", \"--login\", \"-c\" ]\n    \n    ENV UV_CONCURRENT_BUILDS=8\n    ENV TORCH_CUDA_ARCH_LIST=\"8.6\"\n    ENV UV_LINK_MODE=copy\n    \n    RUN mkdir -p /app/libs\n    \n    # absolutely required\n    RUN git clone https://github.com/openai/triton.git /app/libs/triton\n    WORKDIR /app/libs/triton\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -r python/requirements.txt\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -e . --verbose --no-build-isolation\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -e python/triton_kernels --no-deps\n    \n    RUN git clone -b rc1 --depth 1 https://github.com/zyongye/vllm.git /app/libs/vllm\n    WORKDIR /app/libs/vllm\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install -r requirements/build.txt\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install flashinfer-python==0.2.10\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip uninstall pytorch-triton\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install triton==3.4.0 mcp openai_harmony \"transformers[torch]\"\n    #RUN --mount=type=cache,target=/root/.cache/uv uv pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128\n    # torch 2.8\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install torch torchvision\n    RUN python use_existing_torch.py\n    RUN --mount=type=cache,target=/root/.cache/uv uv pip install --no-build-isolation -e . -v\n    \n    COPY &lt;&lt;-\"EOF\" /app/entrypoint\n    #!/bin/bash\n    export VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1\n    export TORCH_CUDA_ARCH_LIST=8.6\n    source /workspace-lib/bin/activate\n    exec python3 -m vllm.entrypoints.openai.api_server --port 8080 \"$@\"\n    EOF\n    \n    RUN chmod +x /app/entrypoint\n    \n    EXPOSE 8080\n    \n    ENTRYPOINT [ \"/app/entrypoint\" ]\n\nbuild might take a while :\n\n    docker build -t vllmgpt . --progress plain\n\n# Running\n\nIf you have already downloaded the model from huggingface, you can mount it inside the container. If not, don't use the volume mount.\n\n    docker run -d --name vllmgpt -v $HOME/.cache/huggingface:/root/.cache/huggingface -p 8080:8080 --runtime nvidia --gpus all --ipc host vllmgpt --model openai/gpt-oss-120b --max-num-batched-tokens 4096 --gpu-memory-utilization 0.85 --max-num-seqs 8 --async-scheduling --max-model-len 32k --tensor-parallel-size 4\n\nThis will serve gpt-oss-120b on port 8080\n\nWith single concurrency, feeding 25K of tokens (quantum cryptography wiki article), results in vllm reporting :\n\nINFO 08-07 22:36:07 \\[loggers.py:123\\] Engine 000: **Avg prompt throughput: 2537.0 tokens/s**, Avg generation throughput: 81.7 tokens/s\n\nINFO 08-07 22:36:17 \\[loggers.py:123\\] Engine 000: Avg prompt throughput: 0.0 tokens/s, **Avg generation throughput: 94.4 tokens/s**",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "gpt-oss-120b running on 4x 3090 with vllm",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 83,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "dn7v2owggohf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/jpg",
                "p": [
                  {
                    "y": 64,
                    "x": 108,
                    "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2f082000683bcc7ed9185805441629c4c7acfa02"
                  },
                  {
                    "y": 128,
                    "x": 216,
                    "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=d11ee240fb0b3d4ff61a3d9401ee669e96232c67"
                  },
                  {
                    "y": 190,
                    "x": 320,
                    "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=fee94ab99ecb2762808e33bc8784fe8d82484cfb"
                  },
                  {
                    "y": 380,
                    "x": 640,
                    "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a548e47ddc6236fedf18ad4d372c3bdf5abf8c56"
                  },
                  {
                    "y": 570,
                    "x": 960,
                    "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=0bc764bc5b19bfbe5b4bd6d7a0d978ea1f1bb8ab"
                  },
                  {
                    "y": 642,
                    "x": 1080,
                    "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98eff423585211a24e6912183ae3a4213d33f582"
                  }
                ],
                "s": {
                  "y": 910,
                  "x": 1530,
                  "u": "https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;format=pjpg&amp;auto=webp&amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616"
                },
                "id": "dn7v2owggohf1"
              }
            },
            "name": "t3_1mkefbx",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.68,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_bjiw45ny",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://a.thumbs.redditmedia.com/8aAMzJM7DyPf26iC2tcnkUfItMzXTNsIQ1vYJ3WLqE8.jpg",
            "edited": 1754607347,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754606483,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616\"&gt;https://preview.redd.it/dn7v2owggohf1.jpg?width=1530&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=cdcafca4dfb311b3b8c8a2023a0061c605557616&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Benchmarks&lt;/h1&gt;\n\n&lt;pre&gt;&lt;code&gt; python3 benchmark_serving.py --backend openai --base-url &amp;quot;http://127.0.0.1:11345&amp;quot; --endpoint=&amp;#39;/v1/completions&amp;#39; --model &amp;#39;openai/gpt-oss-120b&amp;#39; --dataset-name random --num-prompts 20 --max-concurrency 3 --request-rate inf --random-input-len 2048 --random-output-len 4096\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Results&lt;/h1&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th align=\"left\"&gt;Metric&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 1&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 3&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 5&lt;/th&gt;\n&lt;th align=\"left\"&gt;Concurrency: 8&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Request Statistics&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Successful requests&lt;/td&gt;\n&lt;td align=\"left\"&gt;10&lt;/td&gt;\n&lt;td align=\"left\"&gt;20&lt;/td&gt;\n&lt;td align=\"left\"&gt;40&lt;/td&gt;\n&lt;td align=\"left\"&gt;40&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Maximum request concurrency&lt;/td&gt;\n&lt;td align=\"left\"&gt;1&lt;/td&gt;\n&lt;td align=\"left\"&gt;3&lt;/td&gt;\n&lt;td align=\"left\"&gt;5&lt;/td&gt;\n&lt;td align=\"left\"&gt;8&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Benchmark duration (s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;83.21&lt;/td&gt;\n&lt;td align=\"left\"&gt;89.46&lt;/td&gt;\n&lt;td align=\"left\"&gt;160.30&lt;/td&gt;\n&lt;td align=\"left\"&gt;126.58&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Token Metrics&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Total input tokens&lt;/td&gt;\n&lt;td align=\"left\"&gt;20,325&lt;/td&gt;\n&lt;td align=\"left\"&gt;40,805&lt;/td&gt;\n&lt;td align=\"left\"&gt;81,603&lt;/td&gt;\n&lt;td align=\"left\"&gt;81,603&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Total generated tokens&lt;/td&gt;\n&lt;td align=\"left\"&gt;8,442&lt;/td&gt;\n&lt;td align=\"left\"&gt;16,928&lt;/td&gt;\n&lt;td align=\"left\"&gt;46,046&lt;/td&gt;\n&lt;td align=\"left\"&gt;49,813&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Request throughput (req/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.12&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.22&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.25&lt;/td&gt;\n&lt;td align=\"left\"&gt;0.32&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Output token throughput (tok/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;101.45&lt;/td&gt;\n&lt;td align=\"left\"&gt;189.23&lt;/td&gt;\n&lt;td align=\"left\"&gt;287.25&lt;/td&gt;\n&lt;td align=\"left\"&gt;393.53&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Total token throughput (tok/s)&lt;/td&gt;\n&lt;td align=\"left\"&gt;345.71&lt;/td&gt;\n&lt;td align=\"left\"&gt;645.38&lt;/td&gt;\n&lt;td align=\"left\"&gt;796.32&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,038.21&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Time to First Token (TTFT)&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mean TTFT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;787.62&lt;/td&gt;\n&lt;td align=\"left\"&gt;51.83&lt;/td&gt;\n&lt;td align=\"left\"&gt;59.78&lt;/td&gt;\n&lt;td align=\"left\"&gt;881.60&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Median TTFT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;614.22&lt;/td&gt;\n&lt;td align=\"left\"&gt;51.08&lt;/td&gt;\n&lt;td align=\"left\"&gt;58.83&lt;/td&gt;\n&lt;td align=\"left\"&gt;655.81&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;P99 TTFT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;2,726.43&lt;/td&gt;\n&lt;td align=\"left\"&gt;70.12&lt;/td&gt;\n&lt;td align=\"left\"&gt;78.94&lt;/td&gt;\n&lt;td align=\"left\"&gt;1,912.05&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Time per Output Token (TPOT)&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mean TPOT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.83&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.95&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.47&lt;/td&gt;\n&lt;td align=\"left\"&gt;66.61&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Median TPOT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.92&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.19&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.59&lt;/td&gt;\n&lt;td align=\"left\"&gt;62.21&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;P99 TPOT (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;9.33&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.59&lt;/td&gt;\n&lt;td align=\"left\"&gt;17.61&lt;/td&gt;\n&lt;td align=\"left\"&gt;191.42&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;&lt;strong&gt;Inter-token Latency (ITL)&lt;/strong&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;td align=\"left\"&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Mean ITL (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.93&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.72&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.24&lt;/td&gt;\n&lt;td align=\"left\"&gt;15.68&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;Median ITL (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;8.80&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.29&lt;/td&gt;\n&lt;td align=\"left\"&gt;14.58&lt;/td&gt;\n&lt;td align=\"left\"&gt;12.92&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td align=\"left\"&gt;P99 ITL (ms)&lt;/td&gt;\n&lt;td align=\"left\"&gt;11.42&lt;/td&gt;\n&lt;td align=\"left\"&gt;13.73&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.26&lt;/td&gt;\n&lt;td align=\"left\"&gt;16.50&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;h1&gt;Dockerfile&lt;/h1&gt;\n\n&lt;p&gt;This builds &lt;a href=\"https://github.com/zyongye/vllm/tree/rc1\"&gt;https://github.com/zyongye/vllm/tree/rc1&lt;/a&gt; .&lt;br/&gt;\nWhich is behind this pull request &lt;a href=\"https://github.com/vllm-project/vllm/pull/22259\"&gt;https://github.com/vllm-project/vllm/pull/22259&lt;/a&gt;&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;FROM nvidia/cuda:12.8.1-devel-ubuntu24.04\n\nRUN apt update &amp;amp;&amp;amp; DEBIAN_FRONTEND=noninteractive apt install -y python3.12 python3-pip git-core curl build-essential cmake &amp;amp;&amp;amp; apt clean &amp;amp;&amp;amp; rm -rf /var/lib/apt/lists/*\n\nRUN pip install uv --break-system-packages\n\nRUN uv venv --python 3.12 --seed --directory / --prompt workspace workspace-lib\nRUN echo &amp;quot;source /workspace-lib/bin/activate&amp;quot; &amp;gt;&amp;gt; /root/.bash_profile\n\nSHELL [ &amp;quot;/bin/bash&amp;quot;, &amp;quot;--login&amp;quot;, &amp;quot;-c&amp;quot; ]\n\nENV UV_CONCURRENT_BUILDS=8\nENV TORCH_CUDA_ARCH_LIST=&amp;quot;8.6&amp;quot;\nENV UV_LINK_MODE=copy\n\nRUN mkdir -p /app/libs\n\n# absolutely required\nRUN git clone https://github.com/openai/triton.git /app/libs/triton\nWORKDIR /app/libs/triton\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -r python/requirements.txt\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -e . --verbose --no-build-isolation\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -e python/triton_kernels --no-deps\n\nRUN git clone -b rc1 --depth 1 https://github.com/zyongye/vllm.git /app/libs/vllm\nWORKDIR /app/libs/vllm\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install -r requirements/build.txt\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install flashinfer-python==0.2.10\nRUN --mount=type=cache,target=/root/.cache/uv uv pip uninstall pytorch-triton\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install triton==3.4.0 mcp openai_harmony &amp;quot;transformers[torch]&amp;quot;\n#RUN --mount=type=cache,target=/root/.cache/uv uv pip install --pre torch torchvision --index-url https://download.pytorch.org/whl/nightly/cu128\n# torch 2.8\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install torch torchvision\nRUN python use_existing_torch.py\nRUN --mount=type=cache,target=/root/.cache/uv uv pip install --no-build-isolation -e . -v\n\nCOPY &amp;lt;&amp;lt;-&amp;quot;EOF&amp;quot; /app/entrypoint\n#!/bin/bash\nexport VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1\nexport TORCH_CUDA_ARCH_LIST=8.6\nsource /workspace-lib/bin/activate\nexec python3 -m vllm.entrypoints.openai.api_server --port 8080 &amp;quot;$@&amp;quot;\nEOF\n\nRUN chmod +x /app/entrypoint\n\nEXPOSE 8080\n\nENTRYPOINT [ &amp;quot;/app/entrypoint&amp;quot; ]\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;build might take a while :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker build -t vllmgpt . --progress plain\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;h1&gt;Running&lt;/h1&gt;\n\n&lt;p&gt;If you have already downloaded the model from huggingface, you can mount it inside the container. If not, don&amp;#39;t use the volume mount.&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;docker run -d --name vllmgpt -v $HOME/.cache/huggingface:/root/.cache/huggingface -p 8080:8080 --runtime nvidia --gpus all --ipc host vllmgpt --model openai/gpt-oss-120b --max-num-batched-tokens 4096 --gpu-memory-utilization 0.85 --max-num-seqs 8 --async-scheduling --max-model-len 32k --tensor-parallel-size 4\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;This will serve gpt-oss-120b on port 8080&lt;/p&gt;\n\n&lt;p&gt;With single concurrency, feeding 25K of tokens (quantum cryptography wiki article), results in vllm reporting :&lt;/p&gt;\n\n&lt;p&gt;INFO 08-07 22:36:07 [loggers.py:123] Engine 000: &lt;strong&gt;Avg prompt throughput: 2537.0 tokens/s&lt;/strong&gt;, Avg generation throughput: 81.7 tokens/s&lt;/p&gt;\n\n&lt;p&gt;INFO 08-07 22:36:17 [loggers.py:123] Engine 000: Avg prompt throughput: 0.0 tokens/s, &lt;strong&gt;Avg generation throughput: 94.4 tokens/s&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mkefbx",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "rolotamazzi",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/",
            "subreddit_subscribers": 513813,
            "created_utc": 1754606483,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7i7qhk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "rolotamazzi",
                      "can_mod_post": false,
                      "created_utc": 1754607925,
                      "send_replies": true,
                      "parent_id": "t1_n7i6a6m",
                      "score": 6,
                      "author_fullname": "t2_bjiw45ny",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Main rig has 4 RTX 3090 FEs. It also runs openwebui. I connect from the laptop.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7i7qhk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Main rig has 4 RTX 3090 FEs. It also runs openwebui. I connect from the laptop.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkefbx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7i7qhk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754607925,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7i9qpy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DauntingPrawn",
                      "can_mod_post": false,
                      "created_utc": 1754608588,
                      "send_replies": true,
                      "parent_id": "t1_n7i6a6m",
                      "score": 1,
                      "author_fullname": "t2_68eylg8q",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "128MB Macbook Pro",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7i9qpy",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;128MB Macbook Pro&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkefbx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7i9qpy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754608588,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7id1ah",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ortegaalfredo",
                      "can_mod_post": false,
                      "created_utc": 1754609709,
                      "send_replies": true,
                      "parent_id": "t1_n7i6a6m",
                      "score": 0,
                      "author_fullname": "t2_g177e",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Sama was talking about a 128MB macbook but this is a much faster setup.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7id1ah",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "Alpaca"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sama was talking about a 128MB macbook but this is a much faster setup.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkefbx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7id1ah/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754609709,
                      "author_flair_text": "Alpaca",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bd9e9e",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7i6a6m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "BigRepresentative731",
            "can_mod_post": false,
            "created_utc": 1754607432,
            "send_replies": true,
            "parent_id": "t3_1mkefbx",
            "score": 6,
            "author_fullname": "t2_8a78x7h6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "So a single high end consumer laptop?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7i6a6m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So a single high end consumer laptop?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7i6a6m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754607432,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkefbx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "richtext",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n7jxvbn",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "kmouratidis",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n7jd4eg",
                                                              "score": 1,
                                                              "author_fullname": "t2_k6u7rfxb",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "\"max amounts of tokens in the KV cache\" is a better way to put it.\n\nYour command only shows 80K context per request, so at most 2-3 full-context requests, or 10-20 at ~10K context.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n7jxvbn",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;max amounts of tokens in the KV cache&amp;quot; is a better way to put it.&lt;/p&gt;\n\n&lt;p&gt;Your command only shows 80K context per request, so at most 2-3 full-context requests, or 10-20 at ~10K context.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mkefbx",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7jxvbn/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754632303,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754632303,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          },
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n7ktqme",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "bullerwins",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n7jd4eg",
                                                              "score": 1,
                                                              "author_fullname": "t2_d3wk5",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "can you run non 1-2-4-8 gpus on vllm?",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n7ktqme",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;can you run non 1-2-4-8 gpus on vllm?&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mkefbx",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7ktqme/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754649891,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754649891,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7jd4eg",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "ortegaalfredo",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7jbcwa",
                                                    "score": 1,
                                                    "author_fullname": "t2_g177e",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "VLLM tells you the max amount of tokens among all request when it starts. It's 180k for this config. Yes, you can run regular GLM AWQ using about 10x3090, kinda slow though, I get about 20 tok/s",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7jd4eg",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [
                                                      {
                                                        "e": "text",
                                                        "t": "Alpaca"
                                                      }
                                                    ],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;VLLM tells you the max amount of tokens among all request when it starts. It&amp;#39;s 180k for this config. Yes, you can run regular GLM AWQ using about 10x3090, kinda slow though, I get about 20 tok/s&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mkefbx",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": "light",
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7jd4eg/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754622845,
                                                    "author_flair_text": "Alpaca",
                                                    "collapsed": false,
                                                    "created_utc": 1754622845,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": "#bd9e9e",
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7jbcwa",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "cantgetthistowork",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7ilgsl",
                                          "score": 1,
                                          "author_fullname": "t2_j1i0o",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "How do you get the number 180k? Do you have any suggestion for 13x3090s and non air 4.5? Never tried vLLM before",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7jbcwa",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How do you get the number 180k? Do you have any suggestion for 13x3090s and non air 4.5? Never tried vLLM before&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mkefbx",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7jbcwa/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754622136,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754622136,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7ilgsl",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ortegaalfredo",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7ij99j",
                                "score": 3,
                                "author_fullname": "t2_g177e",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "This will get you 80k context on 4x3090, 180k context total, and &gt;90 tok/s single request. Notice I don't even quantize the kv cache, that would get you 160k context and 320k total.\n\n`VLLM_ATTENTION_BACKEND=FLASHINFER python -m vllm.entrypoints.openai.api_server --model cpatonn_GLM-4.5-Air-AWQ --dtype float16 --tensor-parallel-size 2 --pipeline-parallel-size 2 --gpu-memory-utilization 0.93 --swap-space 2 --max-model-len 80000 --max_num_seqs=20`",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7ilgsl",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Alpaca"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This will get you 80k context on 4x3090, 180k context total, and &amp;gt;90 tok/s single request. Notice I don&amp;#39;t even quantize the kv cache, that would get you 160k context and 320k total.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;VLLM_ATTENTION_BACKEND=FLASHINFER python -m vllm.entrypoints.openai.api_server --model cpatonn_GLM-4.5-Air-AWQ --dtype float16 --tensor-parallel-size 2 --pipeline-parallel-size 2 --gpu-memory-utilization 0.93 --swap-space 2 --max-model-len 80000 --max_num_seqs=20&lt;/code&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mkefbx",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7ilgsl/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754612669,
                                "author_flair_text": "Alpaca",
                                "treatment_tags": [],
                                "created_utc": 1754612669,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#bd9e9e",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7ij99j",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "rolotamazzi",
                      "can_mod_post": false,
                      "created_utc": 1754611884,
                      "send_replies": true,
                      "parent_id": "t1_n7icti2",
                      "score": 1,
                      "author_fullname": "t2_bjiw45ny",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "cpatonn/GLM-4.5-Air-AWQ  is fantastic for front end development. Its what I load by default.\n\nit gets 77tps generation for single concurrency using the same benchmark as in the original post. - so you were spot on with 30%\n\nI guess the non-obvious part of the post was that gpt-oss-120b can run 8 concurrent requests, each with 32K context.\n\nI can only manage a single concurrent request with glm air awq and 32K non-quantized context.\n\nWould love to see better option than these to get more context :\n\n    --dtype float16 --tensor-parallel-size 2 --pipeline-parallel-size 2\n\nSo...\n\n if you are into batching - gpt-oss can get about 5x more throughput on the same hardware, based on the benchmarks at least.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7ij99j",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;cpatonn/GLM-4.5-Air-AWQ  is fantastic for front end development. Its what I load by default.&lt;/p&gt;\n\n&lt;p&gt;it gets 77tps generation for single concurrency using the same benchmark as in the original post. - so you were spot on with 30%&lt;/p&gt;\n\n&lt;p&gt;I guess the non-obvious part of the post was that gpt-oss-120b can run 8 concurrent requests, each with 32K context.&lt;/p&gt;\n\n&lt;p&gt;I can only manage a single concurrent request with glm air awq and 32K non-quantized context.&lt;/p&gt;\n\n&lt;p&gt;Would love to see better option than these to get more context :&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;--dtype float16 --tensor-parallel-size 2 --pipeline-parallel-size 2\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;So...&lt;/p&gt;\n\n&lt;p&gt;if you are into batching - gpt-oss can get about 5x more throughput on the same hardware, based on the benchmarks at least.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkefbx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7ij99j/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754611884,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7icti2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1754609635,
            "send_replies": true,
            "parent_id": "t3_1mkefbx",
            "score": 4,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "So about 30% more than GLM-4.5-air-AWQ. I find both models have different strenghts. Air is much better at coding, while GPT-oss is better at general questions, conversation and easier to uncensor.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7icti2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So about 30% more than GLM-4.5-air-AWQ. I find both models have different strenghts. Air is much better at coding, while GPT-oss is better at general questions, conversation and easier to uncensor.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7icti2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754609635,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1mkefbx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7idton",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "secopsml",
            "can_mod_post": false,
            "created_utc": 1754609980,
            "send_replies": true,
            "parent_id": "t3_1mkefbx",
            "score": 2,
            "author_fullname": "t2_pmniwf57y",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "amazing! thanks for this post op!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7idton",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;amazing! thanks for this post op!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7idton/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754609980,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkefbx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": {
                                                                "kind": "Listing",
                                                                "data": {
                                                                  "after": null,
                                                                  "dist": null,
                                                                  "modhash": "",
                                                                  "geo_filter": "",
                                                                  "children": [
                                                                    {
                                                                      "kind": "t1",
                                                                      "data": {
                                                                        "subreddit_id": "t5_81eyvm",
                                                                        "approved_at_utc": null,
                                                                        "author_is_blocked": false,
                                                                        "comment_type": null,
                                                                        "awarders": [],
                                                                        "mod_reason_by": null,
                                                                        "banned_by": null,
                                                                        "author_flair_type": "text",
                                                                        "total_awards_received": 0,
                                                                        "subreddit": "LocalLLaMA",
                                                                        "author_flair_template_id": null,
                                                                        "distinguished": null,
                                                                        "likes": null,
                                                                        "replies": "",
                                                                        "user_reports": [],
                                                                        "saved": false,
                                                                        "id": "n7k18sp",
                                                                        "banned_at_utc": null,
                                                                        "mod_reason_title": null,
                                                                        "gilded": 0,
                                                                        "archived": false,
                                                                        "collapsed_reason_code": null,
                                                                        "no_follow": true,
                                                                        "author": "Conscious-content42",
                                                                        "can_mod_post": false,
                                                                        "send_replies": true,
                                                                        "parent_id": "t1_n7k0cp6",
                                                                        "score": 2,
                                                                        "author_fullname": "t2_174nh08zz4",
                                                                        "approved_by": null,
                                                                        "mod_note": null,
                                                                        "all_awardings": [],
                                                                        "collapsed": false,
                                                                        "body": "You can get away with a lower wattage PSU, but would require power limiting the 3090s to 250-275 watts per card using something like 'nvidia-smi -pl 250', in your command line",
                                                                        "edited": false,
                                                                        "gildings": {},
                                                                        "author_flair_css_class": null,
                                                                        "name": "t1_n7k18sp",
                                                                        "is_submitter": false,
                                                                        "downs": 0,
                                                                        "author_flair_richtext": [],
                                                                        "author_patreon_flair": false,
                                                                        "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can get away with a lower wattage PSU, but would require power limiting the 3090s to 250-275 watts per card using something like &amp;#39;nvidia-smi -pl 250&amp;#39;, in your command line&lt;/p&gt;\n&lt;/div&gt;",
                                                                        "removal_reason": null,
                                                                        "collapsed_reason": null,
                                                                        "link_id": "t3_1mkefbx",
                                                                        "associated_award": null,
                                                                        "stickied": false,
                                                                        "author_premium": false,
                                                                        "can_gild": false,
                                                                        "top_awarded_type": null,
                                                                        "unrepliable_reason": null,
                                                                        "author_flair_text_color": null,
                                                                        "score_hidden": false,
                                                                        "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7k18sp/",
                                                                        "subreddit_type": "public",
                                                                        "locked": false,
                                                                        "report_reasons": null,
                                                                        "created": 1754634080,
                                                                        "author_flair_text": null,
                                                                        "treatment_tags": [],
                                                                        "created_utc": 1754634080,
                                                                        "subreddit_name_prefixed": "r/LocalLLaMA",
                                                                        "controversiality": 0,
                                                                        "depth": 6,
                                                                        "author_flair_background_color": null,
                                                                        "collapsed_because_crowd_control": null,
                                                                        "mod_reports": [],
                                                                        "num_reports": null,
                                                                        "ups": 2
                                                                      }
                                                                    }
                                                                  ],
                                                                  "before": null
                                                                }
                                                              },
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n7k0cp6",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "Conscious-content42",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n7jzybt",
                                                              "score": 1,
                                                              "author_fullname": "t2_174nh08zz4",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "Yup should be fine. Want to make sure you have enough room for the cards, or use risers, and of course a power supply to boot, probably want something like 1300-1600 watts.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n7k0cp6",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup should be fine. Want to make sure you have enough room for the cards, or use risers, and of course a power supply to boot, probably want something like 1300-1600 watts.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1mkefbx",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7k0cp6/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1754633603,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1754633603,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n7jzybt",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "Glittering-Call8746",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n7jv4f6",
                                                    "score": 1,
                                                    "author_fullname": "t2_tqwl6sawb",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "X8 x4 x4 on intel mobo is enough ?",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n7jzybt",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;X8 x4 x4 on intel mobo is enough ?&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mkefbx",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7jzybt/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754633395,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754633395,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7jv4f6",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Conscious-content42",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7jg8ve",
                                          "score": 1,
                                          "author_fullname": "t2_174nh08zz4",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Depends on the quantization, at Q4 you probably want at least 3 to have the weights and prompt processing on the GPUs. If you quantize further like Q2 (2-2.5 bits per weight or so) then you could run it on two 3090s.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7jv4f6",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Depends on the quantization, at Q4 you probably want at least 3 to have the weights and prompt processing on the GPUs. If you quantize further like Q2 (2-2.5 bits per weight or so) then you could run it on two 3090s.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mkefbx",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7jv4f6/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754630876,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754630876,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7jg8ve",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Glittering-Call8746",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7j71xj",
                                "score": 1,
                                "author_fullname": "t2_tqwl6sawb",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "How many 3090 do i need to run 120b ? 4 ?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7jg8ve",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;How many 3090 do i need to run 120b ? 4 ?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mkefbx",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7jg8ve/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754624126,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754624126,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7j71xj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "rolotamazzi",
                      "can_mod_post": false,
                      "created_utc": 1754620470,
                      "send_replies": true,
                      "parent_id": "t1_n7j3sck",
                      "score": 1,
                      "author_fullname": "t2_bjiw45ny",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "New wheels were released a few hours ago with ampere support built in. Negates the need to compile it yourself.\n\n\nDocs were updatedhttps://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#a100",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7j71xj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;New wheels were released a few hours ago with ampere support built in. Negates the need to compile it yourself.&lt;/p&gt;\n\n&lt;p&gt;Docs were updated&lt;a href=\"https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#a100\"&gt;https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html#a100&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mkefbx",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7j71xj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754620470,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7j3sck",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "spac3muffin",
            "can_mod_post": false,
            "created_utc": 1754619248,
            "send_replies": true,
            "parent_id": "t3_1mkefbx",
            "score": 1,
            "author_fullname": "t2_igd7j6y",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thanks this is really useful.  I got to run gpt-oss 20b on a dual 3090.  Not that you needed 2 3090 to run the 20b model, but I wanted to make a vllm that runs Amphere older chips.  I just need to do this on prod for an A100 as current vllm image has an open issue.  [https://github.com/vllm-project/vllm/issues/22331](https://github.com/vllm-project/vllm/issues/22331)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7j3sck",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks this is really useful.  I got to run gpt-oss 20b on a dual 3090.  Not that you needed 2 3090 to run the 20b model, but I wanted to make a vllm that runs Amphere older chips.  I just need to do this on prod for an A100 as current vllm image has an open issue.  &lt;a href=\"https://github.com/vllm-project/vllm/issues/22331\"&gt;https://github.com/vllm-project/vllm/issues/22331&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/n7j3sck/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754619248,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkefbx",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]