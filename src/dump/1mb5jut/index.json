[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Yesterday I bought a 3090 and it works great with vllm (despite some issues in some models, but that is probably my fault). Is there a way that I could use my rtx 2060 (6gb vram) for context (I can only use 8k context in qwen2.5-coder:32b awq using the 3090)? If not for context then maybe to increase the tokens/second. But from what I have seen it could also decrease the tokens/second because its less powerful.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Rtx 3090 + Rtx 2060 for Context Increase and Performance",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mb5jut",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_ti9s05lw",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753672812,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Yesterday I bought a 3090 and it works great with vllm (despite some issues in some models, but that is probably my fault). Is there a way that I could use my rtx 2060 (6gb vram) for context (I can only use 8k context in qwen2.5-coder:32b awq using the 3090)? If not for context then maybe to increase the tokens/second. But from what I have seen it could also decrease the tokens/second because its less powerful.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mb5jut",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "FredericoDev",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mb5jut/rtx_3090_rtx_2060_for_context_increase_and/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mb5jut/rtx_3090_rtx_2060_for_context_increase_and/",
            "subreddit_subscribers": 505879,
            "created_utc": 1753672812,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5jqscf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BusRevolutionary9893",
            "can_mod_post": false,
            "created_utc": 1753673144,
            "send_replies": true,
            "parent_id": "t3_1mb5jut",
            "score": 2,
            "author_fullname": "t2_1by73qs5e5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've got a 3090 and a 2080 ti. Pretty sure the slowest card dictates speed. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5jqscf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve got a 3090 and a 2080 ti. Pretty sure the slowest card dictates speed. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb5jut/rtx_3090_rtx_2060_for_context_increase_and/n5jqscf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753673144,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb5jut",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5jrsmj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zipperlein",
            "can_mod_post": false,
            "created_utc": 1753673575,
            "send_replies": true,
            "parent_id": "t3_1mb5jut",
            "score": 1,
            "author_fullname": "t2_x3duw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Not as far as i know. But you can also run ggufs of dense Qwen3 models on vllm, which would free space for context. Take a look at the file sizes of the Unsloth repo for Qwen3 32B at Huggingface.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5jrsmj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not as far as i know. But you can also run ggufs of dense Qwen3 models on vllm, which would free space for context. Take a look at the file sizes of the Unsloth repo for Qwen3 32B at Huggingface.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb5jut/rtx_3090_rtx_2060_for_context_increase_and/n5jrsmj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753673575,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb5jut",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5jwdsi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lly0571",
            "can_mod_post": false,
            "created_utc": 1753675589,
            "send_replies": true,
            "parent_id": "t3_1mb5jut",
            "score": 1,
            "author_fullname": "t2_70vzcleel",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You should not use vllm tensor parallel, as tp would distribute the model evenly on both GPUs. And using PP on two two GPUs with different architecture is pretty slow. And you need to set VLLM\\_ATTENTION\\_BACKEND=XFORMERS and --quantization awq to match the turing GPU. Here is a RTX 3080(modded) + T10(you can regard it as a 2070Super 16GB) serving Qwen3-32B-AWQ, pretty slow: \n\nhttps://preview.redd.it/e0qbnq07hjff1.png?width=1897&amp;format=png&amp;auto=webp&amp;s=62223aa3f9cc3dceb1e70956654e65a32da8a2d0\n\nThere are `VLLM_PP_LAYER_PARTITION` env which should performs as `tensor_split` in llama.cpp, but I didn't make it working.\n\nIf you are using llama.cpp or llamacpp based app(ollama, lmstudio, etc), you can split layers on both GPUs, but would be slower than using only one 3090.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5jwdsi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You should not use vllm tensor parallel, as tp would distribute the model evenly on both GPUs. And using PP on two two GPUs with different architecture is pretty slow. And you need to set VLLM_ATTENTION_BACKEND=XFORMERS and --quantization awq to match the turing GPU. Here is a RTX 3080(modded) + T10(you can regard it as a 2070Super 16GB) serving Qwen3-32B-AWQ, pretty slow: &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/e0qbnq07hjff1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62223aa3f9cc3dceb1e70956654e65a32da8a2d0\"&gt;https://preview.redd.it/e0qbnq07hjff1.png?width=1897&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62223aa3f9cc3dceb1e70956654e65a32da8a2d0&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;There are &lt;code&gt;VLLM_PP_LAYER_PARTITION&lt;/code&gt; env which should performs as &lt;code&gt;tensor_split&lt;/code&gt; in llama.cpp, but I didn&amp;#39;t make it working.&lt;/p&gt;\n\n&lt;p&gt;If you are using llama.cpp or llamacpp based app(ollama, lmstudio, etc), you can split layers on both GPUs, but would be slower than using only one 3090.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mb5jut/rtx_3090_rtx_2060_for_context_increase_and/n5jwdsi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753675589,
            "media_metadata": {
              "e0qbnq07hjff1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 116,
                    "x": 108,
                    "u": "https://preview.redd.it/e0qbnq07hjff1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7530841922e6c3ece0cda44541be2d4aba3abe90"
                  },
                  {
                    "y": 233,
                    "x": 216,
                    "u": "https://preview.redd.it/e0qbnq07hjff1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8159b5e529920ee7915a0e4836c8794cf61bdd61"
                  },
                  {
                    "y": 346,
                    "x": 320,
                    "u": "https://preview.redd.it/e0qbnq07hjff1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fd9315d6b5edbbf4d80df579cf36bb5beaf07497"
                  },
                  {
                    "y": 692,
                    "x": 640,
                    "u": "https://preview.redd.it/e0qbnq07hjff1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2fbf4dcbba21dadebb026097351364f4a709d88f"
                  },
                  {
                    "y": 1038,
                    "x": 960,
                    "u": "https://preview.redd.it/e0qbnq07hjff1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ee6b7a341956bcfeaa4b9b74dd43c70b4a96971"
                  },
                  {
                    "y": 1168,
                    "x": 1080,
                    "u": "https://preview.redd.it/e0qbnq07hjff1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=49b3e31942476b541774239aaaab90d8ce33681c"
                  }
                ],
                "s": {
                  "y": 2053,
                  "x": 1897,
                  "u": "https://preview.redd.it/e0qbnq07hjff1.png?width=1897&amp;format=png&amp;auto=webp&amp;s=62223aa3f9cc3dceb1e70956654e65a32da8a2d0"
                },
                "id": "e0qbnq07hjff1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mb5jut",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]