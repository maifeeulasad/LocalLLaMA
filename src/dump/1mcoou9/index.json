[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi everyone! üëã\n\nI'm exploring a novel concept in unsupervised neural machine translation and would love to get your feedback. I‚Äôm curious if this approach has been tested before‚Äîor if someone might be interested in giving it a try.\n\n**My idea in a nutshell:**\n\n- I train two simple decoder‚Äëonly models (transformers) **at the character level**, one on English, another on Ukrainian. No encoder, no shared latent space.\n- These two decoders are completely separate and independently trained as **language models**‚Äîeach fluent in its own language.\n\nNow here‚Äôs the twist:\n\n- When we want to translate an English sentence, we feed it **as characters** into the English decoder.\n- We then extract its **inner hidden states** (or attention activations).\n- Those hidden states are passed directly into the Ukrainian decoder (as if they were input).\n- The Ukrainian decoder tries to generate an equivalent Ukrainian sentence.\n\n**No extra layers, no mapper‚Äîjust latent states transferred from one decoder to the other.**\n\n---\n\n### Why I think it *could* work:\n\n1. **Natural language is built on statistical patterns.**  \n   At the character level, both languages contain frequent patterns‚Äîletter combinations, suffixes, morphology‚Äîthat can be learned without semantic knowledge.\n\n2. **English and Ukrainian share some structural similarities** (SVO order, some grammatical forms). A decoder-only model trained character-wise can capture this statistical structure.\n\n3. Even if the language models don‚Äôt ‚Äúunderstand‚Äù each other initially, they can potentially learn to interpret these latent signals through **cross‚Äêlanguage supervision**.\n\n---\n\n### Proposed training strategy:\n\n1. Pre-train `D_en` on English text and `D_uk` on Ukrainian text (character-level modeling).\n2. During translation training:\n   - Use an English sentence `sEn`.\n   - Feed it into `D_en`, capture hidden state matrix `H_en`.\n   - Input `H_en` (frame‚Äëaligned) into `D_uk`, let it generate `sUk_pred`.\n   - Compute loss by comparing `sUk_pred` with the *true* Ukrainian translation `sUk`.\n3. Optionally add a cycle: sEn ‚Üí D_en ‚Üí H_en ‚Üí D_uk ‚Üí sUk_pred sUk_pred ‚Üí D_uk ‚Üí H_uk ‚Üí D_en ‚Üí sEn_restored\n\nand enforce reconstruction (cycle‚Äëconsistency loss).\n\n---\n\n### Challenges I‚Äôm concerned about:\n\n- Feeding hidden states from one decoder into another‚Äîhow should they align?\n- Do hidden states carry enough semantic structure for the second decoder to make sense of them?\n- Would the English decoder still generate fluent English after learning to accept Ukrainian input?\n- Could training converge‚Äîor would this mutual mapping collapse?\n\n---\n\n### My constraints:\n\n- I don‚Äôt have access to GPUs or major compute resources üòÖ\n- I‚Äôd mainly like to get feedback, references, or see if anyone has tried something similar‚Äîor might be able to prototype this.\n\n---\n\n### Would love to hear:\n\n- If anyone has experimented with **decoder‚Äëonly cross‚Äëcommunication**, especially at the hidden‚Äêstate level.\n- Ideas for alignment strategies between decoder hidden states.\n- Training tips: masking, attention mapping, loss design, etc.\n- Any known literature or codebases exploring similar minimal translation approaches.\n\nThanks for your time!  \n‚Äî **Buka Koshmarovich**",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Could two decoder‚Äëonly models communicate directly via latent outputs and translate each other?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mcoou9",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rnaz4818s",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753825851,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi everyone! üëã&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m exploring a novel concept in unsupervised neural machine translation and would love to get your feedback. I‚Äôm curious if this approach has been tested before‚Äîor if someone might be interested in giving it a try.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;My idea in a nutshell:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I train two simple decoder‚Äëonly models (transformers) &lt;strong&gt;at the character level&lt;/strong&gt;, one on English, another on Ukrainian. No encoder, no shared latent space.&lt;/li&gt;\n&lt;li&gt;These two decoders are completely separate and independently trained as &lt;strong&gt;language models&lt;/strong&gt;‚Äîeach fluent in its own language.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Now here‚Äôs the twist:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;When we want to translate an English sentence, we feed it &lt;strong&gt;as characters&lt;/strong&gt; into the English decoder.&lt;/li&gt;\n&lt;li&gt;We then extract its &lt;strong&gt;inner hidden states&lt;/strong&gt; (or attention activations).&lt;/li&gt;\n&lt;li&gt;Those hidden states are passed directly into the Ukrainian decoder (as if they were input).&lt;/li&gt;\n&lt;li&gt;The Ukrainian decoder tries to generate an equivalent Ukrainian sentence.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;&lt;strong&gt;No extra layers, no mapper‚Äîjust latent states transferred from one decoder to the other.&lt;/strong&gt;&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Why I think it &lt;em&gt;could&lt;/em&gt; work:&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Natural language is built on statistical patterns.&lt;/strong&gt;&lt;br/&gt;\nAt the character level, both languages contain frequent patterns‚Äîletter combinations, suffixes, morphology‚Äîthat can be learned without semantic knowledge.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;English and Ukrainian share some structural similarities&lt;/strong&gt; (SVO order, some grammatical forms). A decoder-only model trained character-wise can capture this statistical structure.&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;Even if the language models don‚Äôt ‚Äúunderstand‚Äù each other initially, they can potentially learn to interpret these latent signals through &lt;strong&gt;cross‚Äêlanguage supervision&lt;/strong&gt;.&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Proposed training strategy:&lt;/h3&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Pre-train &lt;code&gt;D_en&lt;/code&gt; on English text and &lt;code&gt;D_uk&lt;/code&gt; on Ukrainian text (character-level modeling).&lt;/li&gt;\n&lt;li&gt;During translation training:\n\n&lt;ul&gt;\n&lt;li&gt;Use an English sentence &lt;code&gt;sEn&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Feed it into &lt;code&gt;D_en&lt;/code&gt;, capture hidden state matrix &lt;code&gt;H_en&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Input &lt;code&gt;H_en&lt;/code&gt; (frame‚Äëaligned) into &lt;code&gt;D_uk&lt;/code&gt;, let it generate &lt;code&gt;sUk_pred&lt;/code&gt;.&lt;/li&gt;\n&lt;li&gt;Compute loss by comparing &lt;code&gt;sUk_pred&lt;/code&gt; with the &lt;em&gt;true&lt;/em&gt; Ukrainian translation &lt;code&gt;sUk&lt;/code&gt;.&lt;/li&gt;\n&lt;/ul&gt;&lt;/li&gt;\n&lt;li&gt;Optionally add a cycle: sEn ‚Üí D_en ‚Üí H_en ‚Üí D_uk ‚Üí sUk_pred sUk_pred ‚Üí D_uk ‚Üí H_uk ‚Üí D_en ‚Üí sEn_restored&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;and enforce reconstruction (cycle‚Äëconsistency loss).&lt;/p&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Challenges I‚Äôm concerned about:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Feeding hidden states from one decoder into another‚Äîhow should they align?&lt;/li&gt;\n&lt;li&gt;Do hidden states carry enough semantic structure for the second decoder to make sense of them?&lt;/li&gt;\n&lt;li&gt;Would the English decoder still generate fluent English after learning to accept Ukrainian input?&lt;/li&gt;\n&lt;li&gt;Could training converge‚Äîor would this mutual mapping collapse?&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;My constraints:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;I don‚Äôt have access to GPUs or major compute resources üòÖ&lt;/li&gt;\n&lt;li&gt;I‚Äôd mainly like to get feedback, references, or see if anyone has tried something similar‚Äîor might be able to prototype this.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;hr/&gt;\n\n&lt;h3&gt;Would love to hear:&lt;/h3&gt;\n\n&lt;ul&gt;\n&lt;li&gt;If anyone has experimented with &lt;strong&gt;decoder‚Äëonly cross‚Äëcommunication&lt;/strong&gt;, especially at the hidden‚Äêstate level.&lt;/li&gt;\n&lt;li&gt;Ideas for alignment strategies between decoder hidden states.&lt;/li&gt;\n&lt;li&gt;Training tips: masking, attention mapping, loss design, etc.&lt;/li&gt;\n&lt;li&gt;Any known literature or codebases exploring similar minimal translation approaches.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Thanks for your time!&lt;br/&gt;\n‚Äî &lt;strong&gt;Buka Koshmarovich&lt;/strong&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mcoou9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "According_Change2007",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mcoou9/could_two_decoderonly_models_communicate_directly/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcoou9/could_two_decoderonly_models_communicate_directly/",
            "subreddit_subscribers": 506973,
            "created_utc": 1753825851,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5vi5qz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753827089,
            "send_replies": true,
            "parent_id": "t3_1mcoou9",
            "score": 9,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;No encoder\n\nNow, you say that, but there is one, you just aren't thinking about the component in the models that will function like one.\n\nSo, suppose you have w1, w2, and w3. All of these are linear weights, possibly with a non linearity between them.\n\nNow, you could imagine training these end to end. So, `Y = ReLU((w3)ReLU((w2)ReLU((w1)))` or something to that effect.\n\nBut you would also do a dual objective term. Like, you could train `y1 = ReLU(w1)` and also `y3 = ReLU(w3)` so that this objective is orthogonal.\n\nYou can, in fact, compose these objectives together, to get a model which can model both of those objectives. You could also start with this extra objective and then transition to the main one later. This isn't like, 100% the same as what you're doing, but conceptually it's similar.\n\nNow, the interesting thing about this, is that in this really simple example, `w2` isn't explicitly an encoder, per se. It's not some sort of module specialized in translating from one latent space to another. It's just a transform. But with that said, due to the needs of this objective, it actually does learn to translate the blend of y1 = ReLU(w1) and the full objective, to get to what w3 needs as input.\n\nNow, in your proposed architecture, you do have an encoder in the same way... Transformer blocks are universal function approximators, and the later layers of the source decoder and the early layers of the target decoder will learn to translate into one another due to your objective.\n\nIn other words, you're taking some of your model's language modelling capacity, and overwriting it to translate between latent spaces.\n\nNow, in principle, there's nothing wrong with this, and it'll probably work. You could train a Tinystories size model (two \\~100m parameter LLMs) and try this and I think it would work. This could train on CPU in probably a day, or on Google Colab.\n\nBut this is...Kind of a crude strategy, I guess?\n\nPersonally I'd prefer to do something like projecting into a shared sentence VAE or projecting into the other model's softprompt via a linear transform, or perhaps using a shared cross attention strategy like CaLM (though that gets quite close to encoder-decoder arches). The latter has the advantage of letting you train a small dedicated Ukrainian LLM (possibly as small as 100m or 200m might work) and you could co-optimize it later with a larger target LLM for a viable complete system.\n\nI'm not really sure how I'd do this for an explicit translation model, personally, because decoder only LLMs are already quite good at this, but obviously, people don't necessarily choose where their curiosity is directed, I suppose.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5vi5qz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;No encoder&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Now, you say that, but there is one, you just aren&amp;#39;t thinking about the component in the models that will function like one.&lt;/p&gt;\n\n&lt;p&gt;So, suppose you have w1, w2, and w3. All of these are linear weights, possibly with a non linearity between them.&lt;/p&gt;\n\n&lt;p&gt;Now, you could imagine training these end to end. So, &lt;code&gt;Y = ReLU((w3)ReLU((w2)ReLU((w1)))&lt;/code&gt; or something to that effect.&lt;/p&gt;\n\n&lt;p&gt;But you would also do a dual objective term. Like, you could train &lt;code&gt;y1 = ReLU(w1)&lt;/code&gt; and also &lt;code&gt;y3 = ReLU(w3)&lt;/code&gt; so that this objective is orthogonal.&lt;/p&gt;\n\n&lt;p&gt;You can, in fact, compose these objectives together, to get a model which can model both of those objectives. You could also start with this extra objective and then transition to the main one later. This isn&amp;#39;t like, 100% the same as what you&amp;#39;re doing, but conceptually it&amp;#39;s similar.&lt;/p&gt;\n\n&lt;p&gt;Now, the interesting thing about this, is that in this really simple example, &lt;code&gt;w2&lt;/code&gt; isn&amp;#39;t explicitly an encoder, per se. It&amp;#39;s not some sort of module specialized in translating from one latent space to another. It&amp;#39;s just a transform. But with that said, due to the needs of this objective, it actually does learn to translate the blend of y1 = ReLU(w1) and the full objective, to get to what w3 needs as input.&lt;/p&gt;\n\n&lt;p&gt;Now, in your proposed architecture, you do have an encoder in the same way... Transformer blocks are universal function approximators, and the later layers of the source decoder and the early layers of the target decoder will learn to translate into one another due to your objective.&lt;/p&gt;\n\n&lt;p&gt;In other words, you&amp;#39;re taking some of your model&amp;#39;s language modelling capacity, and overwriting it to translate between latent spaces.&lt;/p&gt;\n\n&lt;p&gt;Now, in principle, there&amp;#39;s nothing wrong with this, and it&amp;#39;ll probably work. You could train a Tinystories size model (two ~100m parameter LLMs) and try this and I think it would work. This could train on CPU in probably a day, or on Google Colab.&lt;/p&gt;\n\n&lt;p&gt;But this is...Kind of a crude strategy, I guess?&lt;/p&gt;\n\n&lt;p&gt;Personally I&amp;#39;d prefer to do something like projecting into a shared sentence VAE or projecting into the other model&amp;#39;s softprompt via a linear transform, or perhaps using a shared cross attention strategy like CaLM (though that gets quite close to encoder-decoder arches). The latter has the advantage of letting you train a small dedicated Ukrainian LLM (possibly as small as 100m or 200m might work) and you could co-optimize it later with a larger target LLM for a viable complete system.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not really sure how I&amp;#39;d do this for an explicit translation model, personally, because decoder only LLMs are already quite good at this, but obviously, people don&amp;#39;t necessarily choose where their curiosity is directed, I suppose.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcoou9/could_two_decoderonly_models_communicate_directly/n5vi5qz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753827089,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcoou9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        }
      ],
      "before": null
    }
  }
]