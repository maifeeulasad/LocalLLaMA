[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I want to fine-tune a LlaVa model to include new details about an image. Think about medical, I want the model to mention a new condition a group of doctors described after looking at the image. \n\n\n\nI have pairs of images and new details, given in a description. \n\n\n\nI want to fine-tune the model. In my first batch of experiments, I had about 7.8K conversations in the training set, and I always used the same questions. I used QLoRa using different configurations, and when I tested it, it returned gibberish when using greedy decoding, or something that might include some words of the new answers, when trying different \\`temperature\\`/\\`top\\_p\\`. I suspect it just overfitted to my data, resulting in catastrophic forgetting. \n\n\n\nI got back to the drawing table, gathered more data, now I have about 21K observations (currently images and descriptions), and I want to construct a robust training dataset.\n\n\\- [This](https://www.reddit.com/r/LocalLLaMA/s/YTmnITYTPN) post discusses the number of observations required to fine-tune a model, with some members mentioning that they had a successful fine-tuning with only 100 conversations of high quality. \n\n  \nMy question I guess, is how to build the questions (to be attached to the image/description pairs) to make sure my data is of the highest quality possible? \n\n  \n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Data Quality and Size for LoRa",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8zeg8",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.8,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1t6vmqt87p",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753450676,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to fine-tune a LlaVa model to include new details about an image. Think about medical, I want the model to mention a new condition a group of doctors described after looking at the image. &lt;/p&gt;\n\n&lt;p&gt;I have pairs of images and new details, given in a description. &lt;/p&gt;\n\n&lt;p&gt;I want to fine-tune the model. In my first batch of experiments, I had about 7.8K conversations in the training set, and I always used the same questions. I used QLoRa using different configurations, and when I tested it, it returned gibberish when using greedy decoding, or something that might include some words of the new answers, when trying different `temperature`/`top_p`. I suspect it just overfitted to my data, resulting in catastrophic forgetting. &lt;/p&gt;\n\n&lt;p&gt;I got back to the drawing table, gathered more data, now I have about 21K observations (currently images and descriptions), and I want to construct a robust training dataset.&lt;/p&gt;\n\n&lt;p&gt;- &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/s/YTmnITYTPN\"&gt;This&lt;/a&gt; post discusses the number of observations required to fine-tune a model, with some members mentioning that they had a successful fine-tuning with only 100 conversations of high quality. &lt;/p&gt;\n\n&lt;p&gt;My question I guess, is how to build the questions (to be attached to the image/description pairs) to make sure my data is of the highest quality possible? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m8zeg8",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "Emotional-Sundae4075",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8zeg8/data_quality_and_size_for_lora/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8zeg8/data_quality_and_size_for_lora/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753450676,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n58a3om",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MR_-_501",
            "can_mod_post": false,
            "created_utc": 1753515518,
            "send_replies": true,
            "parent_id": "t3_1m8zeg8",
            "score": 1,
            "author_fullname": "t2_14fb6edg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Llava is an old VLM with pretty bad performance for modern standards, i would recommend going with Qwen 2.5 VL instead, even the 3B should outperform it.\n\nFinetuning VLM's is often broken, my experience with Qwen was relatively good. When using a LoRa approach catastrophic forgetting is nearly impossible because you are training so few parameters.\n\nDo you have more details/examples about what this looks like? The amount of data you need varies wildly, depending on how far from the target data you are.  In the past i've needed over 50k image pairs to properly generalize on something.\n\nAlso dont do more than 4 epochs, maybe even freeze the vision encoder. If you have a workload that requires localisation with relatively little data i would recommend staying away from VLM's that use CLIP or SIGLliP. (Which llava also does) Because the VLM just gets very generic embeddings that do not properly adapt to new workloads.\n\nA lot of the time an image classifier will vastly outperform a VLM in the workload you are describing, you can also find these kinds of models on huggingface pretrained on X-Ray data for example.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n58a3om",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Llava is an old VLM with pretty bad performance for modern standards, i would recommend going with Qwen 2.5 VL instead, even the 3B should outperform it.&lt;/p&gt;\n\n&lt;p&gt;Finetuning VLM&amp;#39;s is often broken, my experience with Qwen was relatively good. When using a LoRa approach catastrophic forgetting is nearly impossible because you are training so few parameters.&lt;/p&gt;\n\n&lt;p&gt;Do you have more details/examples about what this looks like? The amount of data you need varies wildly, depending on how far from the target data you are.  In the past i&amp;#39;ve needed over 50k image pairs to properly generalize on something.&lt;/p&gt;\n\n&lt;p&gt;Also dont do more than 4 epochs, maybe even freeze the vision encoder. If you have a workload that requires localisation with relatively little data i would recommend staying away from VLM&amp;#39;s that use CLIP or SIGLliP. (Which llava also does) Because the VLM just gets very generic embeddings that do not properly adapt to new workloads.&lt;/p&gt;\n\n&lt;p&gt;A lot of the time an image classifier will vastly outperform a VLM in the workload you are describing, you can also find these kinds of models on huggingface pretrained on X-Ray data for example.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8zeg8/data_quality_and_size_for_lora/n58a3om/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753515518,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8zeg8",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]