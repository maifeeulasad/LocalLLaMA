[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "What tools and settings enable optimal performance with CPU + GPU inference (partial offloading)? Here's my setup, which runs at \\~7.2 t/s, which is the maximum I've been able to squeeze out experimenting with settings in LM Studio and Llama.cpp. As we get more model releases that often don't fit entirely in VRAM, it seems like making the most of these settings is important.  \n  \n**Model:** Qwen3-235B-A22B 2507 / Unsloth's Q2\\_K\\_XL Quant / 82.67GB\n\n**GPU**: 5090 / 32GB VRAM\n\n**CPU**: AMD Ryzen 9 9900X\n\n**RAM:** 2x32GB DDR5-6000\n\n**Settings:**\n\n* Context: 4096\n* GPU Offload: 42/94 layers\n* CPU Thread Pool Size: 9\n* Batch Size: 512\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Optimizing inference on GPU + CPU",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1m7oolz",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_i5ptpsd5",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753313226,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What tools and settings enable optimal performance with CPU + GPU inference (partial offloading)? Here&amp;#39;s my setup, which runs at ~7.2 t/s, which is the maximum I&amp;#39;ve been able to squeeze out experimenting with settings in LM Studio and Llama.cpp. As we get more model releases that often don&amp;#39;t fit entirely in VRAM, it seems like making the most of these settings is important.  &lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Qwen3-235B-A22B 2507 / Unsloth&amp;#39;s Q2_K_XL Quant / 82.67GB&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: 5090 / 32GB VRAM&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;: AMD Ryzen 9 9900X&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 2x32GB DDR5-6000&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Settings:&lt;/strong&gt;&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Context: 4096&lt;/li&gt;\n&lt;li&gt;GPU Offload: 42/94 layers&lt;/li&gt;\n&lt;li&gt;CPU Thread Pool Size: 9&lt;/li&gt;\n&lt;li&gt;Batch Size: 512&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m7oolz",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "SubstantialSock8002",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/",
            "subreddit_subscribers": 503516,
            "created_utc": 1753313226,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n4t4cg2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1753314289,
            "send_replies": true,
            "parent_id": "t3_1m7oolz",
            "score": 1,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You have upwards potential with that hardware.  \nI tested the latest Qwen3 235b [Q2\\_K](https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/tree/main/Q2_K) (2GB smaller than yours) on my 1500$ workstation and I'm getting **6.5\\~6.8 t/s** with 30K context - 115 token prompt - 1040 generated tokens\n\nSpecs: 2x 16GB Nvidia (RTX 5060 Ti &amp; P5000) + 64GB DDR5 6000Mhz + Intel 13th gen i5\n\nllama-cli -m .\\\\Qwen3-235B-A22B-Instruct-2507-Q2\\_K-00001-of-00002.gguf -ngl 99 -fa -c 30720 -ctk q8\\_0 -ctv q8\\_0 --main-gpu 0 -ot \".ffn\\_(up|down)\\_exps.=CPU\" -t 10 --temp 0.1 -ts 0.95,1\n\nYou can see I used the -ot parameter as explained [here](https://docs.unsloth.ai/basics/qwen3-coder-how-to-run-locally#improving-generation-speed). This gave me an extra 1 t/s compared to standard offloading with -ngl. This method allows for -ngl 99 because of the freed VRAM.",
            "edited": 1753315018,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t4cg2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You have upwards potential with that hardware.&lt;br/&gt;\nI tested the latest Qwen3 235b &lt;a href=\"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/tree/main/Q2_K\"&gt;Q2_K&lt;/a&gt; (2GB smaller than yours) on my 1500$ workstation and I&amp;#39;m getting &lt;strong&gt;6.5~6.8 t/s&lt;/strong&gt; with 30K context - 115 token prompt - 1040 generated tokens&lt;/p&gt;\n\n&lt;p&gt;Specs: 2x 16GB Nvidia (RTX 5060 Ti &amp;amp; P5000) + 64GB DDR5 6000Mhz + Intel 13th gen i5&lt;/p&gt;\n\n&lt;p&gt;llama-cli -m .\\Qwen3-235B-A22B-Instruct-2507-Q2_K-00001-of-00002.gguf -ngl 99 -fa -c 30720 -ctk q8_0 -ctv q8_0 --main-gpu 0 -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot; -t 10 --temp 0.1 -ts 0.95,1&lt;/p&gt;\n\n&lt;p&gt;You can see I used the -ot parameter as explained &lt;a href=\"https://docs.unsloth.ai/basics/qwen3-coder-how-to-run-locally#improving-generation-speed\"&gt;here&lt;/a&gt;. This gave me an extra 1 t/s compared to standard offloading with -ngl. This method allows for -ngl 99 because of the freed VRAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/n4t4cg2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314289,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7oolz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n4tgecn",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "eloquentemu",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n4t9nc8",
                                                    "score": 1,
                                                    "author_fullname": "t2_lpdsy",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Not bad.  I guess it depends on how much the RAM changed, but it's at least competitive.  I was a bit worried that offloading mixed chunks like could have had a meaningful performance impact, but seems not",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n4tgecn",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not bad.  I guess it depends on how much the RAM changed, but it&amp;#39;s at least competitive.  I was a bit worried that offloading mixed chunks like could have had a meaningful performance impact, but seems not&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1m7oolz",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/n4tgecn/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1753318438,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1753318438,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n4t9nc8",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "AdamDhahabi",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n4t8j8g",
                                          "score": 2,
                                          "author_fullname": "t2_x5lnbc2",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "From 5.5 t/s to 6.5 t/s, possibly half of that because I found out my RAM was running too slow when getting that 5.5 t/s.  \n33 layers offloaded (32 GB VRAM) initially.  \nLet's hope small draft models will soon arrive for doing speculative decoding.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n4t9nc8",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;From 5.5 t/s to 6.5 t/s, possibly half of that because I found out my RAM was running too slow when getting that 5.5 t/s.&lt;br/&gt;\n33 layers offloaded (32 GB VRAM) initially.&lt;br/&gt;\nLet&amp;#39;s hope small draft models will soon arrive for doing speculative decoding.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1m7oolz",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/n4t9nc8/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753316095,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753316095,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n4t8j8g",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n4t7fb5",
                                "score": 1,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Yeah, I noticed that in your comment after I posted.  Have you benchmarked the performance of that versus offloading by layer?  If not, I might try.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n4t8j8g",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yeah, I noticed that in your comment after I posted.  Have you benchmarked the performance of that versus offloading by layer?  If not, I might try.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m7oolz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/n4t8j8g/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753315711,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753315711,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n4t7fb5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AdamDhahabi",
                      "can_mod_post": false,
                      "created_utc": 1753315326,
                      "send_replies": true,
                      "parent_id": "t1_n4t5p1x",
                      "score": 1,
                      "author_fullname": "t2_x5lnbc2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I found out today that the slightly smaller quant [Q2\\_K](https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/tree/main/Q2_K) on a 32GB VRAM system allows for offloading elegantly like this: -ot \".ffn\\_(up|down)\\_exps.=CPU\"  \nThat leaves enough space for 30K\\~32K context (q8\\_0) and will fully fill that VRAM.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4t7fb5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I found out today that the slightly smaller quant &lt;a href=\"https://huggingface.co/unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/tree/main/Q2_K\"&gt;Q2_K&lt;/a&gt; on a 32GB VRAM system allows for offloading elegantly like this: -ot &amp;quot;.ffn_(up|down)_exps.=CPU&amp;quot;&lt;br/&gt;\nThat leaves enough space for 30K~32K context (q8_0) and will fully fill that VRAM.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m7oolz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/n4t7fb5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753315326,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4t5p1x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753314736,
            "send_replies": true,
            "parent_id": "t3_1m7oolz",
            "score": 1,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "There isn't a whole lot to it, honestly.  The normal way to handle offloading is to use `-ngl 99 -ot exps=CPU` which tells llama.cpp to offload everything expect tensors with \"exps\" in the name (i.e. experts).\n\nThat underutilizes the GPU, however, coming in about 5GB (for small context).  And since you have 96GB combined for a 82GB model you really need to max that out.  So you can start selecting what layers of experts also go on the GPU: \n\n`-ot '\\.[1-9][0-9]\\..*exps=CPU`\n\nThat basically selects all \"exps\" from 2-digit layers for the CPU.  At small context, this is about 21GB.  FWIW I don't get much speedup, like 10-20%.  If you want to offload layers 11+ You would do:\n\n`-ot '\\.(1[1-9]|[2-9][0-9])\\..*exps=CPU`\n\nThis is normal regular expression stuff so I won't explain too much (Google is better), but the quick primer is `[2-9][0-9]` matches a character in the range 2-9 followed by 0-9, i.e. a number 20+.  The `()` and `|` is a group and OR, matching the part in the `()` before or after the `|`.  The `1[1-9]` matches a 1 followed by a 1-9 so matches 11-19.  Put it all together you have 11-19 or 20-99.\n\nI don't have a 5090 so I can say my 24GB caps out at offloading layers 12+ but only leaves enough room for like 1k context :).  You'll have to experiment to see where the line is for your setup.\n\nEDIT: I realize my numbers are from Q4_K_M rather than your Q2_K_XL so you can probably offload quite a few more layers.",
            "edited": 1753315185,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4t5p1x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There isn&amp;#39;t a whole lot to it, honestly.  The normal way to handle offloading is to use &lt;code&gt;-ngl 99 -ot exps=CPU&lt;/code&gt; which tells llama.cpp to offload everything expect tensors with &amp;quot;exps&amp;quot; in the name (i.e. experts).&lt;/p&gt;\n\n&lt;p&gt;That underutilizes the GPU, however, coming in about 5GB (for small context).  And since you have 96GB combined for a 82GB model you really need to max that out.  So you can start selecting what layers of experts also go on the GPU: &lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-ot &amp;#39;\\.[1-9][0-9]\\..*exps=CPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;That basically selects all &amp;quot;exps&amp;quot; from 2-digit layers for the CPU.  At small context, this is about 21GB.  FWIW I don&amp;#39;t get much speedup, like 10-20%.  If you want to offload layers 11+ You would do:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;-ot &amp;#39;\\.(1[1-9]|[2-9][0-9])\\..*exps=CPU&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;This is normal regular expression stuff so I won&amp;#39;t explain too much (Google is better), but the quick primer is &lt;code&gt;[2-9][0-9]&lt;/code&gt; matches a character in the range 2-9 followed by 0-9, i.e. a number 20+.  The &lt;code&gt;()&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt; is a group and OR, matching the part in the &lt;code&gt;()&lt;/code&gt; before or after the &lt;code&gt;|&lt;/code&gt;.  The &lt;code&gt;1[1-9]&lt;/code&gt; matches a 1 followed by a 1-9 so matches 11-19.  Put it all together you have 11-19 or 20-99.&lt;/p&gt;\n\n&lt;p&gt;I don&amp;#39;t have a 5090 so I can say my 24GB caps out at offloading layers 12+ but only leaves enough room for like 1k context :).  You&amp;#39;ll have to experiment to see where the line is for your setup.&lt;/p&gt;\n\n&lt;p&gt;EDIT: I realize my numbers are from Q4_K_M rather than your Q2_K_XL so you can probably offload quite a few more layers.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m7oolz/optimizing_inference_on_gpu_cpu/n4t5p1x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753314736,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m7oolz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]