[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "People who have hosted LLMs using vLLM, what approach did you guys take?\nListing down some approaches that I am considering. Would like to understand the associated complexity involved, ease of scaling for more models, more production loads, etc.\n\n1. Ec2 (considering g5.xlarge) with ASG\n2. Using k8s \n3. Using frameworks like Anyscale, anything llm, autogen, bentoml etc. (Using AWS is compulsory)\n4. Using integrations like kubeai, kuberay etc.\n\nThe frameworks and integrations are from vLLM docs under deployment. I am not much aware of what they exactly solve for but would like to understand if anyone of you have used those tools.\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Hosting LLM using vLLM for production",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mbf9a9",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_3nk0ww7f",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753706925,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;People who have hosted LLMs using vLLM, what approach did you guys take?\nListing down some approaches that I am considering. Would like to understand the associated complexity involved, ease of scaling for more models, more production loads, etc.&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Ec2 (considering g5.xlarge) with ASG&lt;/li&gt;\n&lt;li&gt;Using k8s &lt;/li&gt;\n&lt;li&gt;Using frameworks like Anyscale, anything llm, autogen, bentoml etc. (Using AWS is compulsory)&lt;/li&gt;\n&lt;li&gt;Using integrations like kubeai, kuberay etc.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;The frameworks and integrations are from vLLM docs under deployment. I am not much aware of what they exactly solve for but would like to understand if anyone of you have used those tools.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mbf9a9",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "everyoneisodd",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/",
            "subreddit_subscribers": 506191,
            "created_utc": 1753706925,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5losuv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Low-Opening25",
            "can_mod_post": false,
            "created_utc": 1753707945,
            "send_replies": true,
            "parent_id": "t3_1mbf9a9",
            "score": 5,
            "author_fullname": "t2_ebfjvj5t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "what is your use case?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5losuv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;what is your use case?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/n5losuv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753707945,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbf9a9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5mwe7k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "secopsml",
            "can_mod_post": false,
            "created_utc": 1753720904,
            "send_replies": true,
            "parent_id": "t3_1mbf9a9",
            "score": 1,
            "author_fullname": "t2_pmniwf57y",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "vllm, litellm, openai compatibile endpoints.\nBare metal vllm configured with ansible playbooks. Litellm containerized.\n\n\nI might use frameworks as context and vibe code per project custom solutions. It is easier to rewrite entire apps than to track breaking changes for me.\n\n\nIn case I need more than single host I use modal autoscaling or use public APIs ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5mwe7k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vllm, litellm, openai compatibile endpoints.\nBare metal vllm configured with ansible playbooks. Litellm containerized.&lt;/p&gt;\n\n&lt;p&gt;I might use frameworks as context and vibe code per project custom solutions. It is easier to rewrite entire apps than to track breaking changes for me.&lt;/p&gt;\n\n&lt;p&gt;In case I need more than single host I use modal autoscaling or use public APIs &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mbf9a9/hosting_llm_using_vllm_for_production/n5mwe7k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753720904,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mbf9a9",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]