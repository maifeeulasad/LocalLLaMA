[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "As per the title. I want to run a model for dnd, the plan is to use Gemma 3 27b and max out the context length so that the model can remember things. Once the context fills up, I plan to ask the model to summarise the session and paste it into a new instance to continue. I have tried it with Gemini 2.5 Pro and the method works well enough.\n\nThe issue I mainly want to ask about is what impacts the filled up context length would have. From my understanding, I will need a stronger gpu chip for the prompt processing, but the vram will get filled up as usual.\n\nWill this just be the same as running a model that progressively gets larger the more I use it?\n\nHow does this work with multiple gpus? \n\nWhat prompt processing speeds can I expect with an mi50 32gb?\n\nHow does prompt processing work actually, each portion loaded into vram is processed by that vram’s corresponding gpu chip right?\n\nSo many questions, I’ll probably ask further clarifying questions in the comments",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How does having a very long context window impact performance?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lxuu5m",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rn6co7q5m",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752306412,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;As per the title. I want to run a model for dnd, the plan is to use Gemma 3 27b and max out the context length so that the model can remember things. Once the context fills up, I plan to ask the model to summarise the session and paste it into a new instance to continue. I have tried it with Gemini 2.5 Pro and the method works well enough.&lt;/p&gt;\n\n&lt;p&gt;The issue I mainly want to ask about is what impacts the filled up context length would have. From my understanding, I will need a stronger gpu chip for the prompt processing, but the vram will get filled up as usual.&lt;/p&gt;\n\n&lt;p&gt;Will this just be the same as running a model that progressively gets larger the more I use it?&lt;/p&gt;\n\n&lt;p&gt;How does this work with multiple gpus? &lt;/p&gt;\n\n&lt;p&gt;What prompt processing speeds can I expect with an mi50 32gb?&lt;/p&gt;\n\n&lt;p&gt;How does prompt processing work actually, each portion loaded into vram is processed by that vram’s corresponding gpu chip right?&lt;/p&gt;\n\n&lt;p&gt;So many questions, I’ll probably ask further clarifying questions in the comments&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lxuu5m",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "opoot_",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/",
            "subreddit_subscribers": 497824,
            "created_utc": 1752306412,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2p2i80",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "opoot_",
                      "can_mod_post": false,
                      "created_utc": 1752308275,
                      "send_replies": true,
                      "parent_id": "t1_n2ozwpu",
                      "score": 2,
                      "author_fullname": "t2_rn6co7q5m",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Really? I would think the raw chip performance of the mi50 would cause it to suffer in prompt processing, especially with it having like 1tb/s of bandwidth but having not a lot of fp16 tflops",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2p2i80",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Really? I would think the raw chip performance of the mi50 would cause it to suffer in prompt processing, especially with it having like 1tb/s of bandwidth but having not a lot of fp16 tflops&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxuu5m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/n2p2i80/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752308275,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2p82wd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AppearanceHeavy6724",
                      "can_mod_post": false,
                      "created_utc": 1752311659,
                      "send_replies": true,
                      "parent_id": "t1_n2ozwpu",
                      "score": 0,
                      "author_fullname": "t2_uz37qfx5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Mi50 has very weak compute",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2p82wd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mi50 has very weak compute&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxuu5m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/n2p82wd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752311659,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2ozwpu",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "404NotAFish",
            "can_mod_post": false,
            "created_utc": 1752306753,
            "send_replies": true,
            "parent_id": "t3_1lxuu5m",
            "score": 3,
            "author_fullname": "t2_1jhbvmdfa6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "When you use a long context window, the model has to do more work every time you send in a prompt. The longer the input, the more time and memory it takes to process, which puts more pressure on your GPU and means slower response times, especially if you're running something large like Gemma 27B.\n\nVRAM usage does increase as the context fills up. It's not like the model gets 'bigger' over time, but the amount of data you ask it to pay attention to grows, so that affects how long it takes to generate each response.\n\nSome new models try to handle this more efficiently. For example, Jamba uses a mix of transformer and state-space model components to reduce slowdown with long contexts. It's designed to work well even when the input is really long. \n\nAs for multi-GPU setups, it depends on whether the model you're using supports splitting the load. Some open models do, but it's not always simple to set up. With something like an MI50, you'll be limited more by memory and bandwidth than by raw processing power, especially if you're pushing the context window to the max.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2ozwpu",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When you use a long context window, the model has to do more work every time you send in a prompt. The longer the input, the more time and memory it takes to process, which puts more pressure on your GPU and means slower response times, especially if you&amp;#39;re running something large like Gemma 27B.&lt;/p&gt;\n\n&lt;p&gt;VRAM usage does increase as the context fills up. It&amp;#39;s not like the model gets &amp;#39;bigger&amp;#39; over time, but the amount of data you ask it to pay attention to grows, so that affects how long it takes to generate each response.&lt;/p&gt;\n\n&lt;p&gt;Some new models try to handle this more efficiently. For example, Jamba uses a mix of transformer and state-space model components to reduce slowdown with long contexts. It&amp;#39;s designed to work well even when the input is really long. &lt;/p&gt;\n\n&lt;p&gt;As for multi-GPU setups, it depends on whether the model you&amp;#39;re using supports splitting the load. Some open models do, but it&amp;#39;s not always simple to set up. With something like an MI50, you&amp;#39;ll be limited more by memory and bandwidth than by raw processing power, especially if you&amp;#39;re pushing the context window to the max.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/n2ozwpu/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752306753,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxuu5m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2p0gkj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SillyLilBear",
            "can_mod_post": false,
            "created_utc": 1752307072,
            "send_replies": true,
            "parent_id": "t3_1lxuu5m",
            "score": 1,
            "author_fullname": "t2_wjjtz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Think of using an LLM like it has amnesia.  You say Hi, it says hi back.  You say How you are doing, it has to read your first conversation before it responds to your last prompt.  As your conversations get longer, it takes longer to process the previous conversation as it is seeing it for the first time and sending it through the neural network every single time.\n\nIf your prompt processing is very slow (i.e. AMD Strix Halo) you can see very slow responses are you get further along in the conversation.  It will always get slower, but if your prompt processing speed is low, it will get exponentially worse.\n\nYou will also use up a lot of tokens as the conversation goes on as you are feeding all the previous input in yet again.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2p0gkj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Think of using an LLM like it has amnesia.  You say Hi, it says hi back.  You say How you are doing, it has to read your first conversation before it responds to your last prompt.  As your conversations get longer, it takes longer to process the previous conversation as it is seeing it for the first time and sending it through the neural network every single time.&lt;/p&gt;\n\n&lt;p&gt;If your prompt processing is very slow (i.e. AMD Strix Halo) you can see very slow responses are you get further along in the conversation.  It will always get slower, but if your prompt processing speed is low, it will get exponentially worse.&lt;/p&gt;\n\n&lt;p&gt;You will also use up a lot of tokens as the conversation goes on as you are feeding all the previous input in yet again.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/n2p0gkj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752307072,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxuu5m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2p5d32",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "mpthouse",
            "can_mod_post": false,
            "created_utc": 1752310000,
            "send_replies": true,
            "parent_id": "t3_1lxuu5m",
            "score": 1,
            "author_fullname": "t2_1ft86zbrel",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Interesting approach to handling long D&amp;D sessions! I'm curious to see what kind of performance you actually get with the MI50.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2p5d32",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Interesting approach to handling long D&amp;amp;D sessions! I&amp;#39;m curious to see what kind of performance you actually get with the MI50.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/n2p5d32/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752310000,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxuu5m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2pk583",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "opoot_",
                      "can_mod_post": false,
                      "created_utc": 1752318588,
                      "send_replies": true,
                      "parent_id": "t1_n2pgb38",
                      "score": 1,
                      "author_fullname": "t2_rn6co7q5m",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I’m not that familiar on auto summarising functions. Beyond the name’s explanation, what does it do exactly and how can I implement it? I use lm studio mainly",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2pk583",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m not that familiar on auto summarising functions. Beyond the name’s explanation, what does it do exactly and how can I implement it? I use lm studio mainly&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxuu5m",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/n2pk583/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752318588,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2pgb38",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Some-Cauliflower4902",
            "can_mod_post": false,
            "created_utc": 1752316555,
            "send_replies": true,
            "parent_id": "t3_1lxuu5m",
            "score": 1,
            "author_fullname": "t2_1e2tnqudlj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My experience with Gemma3 27B is that speed fell off the cliff once the context is more than 4 windows (ie over 4K). I’m using a 5070ti and I get 30t/s using Q3KM model under 4K. Beyond 4K context it requires a lot more compute to look back and remember things is my understanding. So you get loooong prompt processing time. In my case it’s 5t/s… Probably better off using Mistral Small which is what I’m doing, something a bit longer context and less cliff.\n\nYou can also write in auto-summarizing function just before it hits context limit. Or use something that has it. And when the sessions get very long use RAG.",
            "edited": 1752316856,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2pgb38",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My experience with Gemma3 27B is that speed fell off the cliff once the context is more than 4 windows (ie over 4K). I’m using a 5070ti and I get 30t/s using Q3KM model under 4K. Beyond 4K context it requires a lot more compute to look back and remember things is my understanding. So you get loooong prompt processing time. In my case it’s 5t/s… Probably better off using Mistral Small which is what I’m doing, something a bit longer context and less cliff.&lt;/p&gt;\n\n&lt;p&gt;You can also write in auto-summarizing function just before it hits context limit. Or use something that has it. And when the sessions get very long use RAG.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxuu5m/how_does_having_a_very_long_context_window_impact/n2pgb38/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752316555,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxuu5m",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]