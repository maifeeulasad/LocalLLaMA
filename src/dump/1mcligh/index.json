[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "AI Max+ 395 vs nvidia vs m4, which one has higher TPM when running ollama in notebook? thanks",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Notebook, AI Max+ 395 vs nvidia vs m4",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mcligh",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.43,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_354zxl3r",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753818518,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;AI Max+ 395 vs nvidia vs m4, which one has higher TPM when running ollama in notebook? thanks&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mcligh",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "quantrpeter",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/",
            "subreddit_subscribers": 506973,
            "created_utc": 1753818518,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5utxhs",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willdudes",
            "can_mod_post": false,
            "created_utc": 1753820050,
            "send_replies": true,
            "parent_id": "t3_1mcligh",
            "score": 2,
            "author_fullname": "t2_14v7k3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is digits even released.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5utxhs",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is digits even released.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/n5utxhs/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753820050,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcligh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5uzkrg",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "curios-al",
            "can_mod_post": false,
            "created_utc": 1753821631,
            "send_replies": true,
            "parent_id": "t3_1mcligh",
            "score": 2,
            "author_fullname": "t2_te425vy7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Nvidia 5090 (laptop version) is the fastest but model size is limited. M4 MAX will be the second best. M4 Pro and AI Max+ 395 will  be about the same (prompt processing could be faster on Amd).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5uzkrg",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Nvidia 5090 (laptop version) is the fastest but model size is limited. M4 MAX will be the second best. M4 Pro and AI Max+ 395 will  be about the same (prompt processing could be faster on Amd).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/n5uzkrg/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753821631,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcligh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5v3jlx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1753822730,
            "send_replies": true,
            "parent_id": "t3_1mcligh",
            "score": 3,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This question is kind of meaningless. The only specific product here was the AI Max+ 395, so it's hard to compare against the others.\n\nLike, do you mean a base M4? M4 pro? Ultra? How many GB of RAM (the bandwidth, which is very important, changes with the specific configuration).\n\nWhat about Nvidia? Do you mean an Nvidia GPU? Which GPU? What laptops are available to you and what cooling solutions do they have? Do you mean DIGITS (a direct AI Max+ 395 competitor)? If so, do you know how to compile code for ARM?\n\nAll of these will have their own problems.\n\nNvidia laptop GPU: Has both a surcharge for being compact, and a reduction in GPU quality for being a laptop chip, so you're usually getting about half or worse the value compared to a gaming GPU (in ML workloads). You also have limited VRAM.\n\nAI Max+ 395: Uses AMD iGPU probably (unless you run on CPU, I suppose) which is worse supported in software. I'm not sure about setting it up on Windows, but it's at least bearable on Linux now. Vulkan backend tends to be the fastest right now, I believe.\n\nM4: Where AMD is poorly supported in drivers, M4 tends to be poorly supported in software. While it is well supported in the GGUF ecosystem, as soon as you want to do anything more than that it can get a bit tricky. Specific extra projects you want to run may not operate cleanly on your hardware, and there's a lot of really weird edge cases in data types etc. Some projects disable support for MLX and fallback to CPU.\n\nNvidia DIGITS: You have to deal with ARM drivers. Based on people's experiences with GH100 and Jetson Orin, you're likely going to have an awkward time with software packages and support. Similarly to M4, everything within support will work quite well, but if you need a package Nvidia doesn't give you, you'll have to build it yourself, and it won't be immediately obvious which version of everything is compatible or will work at all.\n\nI think the Nvidia dGPU would run the fastest up until the model exceeds the available VRAM, higher end M4 chips are the next fastest, maybe AI Max+ 395 and Digits are about tied, and lower end M4 chips go last.\n\nAlso:\n\nWhy are you running Ollama?\n\nWhy are you running in a notebook?\n\n...What do you mean by running in a notebook? If you mean like a Jupyter Notebook:\n\nI would personally strongly advocate for using LlamaCPP directly. It's more hackable, has more up to date support, better supports a wider variety of hardware more quickly (Ollama and LM Studio are downstream and have to merge LCPP updates so there's a delay), and it's easier to customize for your use case and know exactly what's going on.\n\nIf you're hitting an endpoint, it looks pretty much the same to run LCPP server as Ollama.  \n\n\nIf you mean in a laptop:\n\nI would strongly advocate against doing heavy duty ML workloads in a laptop if at all possible. I know they sound nice (\"Oh, it's portable, I need it to go with me\"), but most people take their laptop to maybe two places in reality, and the cost/benefit of a laptop is so bad you'd almost rather buy PCs at both locations rather than a crazy expensive laptop.\n\nIt's also possible to reverse proxy your endpoint so that you can access it while on the go, and you can host a webUI directly on it, meaning that you really don't necessarily need a device you can carry around with you at all times which can run intensive models. For 90% of people that I've heard say \"I need a laptop for ML\" what they actually needed was a cheap second hand server to run/train models and a cheap laptop to access it. Very few people have actually needed a laptop specifically.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5v3jlx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This question is kind of meaningless. The only specific product here was the AI Max+ 395, so it&amp;#39;s hard to compare against the others.&lt;/p&gt;\n\n&lt;p&gt;Like, do you mean a base M4? M4 pro? Ultra? How many GB of RAM (the bandwidth, which is very important, changes with the specific configuration).&lt;/p&gt;\n\n&lt;p&gt;What about Nvidia? Do you mean an Nvidia GPU? Which GPU? What laptops are available to you and what cooling solutions do they have? Do you mean DIGITS (a direct AI Max+ 395 competitor)? If so, do you know how to compile code for ARM?&lt;/p&gt;\n\n&lt;p&gt;All of these will have their own problems.&lt;/p&gt;\n\n&lt;p&gt;Nvidia laptop GPU: Has both a surcharge for being compact, and a reduction in GPU quality for being a laptop chip, so you&amp;#39;re usually getting about half or worse the value compared to a gaming GPU (in ML workloads). You also have limited VRAM.&lt;/p&gt;\n\n&lt;p&gt;AI Max+ 395: Uses AMD iGPU probably (unless you run on CPU, I suppose) which is worse supported in software. I&amp;#39;m not sure about setting it up on Windows, but it&amp;#39;s at least bearable on Linux now. Vulkan backend tends to be the fastest right now, I believe.&lt;/p&gt;\n\n&lt;p&gt;M4: Where AMD is poorly supported in drivers, M4 tends to be poorly supported in software. While it is well supported in the GGUF ecosystem, as soon as you want to do anything more than that it can get a bit tricky. Specific extra projects you want to run may not operate cleanly on your hardware, and there&amp;#39;s a lot of really weird edge cases in data types etc. Some projects disable support for MLX and fallback to CPU.&lt;/p&gt;\n\n&lt;p&gt;Nvidia DIGITS: You have to deal with ARM drivers. Based on people&amp;#39;s experiences with GH100 and Jetson Orin, you&amp;#39;re likely going to have an awkward time with software packages and support. Similarly to M4, everything within support will work quite well, but if you need a package Nvidia doesn&amp;#39;t give you, you&amp;#39;ll have to build it yourself, and it won&amp;#39;t be immediately obvious which version of everything is compatible or will work at all.&lt;/p&gt;\n\n&lt;p&gt;I think the Nvidia dGPU would run the fastest up until the model exceeds the available VRAM, higher end M4 chips are the next fastest, maybe AI Max+ 395 and Digits are about tied, and lower end M4 chips go last.&lt;/p&gt;\n\n&lt;p&gt;Also:&lt;/p&gt;\n\n&lt;p&gt;Why are you running Ollama?&lt;/p&gt;\n\n&lt;p&gt;Why are you running in a notebook?&lt;/p&gt;\n\n&lt;p&gt;...What do you mean by running in a notebook? If you mean like a Jupyter Notebook:&lt;/p&gt;\n\n&lt;p&gt;I would personally strongly advocate for using LlamaCPP directly. It&amp;#39;s more hackable, has more up to date support, better supports a wider variety of hardware more quickly (Ollama and LM Studio are downstream and have to merge LCPP updates so there&amp;#39;s a delay), and it&amp;#39;s easier to customize for your use case and know exactly what&amp;#39;s going on.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re hitting an endpoint, it looks pretty much the same to run LCPP server as Ollama.  &lt;/p&gt;\n\n&lt;p&gt;If you mean in a laptop:&lt;/p&gt;\n\n&lt;p&gt;I would strongly advocate against doing heavy duty ML workloads in a laptop if at all possible. I know they sound nice (&amp;quot;Oh, it&amp;#39;s portable, I need it to go with me&amp;quot;), but most people take their laptop to maybe two places in reality, and the cost/benefit of a laptop is so bad you&amp;#39;d almost rather buy PCs at both locations rather than a crazy expensive laptop.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s also possible to reverse proxy your endpoint so that you can access it while on the go, and you can host a webUI directly on it, meaning that you really don&amp;#39;t necessarily need a device you can carry around with you at all times which can run intensive models. For 90% of people that I&amp;#39;ve heard say &amp;quot;I need a laptop for ML&amp;quot; what they actually needed was a cheap second hand server to run/train models and a cheap laptop to access it. Very few people have actually needed a laptop specifically.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/n5v3jlx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753822730,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcligh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5uuubp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTrack_ai",
            "can_mod_post": false,
            "created_utc": 1753820307,
            "send_replies": true,
            "parent_id": "t3_1mcligh",
            "score": 1,
            "author_fullname": "t2_1tpuoj72sa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "the devil, belzebub or baal.... Only peope who do not know what the electrolyte in lithium-ion batteries is buy notebooks.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5uuubp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the devil, belzebub or baal.... Only peope who do not know what the electrolyte in lithium-ion batteries is buy notebooks.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcligh/notebook_ai_max_395_vs_nvidia_vs_m4/n5uuubp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753820307,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcligh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]