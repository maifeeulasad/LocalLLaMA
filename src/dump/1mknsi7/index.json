[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Built ollamao to solve the chaos of running multiple LLM backends locally and in production.\n\nðŸŽ¯ \\*\\*The Problem:\\*\\*\n\n\\- Ollama: Great for dev, GGUF models, memory efficient\n\n\\- vLLM: Best for prod, high throughput, GPU optimization\n\n\\- Managing both: Complete nightmare\n\nðŸš€ \\*\\*The Solution:\\*\\*\n\nOne OpenAI-compatible API that intelligently routes between backends:\n\n\\`\\`\\`yaml\n\nmodels:\n\nÂ  chat: llama3.2:3bÂ  Â  Â  # Ollama - fast responses\n\nÂ  analysis: llama3:70b Â  # vLLM - heavy lifting Â \n\nÂ  code: codellama:13bÂ  Â  # Ollama - quick coding\n\n\\`\\`\\`\n\nâœ… Same code from dev to production\n\nâœ… Smart routing (fast models for simple tasks)\n\nâœ… Proper auth, logging, streaming\n\nâœ… Docker compose up and you're running\n\n\\*\\*Current:\\*\\* Ollama support with security fixes\n\n\\*\\*Coming:\\*\\* vLLM integration, cost-aware routing\n\nPerfect for developers who want Ollama simplicity with production-grade features.\n\nPlanning to add more backends - what would you want to see next?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Ollamao: open-source proxy smart serving multipleÂ ollama &amp; vllm instances",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Resources"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 70,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mknsi7",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.17,
            "author_flair_background_color": null,
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_14szmq8370",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Resources",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=677e0f5535b0a834a8b240d7c5d760a180567488",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "link",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754634362,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "github.com",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Built ollamao to solve the chaos of running multiple LLM backends locally and in production.&lt;/p&gt;\n\n&lt;p&gt;ðŸŽ¯ **The Problem:**&lt;/p&gt;\n\n&lt;p&gt;- Ollama: Great for dev, GGUF models, memory efficient&lt;/p&gt;\n\n&lt;p&gt;- vLLM: Best for prod, high throughput, GPU optimization&lt;/p&gt;\n\n&lt;p&gt;- Managing both: Complete nightmare&lt;/p&gt;\n\n&lt;p&gt;ðŸš€ **The Solution:**&lt;/p&gt;\n\n&lt;p&gt;One OpenAI-compatible API that intelligently routes between backends:&lt;/p&gt;\n\n&lt;p&gt;```yaml&lt;/p&gt;\n\n&lt;p&gt;models:&lt;/p&gt;\n\n&lt;p&gt;Â  chat: llama3.2:3bÂ  Â  Â  # Ollama - fast responses&lt;/p&gt;\n\n&lt;p&gt;Â  analysis: llama3:70b Â  # vLLM - heavy lifting Â &lt;/p&gt;\n\n&lt;p&gt;Â  code: codellama:13bÂ  Â  # Ollama - quick coding&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;âœ… Same code from dev to production&lt;/p&gt;\n\n&lt;p&gt;âœ… Smart routing (fast models for simple tasks)&lt;/p&gt;\n\n&lt;p&gt;âœ… Proper auth, logging, streaming&lt;/p&gt;\n\n&lt;p&gt;âœ… Docker compose up and you&amp;#39;re running&lt;/p&gt;\n\n&lt;p&gt;**Current:** Ollama support with security fixes&lt;/p&gt;\n\n&lt;p&gt;**Coming:** vLLM integration, cost-aware routing&lt;/p&gt;\n\n&lt;p&gt;Perfect for developers who want Ollama simplicity with production-grade features.&lt;/p&gt;\n\n&lt;p&gt;Planning to add more backends - what would you want to see next?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://github.com/GeLi2001/ollamao",
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?auto=webp&amp;s=c3e2eda753785e8a94bac54a6b479238f1e99bf2",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a2fbc6957604ab5aa28d5cf69b2ba8674a337e14",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d9be5b6db16d1498eda114747c4ea24fe9b967ca",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4b7d788da17ebe65d62d1e7fa94a874a40e02eaa",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b3cfbc88cfc32a5bf5ee99189c58b21f605382f0",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=94854ea60435bb3af3b8cc8c70682b4d3024dfc7",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=08ec773f4d55301434a4ff267afbcc1861cb2dc5",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "dZtgh5HPP2JdRMNAWbNGZu4XpjKQJqvsqT4gL7VelKo"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ccac2b",
            "id": "1mknsi7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "JadedBlackberry1804",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mknsi7/ollamao_opensource_proxy_smart_serving_multiple/",
            "stickied": false,
            "url": "https://github.com/GeLi2001/ollamao",
            "subreddit_subscribers": 513813,
            "created_utc": 1754634362,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7k61u8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "JadedBlackberry1804",
                      "can_mod_post": false,
                      "created_utc": 1754636670,
                      "send_replies": true,
                      "parent_id": "t1_n7k3q6p",
                      "score": 1,
                      "author_fullname": "t2_14szmq8370",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "appreciate your input! I will try to see if I can put up an example of running comfyui on ollamao",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7k61u8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;appreciate your input! I will try to see if I can put up an example of running comfyui on ollamao&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mknsi7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mknsi7/ollamao_opensource_proxy_smart_serving_multiple/n7k61u8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754636670,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k3q6p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Efficiency_1144",
            "can_mod_post": false,
            "created_utc": 1754635428,
            "send_replies": true,
            "parent_id": "t3_1mknsi7",
            "score": 1,
            "author_fullname": "t2_1nkj9l14b0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Thanks this is useful because some ComfyUI workflows have Ollama nodes so if you want to use a vLLM agent to run the comfy workflows then you need to have both.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k3q6p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks this is useful because some ComfyUI workflows have Ollama nodes so if you want to use a vLLM agent to run the comfy workflows then you need to have both.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mknsi7/ollamao_opensource_proxy_smart_serving_multiple/n7k3q6p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754635428,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mknsi7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7k67de",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "JadedBlackberry1804",
                      "can_mod_post": false,
                      "created_utc": 1754636752,
                      "send_replies": true,
                      "parent_id": "t1_n7k5a2g",
                      "score": 1,
                      "author_fullname": "t2_14szmq8370",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "lol sorry about this, updated; Nothing wrong about vibe code, no need to write simple code if ai can do it. its my bad not being careful.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7k67de",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;lol sorry about this, updated; Nothing wrong about vibe code, no need to write simple code if ai can do it. its my bad not being careful.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mknsi7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mknsi7/ollamao_opensource_proxy_smart_serving_multiple/n7k67de/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754636752,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7k5a2g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1754636253,
            "send_replies": true,
            "parent_id": "t3_1mknsi7",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "    1. Clone the repo\n    git clone https://github.com/yourname/ollamao\n\n\nyet another vibecoded trash. report -&gt; breaks LocalLLaMA rules -&gt; low effort posts",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7k5a2g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;pre&gt;&lt;code&gt;1. Clone the repo\ngit clone https://github.com/yourname/ollamao\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;yet another vibecoded trash. report -&amp;gt; breaks LocalLLaMA rules -&amp;gt; low effort posts&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mknsi7/ollamao_opensource_proxy_smart_serving_multiple/n7k5a2g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754636253,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mknsi7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]