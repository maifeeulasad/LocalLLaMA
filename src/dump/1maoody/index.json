[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Suggestions as to what you've found worth using / keeping vs. not?\n\nWhat specific older models or older model / use case combinations from 2023-2024 would you emphatically NOT consider wholly obsoleted by newer models?\n\nLocal model obsolescence decisions for personal STEM / utility / english / Q&amp;A / RAG / tool use / IT / desktop / workstation use cases?\n\nSo we've had quite a lot of LLM, VLM models released now from the original llama up through what's come out in the past weeks.\n\nRelative to having local models spanning that time frame ready for personal use for desktop / workstation / STEM / english / Q&amp;A / LLM / visual Q&amp;A, speaking of models in the 4B-250B range MoE &amp; dense categories we've had bunches around 7-14B, 20-32B, 70B, 100-250B.\n\nSome of the ones from 6-8 months ago, 12 months ago, 18-24 months ago are / were quite useful / good, but many of the newer ones in similar size ranges are probably better at most things. \n\n70-120B is awkward since there's been less new models in those size ranges though some 32Bs or quants of 230Bs could perform better than old 70-120B dense models in most cases.\n\nAnyway I'm trying to decide for those broad but not all encompassing (no literary fiction compositions, erp, heavy multi-lingual besides casual translation &amp; summarization of web &amp; pub) use cases where to draw the line and just say almost everything before 1H 2024 or whatever criteria one can devise is effectively obsoleted by something free to use / liberally licensed / similar or smaller size with similar or better local runtime performance.\n\ne.g. Deepseek V2.5 vs. Qwen3-235 or such.  LLama2/3.x 7-70B vs newer stuff.  Coding models older than qwen2.5 (obviously qwen-3 small coding models aren't out yet so it's hard to say nothing previous is entirely obsolete..?).\n\nOlder mistral / gemma / command-r / qwen / glm / nous / fine-tunes etc. etc.?\n\nVLMs from the older paligemma up through the early 2024 times vs Q4 2024 and newer releases for casual V-Q&amp;A / OCR / etc.?\n\nBut then even the older QWQ still seems to bench well against newer models.\n\nThe point is not to throw out the baby with the bathwater and keep in mind / availability things that are still gems or outperforming for some use cases.\n\nAlso if new models might \"benchmax\" or limit the width / breadth of training focus to improve and focus performance in narrow areas there's something to be said for ones more generalist or less prone to follow over-trained over-fitted patterns if there's stars in those areas that might be less \"optimized\".",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "LLM / VLM Local model obsolescence decisions for personal STEM / utility / english / Q&amp;A / RAG / tool use / IT desktop / workstation use cases?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1maoody",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_tailqi90",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753628996,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Suggestions as to what you&amp;#39;ve found worth using / keeping vs. not?&lt;/p&gt;\n\n&lt;p&gt;What specific older models or older model / use case combinations from 2023-2024 would you emphatically NOT consider wholly obsoleted by newer models?&lt;/p&gt;\n\n&lt;p&gt;Local model obsolescence decisions for personal STEM / utility / english / Q&amp;amp;A / RAG / tool use / IT / desktop / workstation use cases?&lt;/p&gt;\n\n&lt;p&gt;So we&amp;#39;ve had quite a lot of LLM, VLM models released now from the original llama up through what&amp;#39;s come out in the past weeks.&lt;/p&gt;\n\n&lt;p&gt;Relative to having local models spanning that time frame ready for personal use for desktop / workstation / STEM / english / Q&amp;amp;A / LLM / visual Q&amp;amp;A, speaking of models in the 4B-250B range MoE &amp;amp; dense categories we&amp;#39;ve had bunches around 7-14B, 20-32B, 70B, 100-250B.&lt;/p&gt;\n\n&lt;p&gt;Some of the ones from 6-8 months ago, 12 months ago, 18-24 months ago are / were quite useful / good, but many of the newer ones in similar size ranges are probably better at most things. &lt;/p&gt;\n\n&lt;p&gt;70-120B is awkward since there&amp;#39;s been less new models in those size ranges though some 32Bs or quants of 230Bs could perform better than old 70-120B dense models in most cases.&lt;/p&gt;\n\n&lt;p&gt;Anyway I&amp;#39;m trying to decide for those broad but not all encompassing (no literary fiction compositions, erp, heavy multi-lingual besides casual translation &amp;amp; summarization of web &amp;amp; pub) use cases where to draw the line and just say almost everything before 1H 2024 or whatever criteria one can devise is effectively obsoleted by something free to use / liberally licensed / similar or smaller size with similar or better local runtime performance.&lt;/p&gt;\n\n&lt;p&gt;e.g. Deepseek V2.5 vs. Qwen3-235 or such.  LLama2/3.x 7-70B vs newer stuff.  Coding models older than qwen2.5 (obviously qwen-3 small coding models aren&amp;#39;t out yet so it&amp;#39;s hard to say nothing previous is entirely obsolete..?).&lt;/p&gt;\n\n&lt;p&gt;Older mistral / gemma / command-r / qwen / glm / nous / fine-tunes etc. etc.?&lt;/p&gt;\n\n&lt;p&gt;VLMs from the older paligemma up through the early 2024 times vs Q4 2024 and newer releases for casual V-Q&amp;amp;A / OCR / etc.?&lt;/p&gt;\n\n&lt;p&gt;But then even the older QWQ still seems to bench well against newer models.&lt;/p&gt;\n\n&lt;p&gt;The point is not to throw out the baby with the bathwater and keep in mind / availability things that are still gems or outperforming for some use cases.&lt;/p&gt;\n\n&lt;p&gt;Also if new models might &amp;quot;benchmax&amp;quot; or limit the width / breadth of training focus to improve and focus performance in narrow areas there&amp;#39;s something to be said for ones more generalist or less prone to follow over-trained over-fitted patterns if there&amp;#39;s stars in those areas that might be less &amp;quot;optimized&amp;quot;.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1maoody",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Calcidiol",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/",
            "subreddit_subscribers": 505881,
            "created_utc": 1753628996,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5g8lm3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "custodiam99",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5g7762",
                                "score": 2,
                                "author_fullname": "t2_nqnhgqqf5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "For me [LiveBench](https://livebench.ai/#/) always works. I find them objective.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5g8lm3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For me &lt;a href=\"https://livebench.ai/#/\"&gt;LiveBench&lt;/a&gt; always works. I find them objective.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1maoody",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/n5g8lm3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753631609,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753631609,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5g7762",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Calcidiol",
                      "can_mod_post": false,
                      "created_utc": 1753631198,
                      "send_replies": true,
                      "parent_id": "t1_n5g0kqo",
                      "score": 1,
                      "author_fullname": "t2_tailqi90",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks.  Yeah that's kind of what I'm wondering.\n\nWould I really be losing anything if I just pick a \"top 8\" or \"top 10\" models that are \"the hottest new versions\" that bench well / get good overall reviews and call it good for casual use to just use those and stop worrying about older / other stuff since it's getting too hard to keep up\nwith all the old models, all the new models.\n\nIt's (LLM) a utilitarian casual part time tool not a job issue to keep on top of what niche model X is better than model Y in over time and it's getting impossible to keep straight what a \"go to\" model list should look like if it's not just a few overall winners.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5g7762",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks.  Yeah that&amp;#39;s kind of what I&amp;#39;m wondering.&lt;/p&gt;\n\n&lt;p&gt;Would I really be losing anything if I just pick a &amp;quot;top 8&amp;quot; or &amp;quot;top 10&amp;quot; models that are &amp;quot;the hottest new versions&amp;quot; that bench well / get good overall reviews and call it good for casual use to just use those and stop worrying about older / other stuff since it&amp;#39;s getting too hard to keep up\nwith all the old models, all the new models.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s (LLM) a utilitarian casual part time tool not a job issue to keep on top of what niche model X is better than model Y in over time and it&amp;#39;s getting impossible to keep straight what a &amp;quot;go to&amp;quot; model list should look like if it&amp;#39;s not just a few overall winners.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maoody",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/n5g7762/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753631198,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5g0kqo",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "custodiam99",
            "can_mod_post": false,
            "created_utc": 1753629204,
            "send_replies": true,
            "parent_id": "t3_1maoody",
            "score": 3,
            "author_fullname": "t2_nqnhgqqf5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I can't really use any older models. I use Qwen3 14b, 32b and 235b now and Gemma3 12b and 27b, but rarely. Oh yeah, QwQ 32b is still nice, but very slow.",
            "edited": 1753629413,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5g0kqo",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t really use any older models. I use Qwen3 14b, 32b and 235b now and Gemma3 12b and 27b, but rarely. Oh yeah, QwQ 32b is still nice, but very slow.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/n5g0kqo/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753629204,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maoody",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5g5xfs",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Calcidiol",
                      "can_mod_post": false,
                      "created_utc": 1753630824,
                      "send_replies": true,
                      "parent_id": "t1_n5g1ru7",
                      "score": 1,
                      "author_fullname": "t2_tailqi90",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Sure, if a model works \"well enough\" to satisfy any need then it'll keep being that good forever and a practical solution.\n\nMy question is though whether at some point we've broadly reached that newer more modern models tend to have become overall almost generally superior to older \"generations\" of models so for anything older models could do, newer ones might (I'm asking where this logic breaks down / has big exceptions) do all that and more better quality / performance / whatever.\n\nSome problems are just pass / fail and models either work or not regardless of type / age.  But many are more qualitative \"give me grammar suggestions on my document\", \"translate this document to my language\", \"write clean code to implement X program\" and one gets qualitatively different / better results depending on what model you ask to do a given task, many solutions may be useful, but probably some particular capability / results will be outstanding / superior over others.\n\nAnd saving models has an opportunity cost (N TBy storage, maintaining usage configurations, testing / comparing them A vs B vs C over time, etc.) so in many ways it's easier if one can simplify and just say that except X, Y, Z exceptions anything in LLM/VLM category from 2022-2023, 1H 2024 is just about always going to be not better than a similar size local open model from later generations / makers.  But I'm sure there are exceptions and nuances, so I'm asking but at some point one can't maintain a data center with everything historically made that was once good for X, Y, Z if there's no reason to prefer that vs. something more capable and modern.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5g5xfs",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sure, if a model works &amp;quot;well enough&amp;quot; to satisfy any need then it&amp;#39;ll keep being that good forever and a practical solution.&lt;/p&gt;\n\n&lt;p&gt;My question is though whether at some point we&amp;#39;ve broadly reached that newer more modern models tend to have become overall almost generally superior to older &amp;quot;generations&amp;quot; of models so for anything older models could do, newer ones might (I&amp;#39;m asking where this logic breaks down / has big exceptions) do all that and more better quality / performance / whatever.&lt;/p&gt;\n\n&lt;p&gt;Some problems are just pass / fail and models either work or not regardless of type / age.  But many are more qualitative &amp;quot;give me grammar suggestions on my document&amp;quot;, &amp;quot;translate this document to my language&amp;quot;, &amp;quot;write clean code to implement X program&amp;quot; and one gets qualitatively different / better results depending on what model you ask to do a given task, many solutions may be useful, but probably some particular capability / results will be outstanding / superior over others.&lt;/p&gt;\n\n&lt;p&gt;And saving models has an opportunity cost (N TBy storage, maintaining usage configurations, testing / comparing them A vs B vs C over time, etc.) so in many ways it&amp;#39;s easier if one can simplify and just say that except X, Y, Z exceptions anything in LLM/VLM category from 2022-2023, 1H 2024 is just about always going to be not better than a similar size local open model from later generations / makers.  But I&amp;#39;m sure there are exceptions and nuances, so I&amp;#39;m asking but at some point one can&amp;#39;t maintain a data center with everything historically made that was once good for X, Y, Z if there&amp;#39;s no reason to prefer that vs. something more capable and modern.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1maoody",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/n5g5xfs/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753630824,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5g1ru7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BusRevolutionary9893",
            "can_mod_post": false,
            "created_utc": 1753629570,
            "send_replies": true,
            "parent_id": "t3_1maoody",
            "score": 2,
            "author_fullname": "t2_1by73qs5e5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That's not a thing. Why would models get worse in any area? The best models are iterations of the best models from 2023-2024. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5g1ru7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s not a thing. Why would models get worse in any area? The best models are iterations of the best models from 2023-2024. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/n5g1ru7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753629570,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maoody",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5gt16x",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Awwtifishal",
            "can_mod_post": false,
            "created_utc": 1753637563,
            "send_replies": true,
            "parent_id": "t3_1maoody",
            "score": 1,
            "author_fullname": "t2_1d96a8k10t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For general purpose tasks it makes no sense to use models older than Llama 3.3 70B. Or for vision, anything older than Qwen 2.5 VL 72B. All of them before that have been surpassed by other models released since.\n\nPurpose specific fine tunes is a different story. For example, some people still enjoy older roleplay fine tunes, because newer ones may be better overall but lack some quality they liked about some older releases.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5gt16x",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For general purpose tasks it makes no sense to use models older than Llama 3.3 70B. Or for vision, anything older than Qwen 2.5 VL 72B. All of them before that have been surpassed by other models released since.&lt;/p&gt;\n\n&lt;p&gt;Purpose specific fine tunes is a different story. For example, some people still enjoy older roleplay fine tunes, because newer ones may be better overall but lack some quality they liked about some older releases.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/n5gt16x/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753637563,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maoody",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hl2mt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "randomqhacker",
            "can_mod_post": false,
            "created_utc": 1753645946,
            "send_replies": true,
            "parent_id": "t3_1maoody",
            "score": 1,
            "author_fullname": "t2_4nw3v",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've been enjoying dots.llm1 for casual reference / conversation, perhaps because it is only trained on human data (which will probably get more and more rare as time goes on).  I've heard some people here say the old Mistral Small 22B was better for fiction/RP than the newer 24B versions which are more focused on benchmarkable skills.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hl2mt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been enjoying dots.llm1 for casual reference / conversation, perhaps because it is only trained on human data (which will probably get more and more rare as time goes on).  I&amp;#39;ve heard some people here say the old Mistral Small 22B was better for fiction/RP than the newer 24B versions which are more focused on benchmarkable skills.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1maoody/llm_vlm_local_model_obsolescence_decisions_for/n5hl2mt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753645946,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1maoody",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]