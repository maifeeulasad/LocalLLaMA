[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "This is less related to models, and more related to model interactions, but would love for the community to offer feedback on an internal debate.\n\nWe see a lot of traffic flow through our oss edge/service proxy for LLM-based apps. This includes local models served via vLLM and Ollama. One failure mode that most recently tripped us up (as we scaled deployments of [archgw](https://github.com/katanemo/archgw) at a F500 telco) were transient errors in streaming LLM responses. Specifically, if the upstream LLM hangs midstream (this could be an API-based LLM or a local model running via vLLM or ollama) while streaming we fail rather painfully today. \n\nBy default we have timeouts for connections made upstream and backoff/retry policies, But that resiliency logic doesn't incorporate the more nuanced failure modes where LLMs can hang mid stream, and then the retry behavior isn't obvious. Here are two immediate strategies we are debating, and would love the feedback:\n\n1/ If we detect the stream to be hung for say X seconds, we could buffer the state up until that point, reconstruct the assistant messages and try again. This would replay the state back to the LLM up until that point and have it try generate its messages from that point. For example, lets say we are calling the chat.completions endpoint, with the following user message:\n\n*{\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},*\n\nAnd mid stream the LLM hangs at this point\n\n*\\[{\"type\": \"text\", \"text\": \"The best answer is (\"}\\]*\n\nWe could then try with the following message to the upstream LLM\n\n*\\[*  \n*{\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},*  \n*{\"role\": \"assistant\", \"content\": \"The best answer is (\"}*  \n*\\]*\n\nWhich would result in a response like\n\n*\\[{\"type\": \"text\", \"text\": \"B)\"}\\]*\n\nThis would be elegant, but we'll have to contend with potentially long buffer sizes, image content (although that is base64'd) and iron out any gotchas with how we use multiplexing to reduce connection overhead. But because the stream replay is stateful, I am not sure if we will expose ourselves to different downstream issues.\n\n2/ fail hard, and don't retry. Two options here a) simply to break the connection upstream and have the client handle the error like a fatal failures or b) send a streaming error event. We could end up sending something like:  \n*event: error*  \n*data: {\"error\":\"502 Bad Gateway\", \"message\":\"upstream failure\"}*\n\nBecause we would have already send partial data to the upstream client, we won't be able to modify the HTTP response code to 502. There are trade offs on both approaches, but from a great developer experience vs. control and visibility where would you lean and why?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Strategies for handling transient Server-Sent Events (SSE) from LLM responses",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mab16n",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_gwq7fd01b",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753583164,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;This is less related to models, and more related to model interactions, but would love for the community to offer feedback on an internal debate.&lt;/p&gt;\n\n&lt;p&gt;We see a lot of traffic flow through our oss edge/service proxy for LLM-based apps. This includes local models served via vLLM and Ollama. One failure mode that most recently tripped us up (as we scaled deployments of &lt;a href=\"https://github.com/katanemo/archgw\"&gt;archgw&lt;/a&gt; at a F500 telco) were transient errors in streaming LLM responses. Specifically, if the upstream LLM hangs midstream (this could be an API-based LLM or a local model running via vLLM or ollama) while streaming we fail rather painfully today. &lt;/p&gt;\n\n&lt;p&gt;By default we have timeouts for connections made upstream and backoff/retry policies, But that resiliency logic doesn&amp;#39;t incorporate the more nuanced failure modes where LLMs can hang mid stream, and then the retry behavior isn&amp;#39;t obvious. Here are two immediate strategies we are debating, and would love the feedback:&lt;/p&gt;\n\n&lt;p&gt;1/ If we detect the stream to be hung for say X seconds, we could buffer the state up until that point, reconstruct the assistant messages and try again. This would replay the state back to the LLM up until that point and have it try generate its messages from that point. For example, lets say we are calling the chat.completions endpoint, with the following user message:&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the Greek name for Sun? (A) Sol (B) Helios (C) Sun&amp;quot;},&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;And mid stream the LLM hangs at this point&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[{&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;The best answer is (&amp;quot;}]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;We could then try with the following message to the upstream LLM&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;What&amp;#39;s the Greek name for Sun? (A) Sol (B) Helios (C) Sun&amp;quot;},&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;{&amp;quot;role&amp;quot;: &amp;quot;assistant&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;The best answer is (&amp;quot;}&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Which would result in a response like&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;[{&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;B)&amp;quot;}]&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;This would be elegant, but we&amp;#39;ll have to contend with potentially long buffer sizes, image content (although that is base64&amp;#39;d) and iron out any gotchas with how we use multiplexing to reduce connection overhead. But because the stream replay is stateful, I am not sure if we will expose ourselves to different downstream issues.&lt;/p&gt;\n\n&lt;p&gt;2/ fail hard, and don&amp;#39;t retry. Two options here a) simply to break the connection upstream and have the client handle the error like a fatal failures or b) send a streaming error event. We could end up sending something like:&lt;br/&gt;\n&lt;em&gt;event: error&lt;/em&gt;&lt;br/&gt;\n&lt;em&gt;data: {&amp;quot;error&amp;quot;:&amp;quot;502 Bad Gateway&amp;quot;, &amp;quot;message&amp;quot;:&amp;quot;upstream failure&amp;quot;}&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Because we would have already send partial data to the upstream client, we won&amp;#39;t be able to modify the HTTP response code to 502. There are trade offs on both approaches, but from a great developer experience vs. control and visibility where would you lean and why?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?auto=webp&amp;s=36b4d1b414cc6cee73ea40ffdc363688d2e9e7d3",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e36c3744ed0552e2b02b7b82f9390cbd418feb4b",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a73059070937e3d39089f8f5256a83264a7c1f37",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6131ad0d3e101ec5b9f07464fcac56396cf950af",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8391306b7ffc6166dc332548393944370f7db6bf",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=86e9a448892e718c7d223c1f239342afc88a6815",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=06fa6467d8bacb728da4ddc3aca382a33fc9beb7",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "0ZWoFzGweGNW0rtaFiJo7cgwtA2lmAaS7it_7nc7p60"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mab16n",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "AdditionalWeb107",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/",
            "subreddit_subscribers": 505616,
            "created_utc": 1753583164,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5iyizh",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Zc5Gwu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5dnbw2",
                                "score": 1,
                                "author_fullname": "t2_67qrvlir",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "As a developer, I think I would find option 1 surprising behavior but option 2 would make sense to me.\n\nFor option 1, it’s not necessarily clear whether there is a difference in behavior when the assistant’s message is fully supplied or partially supplied.\n\nI think that assistant prefilled implies that you supply the “whole” message but I could be wrong.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5iyizh",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As a developer, I think I would find option 1 surprising behavior but option 2 would make sense to me.&lt;/p&gt;\n\n&lt;p&gt;For option 1, it’s not necessarily clear whether there is a difference in behavior when the assistant’s message is fully supplied or partially supplied.&lt;/p&gt;\n\n&lt;p&gt;I think that assistant prefilled implies that you supply the “whole” message but I could be wrong.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mab16n",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/n5iyizh/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753662274,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753662274,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5dnbw2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AdditionalWeb107",
                      "can_mod_post": false,
                      "created_utc": 1753589114,
                      "send_replies": true,
                      "parent_id": "t1_n5dm7fk",
                      "score": 1,
                      "author_fullname": "t2_gwq7fd01b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "So some models offer \"assistant pre-fill\" like Claude. And more and more of the industry is headed that way. But you are correct to point out that not all models will do that deterministically, but we can do one best effort and if there is suffix overlap then we can simply discard the request and error out. This way we can offer a better user experience without it being too token expensive. Or be specific about named providers that offer that functionality. \n\nAgree that the latter has more predictability. And yes SSE is only for UX to make the LLM response appear responsive to the user, so the strategies shared above only make sense in the context of stream =true. \n\nWould it be helpful to give this as a config option and let developers choose? Feels the complexity might not be worth the effort - but also there is some developer delight in there, where they don't have to figure out this low-level logic and it \"just works\"",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5dnbw2",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So some models offer &amp;quot;assistant pre-fill&amp;quot; like Claude. And more and more of the industry is headed that way. But you are correct to point out that not all models will do that deterministically, but we can do one best effort and if there is suffix overlap then we can simply discard the request and error out. This way we can offer a better user experience without it being too token expensive. Or be specific about named providers that offer that functionality. &lt;/p&gt;\n\n&lt;p&gt;Agree that the latter has more predictability. And yes SSE is only for UX to make the LLM response appear responsive to the user, so the strategies shared above only make sense in the context of stream =true. &lt;/p&gt;\n\n&lt;p&gt;Would it be helpful to give this as a config option and let developers choose? Feels the complexity might not be worth the effort - but also there is some developer delight in there, where they don&amp;#39;t have to figure out this low-level logic and it &amp;quot;just works&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mab16n",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/n5dnbw2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753589114,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5dm7fk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "weizien",
            "can_mod_post": false,
            "created_utc": 1753588604,
            "send_replies": true,
            "parent_id": "t3_1mab16n",
            "score": 2,
            "author_fullname": "t2_jn6lu8rpm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Does option 1 really guarantee the LLM actually finish whatever it supposed to send or it end up reconstructing the sentence again? Is this a behavior for all models? If you can’t guarantee that, then the 2nd option is a better option. It’s better to error and let the client/caller to implement retry instead of server end. This is more of UX issue as well so giving the client retry will allow client to control the UX. Streaming is mainly for UX because  I’m pretty sure if it’s pure backend to backend call, it’s almost no reason to use streaming unless you are trying to find a keyword and it’s super time sensitive",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5dm7fk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does option 1 really guarantee the LLM actually finish whatever it supposed to send or it end up reconstructing the sentence again? Is this a behavior for all models? If you can’t guarantee that, then the 2nd option is a better option. It’s better to error and let the client/caller to implement retry instead of server end. This is more of UX issue as well so giving the client retry will allow client to control the UX. Streaming is mainly for UX because  I’m pretty sure if it’s pure backend to backend call, it’s almost no reason to use streaming unless you are trying to find a keyword and it’s super time sensitive&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mab16n/strategies_for_handling_transient_serversent/n5dm7fk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753588604,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mab16n",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]