[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I've been working with artificial intelligence, It's been working for the past few months but I've gotten annoyed with the performance. I've just switched to **IK\\_llama.cpp**, and I'm looking to optimize my command although I haven't found any documentation. I've managed to get it around **12-15 t/s** (quite good, but I'm looking to see if it can get better &lt;3)  \nSorry if it's alot, I'm just asking somebody to help create an optimized command for running the models.\n\n\\[**specs**\\]: RTX 3060 12gb, 16gb ram, ryzen 5 2600.  \n\\[**I'm using IK\\_llama.cpp on arch linux**\\]\n\n\\[**model**\\] latest Qwen3 30B-A3B",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How to optimize TPS using IK_llama.cpp?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": true,
            "name": "t3_1mdlss2",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_11jbo8mpsx",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753918723,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753917687,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve been working with artificial intelligence, It&amp;#39;s been working for the past few months but I&amp;#39;ve gotten annoyed with the performance. I&amp;#39;ve just switched to &lt;strong&gt;IK_llama.cpp&lt;/strong&gt;, and I&amp;#39;m looking to optimize my command although I haven&amp;#39;t found any documentation. I&amp;#39;ve managed to get it around &lt;strong&gt;12-15 t/s&lt;/strong&gt; (quite good, but I&amp;#39;m looking to see if it can get better &amp;lt;3)&lt;br/&gt;\nSorry if it&amp;#39;s alot, I&amp;#39;m just asking somebody to help create an optimized command for running the models.&lt;/p&gt;\n\n&lt;p&gt;[&lt;strong&gt;specs&lt;/strong&gt;]: RTX 3060 12gb, 16gb ram, ryzen 5 2600.&lt;br/&gt;\n[&lt;strong&gt;I&amp;#39;m using IK_llama.cpp on arch linux&lt;/strong&gt;]&lt;/p&gt;\n\n&lt;p&gt;[&lt;strong&gt;model&lt;/strong&gt;] latest Qwen3 30B-A3B&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdlss2",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Final-Message2150",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdlss2/how_to_optimize_tps_using_ik_llamacpp/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdlss2/how_to_optimize_tps_using_ik_llamacpp/",
            "subreddit_subscribers": 507274,
            "created_utc": 1753917687,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n62pjfx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "un_passant",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n62mtjl",
                                "score": 1,
                                "author_fullname": "t2_7rqtc",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "which quant ?\n\nYou should get one from [https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF](https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF)\n\nor [https://huggingface.co/ubergarm/Qwen3-30B-A3B-Instruct-2507-GGUF](https://huggingface.co/ubergarm/Qwen3-30B-A3B-Instruct-2507-GGUF)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n62pjfx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;which quant ?&lt;/p&gt;\n\n&lt;p&gt;You should get one from &lt;a href=\"https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF\"&gt;https://huggingface.co/ubergarm/Qwen3-30B-A3B-Thinking-2507-GGUF&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;or &lt;a href=\"https://huggingface.co/ubergarm/Qwen3-30B-A3B-Instruct-2507-GGUF\"&gt;https://huggingface.co/ubergarm/Qwen3-30B-A3B-Instruct-2507-GGUF&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdlss2",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdlss2/how_to_optimize_tps_using_ik_llamacpp/n62pjfx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753919596,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753919596,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n62mtjl",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Final-Message2150",
                      "can_mod_post": false,
                      "created_utc": 1753918698,
                      "send_replies": true,
                      "parent_id": "t1_n62lk7c",
                      "score": 1,
                      "author_fullname": "t2_11jbo8mpsx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I'm sorry I've completely forgotten to provide the model I was running, which is qwen3 30b-a3b\n\nediting it now\n\nI'll also check it out",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n62mtjl",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m sorry I&amp;#39;ve completely forgotten to provide the model I was running, which is qwen3 30b-a3b&lt;/p&gt;\n\n&lt;p&gt;editing it now&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll also check it out&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdlss2",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdlss2/how_to_optimize_tps_using_ik_llamacpp/n62mtjl/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753918698,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n62lk7c",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lissanro",
            "can_mod_post": false,
            "created_utc": 1753918285,
            "send_replies": true,
            "parent_id": "t3_1mdlss2",
            "score": 1,
            "author_fullname": "t2_fpfao9g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I shared [here](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) how to get started with ik\\_llama.cpp from git cloning and building to usage examples. Even if you already figured out building part, it is still may worth it to double check that you are using right build options for the best performance.\n\nAlso, you speed will greatly depend on a model you are running, and what quant. On your rig, you may consider running something like Qwen3 30B-A3B (for example, IQ3 quant with partial offloading to RAM) for the best ratio of performance/quality, or try 14B or smaller models if it turns out to be too heavy.",
            "edited": 1753918559,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n62lk7c",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I shared &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\"&gt;here&lt;/a&gt; how to get started with ik_llama.cpp from git cloning and building to usage examples. Even if you already figured out building part, it is still may worth it to double check that you are using right build options for the best performance.&lt;/p&gt;\n\n&lt;p&gt;Also, you speed will greatly depend on a model you are running, and what quant. On your rig, you may consider running something like Qwen3 30B-A3B (for example, IQ3 quant with partial offloading to RAM) for the best ratio of performance/quality, or try 14B or smaller models if it turns out to be too heavy.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdlss2/how_to_optimize_tps_using_ik_llamacpp/n62lk7c/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753918285,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdlss2",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]