[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Prompt processing isn't as simple as token generation (memory bandwidth/active parameter size). Are there any good sources on that (I suspect there is no simple answer)?\n\nIt depends on TFlops of the GPU, architecture etc.\n\nWorse, how does it depend when only part of model is on GPUs VRAM, and part is on CPUs RAM? How it depends when KV cache is offloaded to GPU and when not (e.g. --no-kv-offload in llama.cpp)?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How to estimate prompt processing speed for given (multi-)GPU and model?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m80kuh",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_jti45lwl",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753352215,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Prompt processing isn&amp;#39;t as simple as token generation (memory bandwidth/active parameter size). Are there any good sources on that (I suspect there is no simple answer)?&lt;/p&gt;\n\n&lt;p&gt;It depends on TFlops of the GPU, architecture etc.&lt;/p&gt;\n\n&lt;p&gt;Worse, how does it depend when only part of model is on GPUs VRAM, and part is on CPUs RAM? How it depends when KV cache is offloaded to GPU and when not (e.g. --no-kv-offload in llama.cpp)?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m80kuh",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "EmilPi",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m80kuh/how_to_estimate_prompt_processing_speed_for_given/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m80kuh/how_to_estimate_prompt_processing_speed_for_given/",
            "subreddit_subscribers": 504023,
            "created_utc": 1753352215,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n4y10u4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FlippNdip",
                      "can_mod_post": false,
                      "created_utc": 1753381110,
                      "send_replies": true,
                      "parent_id": "t1_n4vneps",
                      "score": 2,
                      "author_fullname": "t2_5kxn6beg",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This may be a little naive but why can't the GPU do all the processing even if some of the model layers are in system RAM?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n4y10u4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This may be a little naive but why can&amp;#39;t the GPU do all the processing even if some of the model layers are in system RAM?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m80kuh",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m80kuh/how_to_estimate_prompt_processing_speed_for_given/n4y10u4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753381110,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n4vneps",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lly0571",
            "can_mod_post": false,
            "created_utc": 1753355127,
            "send_replies": true,
            "parent_id": "t3_1m80kuh",
            "score": 3,
            "author_fullname": "t2_70vzcleel",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You need roughly 2x \\[parameter count\\] x \\[sequence length\\] FLOPs in prefill.\n\neg: For processing 1024 tokens with a 32.5B dense model, 32.5x10^9 *2 %*1024=66.56TFLOPs are needed. I think you can use about 40%(llama3 tech report) to 70%(From [FA2 paper](https://tridao.me/publications/flash2/flash2.pdf)) of your GPU's FP16 FLOPs if you use GPU only.\n\nIf you offload several layers to GPU, you should count each part of the models separately. If you use `-ot`, that could be hard to compute as there are more PCIe communication bottleneck.",
            "edited": 1753355318,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n4vneps",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You need roughly 2x [parameter count] x [sequence length] FLOPs in prefill.&lt;/p&gt;\n\n&lt;p&gt;eg: For processing 1024 tokens with a 32.5B dense model, 32.5x10&lt;sup&gt;9&lt;/sup&gt; &lt;em&gt;2 %&lt;/em&gt;1024=66.56TFLOPs are needed. I think you can use about 40%(llama3 tech report) to 70%(From &lt;a href=\"https://tridao.me/publications/flash2/flash2.pdf\"&gt;FA2 paper&lt;/a&gt;) of your GPU&amp;#39;s FP16 FLOPs if you use GPU only.&lt;/p&gt;\n\n&lt;p&gt;If you offload several layers to GPU, you should count each part of the models separately. If you use &lt;code&gt;-ot&lt;/code&gt;, that could be hard to compute as there are more PCIe communication bottleneck.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m80kuh/how_to_estimate_prompt_processing_speed_for_given/n4vneps/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753355127,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m80kuh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]