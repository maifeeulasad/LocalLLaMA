[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Apart from RAM &amp; GPU upgrades. I use Jan &amp; Kobaldcpp.\n\nFound few things from online on this.\n\n* Picking Quantized model fittable to System VRAM\n* Set Q8\\_0(instead of 16) for KV Cache\n* Use Recommended Settings(Temperature, TopP, TopK, MinP) for models(Mostly from Model cards on HuggingFace)\n* Decent Prompts\n\nWhat else could help to get faster response with some more tokens?\n\nI'm not expecting too much for my 8GB VRAM(32 GB RAM), just even another bunch of additional tokens fine for me.\n\nSystem Spec : Intel(R) Core(TM) i7-14700HX 2.10 GHz NVIDIA GeForce RTX 4060\n\nTried below simple prompt to test some models with Context 32768, GPU Layers -1:\n\nTemperature 0.7, TopK 20, TopP 0.8, MinP 0.\n\nwho are you? Provide all details about you /no\\_think\n\n* Qwen3 0.6B Q8 - 120 tokens/sec (Typically **70-80** tokens/sec)\n* Qwen3 1.7B Q8 -   65 tokens/sec (Typically **50-60** tokens/sec)\n* Qwen3 4B Q6   -   25 tokens/sec (Typically **20** tokens/sec)\n* Qwen3 8B Q4   -   10 tokens/sec (Typically **7-9** tokens/sec)\n* Qwen3 30B A3B Q4 - 2 tokens/sec (Typically **1** tokens/sec)\n\nPoor GPU Club members(\\~8GB VRAM) .... Are you getting similar tokens/sec? If you're getting more tokens, what have you done for that? please share.\n\nI'm sure I'm doing something wrong on few things here, please help me on this. Thanks.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How to increase tps Tokens/Second? Other ways to optimize things to get faster response",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mau1nz",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1deiadfhb1",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1753642651,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753641743,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Apart from RAM &amp;amp; GPU upgrades. I use Jan &amp;amp; Kobaldcpp.&lt;/p&gt;\n\n&lt;p&gt;Found few things from online on this.&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Picking Quantized model fittable to System VRAM&lt;/li&gt;\n&lt;li&gt;Set Q8_0(instead of 16) for KV Cache&lt;/li&gt;\n&lt;li&gt;Use Recommended Settings(Temperature, TopP, TopK, MinP) for models(Mostly from Model cards on HuggingFace)&lt;/li&gt;\n&lt;li&gt;Decent Prompts&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;What else could help to get faster response with some more tokens?&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not expecting too much for my 8GB VRAM(32 GB RAM), just even another bunch of additional tokens fine for me.&lt;/p&gt;\n\n&lt;p&gt;System Spec : Intel(R) Core(TM) i7-14700HX 2.10 GHz NVIDIA GeForce RTX 4060&lt;/p&gt;\n\n&lt;p&gt;Tried below simple prompt to test some models with Context 32768, GPU Layers -1:&lt;/p&gt;\n\n&lt;p&gt;Temperature 0.7, TopK 20, TopP 0.8, MinP 0.&lt;/p&gt;\n\n&lt;p&gt;who are you? Provide all details about you /no_think&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Qwen3 0.6B Q8 - 120 tokens/sec (Typically &lt;strong&gt;70-80&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 1.7B Q8 -   65 tokens/sec (Typically &lt;strong&gt;50-60&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 4B Q6   -   25 tokens/sec (Typically &lt;strong&gt;20&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 8B Q4   -   10 tokens/sec (Typically &lt;strong&gt;7-9&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;li&gt;Qwen3 30B A3B Q4 - 2 tokens/sec (Typically &lt;strong&gt;1&lt;/strong&gt; tokens/sec)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Poor GPU Club members(~8GB VRAM) .... Are you getting similar tokens/sec? If you&amp;#39;re getting more tokens, what have you done for that? please share.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m sure I&amp;#39;m doing something wrong on few things here, please help me on this. Thanks.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mau1nz",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "pmttyji",
            "discussion_type": null,
            "num_comments": 17,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/",
            "subreddit_subscribers": 505617,
            "created_utc": 1753641743,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5hcocw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "LagOps91",
                      "can_mod_post": false,
                      "created_utc": 1753643375,
                      "send_replies": true,
                      "parent_id": "t1_n5h9jod",
                      "score": 5,
                      "author_fullname": "t2_3wi6j7vwh",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "to make use of this feature, you need to supply a command line argument that contains a regex to specify where to load certain tensors to. in kobold cpp you can also directly entire it in the \"tokens\" tab under \"overwrite tensors\". this needs to be combined with loading all layers on gpu (basically you specify what you \\*dont\\* want to have on gpu with the regex)\n\na regex can look like this:\n\n\\--ot \"blk\\\\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51).ffn\\_.\\*\\_exps.=CPU\"\n\nhere you keep all shared weights including kv cache on gpu and put all experts onto the cpu.\n\nthis is a starting point for further optimization. you can check how much space you have left on gpu and then reduce the amount of layers where you offload the expert weights to cpu/ram. simply remove some layers in the overwrite tensor regex until you properly utilize your gpu.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5hcocw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;to make use of this feature, you need to supply a command line argument that contains a regex to specify where to load certain tensors to. in kobold cpp you can also directly entire it in the &amp;quot;tokens&amp;quot; tab under &amp;quot;overwrite tensors&amp;quot;. this needs to be combined with loading all layers on gpu (basically you specify what you *dont* want to have on gpu with the regex)&lt;/p&gt;\n\n&lt;p&gt;a regex can look like this:&lt;/p&gt;\n\n&lt;p&gt;--ot &amp;quot;blk\\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51).ffn_.*_exps.=CPU&amp;quot;&lt;/p&gt;\n\n&lt;p&gt;here you keep all shared weights including kv cache on gpu and put all experts onto the cpu.&lt;/p&gt;\n\n&lt;p&gt;this is a starting point for further optimization. you can check how much space you have left on gpu and then reduce the amount of layers where you offload the expert weights to cpu/ram. simply remove some layers in the overwrite tensor regex until you properly utilize your gpu.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mau1nz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hcocw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753643375,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5h9jod",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753642427,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 4,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can offload specifc tensors to ram to increase performance instead of just offloading a certain amount of layers. it has little impact for dense models, but it's worthwhile when using MoE models.\n\nQwen 30B A3 should run much faster on your system! this is likely because you didn't offload specific tensors and have your KV Cache split between VRAM and RAM.\n\nI would expect Qwen 3 30B A3 (or other comparable small MoE models) to make the best out of your hardware. Qwen 3 30B A3 i would expect to run with 10+ t/s.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5h9jod",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can offload specifc tensors to ram to increase performance instead of just offloading a certain amount of layers. it has little impact for dense models, but it&amp;#39;s worthwhile when using MoE models.&lt;/p&gt;\n\n&lt;p&gt;Qwen 30B A3 should run much faster on your system! this is likely because you didn&amp;#39;t offload specific tensors and have your KV Cache split between VRAM and RAM.&lt;/p&gt;\n\n&lt;p&gt;I would expect Qwen 3 30B A3 (or other comparable small MoE models) to make the best out of your hardware. Qwen 3 30B A3 i would expect to run with 10+ t/s.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5h9jod/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753642427,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hd7kb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753643536,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 2,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "you can also reduce memory footprint a bit by using flash attention (reduces prompt processing speed noticably for me) and by reducing BLAS Batch Size in the hardware tab of kobold cpp. reducing it typically reduces prompt processing by a small amount, but also reduces memory footprint. the default of 512 is a bit high imo, i typically go with 256.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hd7kb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;you can also reduce memory footprint a bit by using flash attention (reduces prompt processing speed noticably for me) and by reducing BLAS Batch Size in the hardware tab of kobold cpp. reducing it typically reduces prompt processing by a small amount, but also reduces memory footprint. the default of 512 is a bit high imo, i typically go with 256.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hd7kb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753643536,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5i2eb6",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "kironlau",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5i1ce3",
                                          "score": 1,
                                          "author_fullname": "t2_tb0dz2ds",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "i am using in Window CMD, if you're using Linux, replace '\\^\" with \"\\\\\" at the end of each line",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5i2eb6",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i am using in Window CMD, if you&amp;#39;re using Linux, replace &amp;#39;^&amp;quot; with &amp;quot;\\&amp;quot; at the end of each line&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mau1nz",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5i2eb6/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753651268,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753651268,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5i1ce3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kironlau",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5hoimx",
                                "score": 1,
                                "author_fullname": "t2_tb0dz2ds",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "just more or less same as mainline llama.cpp, I just use llama-server.exe to host an open-ai format API\n\ncommand as like:  \n\\`\\`\\`  \n.\\\\ik\\_llama-bin-win-cuda-12.8-x64-avx2\\\\llama-server \\^\n\n  \\--model \"G:\\\\lm-studio\\\\models\\\\ubergarm\\\\Qwen3-30B-A3B-GGUF\\\\Qwen3-30B-A3B-mix-IQ4\\_K.gguf\" \\^\n\n  \\--alias Qwen/Qwen3-30B-A3B \\^\n\n  \\-fa \\^\n\n  \\-c 32768 \\^\n\n  \\-ctk q8\\_0 -ctv q8\\_0 \\^\n\n  \\-fmoe \\^\n\n  \\-rtr \\^\n\n  \\-ot \"blk\\\\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23)\\\\.ffn.\\*exps=CUDA0\" \\^\n\n  \\-ot exps=CPU\\^\n\n  \\-ngl 99\\^\n\n  \\--threads 8 \\^\n\n  \\--port 8080\n\n\\`\\`\\`\n\nthen, you could use any gui supporting for LLM completions",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5i1ce3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;just more or less same as mainline llama.cpp, I just use llama-server.exe to host an open-ai format API&lt;/p&gt;\n\n&lt;p&gt;command as like:&lt;br/&gt;\n```&lt;br/&gt;\n.\\ik_llama-bin-win-cuda-12.8-x64-avx2\\llama-server ^&lt;/p&gt;\n\n&lt;p&gt;--model &amp;quot;G:\\lm-studio\\models\\ubergarm\\Qwen3-30B-A3B-GGUF\\Qwen3-30B-A3B-mix-IQ4_K.gguf&amp;quot; ^&lt;/p&gt;\n\n&lt;p&gt;--alias Qwen/Qwen3-30B-A3B ^&lt;/p&gt;\n\n&lt;p&gt;-fa ^&lt;/p&gt;\n\n&lt;p&gt;-c 32768 ^&lt;/p&gt;\n\n&lt;p&gt;-ctk q8_0 -ctv q8_0 ^&lt;/p&gt;\n\n&lt;p&gt;-fmoe ^&lt;/p&gt;\n\n&lt;p&gt;-rtr ^&lt;/p&gt;\n\n&lt;p&gt;-ot &amp;quot;blk\\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23)\\.ffn.*exps=CUDA0&amp;quot; ^&lt;/p&gt;\n\n&lt;p&gt;-ot exps=CPU^&lt;/p&gt;\n\n&lt;p&gt;-ngl 99^&lt;/p&gt;\n\n&lt;p&gt;--threads 8 ^&lt;/p&gt;\n\n&lt;p&gt;--port 8080&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;then, you could use any gui supporting for LLM completions&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mau1nz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5i1ce3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753650933,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753650933,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5i26bb",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kironlau",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5hoimx",
                                "score": 1,
                                "author_fullname": "t2_tb0dz2ds",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "a precomple release (for Win-cuda only): [Release main-b3974-bb4c917 · Thireus/ik\\_llama.cpp](https://github.com/Thireus/ik_llama.cpp/releases/tag/main-b3974-bb4c917)\n\nfor linux, it's easily to compile, here is the guide: [Quick-start Guide coming over from llama.cpp and ktransformers! · ikawrakow/ik\\_llama.cpp · Discussion #258](https://github.com/ikawrakow/ik_llama.cpp/discussions/258)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5i26bb",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;a precomple release (for Win-cuda only): &lt;a href=\"https://github.com/Thireus/ik_llama.cpp/releases/tag/main-b3974-bb4c917\"&gt;Release main-b3974-bb4c917 · Thireus/ik_llama.cpp&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;for linux, it&amp;#39;s easily to compile, here is the guide: &lt;a href=\"https://github.com/ikawrakow/ik_llama.cpp/discussions/258\"&gt;Quick-start Guide coming over from llama.cpp and ktransformers! · ikawrakow/ik_llama.cpp · Discussion #258&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mau1nz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5i26bb/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753651197,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753651197,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5hoimx",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "lacerating_aura",
                      "can_mod_post": false,
                      "created_utc": 1753647008,
                      "send_replies": true,
                      "parent_id": "t1_n5hgdre",
                      "score": 1,
                      "author_fullname": "t2_9nex5np2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Hi, sorry for an off topic question, do you know how to use text completion with llama serve in ik_llama.cpp?\n\nI have it installed and am trying to connect it to silly tavern using text completion, and both communicate when prompted, but the responses generated are empty. If I use chat completion, I get some response, but I would like to keep using text completion.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5hoimx",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi, sorry for an off topic question, do you know how to use text completion with llama serve in ik_llama.cpp?&lt;/p&gt;\n\n&lt;p&gt;I have it installed and am trying to connect it to silly tavern using text completion, and both communicate when prompted, but the responses generated are empty. If I use chat completion, I get some response, but I would like to keep using text completion.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mau1nz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hoimx/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753647008,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5hgdre",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kironlau",
            "can_mod_post": false,
            "created_utc": 1753644504,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 2,
            "author_fullname": "t2_tb0dz2ds",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "use ik\\_llama, I got 30 tokens/sec using Qwen3 30B A3B Q4 (IQ4\\_KS)  \nmy system config : Ryzen 5700x, with ddr4 oc at 3733, rtx 4070 12gb  \nyou should get 15-20 tk/sec at least, even 8gb 4060 (laptop version), if you optimize well (using ik\\_llama for moe)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hgdre",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;use ik_llama, I got 30 tokens/sec using Qwen3 30B A3B Q4 (IQ4_KS)&lt;br/&gt;\nmy system config : Ryzen 5700x, with ddr4 oc at 3733, rtx 4070 12gb&lt;br/&gt;\nyou should get 15-20 tk/sec at least, even 8gb 4060 (laptop version), if you optimize well (using ik_llama for moe)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hgdre/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753644504,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hmrml",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TacGibs",
            "can_mod_post": false,
            "created_utc": 1753646469,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 2,
            "author_fullname": "t2_8w0y7ezw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Use a better inference engine like SGLang or vLLM :)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hmrml",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Use a better inference engine like SGLang or vLLM :)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hmrml/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753646469,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ha8hj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753642633,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 1,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "prompts and sampler settings don't impact inference speed (but impact output length - so technically they do matter a bit). quanting KV cache helps by reducing KV cache size, but it also affects performance. i wouldn't use that option unless i had to, especially for smaller models. in terms of quants, Q5 is recommended for smaller models (12b or below imo) and Q4 is fine for anything else. large models can be good with Q3 or less, but this isn't really relevant for your system as you can't run those anyway.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ha8hj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;prompts and sampler settings don&amp;#39;t impact inference speed (but impact output length - so technically they do matter a bit). quanting KV cache helps by reducing KV cache size, but it also affects performance. i wouldn&amp;#39;t use that option unless i had to, especially for smaller models. in terms of quants, Q5 is recommended for smaller models (12b or below imo) and Q4 is fine for anything else. large models can be good with Q3 or less, but this isn&amp;#39;t really relevant for your system as you can&amp;#39;t run those anyway.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5ha8hj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753642633,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5he357",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753643804,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 1,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You should also always adjust the GPU layers manually. kobold cpp is very conservative here and typically underutilized the hardware quite a bit. simply enter a number and check what happens when you load the model. ideally, you use as much of your VRAM as possible without spilling over into system ram. feel free to make use of the benchmark (under hardware tab) to find the best split.\n\nfor MoE models, use tensor offloading and enter 999 for layer count (load everything on gpu) as described in another comment i made",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5he357",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You should also always adjust the GPU layers manually. kobold cpp is very conservative here and typically underutilized the hardware quite a bit. simply enter a number and check what happens when you load the model. ideally, you use as much of your VRAM as possible without spilling over into system ram. feel free to make use of the benchmark (under hardware tab) to find the best split.&lt;/p&gt;\n\n&lt;p&gt;for MoE models, use tensor offloading and enter 999 for layer count (load everything on gpu) as described in another comment i made&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5he357/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753643804,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hefvt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753643911,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 1,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "32k context can be quite memory heavy depending on the model. consider using 16k context instead or perhaps even 8k depending on your use-case. use this site to find out how costly KV cache is going to be: [https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hefvt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;32k context can be quite memory heavy depending on the model. consider using 16k context instead or perhaps even 8k depending on your use-case. use this site to find out how costly KV cache is going to be: &lt;a href=\"https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator\"&gt;https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hefvt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753643911,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n5iacj9",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Toooooool",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5i7x1t",
                                "score": 1,
                                "author_fullname": "t2_8llornh4",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "interesting.  \ni've been daily driving Q4 KV's for months and only had to occasionally regenerate, you'd for sure advice bumping the KV up to Q8 or even FP16 at the expense of i.e. half the context size?",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5iacj9",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;interesting.&lt;br/&gt;\ni&amp;#39;ve been daily driving Q4 KV&amp;#39;s for months and only had to occasionally regenerate, you&amp;#39;d for sure advice bumping the KV up to Q8 or even FP16 at the expense of i.e. half the context size?&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mau1nz",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5iacj9/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753653804,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753653804,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5i7x1t",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MelodicRecognition7",
                      "can_mod_post": false,
                      "created_utc": 1753653026,
                      "send_replies": true,
                      "parent_id": "t1_n5hpjq8",
                      "score": 2,
                      "author_fullname": "t2_1eex9ug5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "even at Q4 things get wierd, going below Q8 in KV cache strongly not recommended. And I advise even against Q8.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5i7x1t",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;even at Q4 things get wierd, going below Q8 in KV cache strongly not recommended. And I advise even against Q8.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mau1nz",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5i7x1t/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753653026,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5hpjq8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Toooooool",
            "can_mod_post": false,
            "created_utc": 1753647323,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 1,
            "author_fullname": "t2_8llornh4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Run a Q4 model and lower KV Cache to Q4 as well, that's going to be the best balance between speed and size. Below Q4 things get weird and it's generally not worth it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hpjq8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Run a Q4 model and lower KV Cache to Q4 as well, that&amp;#39;s going to be the best balance between speed and size. Below Q4 things get weird and it&amp;#39;s generally not worth it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hpjq8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753647323,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5hvt7d",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1753649222,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 1,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Install MSI Afterburner and pump up the memory clock of your RTX 4060.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5hvt7d",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Install MSI Afterburner and pump up the memory clock of your RTX 4060.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5hvt7d/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753649222,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5irzz9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fooo12gh",
            "can_mod_post": false,
            "created_utc": 1753659891,
            "send_replies": true,
            "parent_id": "t3_1mau1nz",
            "score": 1,
            "author_fullname": "t2_3vvvbmh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Looks like some issue on your side.\n\nI use aforementioned model also on laptop, and tried to run it exclusively on CPU only. In my case, when using pretty much similar parameters - qwen3 30b a3b, q8\\_k\\_xl, 32768 context length - I get \\~10 tokens/second.\n\nI have 8845HS+4060, 2x48gb ddr5 5600mhz, running via LMStudio with default settings except of context length and running completely on CPU, Fedora 42.\n\nq4 gets to 17-19 tokens/second with that setup.\n\nDouble check maybe your RAM - do you use one or two sticks, what speed, maybe some additional settings on them in BIOS (though unlikely). Also you can run some memory speed tests to ensure you have no issues with RAM.",
            "edited": 1753660547,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5irzz9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Looks like some issue on your side.&lt;/p&gt;\n\n&lt;p&gt;I use aforementioned model also on laptop, and tried to run it exclusively on CPU only. In my case, when using pretty much similar parameters - qwen3 30b a3b, q8_k_xl, 32768 context length - I get ~10 tokens/second.&lt;/p&gt;\n\n&lt;p&gt;I have 8845HS+4060, 2x48gb ddr5 5600mhz, running via LMStudio with default settings except of context length and running completely on CPU, Fedora 42.&lt;/p&gt;\n\n&lt;p&gt;q4 gets to 17-19 tokens/second with that setup.&lt;/p&gt;\n\n&lt;p&gt;Double check maybe your RAM - do you use one or two sticks, what speed, maybe some additional settings on them in BIOS (though unlikely). Also you can run some memory speed tests to ensure you have no issues with RAM.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mau1nz/how_to_increase_tps_tokenssecond_other_ways_to/n5irzz9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753659891,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mau1nz",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]