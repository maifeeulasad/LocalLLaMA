[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey[ ](https://www.reddit.com/r/MachineLearning/)r/LocalLLaMA !\n\nA while back, we shared our open-source project LMCache here and were blown away by the incredible support and feedback. Today, our team is thrilled to share more about one of our core components: **CacheBlend**. Recognized with a **Best Paper Award at ACM EuroSys 2025,** this technique is a pain killer for efficient RAG applications \n\n# The Problem: Your KV Cache is Wasting Potential\n\nIn modern LLM applications like RAG and Agents, we constantly feed the model new context. For example, in RAG, we retrieve relevant documents and stuff them into the prompt.\n\nThe issue is that this dynamically retrieved context doesn't always appear at the beginning of the input sequence. Traditional KV caching only reuses a \"common prefix,\" so if the new information isn't at the very start, the cache hit rate plummets, and your GPU ends up recomputing the same things over and over.\n\n**The Solution: CacheBlend - 100% Hit Rate, No Compromises**\n\nCacheBlend changes the game by allowing for the reuse of pre-computed KV caches **regardless of their position in the input sequence**.\n\nThis means we can finally achieve a **100% KV Cache hit rate** in applications like RAG. The performance gains are significant:\n\n* **Faster Time-To-First-Token (TTFT):** Get your initial response much quicker.\n* **More Throughput:** Serve significantly more users with the same hardware.\n* **Almost lossless Output Quality:** All of this is achieved with little degradation in the model's generation quality.\n\n# How does it work?\n\nCacheBlend intelligently handles the two main challenges of reusing non-prefix caches:\n\n1. **Positional Encoding Update:** It efficiently updates positional encodings to ensure the model always knows the correct position of each token, even when we're stitching together cached and new data.\n2. **Selective Attention Recalculation:** Instead of recomputing everything, it strategically recalculates only the minimal cross-attention needed between the new and cached chunks to maintain perfect generation quality.\n\nFor detailed analysis, please refer to the official paper: [https://dl.acm.org/doi/10.1145/3689031.3696098](https://dl.acm.org/doi/10.1145/3689031.3696098)\n\n# Where can I try it?\n\nTry the newest interactive CacheBlend demo at: [https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending](https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending)\n\n\n\nAsk us anything!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Reuse non-prefix KV Cache and speed up RAG by 3X with LMCache.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 77,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lp653l",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.96,
            "author_flair_background_color": null,
            "ups": 131,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_7zeld9y7",
            "secure_media": null,
            "is_reddit_media_domain": true,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 131,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://a.thumbs.redditmedia.com/SsFdwYLG_92_xEbJs207_a0YHI0TAqT7jOZYCY0xnE8.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "image",
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1751387163,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "i.redd.it",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey&lt;a href=\"https://www.reddit.com/r/MachineLearning/\"&gt; &lt;/a&gt;&lt;a href=\"/r/LocalLLaMA\"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt;\n\n&lt;p&gt;A while back, we shared our open-source project LMCache here and were blown away by the incredible support and feedback. Today, our team is thrilled to share more about one of our core components: &lt;strong&gt;CacheBlend&lt;/strong&gt;. Recognized with a &lt;strong&gt;Best Paper Award at ACM EuroSys 2025,&lt;/strong&gt; this technique is a pain killer for efficient RAG applications &lt;/p&gt;\n\n&lt;h1&gt;The Problem: Your KV Cache is Wasting Potential&lt;/h1&gt;\n\n&lt;p&gt;In modern LLM applications like RAG and Agents, we constantly feed the model new context. For example, in RAG, we retrieve relevant documents and stuff them into the prompt.&lt;/p&gt;\n\n&lt;p&gt;The issue is that this dynamically retrieved context doesn&amp;#39;t always appear at the beginning of the input sequence. Traditional KV caching only reuses a &amp;quot;common prefix,&amp;quot; so if the new information isn&amp;#39;t at the very start, the cache hit rate plummets, and your GPU ends up recomputing the same things over and over.&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;The Solution: CacheBlend - 100% Hit Rate, No Compromises&lt;/strong&gt;&lt;/p&gt;\n\n&lt;p&gt;CacheBlend changes the game by allowing for the reuse of pre-computed KV caches &lt;strong&gt;regardless of their position in the input sequence&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;p&gt;This means we can finally achieve a &lt;strong&gt;100% KV Cache hit rate&lt;/strong&gt; in applications like RAG. The performance gains are significant:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;Faster Time-To-First-Token (TTFT):&lt;/strong&gt; Get your initial response much quicker.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;More Throughput:&lt;/strong&gt; Serve significantly more users with the same hardware.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Almost lossless Output Quality:&lt;/strong&gt; All of this is achieved with little degradation in the model&amp;#39;s generation quality.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;h1&gt;How does it work?&lt;/h1&gt;\n\n&lt;p&gt;CacheBlend intelligently handles the two main challenges of reusing non-prefix caches:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;strong&gt;Positional Encoding Update:&lt;/strong&gt; It efficiently updates positional encodings to ensure the model always knows the correct position of each token, even when we&amp;#39;re stitching together cached and new data.&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;Selective Attention Recalculation:&lt;/strong&gt; Instead of recomputing everything, it strategically recalculates only the minimal cross-attention needed between the new and cached chunks to maintain perfect generation quality.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;For detailed analysis, please refer to the official paper: &lt;a href=\"https://dl.acm.org/doi/10.1145/3689031.3696098\"&gt;https://dl.acm.org/doi/10.1145/3689031.3696098&lt;/a&gt;&lt;/p&gt;\n\n&lt;h1&gt;Where can I try it?&lt;/h1&gt;\n\n&lt;p&gt;Try the newest interactive CacheBlend demo at: &lt;a href=\"https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending\"&gt;https://github.com/LMCache/LMCache-Examples/tree/main/demo-rag-blending&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Ask us anything!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://i.redd.it/9eq6ted4haaf1.jpeg",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://preview.redd.it/9eq6ted4haaf1.jpeg?auto=webp&amp;s=46318e6820a5614ae6c6ec05ed3e6d6f5460fa9c",
                    "width": 1600,
                    "height": 885
                  },
                  "resolutions": [
                    {
                      "url": "https://preview.redd.it/9eq6ted4haaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0a0e24393dd73d59d6b136cc461e9599fc9e7f57",
                      "width": 108,
                      "height": 59
                    },
                    {
                      "url": "https://preview.redd.it/9eq6ted4haaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ffa73499cab94425a89ba73fe53be6e2d751620d",
                      "width": 216,
                      "height": 119
                    },
                    {
                      "url": "https://preview.redd.it/9eq6ted4haaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=e673a0d0c0213d54c8d77eeea5c6fb03e7a52bd0",
                      "width": 320,
                      "height": 177
                    },
                    {
                      "url": "https://preview.redd.it/9eq6ted4haaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=576ff3381e11049410b474e07e3ae6108a604a27",
                      "width": 640,
                      "height": 354
                    },
                    {
                      "url": "https://preview.redd.it/9eq6ted4haaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7c2b5202de1dcd13b92bbdea8f3c237e2b968c19",
                      "width": 960,
                      "height": 531
                    },
                    {
                      "url": "https://preview.redd.it/9eq6ted4haaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c5e84cc7aa886c6c0fcfe1d258eeb3323e8c3b39",
                      "width": 1080,
                      "height": 597
                    }
                  ],
                  "variants": {},
                  "id": "12k_d9RLqmhi12Epl-03t4Q8GWO-f2zhPUu9yNyQa40"
                }
              ],
              "enabled": true
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lp653l",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Nice-Comfortable-650",
            "discussion_type": null,
            "num_comments": 22,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/",
            "stickied": false,
            "url": "https://i.redd.it/9eq6ted4haaf1.jpeg",
            "subreddit_subscribers": 494001,
            "created_utc": 1751387163,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n0ss9k0",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Nice-Comfortable-650",
                      "can_mod_post": false,
                      "created_utc": 1751392763,
                      "send_replies": true,
                      "parent_id": "t1_n0sap6g",
                      "score": 4,
                      "author_fullname": "t2_7zeld9y7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is correct!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n0ss9k0",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is correct!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lp653l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0ss9k0/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751392763,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n0sap6g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "rainbowColoredBalls",
            "can_mod_post": false,
            "created_utc": 1751388002,
            "send_replies": true,
            "parent_id": "t3_1lp653l",
            "score": 7,
            "author_fullname": "t2_23ovx259",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For the selective attention calculation, if I understand correctly, you drop the complexity from O(n^2) to O(n*k) where k is the length of new tokens and k &lt;&lt; n? ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0sap6g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For the selective attention calculation, if I understand correctly, you drop the complexity from O(n&lt;sup&gt;2)&lt;/sup&gt; to O(n*k) where k is the length of new tokens and k &amp;lt;&amp;lt; n? &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0sap6g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751388002,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lp653l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": {
                                                      "kind": "Listing",
                                                      "data": {
                                                        "after": null,
                                                        "dist": null,
                                                        "modhash": "",
                                                        "geo_filter": "",
                                                        "children": [
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n0u5t29",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "__JockY__",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n0u2wbf",
                                                              "score": 1,
                                                              "author_fullname": "t2_qf8h7ka8",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "That’s actually super useful. Thanks for taking the time.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n0u5t29",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s actually super useful. Thanks for taking the time.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1lp653l",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0u5t29/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1751407030,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1751407030,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          },
                                                          {
                                                            "kind": "t1",
                                                            "data": {
                                                              "subreddit_id": "t5_81eyvm",
                                                              "approved_at_utc": null,
                                                              "author_is_blocked": false,
                                                              "comment_type": null,
                                                              "awarders": [],
                                                              "mod_reason_by": null,
                                                              "banned_by": null,
                                                              "author_flair_type": "text",
                                                              "total_awards_received": 0,
                                                              "subreddit": "LocalLLaMA",
                                                              "author_flair_template_id": null,
                                                              "distinguished": null,
                                                              "likes": null,
                                                              "replies": "",
                                                              "user_reports": [],
                                                              "saved": false,
                                                              "id": "n0w9053",
                                                              "banned_at_utc": null,
                                                              "mod_reason_title": null,
                                                              "gilded": 0,
                                                              "archived": false,
                                                              "collapsed_reason_code": null,
                                                              "no_follow": true,
                                                              "author": "jazir5",
                                                              "can_mod_post": false,
                                                              "send_replies": true,
                                                              "parent_id": "t1_n0u2wbf",
                                                              "score": 1,
                                                              "author_fullname": "t2_8u27g",
                                                              "approved_by": null,
                                                              "mod_note": null,
                                                              "all_awardings": [],
                                                              "body": "So it just fragment caches different sections and can reassembles them as needed calling the individually cached parts in whatever order? Neat.",
                                                              "edited": false,
                                                              "gildings": {},
                                                              "downs": 0,
                                                              "author_flair_css_class": null,
                                                              "name": "t1_n0w9053",
                                                              "is_submitter": false,
                                                              "collapsed": false,
                                                              "author_flair_richtext": [],
                                                              "author_patreon_flair": false,
                                                              "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So it just fragment caches different sections and can reassembles them as needed calling the individually cached parts in whatever order? Neat.&lt;/p&gt;\n&lt;/div&gt;",
                                                              "removal_reason": null,
                                                              "collapsed_reason": null,
                                                              "link_id": "t3_1lp653l",
                                                              "associated_award": null,
                                                              "stickied": false,
                                                              "author_premium": false,
                                                              "can_gild": false,
                                                              "top_awarded_type": null,
                                                              "unrepliable_reason": null,
                                                              "author_flair_text_color": null,
                                                              "score_hidden": false,
                                                              "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0w9053/",
                                                              "subreddit_type": "public",
                                                              "locked": false,
                                                              "report_reasons": null,
                                                              "created": 1751435123,
                                                              "author_flair_text": null,
                                                              "treatment_tags": [],
                                                              "created_utc": 1751435123,
                                                              "subreddit_name_prefixed": "r/LocalLLaMA",
                                                              "controversiality": 0,
                                                              "depth": 5,
                                                              "author_flair_background_color": null,
                                                              "collapsed_because_crowd_control": null,
                                                              "mod_reports": [],
                                                              "num_reports": null,
                                                              "ups": 1
                                                            }
                                                          }
                                                        ],
                                                        "before": null
                                                      }
                                                    },
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n0u2wbf",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": false,
                                                    "author": "sautdepage",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n0tqvmo",
                                                    "score": 6,
                                                    "author_fullname": "t2_6qdom3n2",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "This is for multi-session. Basic cache only looks at the common \"starts with\" part -- like Claude's huge standard prompt is certainly cached fully for all requests.\n\nLooking at github it seems the key feature is multiple chunks of context can be combined together in a prompt, in any order, and each part can be retrieved from cache and put together.\n\nSo say the app initializes the new prompt for a session by combining: 1) a standard prompt, 2) a user-specific prompt, 3) a feature or usage-specific prompt + 4) a couple of RAG snippets relevant for that session. If I understood correctly, now most of them can be retrieved from cache if they've been seen before individually to form the new context.",
                                                    "edited": 1751406428,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n0u2wbf",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is for multi-session. Basic cache only looks at the common &amp;quot;starts with&amp;quot; part -- like Claude&amp;#39;s huge standard prompt is certainly cached fully for all requests.&lt;/p&gt;\n\n&lt;p&gt;Looking at github it seems the key feature is multiple chunks of context can be combined together in a prompt, in any order, and each part can be retrieved from cache and put together.&lt;/p&gt;\n\n&lt;p&gt;So say the app initializes the new prompt for a session by combining: 1) a standard prompt, 2) a user-specific prompt, 3) a feature or usage-specific prompt + 4) a couple of RAG snippets relevant for that session. If I understood correctly, now most of them can be retrieved from cache if they&amp;#39;ve been seen before individually to form the new context.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1lp653l",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0u2wbf/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1751406148,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1751406148,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 6
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n0tqvmo",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": false,
                                          "author": "__JockY__",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n0sf2un",
                                          "score": 4,
                                          "author_fullname": "t2_qf8h7ka8",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Today I learned that people change the system prompt mid-session.\n\nMay I ask why this would be done?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n0tqvmo",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Today I learned that people change the system prompt mid-session.&lt;/p&gt;\n\n&lt;p&gt;May I ask why this would be done?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1lp653l",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0tqvmo/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751402620,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1751402620,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 4
                                        }
                                      },
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "richtext",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n0szd9w",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "MoffKalast",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n0shjrn",
                                                    "score": 1,
                                                    "author_fullname": "t2_d2nyh",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Doesn't this mean that the VRAM/RAM usage for storing old cache will balloon into infinity? I mean KV cache is already most of what we need to allocate if you go for longer context.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n0szd9w",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Doesn&amp;#39;t this mean that the VRAM/RAM usage for storing old cache will balloon into infinity? I mean KV cache is already most of what we need to allocate if you go for longer context.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1lp653l",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0szd9w/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1751394754,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1751394754,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n0shjrn",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "LinkSea8324",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n0sf2un",
                                          "score": 2,
                                          "author_fullname": "t2_152zyn72n4",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I could be misunderstanding something but right now, VLLM got what `--cache-reuse 0` just the prefix\n\naccording to ggerganov , : \n\n&gt;--cache-reuse 1: the entire aaaaaccccccceeeeeeffhhhhhhh will be reused",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n0shjrn",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [
                                            {
                                              "e": "text",
                                              "t": "llama.cpp"
                                            }
                                          ],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I could be misunderstanding something but right now, VLLM got what &lt;code&gt;--cache-reuse 0&lt;/code&gt; just the prefix&lt;/p&gt;\n\n&lt;p&gt;according to ggerganov , : &lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;--cache-reuse 1: the entire aaaaaccccccceeeeeeffhhhhhhh will be reused&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1lp653l",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": "light",
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0shjrn/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1751389884,
                                          "author_flair_text": "llama.cpp",
                                          "treatment_tags": [],
                                          "created_utc": 1751389884,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": "#bbbdbf",
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n0sf2un",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "dampflokfreund",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n0scxam",
                                "score": 8,
                                "author_fullname": "t2_lis7z",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "the limitation of this PR is that context reusing only works if the system prompt remains static. When you change it or other parts of the prompt, which is the case during RAG or using memory such as vector DB, then it will process the entire context again. This is what LM Cache would solve. ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n0sf2un",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;the limitation of this PR is that context reusing only works if the system prompt remains static. When you change it or other parts of the prompt, which is the case during RAG or using memory such as vector DB, then it will process the entire context again. This is what LM Cache would solve. &lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lp653l",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0sf2un/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751389216,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751389216,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 8
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n0sennx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "LagOps91",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n0scxam",
                                "score": 1,
                                "author_fullname": "t2_3wi6j7vwh",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "is that actually it? the PR is quite old, no? sounds like something different.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n0sennx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;is that actually it? the PR is quite old, no? sounds like something different.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lp653l",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0sennx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1751389102,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1751389102,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n0scxam",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "LinkSea8324",
                      "can_mod_post": false,
                      "created_utc": 1751388622,
                      "send_replies": true,
                      "parent_id": "t1_n0sa0m2",
                      "score": 7,
                      "author_fullname": "t2_152zyn72n4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Isn't it already implemented ?\nhttps://github.com/ggml-org/llama.cpp/pull/9866",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n0scxam",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "llama.cpp"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Isn&amp;#39;t it already implemented ?\n&lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/9866\"&gt;https://github.com/ggml-org/llama.cpp/pull/9866&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lp653l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0scxam/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751388622,
                      "author_flair_text": "llama.cpp",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 7
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n0sa0m2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "dampflokfreund",
            "can_mod_post": false,
            "created_utc": 1751387811,
            "send_replies": true,
            "parent_id": "t3_1lp653l",
            "score": 8,
            "author_fullname": "t2_lis7z",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is it possible to implement this in llama.cpp?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0sa0m2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is it possible to implement this in llama.cpp?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0sa0m2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751387811,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lp653l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n0zlynt",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "rakarsky",
                      "can_mod_post": false,
                      "created_utc": 1751481337,
                      "send_replies": true,
                      "parent_id": "t1_n0upo16",
                      "score": 2,
                      "author_fullname": "t2_dfnr8",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "The cached KV cache can be stored in RAM and/or disk.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n0zlynt",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The cached KV cache can be stored in RAM and/or disk.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lp653l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0zlynt/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751481337,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n0upo16",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "k-en",
            "can_mod_post": false,
            "created_utc": 1751413526,
            "send_replies": true,
            "parent_id": "t3_1lp653l",
            "score": 2,
            "author_fullname": "t2_ckear9l",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "This looks very interesting. What about memory usage? Will this eat infinite memory (incrementing with model usage) or is there an option to control for memory? for example, when VRAM reaches a certain threshold, delete oldest KV cache",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0upo16",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This looks very interesting. What about memory usage? Will this eat infinite memory (incrementing with model usage) or is there an option to control for memory? for example, when VRAM reaches a certain threshold, delete oldest KV cache&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0upo16/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751413526,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lp653l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n0tj744",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Nice-Comfortable-650",
                      "can_mod_post": false,
                      "created_utc": 1751400449,
                      "send_replies": true,
                      "parent_id": "t1_n0t7kfp",
                      "score": 1,
                      "author_fullname": "t2_7zeld9y7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Right now the recognition is by manual modification of the context that you need to specify each chunk. This requires the agent programmer to slightly modify the input to the LLM API server.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n0tj744",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Right now the recognition is by manual modification of the context that you need to specify each chunk. This requires the agent programmer to slightly modify the input to the LLM API server.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lp653l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0tj744/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751400449,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n0t7kfp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Baldur-Norddahl",
            "can_mod_post": false,
            "created_utc": 1751397084,
            "send_replies": true,
            "parent_id": "t3_1lp653l",
            "score": 6,
            "author_fullname": "t2_bvqb8ng0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I hope this gets adopted quickly into the major programs. It should really make a huge difference when using agentic coding locally such as Cline, Roo Code and Aider. We are likely uploading the same small pieces of source files over and over.\n\nDoes the technique allow automatic recognition of parts of context, that has been seen before? Say the agent presents a source file to the LLM and that results in a diff for modifying the file. On the next task we get the same file uploaded again and it might be slightly modified, but most lines would be unmodified. Could we fetch cached values for the unmodified lines instead of starting all over?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0t7kfp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I hope this gets adopted quickly into the major programs. It should really make a huge difference when using agentic coding locally such as Cline, Roo Code and Aider. We are likely uploading the same small pieces of source files over and over.&lt;/p&gt;\n\n&lt;p&gt;Does the technique allow automatic recognition of parts of context, that has been seen before? Say the agent presents a source file to the LLM and that results in a diff for modifying the file. On the next task we get the same file uploaded again and it might be slightly modified, but most lines would be unmodified. Could we fetch cached values for the unmodified lines instead of starting all over?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0t7kfp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751397084,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lp653l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "total_awards_received": 0,
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "ups": 1,
            "removal_reason": null,
            "link_id": "t3_1lp653l",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n0tiq2y",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Nice-Comfortable-650",
                      "can_mod_post": false,
                      "created_utc": 1751400313,
                      "send_replies": true,
                      "parent_id": "t1_n0ta12o",
                      "score": 1,
                      "author_fullname": "t2_7zeld9y7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not currently",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n0tiq2y",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not currently&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lp653l",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0tiq2y/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1751400313,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n0ta12o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "DELETED",
            "no_follow": true,
            "author": "[deleted]",
            "can_mod_post": false,
            "send_replies": true,
            "parent_id": "t3_1lp653l",
            "score": 1,
            "approved_by": null,
            "report_reasons": null,
            "all_awardings": [],
            "subreddit_id": "t5_81eyvm",
            "body": "[removed]",
            "edited": false,
            "downs": 0,
            "author_flair_css_class": null,
            "collapsed": true,
            "is_submitter": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\n&lt;/div&gt;",
            "gildings": {},
            "collapsed_reason": null,
            "associated_award": null,
            "stickied": false,
            "subreddit_type": "public",
            "can_gild": false,
            "top_awarded_type": null,
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0ta12o/",
            "num_reports": null,
            "locked": false,
            "name": "t1_n0ta12o",
            "created": 1751397791,
            "subreddit": "LocalLLaMA",
            "author_flair_text": null,
            "treatment_tags": [],
            "created_utc": 1751397791,
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "mod_note": null,
            "distinguished": null
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n0unmmh",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MargretTatchersParty",
            "can_mod_post": false,
            "created_utc": 1751412847,
            "send_replies": true,
            "parent_id": "t3_1lp653l",
            "score": 1,
            "author_fullname": "t2_bbgrq",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Is this something that I can implement and run with in Ollama/OpenWebUI today? How much work would it be to bring that in?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0unmmh",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is this something that I can implement and run with in Ollama/OpenWebUI today? How much work would it be to bring that in?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0unmmh/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751412847,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lp653l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n0w9821",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "jazir5",
            "can_mod_post": false,
            "created_utc": 1751435236,
            "send_replies": true,
            "parent_id": "t3_1lp653l",
            "score": 1,
            "author_fullname": "t2_8u27g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Can you submit this system to RooCode on their GitHub? I think they would want to implement this very quickly.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n0w9821",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can you submit this system to RooCode on their GitHub? I think they would want to implement this very quickly.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lp653l/reuse_nonprefix_kv_cache_and_speed_up_rag_by_3x/n0w9821/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751435236,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lp653l",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]