[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Greetings, we're a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget\n\n  \n1. One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM\n\n2. One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM\n\n  \nBoth models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.\n\nWe're not extremely concerned about RAM (we can buy RAM later using a different budget) but we're concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?\n\nThanks a bunch  \n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How important is to have PRO 6000 Blackwell running on 16 PCIE lanes?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m8wuy7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.81,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 10,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_lavl5",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 10,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753443620,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Greetings, we&amp;#39;re a state-owned college, and we want to acquire an IA workstation. We have a strict budget and cannot surpass it, so working with our providers, they gave us two options with our budget&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;One Threadripper PRO 9955WX, with WS WRX90E-SAGE SE, 1 PRO 6000 Blackwell, and 256 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;One AMD Ryzen 9 9950X with a ProArt X870E-CREATOR, 2 PRO 6000 Blackwells and 128 GB RAM&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Both models have a 1600W PSU. The idea on the first model is to try to get another budget the next year in order to buy a second PRO 6000 Blackwell.&lt;/p&gt;\n\n&lt;p&gt;We&amp;#39;re not extremely concerned about RAM (we can buy RAM later using a different budget) but we&amp;#39;re concerned that the Ryzen 9950X only has enough PCIE lanes to run the blackwell on PCIE x8, instead of x16. Our provider told us that this is not very important unless we want to load and unload models all the time, but we have some reservations about that. So, can you guide us a little on that?&lt;/p&gt;\n\n&lt;p&gt;Thanks a bunch  &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m8wuy7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "ferkte",
            "discussion_type": null,
            "num_comments": 34,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/",
            "subreddit_subscribers": 504487,
            "created_utc": 1753443620,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n53jpo4",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Faux_Grey",
                      "can_mod_post": false,
                      "created_utc": 1753455882,
                      "send_replies": true,
                      "parent_id": "t1_n52lgnv",
                      "score": 3,
                      "author_fullname": "t2_ra3yb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "\\+1 to this, and as long as the full model fits into memory, bus width doesn't really matter except on load.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n53jpo4",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;+1 to this, and as long as the full model fits into memory, bus width doesn&amp;#39;t really matter except on load.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n53jpo4/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753455882,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n55nsyw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "teleprax",
                      "can_mod_post": false,
                      "created_utc": 1753477577,
                      "send_replies": true,
                      "parent_id": "t1_n52lgnv",
                      "score": 1,
                      "author_fullname": "t2_1nu0curp6b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Strongly agree with threadripper especially since this will be used in an educational context where just having the capability is more important than tokens per second.\n\nYou could still evaluate massive models with that 2TB of ram, like set up a batch of evals and let them run over night. With the Ryzen build you simply couldnt evaluate a model over 256GB",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n55nsyw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Strongly agree with threadripper especially since this will be used in an educational context where just having the capability is more important than tokens per second.&lt;/p&gt;\n\n&lt;p&gt;You could still evaluate massive models with that 2TB of ram, like set up a batch of evals and let them run over night. With the Ryzen build you simply couldnt evaluate a model over 256GB&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n55nsyw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753477577,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52lgnv",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "TableSurface",
            "can_mod_post": false,
            "created_utc": 1753444953,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 19,
            "author_fullname": "t2_r5ot7",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You should be more concerned about RAM, especially because of how MOE models work. The X870E platform only allows you a maximum of 256GB, while the Threadripper maxes out at 2TB while also having 4x more memory bandwidth.\n\n\nSince you're allowed to buy more RAM and can potentially buy more GPUs as early as next year, getting the Threadripper would be a more forward-compatible solution.\n\n\nThe consumer platform is more suited for budget constrained builds where you might only be able to buy hardware once every 5 years.",
            "edited": 1753448486,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52lgnv",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You should be more concerned about RAM, especially because of how MOE models work. The X870E platform only allows you a maximum of 256GB, while the Threadripper maxes out at 2TB while also having 4x more memory bandwidth.&lt;/p&gt;\n\n&lt;p&gt;Since you&amp;#39;re allowed to buy more RAM and can potentially buy more GPUs as early as next year, getting the Threadripper would be a more forward-compatible solution.&lt;/p&gt;\n\n&lt;p&gt;The consumer platform is more suited for budget constrained builds where you might only be able to buy hardware once every 5 years.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52lgnv/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753444953,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 19
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n52p4y1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Traditional-Gap-3313",
                      "can_mod_post": false,
                      "created_utc": 1753446340,
                      "send_replies": true,
                      "parent_id": "t1_n52og2o",
                      "score": 2,
                      "author_fullname": "t2_1g8b3m94rt",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This! It will be cheaper in the future to replace everything non-GPU then to buy another GPU.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52p4y1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This! It will be cheaper in the future to replace everything non-GPU then to buy another GPU.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52p4y1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753446340,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52og2o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "BrilliantAudience497",
            "can_mod_post": false,
            "created_utc": 1753446084,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 7,
            "author_fullname": "t2_1p34vnz066",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The vendor is mostly right, it doesn't matter too much if you're only doing things on a single GPU. It *will* have a bigger effect if you're planning to split a model either between cards or between a card and system ram, but its more on the order of a 5-10% performance hit, not half performance.\n\n\nWith that said, have 2x6000s is a way bigger boost than the 2x pcie lanes for performance. You'll be able to run bigger models and/or more models on that system, even if they get slowed down a bit by the pcie lanes.\n\n\nBeyond that, a single rtx 6000 pro should cost more than the non-gpu portion of that workstation. You'll get better performance today with the 2x6000 system, and if you decide you need the extra lanes next year it would be cheaper to replace motherboard/cpu/ram than buying a new 6000.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52og2o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The vendor is mostly right, it doesn&amp;#39;t matter too much if you&amp;#39;re only doing things on a single GPU. It &lt;em&gt;will&lt;/em&gt; have a bigger effect if you&amp;#39;re planning to split a model either between cards or between a card and system ram, but its more on the order of a 5-10% performance hit, not half performance.&lt;/p&gt;\n\n&lt;p&gt;With that said, have 2x6000s is a way bigger boost than the 2x pcie lanes for performance. You&amp;#39;ll be able to run bigger models and/or more models on that system, even if they get slowed down a bit by the pcie lanes.&lt;/p&gt;\n\n&lt;p&gt;Beyond that, a single rtx 6000 pro should cost more than the non-gpu portion of that workstation. You&amp;#39;ll get better performance today with the 2x6000 system, and if you decide you need the extra lanes next year it would be cheaper to replace motherboard/cpu/ram than buying a new 6000.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52og2o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753446084,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n54ypqt",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "abnormal_human",
                                "can_mod_post": false,
                                "send_replies": false,
                                "parent_id": "t1_n54nzom",
                                "score": 1,
                                "author_fullname": "t2_5y02z",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I don't use llama.cpp, but I have gotten up to \\~11gbps model loading on a pair of RAID0 PCIe4.0 SSDs with xfs filesystem in other contexts. Not quite the theoretical max, but I'll take the 50% increase in speed given that it didn't really cost me anything compared to one 8GB SSD. I have two Samsung 990 Pros in that machine, so not quite enterprise grade, but close enough.\n\nI haven't done a DDR5/PCIe5.0 rig yet. The 4x6000Ada machine is meeting my needs just fine, and is better for what I do (mostly training image models) than 2x6000Blackwell would be. Would love to get one of those for inference, I just don't have a free slot anywhere worth sticking it into and don't feel like building/housing yet another machine.\n\nI think IPMI is required even for workstations. I don't have anything anyone would describe as a server, and I've both gotten stuck with a wedged machine while hundreds of miles away, and used IPMI to get myself out of trouble many times. Maybe less relevant for a mostly-interactive user, but I do multi-day training runs on these boxes, so they're largely unattended for long periods.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n54ypqt",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t use llama.cpp, but I have gotten up to ~11gbps model loading on a pair of RAID0 PCIe4.0 SSDs with xfs filesystem in other contexts. Not quite the theoretical max, but I&amp;#39;ll take the 50% increase in speed given that it didn&amp;#39;t really cost me anything compared to one 8GB SSD. I have two Samsung 990 Pros in that machine, so not quite enterprise grade, but close enough.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t done a DDR5/PCIe5.0 rig yet. The 4x6000Ada machine is meeting my needs just fine, and is better for what I do (mostly training image models) than 2x6000Blackwell would be. Would love to get one of those for inference, I just don&amp;#39;t have a free slot anywhere worth sticking it into and don&amp;#39;t feel like building/housing yet another machine.&lt;/p&gt;\n\n&lt;p&gt;I think IPMI is required even for workstations. I don&amp;#39;t have anything anyone would describe as a server, and I&amp;#39;ve both gotten stuck with a wedged machine while hundreds of miles away, and used IPMI to get myself out of trouble many times. Maybe less relevant for a mostly-interactive user, but I do multi-day training runs on these boxes, so they&amp;#39;re largely unattended for long periods.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m8wuy7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n54ypqt/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753470189,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753470189,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n54nzom",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "eloquentemu",
                      "can_mod_post": false,
                      "created_utc": 1753467085,
                      "send_replies": true,
                      "parent_id": "t1_n52sk34",
                      "score": 1,
                      "author_fullname": "t2_lpdsy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; The TR Pro is just a bad deal. It's very expensive, and the only thing you get over a preowned genoa CPU is higher single thread performance, maybe. But You can literally be spending 2-4x more for Threadripper over Epyc and getting nothing out of it that matters to your use case.\n\nIn fairness, the TR Pro doesn't just offer higher boost clocks but can also run DDR5-8000 while Genoa is stuck a 4800, maybe 5200, so actually edges out Genoa despite having only 8ch.  I do agree though, that TR is bad _value_ and the 9955WX _specifically_ is a 2 CCD trap that won't be able to use half the memory bandwidth.  The 9965WX is the bare minimum and anyone ~~wasting~~ spending this much money should get the 9975WX at a bare minimum - even the extra core will be useful for MoE models.  (I'm assuming that the CCD-IO comms are the same as Epyc Turin, but can't find info on it.)\n\n&gt; Also, you didn't talk about storage architecture, but please get the fastest NVMe you can, and then RAID0 it. Makes a huge diff loading/unloading models at the scale these GPUs can support. \n\nCurious if you've benchmarked this.  I only have a 2xGen4 RAID0 but it still caps out at 7GBps on model load with llama.cpp but can get 12GBps in tests.  I'm curious if it scales with more drives or is limited to a flat 7 (or similar base on CPU etc).\n\nI'll also throw out there (more for others than OP, since budget) but broadly don't ignore storage and consider getting _good_ storage and not consumer M.2.  Those are fine for gaming computers, but writing out a 10++GB model they rapidly run out of fast flash and will throttle to &lt;100MBps - worse than a spinning rust and maybe even your internet connection.\n\n&gt; Finally, don't even think about a motherboard without IPMI. You *will* get into a situation where the machine wedges up and you need to intervene, and it will happen when you're out of town or away from it. Put it on UPS too.\n\nThey do say workstation and not server so this is optional IMHO.  Still good advice and definitely get a UPS.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n54nzom",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;The TR Pro is just a bad deal. It&amp;#39;s very expensive, and the only thing you get over a preowned genoa CPU is higher single thread performance, maybe. But You can literally be spending 2-4x more for Threadripper over Epyc and getting nothing out of it that matters to your use case.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;In fairness, the TR Pro doesn&amp;#39;t just offer higher boost clocks but can also run DDR5-8000 while Genoa is stuck a 4800, maybe 5200, so actually edges out Genoa despite having only 8ch.  I do agree though, that TR is bad &lt;em&gt;value&lt;/em&gt; and the 9955WX &lt;em&gt;specifically&lt;/em&gt; is a 2 CCD trap that won&amp;#39;t be able to use half the memory bandwidth.  The 9965WX is the bare minimum and anyone &lt;del&gt;wasting&lt;/del&gt; spending this much money should get the 9975WX at a bare minimum - even the extra core will be useful for MoE models.  (I&amp;#39;m assuming that the CCD-IO comms are the same as Epyc Turin, but can&amp;#39;t find info on it.)&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Also, you didn&amp;#39;t talk about storage architecture, but please get the fastest NVMe you can, and then RAID0 it. Makes a huge diff loading/unloading models at the scale these GPUs can support. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Curious if you&amp;#39;ve benchmarked this.  I only have a 2xGen4 RAID0 but it still caps out at 7GBps on model load with llama.cpp but can get 12GBps in tests.  I&amp;#39;m curious if it scales with more drives or is limited to a flat 7 (or similar base on CPU etc).&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;ll also throw out there (more for others than OP, since budget) but broadly don&amp;#39;t ignore storage and consider getting &lt;em&gt;good&lt;/em&gt; storage and not consumer M.2.  Those are fine for gaming computers, but writing out a 10++GB model they rapidly run out of fast flash and will throttle to &amp;lt;100MBps - worse than a spinning rust and maybe even your internet connection.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Finally, don&amp;#39;t even think about a motherboard without IPMI. You &lt;em&gt;will&lt;/em&gt; get into a situation where the machine wedges up and you need to intervene, and it will happen when you&amp;#39;re out of town or away from it. Put it on UPS too.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;They do say workstation and not server so this is optional IMHO.  Still good advice and definitely get a UPS.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n54nzom/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753467085,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n553iu5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "sub_RedditTor",
                      "can_mod_post": false,
                      "created_utc": 1753471614,
                      "send_replies": true,
                      "parent_id": "t1_n52sk34",
                      "score": 1,
                      "author_fullname": "t2_oy3c84euj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I would've gone with AMD EPYC 9274F server/workstation build ..\n\nMuch cheaper and wat better memory bandwidth",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n553iu5",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would&amp;#39;ve gone with AMD EPYC 9274F server/workstation build ..&lt;/p&gt;\n\n&lt;p&gt;Much cheaper and wat better memory bandwidth&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n553iu5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753471614,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52sk34",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "abnormal_human",
            "can_mod_post": false,
            "created_utc": 1753447566,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 7,
            "author_fullname": "t2_5y02z",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "As someone who's built several machines like this, both of these options are stupid.\n\nYou didn't say what your budget is, but often buying from \"providers\" inflates cost 30% or more, so piecing something together may get you what you want with less stress. \n\nThe TR Pro is just a bad deal. It's very expensive, and the only thing you get over a preowned genoa CPU is higher single thread performance, maybe. But You can literally be spending 2-4x more for Threadripper over Epyc and getting nothing out of it that matters to your use case.\n\nFor 2x RTX 6000 you want 512GB RAM so that you can have a large enough filesystem cache so you're not pushing everything out during model loading. I try to spec 2x the conventional RAM as VRAM for this reason.\n\nIf you have dual GPUs you want full lanes. I don't know exactly what you're doing, but it matters for training for sure, and impacts other situations. You want the important components running at full potential.\n\nAlso, you didn't talk about storage architecture, but please get the fastest NVMe you can, and then RAID0 it. Makes a huge diff loading/unloading models at the scale these GPUs can support. \n\nFinally, don't even think about a motherboard without IPMI. You \\*will\\* get into a situation where the machine wedges up and you need to intervene, and it will happen when you're out of town or away from it. Put it on UPS too.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52sk34",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As someone who&amp;#39;s built several machines like this, both of these options are stupid.&lt;/p&gt;\n\n&lt;p&gt;You didn&amp;#39;t say what your budget is, but often buying from &amp;quot;providers&amp;quot; inflates cost 30% or more, so piecing something together may get you what you want with less stress. &lt;/p&gt;\n\n&lt;p&gt;The TR Pro is just a bad deal. It&amp;#39;s very expensive, and the only thing you get over a preowned genoa CPU is higher single thread performance, maybe. But You can literally be spending 2-4x more for Threadripper over Epyc and getting nothing out of it that matters to your use case.&lt;/p&gt;\n\n&lt;p&gt;For 2x RTX 6000 you want 512GB RAM so that you can have a large enough filesystem cache so you&amp;#39;re not pushing everything out during model loading. I try to spec 2x the conventional RAM as VRAM for this reason.&lt;/p&gt;\n\n&lt;p&gt;If you have dual GPUs you want full lanes. I don&amp;#39;t know exactly what you&amp;#39;re doing, but it matters for training for sure, and impacts other situations. You want the important components running at full potential.&lt;/p&gt;\n\n&lt;p&gt;Also, you didn&amp;#39;t talk about storage architecture, but please get the fastest NVMe you can, and then RAID0 it. Makes a huge diff loading/unloading models at the scale these GPUs can support. &lt;/p&gt;\n\n&lt;p&gt;Finally, don&amp;#39;t even think about a motherboard without IPMI. You *will* get into a situation where the machine wedges up and you need to intervene, and it will happen when you&amp;#39;re out of town or away from it. Put it on UPS too.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52sk34/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753447566,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52l0c0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "MaxKruse96",
            "can_mod_post": false,
            "created_utc": 1753444778,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 4,
            "author_fullname": "t2_pfi81",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "french spotted (noone says IA, its not intelligence artificielle, its Artificial Intelligence).\n\nThe load/unload aspect is valid. If you expect to scale to more than 2 GPUs, get the Threadripper. Otherwise, 2x x8 is fine for you.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52l0c0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;french spotted (noone says IA, its not intelligence artificielle, its Artificial Intelligence).&lt;/p&gt;\n\n&lt;p&gt;The load/unload aspect is valid. If you expect to scale to more than 2 GPUs, get the Threadripper. Otherwise, 2x x8 is fine for you.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52l0c0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753444778,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52m08f",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "BenniB99",
            "can_mod_post": false,
            "created_utc": 1753445165,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 2,
            "author_fullname": "t2_17xncyy5vl",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Mhh this is tricky and depends largely on what exactly you are going to use this for.  \nThat being said more VRAM and GPU compute usually trumps everything else.\n\nI think having two GPUs running on PCIE 5.0 x8 (which is basically PCIE 4.0 x 16) is negligble, if you are splitting models across GPUs especially when running training workloads this might still become a bottleneck though.\n\nFor future upgrades the Threadripper + WRX90E-SAGE combo would definitely be better (adding more GPUs).\n\nI would say go for option 2., because you can not really beat 192GB VRAM.  \nPlus you can always buy a better motherboard and CPU with a potential budget the next year :D  \nAnd if you end up not getting that budget the GPUs might get you much further than a Threadripper with more RAM will (again this will depend on what you are planning to use this for primarily).\n\nTwo GPUs in the hand is worth one in the bush.",
            "edited": 1753445463,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52m08f",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Mhh this is tricky and depends largely on what exactly you are going to use this for.&lt;br/&gt;\nThat being said more VRAM and GPU compute usually trumps everything else.&lt;/p&gt;\n\n&lt;p&gt;I think having two GPUs running on PCIE 5.0 x8 (which is basically PCIE 4.0 x 16) is negligble, if you are splitting models across GPUs especially when running training workloads this might still become a bottleneck though.&lt;/p&gt;\n\n&lt;p&gt;For future upgrades the Threadripper + WRX90E-SAGE combo would definitely be better (adding more GPUs).&lt;/p&gt;\n\n&lt;p&gt;I would say go for option 2., because you can not really beat 192GB VRAM.&lt;br/&gt;\nPlus you can always buy a better motherboard and CPU with a potential budget the next year :D&lt;br/&gt;\nAnd if you end up not getting that budget the GPUs might get you much further than a Threadripper with more RAM will (again this will depend on what you are planning to use this for primarily).&lt;/p&gt;\n\n&lt;p&gt;Two GPUs in the hand is worth one in the bush.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52m08f/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753445165,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52t2x0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "smflx",
            "can_mod_post": false,
            "created_utc": 1753447744,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 2,
            "author_fullname": "t2_16qe65sqwe",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If it's for interference only, it's ok with x8.  Actually, it's the same speed of x16 gen4.\n\nIf you're going to train, it depends on model size.  If the model &amp; all the training state fit in one gpu, no problem. Not much communication between gpu. It's called DDP.\n\nFor the bigger model, you have to split the model &amp; training state into gpus, then a lot of communication needed. Communication speed between GPUs is very crucial. That's why nvidia put nvlink only on expensive server GPUs.  In this case, full pcie gen5 will be beneficial.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52t2x0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If it&amp;#39;s for interference only, it&amp;#39;s ok with x8.  Actually, it&amp;#39;s the same speed of x16 gen4.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re going to train, it depends on model size.  If the model &amp;amp; all the training state fit in one gpu, no problem. Not much communication between gpu. It&amp;#39;s called DDP.&lt;/p&gt;\n\n&lt;p&gt;For the bigger model, you have to split the model &amp;amp; training state into gpus, then a lot of communication needed. Communication speed between GPUs is very crucial. That&amp;#39;s why nvidia put nvlink only on expensive server GPUs.  In this case, full pcie gen5 will be beneficial.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52t2x0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753447744,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52mpw9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "TacGibs",
            "can_mod_post": false,
            "created_utc": 1753445436,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_8w0y7ezw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Le chargement sera limité par la vitesse de ton support de stockage (je suppose que tu ne comptes pas stocker les modèles en RAMdisk), par conséquent il est plus intelligent de partir sur le Ryzen et de prendre une deuxième machine plus tard.\n\nL'avantage du Threadripper est d'avoir beaucoup plus de channels pour la RAM, et donc de pouvoir faire de l'offloading CPU (charger un modèle trop gros pour tenir uniquement sur la VRAM) en étant beaucoup moins pénalisé qu'avec le Ryzen, où tout offloading fera immédiatement chuter les performances de façon drastique.\n\nN'hésite pas à venir en MP, je serais content d'aider au bon emploi de l'argent public (pour une fois 😂).",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52mpw9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Le chargement sera limité par la vitesse de ton support de stockage (je suppose que tu ne comptes pas stocker les modèles en RAMdisk), par conséquent il est plus intelligent de partir sur le Ryzen et de prendre une deuxième machine plus tard.&lt;/p&gt;\n\n&lt;p&gt;L&amp;#39;avantage du Threadripper est d&amp;#39;avoir beaucoup plus de channels pour la RAM, et donc de pouvoir faire de l&amp;#39;offloading CPU (charger un modèle trop gros pour tenir uniquement sur la VRAM) en étant beaucoup moins pénalisé qu&amp;#39;avec le Ryzen, où tout offloading fera immédiatement chuter les performances de façon drastique.&lt;/p&gt;\n\n&lt;p&gt;N&amp;#39;hésite pas à venir en MP, je serais content d&amp;#39;aider au bon emploi de l&amp;#39;argent public (pour une fois 😂).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52mpw9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753445436,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52noju",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Goldandsilverape99",
            "can_mod_post": false,
            "created_utc": 1753445801,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_l8tixwyg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a 5090 and a 4080 super in the same system. Only using the 5090 i got 54t / sek for a 32B Q5 23 GB model for a particular prompt. Activating both GPU's i got 33.27 t/sek, loading the same model with \"split evenly\". The 4080 super is slower, so if feel like that is the limit and not the PCI express bus. My motherboard only support 8x for the 5090 and 4x for the 4080 super in multi gpu mode.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52noju",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a 5090 and a 4080 super in the same system. Only using the 5090 i got 54t / sek for a 32B Q5 23 GB model for a particular prompt. Activating both GPU&amp;#39;s i got 33.27 t/sek, loading the same model with &amp;quot;split evenly&amp;quot;. The 4080 super is slower, so if feel like that is the limit and not the PCI express bus. My motherboard only support 8x for the 5090 and 4x for the 4080 super in multi gpu mode.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52noju/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753445801,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52nt79",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DAlmighty",
            "can_mod_post": false,
            "created_utc": 1753445849,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_a04uj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I’m kinda shocked no one actually has asked the most important question yet… what will this machine actually be doing? \n\nThe use case matters a lot. If you want to default to “everything” or “I don’t know” go with less GPU and more available PCIE lanes for future growth.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52nt79",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I’m kinda shocked no one actually has asked the most important question yet… what will this machine actually be doing? &lt;/p&gt;\n\n&lt;p&gt;The use case matters a lot. If you want to default to “everything” or “I don’t know” go with less GPU and more available PCIE lanes for future growth.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52nt79/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753445849,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n53m4oj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "GPTrack_ai",
                      "can_mod_post": false,
                      "created_utc": 1753456557,
                      "send_replies": true,
                      "parent_id": "t1_n52ppm6",
                      "score": 0,
                      "author_fullname": "t2_1tpuoj72sa",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "wrong!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n53m4oj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;wrong!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n53m4oj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753456557,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 0
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52ppm6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "chisleu",
            "can_mod_post": false,
            "created_utc": 1753446550,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_cbxyn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "They are right. Inference doesn't use a lot of bandwidth. Just loading and unloading models.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52ppm6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They are right. Inference doesn&amp;#39;t use a lot of bandwidth. Just loading and unloading models.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52ppm6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753446550,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52tku9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SEC_intern_",
            "can_mod_post": false,
            "created_utc": 1753447914,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_39mk9lfr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'd go with option 2. VRAM is king, [PCIe not so much](https://youtu.be/X5MW45zJ4Jg?si=L_Uurz9AqRVYcNIv). (Strictly for inferencing though)\n\nFWIW I have a 10980XE + x299 mobo which are only PCIe 3.0 capable (48 lanes). I'm using it with 5090 (x16 lanes), 4090 (x16), 1x 4070ti Sup (x8) and RTX 2000 ADA (x8). It's a 88GB behemoth paired with 256G RAM \n\nIt can crunch any dense model that fits in the under 72G VRAM while leaving the slowest 16G for context. Eats Deepseek R1 70b for breakfast. For MoE architectures, if you carefully offload the layers, you can easily achieve \\~15 TPS for models like Qwen3 235B - A22B at Q8.",
            "edited": 1753448396,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52tku9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d go with option 2. VRAM is king, &lt;a href=\"https://youtu.be/X5MW45zJ4Jg?si=L_Uurz9AqRVYcNIv\"&gt;PCIe not so much&lt;/a&gt;. (Strictly for inferencing though)&lt;/p&gt;\n\n&lt;p&gt;FWIW I have a 10980XE + x299 mobo which are only PCIe 3.0 capable (48 lanes). I&amp;#39;m using it with 5090 (x16 lanes), 4090 (x16), 1x 4070ti Sup (x8) and RTX 2000 ADA (x8). It&amp;#39;s a 88GB behemoth paired with 256G RAM &lt;/p&gt;\n\n&lt;p&gt;It can crunch any dense model that fits in the under 72G VRAM while leaving the slowest 16G for context. Eats Deepseek R1 70b for breakfast. For MoE architectures, if you carefully offload the layers, you can easily achieve ~15 TPS for models like Qwen3 235B - A22B at Q8.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52tku9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753447914,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n534adf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Goober_94",
            "can_mod_post": false,
            "created_utc": 1753451403,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_1tgehitb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The Ryzen 9950X is NOT a workstation CPU, it is a home use / gaming CPU., You should not use the 9950X as a workstation CPU, you will be extremely disappointed.\n\nFirst of all, 9950X only has two memory channels, that is terrible. The AMD Ryzen memory subsystem is terrible, even with two channels the \"infinity fabric\" gets absolutely saturated and is a massive bottleneck. Even if you overclock the IF up to 2200mhz, it doesn't matter as AMD's un-core (IOD) is another serious bottle neck for even light weight compute tasks, and again, even if you get a golden sample and overclock the snot out of it at ridiculously high vSOC voltages; you MIGHT get the uclk to run at what? 3200-3300mhz.\n\nNote: It goes without saying that you shouldn't be overclocking and overvolting a workstation where reliability and stability is key. \n\nAdd in the fact that it doesn't have enough PCI-e lanes for multiple GPU's and fast storage arrays etc. Just a terrible idea.\n\nThe threadripper pro series is better, but a lot of the 9950X issues are also present in the threadrippers. The un-core and infinity fabric issues remain (and in fact amplified) and crossing IOD's for memory access results in pretty huge performance penalty.\n\nYou would be much better off just to pick up a 24+ core Xeon W-3xxx CPU, for AMX if nothing else, and build from there. They are simply much better workstation platforms, especially for AI, IMHO.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n534adf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The Ryzen 9950X is NOT a workstation CPU, it is a home use / gaming CPU., You should not use the 9950X as a workstation CPU, you will be extremely disappointed.&lt;/p&gt;\n\n&lt;p&gt;First of all, 9950X only has two memory channels, that is terrible. The AMD Ryzen memory subsystem is terrible, even with two channels the &amp;quot;infinity fabric&amp;quot; gets absolutely saturated and is a massive bottleneck. Even if you overclock the IF up to 2200mhz, it doesn&amp;#39;t matter as AMD&amp;#39;s un-core (IOD) is another serious bottle neck for even light weight compute tasks, and again, even if you get a golden sample and overclock the snot out of it at ridiculously high vSOC voltages; you MIGHT get the uclk to run at what? 3200-3300mhz.&lt;/p&gt;\n\n&lt;p&gt;Note: It goes without saying that you shouldn&amp;#39;t be overclocking and overvolting a workstation where reliability and stability is key. &lt;/p&gt;\n\n&lt;p&gt;Add in the fact that it doesn&amp;#39;t have enough PCI-e lanes for multiple GPU&amp;#39;s and fast storage arrays etc. Just a terrible idea.&lt;/p&gt;\n\n&lt;p&gt;The threadripper pro series is better, but a lot of the 9950X issues are also present in the threadrippers. The un-core and infinity fabric issues remain (and in fact amplified) and crossing IOD&amp;#39;s for memory access results in pretty huge performance penalty.&lt;/p&gt;\n\n&lt;p&gt;You would be much better off just to pick up a 24+ core Xeon W-3xxx CPU, for AMX if nothing else, and build from there. They are simply much better workstation platforms, especially for AI, IMHO.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n534adf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753451403,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n53aaiy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "createthiscom",
            "can_mod_post": false,
            "created_utc": 1753453214,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_ozxxf",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Spends 8 or 9k on a GPU and skimps on ram and PCIe lanes. lol.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n53aaiy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Spends 8 or 9k on a GPU and skimps on ram and PCIe lanes. lol.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n53aaiy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753453214,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n53dkkp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Aggravating-View9462",
            "can_mod_post": false,
            "created_utc": 1753454156,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_1tfbhaf9be",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you have the funds available take a look at c-paynes PCIE gen 5 switch - https://c-payne.com/products/pcie-gen5-mcio-switch-100-lane-mircochip-switchtec-pm50100?variant=51589360058635 . Allows for two devices to be hosted at x16 on a single x16 slot. Additionally massively improves the one thing no one ever mentions, which has a FAR bigger influence on multi device inference speed, namely response time - cuts out the devices needing to go through the root PCIE complex to req DMA read/writes and therefore massively speeds up inter device communication.\n\n\nThere is not a huge amount of data used for multi device inference.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n53dkkp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you have the funds available take a look at c-paynes PCIE gen 5 switch - &lt;a href=\"https://c-payne.com/products/pcie-gen5-mcio-switch-100-lane-mircochip-switchtec-pm50100?variant=51589360058635\"&gt;https://c-payne.com/products/pcie-gen5-mcio-switch-100-lane-mircochip-switchtec-pm50100?variant=51589360058635&lt;/a&gt; . Allows for two devices to be hosted at x16 on a single x16 slot. Additionally massively improves the one thing no one ever mentions, which has a FAR bigger influence on multi device inference speed, namely response time - cuts out the devices needing to go through the root PCIE complex to req DMA read/writes and therefore massively speeds up inter device communication.&lt;/p&gt;\n\n&lt;p&gt;There is not a huge amount of data used for multi device inference.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n53dkkp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753454156,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n544aof",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "maifee",
            "can_mod_post": false,
            "created_utc": 1753461637,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_1fuhylzi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Here are some GPU related info.\n\n \\- PCIe 4.0 x16 = 32 GB/s\n\n \\- PCIe 4.0 x8 = 16 GB/s\n\n \\- PCIe 5.0 x16 = 64 GB/s\n\n \\- PCIe 5.0 x8 = 32 GB/s\n\nSo I would leave it up to you.I would recommend at least 32 GB/s.\n\nAnd try to get more RAM, with high speed. For personal use 64 GB with 3200 MHz. But for server try something with at least 256 GB with 4500+ MHz. Cause personal use computer generally has only two actual channel. But in server it's more than that, try to benifit from there.\n\n  \nHere are my two cents. *If you want some tools for free for your institute I would love to do so.*",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n544aof",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Ollama"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here are some GPU related info.&lt;/p&gt;\n\n&lt;p&gt;- PCIe 4.0 x16 = 32 GB/s&lt;/p&gt;\n\n&lt;p&gt;- PCIe 4.0 x8 = 16 GB/s&lt;/p&gt;\n\n&lt;p&gt;- PCIe 5.0 x16 = 64 GB/s&lt;/p&gt;\n\n&lt;p&gt;- PCIe 5.0 x8 = 32 GB/s&lt;/p&gt;\n\n&lt;p&gt;So I would leave it up to you.I would recommend at least 32 GB/s.&lt;/p&gt;\n\n&lt;p&gt;And try to get more RAM, with high speed. For personal use 64 GB with 3200 MHz. But for server try something with at least 256 GB with 4500+ MHz. Cause personal use computer generally has only two actual channel. But in server it&amp;#39;s more than that, try to benifit from there.&lt;/p&gt;\n\n&lt;p&gt;Here are my two cents. &lt;em&gt;If you want some tools for free for your institute I would love to do so.&lt;/em&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n544aof/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753461637,
            "author_flair_text": "Ollama",
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n545w8i",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Roland_Bodel_the_2nd",
            "can_mod_post": false,
            "created_utc": 1753462090,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_i80v0xfb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Can you say more about the exact use case?  In our case we buy this kind of spec when people need to do interactive 3d visualization.  If a person doesn't need to sit in front of it, you have more options.\n\nIf you're doing inference, then which target models?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n545w8i",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Can you say more about the exact use case?  In our case we buy this kind of spec when people need to do interactive 3d visualization.  If a person doesn&amp;#39;t need to sit in front of it, you have more options.&lt;/p&gt;\n\n&lt;p&gt;If you&amp;#39;re doing inference, then which target models?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n545w8i/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753462090,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n54zhnn",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MelodicRecognition7",
            "can_mod_post": false,
            "created_utc": 1753470414,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_1eex9ug5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ryzen has lower memory bandwidth than Threadripper so you should get the first option. If you have a chance to get Epyc5 instead of Threadripper then get an Epyc5 because its memory bandwidth is even higher.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n54zhnn",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ryzen has lower memory bandwidth than Threadripper so you should get the first option. If you have a chance to get Epyc5 instead of Threadripper then get an Epyc5 because its memory bandwidth is even higher.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n54zhnn/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753470414,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n550o7q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sub_RedditTor",
            "can_mod_post": false,
            "created_utc": 1753470764,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_oy3c84euj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Very poor choice of components.. \n\nWhy not go with last gen but much better CPU which has more CCD's and most likely will haveuch better memory bandwidth..",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n550o7q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Very poor choice of components.. &lt;/p&gt;\n\n&lt;p&gt;Why not go with last gen but much better CPU which has more CCD&amp;#39;s and most likely will haveuch better memory bandwidth..&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n550o7q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753470764,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5514gy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Conscious_Cut_6144",
            "can_mod_post": false,
            "created_utc": 1753470897,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_9hl4ymvj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Use case matters, need more details on what you are actually doing with it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5514gy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Use case matters, need more details on what you are actually doing with it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n5514gy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753470897,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n552zu0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "sub_RedditTor",
            "can_mod_post": false,
            "created_utc": 1753471455,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_oy3c84euj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Forget about Threadripper..\n\nAll you need is  9004 series AMD EPYC 9274F and 64MT/s memory . \n\nThat set-up will run circles around most Threadripper CPU's in terms of memory bandwidth..\n\nIt'such much cheaper than Threadripper..\n\nIf you want something better invest in Xeon 6 with MRDRIMM memory..",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n552zu0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Forget about Threadripper..&lt;/p&gt;\n\n&lt;p&gt;All you need is  9004 series AMD EPYC 9274F and 64MT/s memory . &lt;/p&gt;\n\n&lt;p&gt;That set-up will run circles around most Threadripper CPU&amp;#39;s in terms of memory bandwidth..&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;such much cheaper than Threadripper..&lt;/p&gt;\n\n&lt;p&gt;If you want something better invest in Xeon 6 with MRDRIMM memory..&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n552zu0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753471455,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "7dba5c08-72f1-11ee-9b6f-ca195bc297d4",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n559cc7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "bigmanbananas",
            "can_mod_post": false,
            "created_utc": 1753473353,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 1,
            "author_fullname": "t2_dkaj7vn2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I had almost this exact query (albeit about dual 5090s) a couple of months ago. \n\nThe answer is, as always, \"it Ddepends!\" if you are running llm inference, you'll lose a few seconds some times when loading new models. If you start running training, how much throughput do you need? Going from 64GBps in each direction down to 32GBps sounds like a lot, but considering the fastest consumer NVME drive can output data at a Max of less than 15GBps, you would probably have some lee way.\n\nYou would be fine in almost all circumstances and when you reach the point of saturating that bandwidth, you'll probably need a new machine anyway.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n559cc7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 70B"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I had almost this exact query (albeit about dual 5090s) a couple of months ago. &lt;/p&gt;\n\n&lt;p&gt;The answer is, as always, &amp;quot;it Ddepends!&amp;quot; if you are running llm inference, you&amp;#39;ll lose a few seconds some times when loading new models. If you start running training, how much throughput do you need? Going from 64GBps in each direction down to 32GBps sounds like a lot, but considering the fastest consumer NVME drive can output data at a Max of less than 15GBps, you would probably have some lee way.&lt;/p&gt;\n\n&lt;p&gt;You would be fine in almost all circumstances and when you reach the point of saturating that bandwidth, you&amp;#39;ll probably need a new machine anyway.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n559cc7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753473353,
            "author_flair_text": "Llama 70B",
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n52lo6v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Defiant_Diet9085",
            "can_mod_post": false,
            "created_utc": 1753445034,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": -1,
            "author_fullname": "t2_1airv0szt9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I suggest 5090 + 1TB RAM\n\nThis way you will be able to run any models that will be released in a year or two.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n52lo6v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I suggest 5090 + 1TB RAM&lt;/p&gt;\n\n&lt;p&gt;This way you will be able to run any models that will be released in a year or two.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52lo6v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753445034,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n53lu0k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTrack_ai",
            "can_mod_post": false,
            "created_utc": 1753456475,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": 0,
            "author_fullname": "t2_1tpuoj72sa",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For inferencing with mutiple GPUs PCIe bandwidth matters extremely. PCIe x8 will cut peformance in half.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n53lu0k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For inferencing with mutiple GPUs PCIe bandwidth matters extremely. PCIe x8 will cut peformance in half.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n53lu0k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753456475,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m8wuy7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "total_awards_received": 0,
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "ups": -7,
            "removal_reason": null,
            "link_id": "t3_1m8wuy7",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n52inn8",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "ferkte",
                      "can_mod_post": false,
                      "created_utc": 1753443855,
                      "send_replies": true,
                      "parent_id": "t1_n52ifgt",
                      "score": 4,
                      "author_fullname": "t2_lavl5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Fair point, but we cannot hire consultants due to previous corruption issues on our government part",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52inn8",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Fair point, but we cannot hire consultants due to previous corruption issues on our government part&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52inn8/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753443855,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 4
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n52k00d",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "ShinyAnkleBalls",
                      "can_mod_post": false,
                      "created_utc": 1753444387,
                      "send_replies": true,
                      "parent_id": "t1_n52ifgt",
                      "score": 3,
                      "author_fullname": "t2_2m3au2xb",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not really how it works in non-ivy academic institutions.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52k00d",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not really how it works in non-ivy academic institutions.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52k00d/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753444387,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n52kfjj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "VihmaVillu",
                      "can_mod_post": false,
                      "created_utc": 1753444555,
                      "send_replies": true,
                      "parent_id": "t1_n52ifgt",
                      "score": 2,
                      "author_fullname": "t2_qrnpi",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "What's your cap limit for free reddit advice?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52kfjj",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your cap limit for free reddit advice?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52kfjj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753444555,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n52khb6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "maifee",
                      "can_mod_post": false,
                      "created_utc": 1753444574,
                      "send_replies": true,
                      "parent_id": "t1_n52ifgt",
                      "score": 1,
                      "author_fullname": "t2_1fuhylzi",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "At least they are being honest, let's appreciate that part.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n52khb6",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "Ollama"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;At least they are being honest, let&amp;#39;s appreciate that part.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m8wuy7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52khb6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753444574,
                      "author_flair_text": "Ollama",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n52ifgt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": "DELETED",
            "no_follow": true,
            "author": "[deleted]",
            "can_mod_post": false,
            "send_replies": true,
            "parent_id": "t3_1m8wuy7",
            "score": -7,
            "approved_by": null,
            "report_reasons": null,
            "all_awardings": [],
            "subreddit_id": "t5_81eyvm",
            "body": "[deleted]",
            "edited": false,
            "downs": 0,
            "author_flair_css_class": null,
            "collapsed": true,
            "is_submitter": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\n&lt;/div&gt;",
            "gildings": {},
            "collapsed_reason": null,
            "associated_award": null,
            "stickied": false,
            "subreddit_type": "public",
            "can_gild": false,
            "top_awarded_type": null,
            "unrepliable_reason": null,
            "author_flair_text_color": "dark",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m8wuy7/how_important_is_to_have_pro_6000_blackwell/n52ifgt/",
            "num_reports": null,
            "locked": false,
            "name": "t1_n52ifgt",
            "created": 1753443764,
            "subreddit": "LocalLLaMA",
            "author_flair_text": null,
            "treatment_tags": [],
            "created_utc": 1753443764,
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "mod_note": null,
            "distinguished": null
          }
        }
      ],
      "before": null
    }
  }
]