[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone!\n\nSo i need help with running the gguf files\nI am using LM Studio and everything is ok.\n\nI have 2 GPU and i want to test out Tensor Parallelism so i can get more speed, but i am facing some issues so i had some questions\n\nIs TP with GGUF even possible? And if yes what backend to use?\nI tried it with Vllm and i got all kinds of error so i dont know what did i do wrong.\n\nAny help is appreciated ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Running GGUF models with TP",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mcl17g",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_clhgguip",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753817429,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\n\n&lt;p&gt;So i need help with running the gguf files\nI am using LM Studio and everything is ok.&lt;/p&gt;\n\n&lt;p&gt;I have 2 GPU and i want to test out Tensor Parallelism so i can get more speed, but i am facing some issues so i had some questions&lt;/p&gt;\n\n&lt;p&gt;Is TP with GGUF even possible? And if yes what backend to use?\nI tried it with Vllm and i got all kinds of error so i dont know what did i do wrong.&lt;/p&gt;\n\n&lt;p&gt;Any help is appreciated &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mcl17g",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Physical-Citron5153",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/",
            "subreddit_subscribers": 506973,
            "created_utc": 1753817429,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5xkhfp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Physical-Citron5153",
                      "can_mod_post": false,
                      "created_utc": 1753855058,
                      "send_replies": true,
                      "parent_id": "t1_n5xbwk5",
                      "score": 1,
                      "author_fullname": "t2_clhgguip",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "So, based on what you say for TP, i need to stick with AWQ and GPTQ as they provide better support for it instead of GGUF quantization marthod? Correct?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5xkhfp",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;So, based on what you say for TP, i need to stick with AWQ and GPTQ as they provide better support for it instead of GGUF quantization marthod? Correct?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mcl17g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/n5xkhfp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753855058,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5xbwk5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "deepnet101",
            "can_mod_post": false,
            "created_utc": 1753850717,
            "send_replies": true,
            "parent_id": "t3_1mcl17g",
            "score": 2,
            "author_fullname": "t2_2vpsoscg",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "sglang and vllm have experimental gguf support",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5xbwk5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;sglang and vllm have experimental gguf support&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/n5xbwk5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753850717,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcl17g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5xkb4c",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Physical-Citron5153",
                      "can_mod_post": false,
                      "created_utc": 1753854966,
                      "send_replies": true,
                      "parent_id": "t1_n5xjiz3",
                      "score": 1,
                      "author_fullname": "t2_clhgguip",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have two RTX 3090 so i think most of my errors are because of my outdated card, as i cant use FP8 models \n I am looking for a platform that has support for most new Local LLMs and i could do TP i will switch over AWQ of GPTQ if needed and support for gguf is limited",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5xkb4c",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have two RTX 3090 so i think most of my errors are because of my outdated card, as i cant use FP8 models \n I am looking for a platform that has support for most new Local LLMs and i could do TP i will switch over AWQ of GPTQ if needed and support for gguf is limited&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mcl17g",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/n5xkb4c/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753854966,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5xjiz3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SandboChang",
            "can_mod_post": false,
            "created_utc": 1753854556,
            "send_replies": true,
            "parent_id": "t3_1mcl17g",
            "score": 1,
            "author_fullname": "t2_10icmj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For vLLM you can show us what config did you use and the error you saw. Also, if you are using vLLM you can try to find if there is GPTQ or AWQ version of the model you are trying.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5xjiz3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For vLLM you can show us what config did you use and the error you saw. Also, if you are using vLLM you can try to find if there is GPTQ or AWQ version of the model you are trying.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcl17g/running_gguf_models_with_tp/n5xjiz3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753854556,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcl17g",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]