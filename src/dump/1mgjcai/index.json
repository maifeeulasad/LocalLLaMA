[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I play The Expanse role-playing game with some friends every week over Zoom. I've captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. \n\nFrom a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through [roll20.net](http://roll20.net) (a virtual tabletop website) as if it were a chat participant.\n\nIt's been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I've defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.\n\nI haven't learned much about training/retraining/pretraining/fine-tuning yes, and I'm wondering if those are better approaches than just bootstrapping the convo\n\n\n\nOther Features in flight:\n\nIntegrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as \"how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration\". \n\nLoading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I'm sure I'm using it incorrectly or perhaps enabling it during inference when it shouldn't be. I could definitely use some advice on that.\n\n\n\nThis must be a common idea, and I'm sure others are working on similar applications; how do I find them?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Roleplay with large historical context and RAG",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgjcai",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_bpv2j",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754227908,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I play The Expanse role-playing game with some friends every week over Zoom. I&amp;#39;ve captured the transcripts for every session.  I intend to run an LLM locally for players to interact with during the game and so it should act as if it were the AI of the ship. &lt;/p&gt;\n\n&lt;p&gt;From a high level, the pipeline goes like this; After every session, I download a transcript from Zoom, I put it through some basic pre-processing to clean it up and minimize the size. I run it through Claude Opus 4 with a very specific prompt on how to best summarize it and that is stored for later use. I run LM studio locally on an M4 MacBook with 48 gigs of RAM. The summaries are appended together into one large historical record for the campaign. That historical record is sent as the first message in the conversation. I have a scripting system that allows the players to interact with the LLM through &lt;a href=\"http://roll20.net\"&gt;roll20.net&lt;/a&gt; (a virtual tabletop website) as if it were a chat participant.&lt;/p&gt;\n\n&lt;p&gt;It&amp;#39;s been a while since I explored the state of the art for this problem space and it seems that a large number of Chinese models have been opened sourced, and so I am wondering if any of them are particularly good at role-play applications.  I&amp;#39;ve defaulted to using mlx-community/Meta-Llama-3.1-8B-Instruct-8bit (64k context tokens) for now , but it seems to be really bad at accurately recalling historical events. It regularly mixes up facts and conflates events.&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t learned much about training/retraining/pretraining/fine-tuning yes, and I&amp;#39;m wondering if those are better approaches than just bootstrapping the convo&lt;/p&gt;\n\n&lt;p&gt;Other Features in flight:&lt;/p&gt;\n\n&lt;p&gt;Integrating with WolframAlpha over MCP so that players can ask for the AI to execute astronomical tasks, such as &amp;quot;how long will it take us to get to Callisto from Himalia if we travel at .3 G acceleration&amp;quot;. &lt;/p&gt;\n\n&lt;p&gt;Loading the core role book and supplement PDFs into the system for searching via RAG. Ideally, this could be used for looking up rules during gameplay. My experiences with RAG has been not great. I&amp;#39;m sure I&amp;#39;m using it incorrectly or perhaps enabling it during inference when it shouldn&amp;#39;t be. I could definitely use some advice on that.&lt;/p&gt;\n\n&lt;p&gt;This must be a common idea, and I&amp;#39;m sure others are working on similar applications; how do I find them?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgjcai",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "RoboCopsGoneMad",
            "discussion_type": null,
            "num_comments": 2,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/",
            "subreddit_subscribers": 509625,
            "created_utc": 1754227908,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6pv0z8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "CV514",
            "can_mod_post": false,
            "created_utc": 1754238562,
            "send_replies": true,
            "parent_id": "t3_1mgjcai",
            "score": 1,
            "author_fullname": "t2_n4zvv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think your scope is quite large; it feels like a whole UX package within an app. I do something similar for my own local stories, summarizing them into lore book entries, gradually compressing or disabling irrelevant ones as the main big story goes on. However, even the smartest large models through paid APIs sometimes struggle with precise data extraction. I don't mind this, since I'm the only one who sees the stories, and manual correction is always an option; this is not the case for you, though. If 100% level of precision is mission critical and intended for player eyes only, I suggest separating data into a robust, stable system, such as an app with some SQL database instead of relying on LLM of any kind. Perhaps even smaller models could benefit from low-depth injection of said database extraction as part of the prompt guidance.\n\nSadly, I can't say that I've found RAG useful in this regard either. However, I'll stick around to read comments to follow, perhaps someone else has had good practical results with it.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6pv0z8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think your scope is quite large; it feels like a whole UX package within an app. I do something similar for my own local stories, summarizing them into lore book entries, gradually compressing or disabling irrelevant ones as the main big story goes on. However, even the smartest large models through paid APIs sometimes struggle with precise data extraction. I don&amp;#39;t mind this, since I&amp;#39;m the only one who sees the stories, and manual correction is always an option; this is not the case for you, though. If 100% level of precision is mission critical and intended for player eyes only, I suggest separating data into a robust, stable system, such as an app with some SQL database instead of relying on LLM of any kind. Perhaps even smaller models could benefit from low-depth injection of said database extraction as part of the prompt guidance.&lt;/p&gt;\n\n&lt;p&gt;Sadly, I can&amp;#39;t say that I&amp;#39;ve found RAG useful in this regard either. However, I&amp;#39;ll stick around to read comments to follow, perhaps someone else has had good practical results with it.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/n6pv0z8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754238562,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgjcai",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6r706y",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArsNeph",
            "can_mod_post": false,
            "created_utc": 1754253246,
            "send_replies": true,
            "parent_id": "t3_1mgjcai",
            "score": 1,
            "author_fullname": "t2_vt0xkv60d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You have a MacBook with 48GB of RAM and you're running Llama 3.1 8B?!??! Llama 3.1 8B is a downright ancient model, and due to it's size and context length, it's unlikely to keep a coherent story going. There are a lot of RP tunes at 12B, but those only support 16K context. I'd recommend looking into larger models, Mistral Small 3.2 24B isn't terrible, nor is Gemma 3 27B. But I'd recommend something bigger, like Valkyrie 49B, or a Llama 3.3 70B fine-tune at 4 bit. You could likely also fit a 3-bit quant of GLM air 100B",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6r706y",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You have a MacBook with 48GB of RAM and you&amp;#39;re running Llama 3.1 8B?!??! Llama 3.1 8B is a downright ancient model, and due to it&amp;#39;s size and context length, it&amp;#39;s unlikely to keep a coherent story going. There are a lot of RP tunes at 12B, but those only support 16K context. I&amp;#39;d recommend looking into larger models, Mistral Small 3.2 24B isn&amp;#39;t terrible, nor is Gemma 3 27B. But I&amp;#39;d recommend something bigger, like Valkyrie 49B, or a Llama 3.3 70B fine-tune at 4 bit. You could likely also fit a 3-bit quant of GLM air 100B&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgjcai/roleplay_with_large_historical_context_and_rag/n6r706y/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754253246,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgjcai",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]