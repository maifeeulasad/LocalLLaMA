[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi, i have a llama and webui running deepseek R1, but i'd like to know some things\n\nfirst how much upgradeability  do i have to the future considering i have only a rtx 4060 in laptop? i dont intend anything crazy i just want some personal organizer\n\nid like it to have long term memory and be able to acces a folder in which ill put documents mainly to schoolar use, to help me study \n\ni dont know if possible but if it could be use to help me research like suggesting search terms\n\nim new to this and ill probably keep it casual, but still want to know how far i can take it to make my life a bit easier",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "I'd like to add some features to my ai if possible",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1ltk7yh",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.57,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_be39tps8a",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1751859832,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi, i have a llama and webui running deepseek R1, but i&amp;#39;d like to know some things&lt;/p&gt;\n\n&lt;p&gt;first how much upgradeability  do i have to the future considering i have only a rtx 4060 in laptop? i dont intend anything crazy i just want some personal organizer&lt;/p&gt;\n\n&lt;p&gt;id like it to have long term memory and be able to acces a folder in which ill put documents mainly to schoolar use, to help me study &lt;/p&gt;\n\n&lt;p&gt;i dont know if possible but if it could be use to help me research like suggesting search terms&lt;/p&gt;\n\n&lt;p&gt;im new to this and ill probably keep it casual, but still want to know how far i can take it to make my life a bit easier&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1ltk7yh",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "RestaurantUnusual456",
            "discussion_type": null,
            "num_comments": 1,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1ltk7yh/id_like_to_add_some_features_to_my_ai_if_possible/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1ltk7yh/id_like_to_add_some_features_to_my_ai_if_possible/",
            "subreddit_subscribers": 496034,
            "created_utc": 1751859832,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n1ruzmt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fizzy1242",
            "can_mod_post": false,
            "created_utc": 1751875700,
            "send_replies": true,
            "parent_id": "t3_1ltk7yh",
            "score": 3,
            "author_fullname": "t2_16zcsx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "upgrading laptops usually isn't worth the hassle.\n\nlook into RAG for reading documents in a folder. many frontends can do this (open-webui, lmstudio...). you'll probably want a semi-small model for long contexts on 4060 (unless you're ok with cpu offloading)\n\nlong-term memory is tricky with llms, summarization and vector storage can help this issue",
            "edited": 1751875925,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n1ruzmt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;upgrading laptops usually isn&amp;#39;t worth the hassle.&lt;/p&gt;\n\n&lt;p&gt;look into RAG for reading documents in a folder. many frontends can do this (open-webui, lmstudio...). you&amp;#39;ll probably want a semi-small model for long contexts on 4060 (unless you&amp;#39;re ok with cpu offloading)&lt;/p&gt;\n\n&lt;p&gt;long-term memory is tricky with llms, summarization and vector storage can help this issue&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1ltk7yh/id_like_to_add_some_features_to_my_ai_if_possible/n1ruzmt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1751875700,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1ltk7yh",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]