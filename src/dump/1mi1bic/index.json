[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi all,\n\nI'm a frequent reader but too poor to actually invest. \nWith all new models and upcomming hardware release I think it is the time to start planning.\n\nMy use case is quite straight foward, just code agent and design doc (md/mermaid) generation. With the rising of AI tool I'm actually spending more and more time on doc generation.\n\nSo what do you guys think from your experience ? Does smaller model but much faster token/s better for your daily work ? Or will the GX10 (x2) beat everything else as openAI server once released",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What should I pick ? 5090 or Asus GX10 or Halo Strix MiniPC at similar prices",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Tutorial | Guide"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mi1bic",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.33,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_g1o0v",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Tutorial | Guide",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754373767,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m a frequent reader but too poor to actually invest. \nWith all new models and upcomming hardware release I think it is the time to start planning.&lt;/p&gt;\n\n&lt;p&gt;My use case is quite straight foward, just code agent and design doc (md/mermaid) generation. With the rising of AI tool I&amp;#39;m actually spending more and more time on doc generation.&lt;/p&gt;\n\n&lt;p&gt;So what do you guys think from your experience ? Does smaller model but much faster token/s better for your daily work ? Or will the GX10 (x2) beat everything else as openAI server once released&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "449b05a6-bf8e-11ed-b4bd-66961e47bd50",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#0079d3",
            "id": "1mi1bic",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "quyetnd",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754373767,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70bq8v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "__JockY__",
            "can_mod_post": false,
            "created_utc": 1754374830,
            "send_replies": true,
            "parent_id": "t3_1mi1bic",
            "score": 4,
            "author_fullname": "t2_qf8h7ka8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "These things are pretty compromised right out of the gate with only 128GB RAM and no PCIe or MCIO expansion.\n\nI’d love to see 256 and 512GB versions that can run some of the newer models released since the GX10’s announcement, like GLM, Kimi, Qwen3 235B, etc. \n\nAlso, without expansion you’ll be stuck without a real GPU, so prompt processing times will be awful, much like a Mac.\n\nIt’s a shame, really. Could’ve been great. But it’s quickly going to slide into non-upgradable obsolescence.\n\nA 5090 is fine, but it’s only got 32GB VRAM so you’ll be running larger models on CPU, which really needs DDR5 RAM and 8- or 12- memory channel CPU. Expensive. But with the 5090 for PP it’ll be a low latency rig that will destroy a Strix, GX10, etc. on prompt processing times.",
            "edited": 1754375090,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70bq8v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;These things are pretty compromised right out of the gate with only 128GB RAM and no PCIe or MCIO expansion.&lt;/p&gt;\n\n&lt;p&gt;I’d love to see 256 and 512GB versions that can run some of the newer models released since the GX10’s announcement, like GLM, Kimi, Qwen3 235B, etc. &lt;/p&gt;\n\n&lt;p&gt;Also, without expansion you’ll be stuck without a real GPU, so prompt processing times will be awful, much like a Mac.&lt;/p&gt;\n\n&lt;p&gt;It’s a shame, really. Could’ve been great. But it’s quickly going to slide into non-upgradable obsolescence.&lt;/p&gt;\n\n&lt;p&gt;A 5090 is fine, but it’s only got 32GB VRAM so you’ll be running larger models on CPU, which really needs DDR5 RAM and 8- or 12- memory channel CPU. Expensive. But with the 5090 for PP it’ll be a low latency rig that will destroy a Strix, GX10, etc. on prompt processing times.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/n70bq8v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754374830,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi1bic",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n712ddx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Only-Letterhead-3411",
            "can_mod_post": false,
            "created_utc": 1754389906,
            "send_replies": true,
            "parent_id": "t3_1mi1bic",
            "score": 2,
            "author_fullname": "t2_pbfqmgf8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a mini pc I use for homelab purposes. It has a mid tier intel cpu (125h) and consumes like 15-30W only while running cpu heavy tasks and runs dockers and local services. It has DDR5 ram. I also run it on turbo mode off so it never exceeds 60C cpu temp even at 100% cpu usage. For fun, I tested running qwen3 30B Q6\\_K on it and I got like 7 t/s on it generating like 3k tokens. If I turned on turbo mode I think I could get close to 10 t/s. It's quite usable tbh and it sips power.\n\nWhat I am trying to say is, if you get something like Halo Strix with 128gb ram or so, you can run big models like GML Air 4.5 or qwen3 235B. It won't be blazing fast like cloud Apis or setups with 8x 3090 but you'll run them at usable speeds and won't have to worry about power usage. You can leave it on 24/7. New trend of model makers are huge MoE models with very low active parameters. This is great for cpu only inference. When you do partial offloading on these models, the t/s gains aren't as big as the dense models. So lots of people go for cpu only inference to save up on power and hardware costs. Something like Mac Studio or maxed out Mac Mini would be faster though but they are expensive af",
            "edited": 1754390127,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n712ddx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a mini pc I use for homelab purposes. It has a mid tier intel cpu (125h) and consumes like 15-30W only while running cpu heavy tasks and runs dockers and local services. It has DDR5 ram. I also run it on turbo mode off so it never exceeds 60C cpu temp even at 100% cpu usage. For fun, I tested running qwen3 30B Q6_K on it and I got like 7 t/s on it generating like 3k tokens. If I turned on turbo mode I think I could get close to 10 t/s. It&amp;#39;s quite usable tbh and it sips power.&lt;/p&gt;\n\n&lt;p&gt;What I am trying to say is, if you get something like Halo Strix with 128gb ram or so, you can run big models like GML Air 4.5 or qwen3 235B. It won&amp;#39;t be blazing fast like cloud Apis or setups with 8x 3090 but you&amp;#39;ll run them at usable speeds and won&amp;#39;t have to worry about power usage. You can leave it on 24/7. New trend of model makers are huge MoE models with very low active parameters. This is great for cpu only inference. When you do partial offloading on these models, the t/s gains aren&amp;#39;t as big as the dense models. So lots of people go for cpu only inference to save up on power and hardware costs. Something like Mac Studio or maxed out Mac Mini would be faster though but they are expensive af&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/n712ddx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754389906,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi1bic",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n71ibh2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1754396788,
            "send_replies": true,
            "parent_id": "t3_1mi1bic",
            "score": 1,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I would say look at the Mi50 on an ATX Epyc board. Four Mi50s will net you 128GB VRAM, and that Epyc will let you spill to the CPU still with decent performance. An Epyc Rome or Milan has 80% the memory bandwidth of Strix Halo, and the Mi50s will provide a nice boost. Won't be as compact nor as power efficient as GX10 or Strix Halo, but it'll be much much much cheaper.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n71ibh2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I would say look at the Mi50 on an ATX Epyc board. Four Mi50s will net you 128GB VRAM, and that Epyc will let you spill to the CPU still with decent performance. An Epyc Rome or Milan has 80% the memory bandwidth of Strix Halo, and the Mi50s will provide a nice boost. Won&amp;#39;t be as compact nor as power efficient as GX10 or Strix Halo, but it&amp;#39;ll be much much much cheaper.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/n71ibh2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754396788,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi1bic",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n70mc4v",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "atape_1",
            "can_mod_post": false,
            "created_utc": 1754380819,
            "send_replies": true,
            "parent_id": "t3_1mi1bic",
            "score": 0,
            "author_fullname": "t2_k35bw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "rtx 4080 48gb",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70mc4v",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;rtx 4080 48gb&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi1bic/what_should_i_pick_5090_or_asus_gx10_or_halo/n70mc4v/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754380819,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mi1bic",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]