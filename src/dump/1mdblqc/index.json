[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi,\n\n  \nSimple question which bugs me - why aren't there more models out there with larger expert sizes?\n\nLike A10B?\n\nMy naive thinking is that Qwen3-50B-A10B would be really powerful. since 30B-A3B is so impressive. But I'm probably missing a lot here :) \n\nActually why did Qwen3 architecture chose A3B, and not say, A4B or A5B? Is there any rule for saying \"this is the optimal expert size\"?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "MoE models with bigger active layers",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdblqc",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_133m0xy6vg",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753893817,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\n\n&lt;p&gt;Simple question which bugs me - why aren&amp;#39;t there more models out there with larger expert sizes?&lt;/p&gt;\n\n&lt;p&gt;Like A10B?&lt;/p&gt;\n\n&lt;p&gt;My naive thinking is that Qwen3-50B-A10B would be really powerful. since 30B-A3B is so impressive. But I&amp;#39;m probably missing a lot here :) &lt;/p&gt;\n\n&lt;p&gt;Actually why did Qwen3 architecture chose A3B, and not say, A4B or A5B? Is there any rule for saying &amp;quot;this is the optimal expert size&amp;quot;?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mdblqc",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Acrobatic_Cat_3448",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/",
            "subreddit_subscribers": 507275,
            "created_utc": 1753893817,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n609yrb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "zennaxxarion",
            "can_mod_post": false,
            "created_utc": 1753894134,
            "send_replies": true,
            "parent_id": "t3_1mdblqc",
            "score": 5,
            "author_fullname": "t2_1gbxvt3ksv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "yeah this bugs me too tbh. like in theory a Qwen3-50B-A10B should be crazy strong, especially if 30B-A3B is already this good. but there’s a bunch of tradeoffs.\n\nonce you go past A3B or A4B the compute per token starts getting high, so you lose a lot of the efficiencygains of MoE. inference gets more expensive, latency goes up etc plus training becomes trickier.\n\nthere’s no golden rule for “best” expert size. it’s kinda like… A2B and A3B tend to hit a sweet spot for cost vs performance.i think Qwen just chose what made sense for their infra and target use cases. but yeah i’d love to see someone train a 50B-A10B beast and see what happens lol.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n609yrb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah this bugs me too tbh. like in theory a Qwen3-50B-A10B should be crazy strong, especially if 30B-A3B is already this good. but there’s a bunch of tradeoffs.&lt;/p&gt;\n\n&lt;p&gt;once you go past A3B or A4B the compute per token starts getting high, so you lose a lot of the efficiencygains of MoE. inference gets more expensive, latency goes up etc plus training becomes trickier.&lt;/p&gt;\n\n&lt;p&gt;there’s no golden rule for “best” expert size. it’s kinda like… A2B and A3B tend to hit a sweet spot for cost vs performance.i think Qwen just chose what made sense for their infra and target use cases. but yeah i’d love to see someone train a 50B-A10B beast and see what happens lol.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/n609yrb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753894134,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdblqc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n60gi21",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1753895891,
            "send_replies": true,
            "parent_id": "t3_1mdblqc",
            "score": 2,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MoE is still pretty actively under research, so I think there's a open question about what is best.  The higher the active parameters, however, the slower the model is, so there's a tradeoff there.  (And note that's not just on inference but training too!)  Huge models like Deepseek 671B and Kimi 1000B use small fractions (~5%) to be affordable and fast.  Qwen3 seems to be broadly ~10% across all sizes (30-A3, 235-A22, 480-A35) because that seemed good to them as a balance of performance vs cost.\n\nThere is one paper indicating that ~20% active might be a sweet spot for quality vs training cost.  Other studies seem to indicate that knowledge scales directly with total parameters while reasoning scales more with sqrt(active*total), so you get more value out of doubling total size than doubling active, and doubling total is generally cheaper too.\n\nFinally, there are some deeper architectural concepts than just number of active params.  In particular, the active parameter count includes plenty of non-experts (e.g. attention tensors) and might include shared experts (ones active every token).  So if you doubled the experts used in Qwen3-234B you'd go from 22B active to 37B.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60gi21",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MoE is still pretty actively under research, so I think there&amp;#39;s a open question about what is best.  The higher the active parameters, however, the slower the model is, so there&amp;#39;s a tradeoff there.  (And note that&amp;#39;s not just on inference but training too!)  Huge models like Deepseek 671B and Kimi 1000B use small fractions (~5%) to be affordable and fast.  Qwen3 seems to be broadly ~10% across all sizes (30-A3, 235-A22, 480-A35) because that seemed good to them as a balance of performance vs cost.&lt;/p&gt;\n\n&lt;p&gt;There is one paper indicating that ~20% active might be a sweet spot for quality vs training cost.  Other studies seem to indicate that knowledge scales directly with total parameters while reasoning scales more with sqrt(active*total), so you get more value out of doubling total size than doubling active, and doubling total is generally cheaper too.&lt;/p&gt;\n\n&lt;p&gt;Finally, there are some deeper architectural concepts than just number of active params.  In particular, the active parameter count includes plenty of non-experts (e.g. attention tensors) and might include shared experts (ones active every token).  So if you doubled the experts used in Qwen3-234B you&amp;#39;d go from 22B active to 37B.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/n60gi21/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753895891,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdblqc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n60j8v4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LagOps91",
            "can_mod_post": false,
            "created_utc": 1753896623,
            "send_replies": true,
            "parent_id": "t3_1mdblqc",
            "score": 3,
            "author_fullname": "t2_3wi6j7vwh",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I think they did a hyperparameter search to see what ratio between active and total parameters best preserves quality while increasing speed. it might very well be possible that something like 50-10 is just marginally better than 50-5 and just not worth spending more compute on and getting slower inference.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60j8v4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think they did a hyperparameter search to see what ratio between active and total parameters best preserves quality while increasing speed. it might very well be possible that something like 50-10 is just marginally better than 50-5 and just not worth spending more compute on and getting slower inference.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/n60j8v4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753896623,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdblqc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n60axz4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1753894400,
            "send_replies": true,
            "parent_id": "t3_1mdblqc",
            "score": 1,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Because they are slow.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60axz4",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because they are slow.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/n60axz4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753894400,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1mdblqc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n60yfg3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "triynizzles1",
            "can_mod_post": false,
            "created_utc": 1753900771,
            "send_replies": true,
            "parent_id": "t3_1mdblqc",
            "score": 1,
            "author_fullname": "t2_zr0g49ixt",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "My initial thoughts is it is probably not on the road map for companies. It appears there is a split in model sizes in the industry. Large parameter 100b+ for showcasing the best SOTA an architecture/ dataset can be. Then smaller 32b and lower size models that are designed with a few of the following in mind: 1. Develop technology to maximize intelligence per parameter. 2. Low cost development of a new architecture. 3. Run on consumer hardware like 3090.\n\nThe number of new releases in the 50,70,90b range have been few and far between. All of the recent releases, like nemotron have all been fine-tune versions of existing models rather than brand new, from scratch architecture.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n60yfg3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My initial thoughts is it is probably not on the road map for companies. It appears there is a split in model sizes in the industry. Large parameter 100b+ for showcasing the best SOTA an architecture/ dataset can be. Then smaller 32b and lower size models that are designed with a few of the following in mind: 1. Develop technology to maximize intelligence per parameter. 2. Low cost development of a new architecture. 3. Run on consumer hardware like 3090.&lt;/p&gt;\n\n&lt;p&gt;The number of new releases in the 50,70,90b range have been few and far between. All of the recent releases, like nemotron have all been fine-tune versions of existing models rather than brand new, from scratch architecture.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/n60yfg3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753900771,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdblqc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61958q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1753903849,
            "send_replies": true,
            "parent_id": "t3_1mdblqc",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Like their A22B or A35B models?\n\n&gt;Is there any rule for saying \"this is the optimal expert size\"?\n\nProbably not, since they did 3 separate MoE sizes already.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61958q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Like their A22B or A35B models?&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Is there any rule for saying &amp;quot;this is the optimal expert size&amp;quot;?&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Probably not, since they did 3 separate MoE sizes already.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/n61958q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753903849,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdblqc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n61m3od",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "dazl1212",
            "can_mod_post": false,
            "created_utc": 1753907507,
            "send_replies": true,
            "parent_id": "t3_1mdblqc",
            "score": 2,
            "author_fullname": "t2_14mcti",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Jamba mini 1.7? 51b Active 12b.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n61m3od",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Jamba mini 1.7? 51b Active 12b.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdblqc/moe_models_with_bigger_active_layers/n61m3od/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753907507,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdblqc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        }
      ],
      "before": null
    }
  }
]