[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?\n\nI know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.\n\nBut I wanted to ask what happens if the plan is to exceed the vram and use system ram?\n\nIf I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?\n\nRight now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?\n\nIn the scenarios I am assuming that the model spills out into the ram either way.\n\nIf the llm spills out into the ram, is it cpu inference now?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Ram Speed importance when exceeding VRAM",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lw8lvt",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rn6co7q5m",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752140750,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?&lt;/p&gt;\n\n&lt;p&gt;I know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.&lt;/p&gt;\n\n&lt;p&gt;But I wanted to ask what happens if the plan is to exceed the vram and use system ram?&lt;/p&gt;\n\n&lt;p&gt;If I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?&lt;/p&gt;\n\n&lt;p&gt;Right now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?&lt;/p&gt;\n\n&lt;p&gt;In the scenarios I am assuming that the model spills out into the ram either way.&lt;/p&gt;\n\n&lt;p&gt;If the llm spills out into the ram, is it cpu inference now?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lw8lvt",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "opoot_",
            "discussion_type": null,
            "num_comments": 5,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/",
            "subreddit_subscribers": 497021,
            "created_utc": 1752140750,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2c3ray",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fizzy1242",
            "can_mod_post": false,
            "created_utc": 1752141077,
            "send_replies": true,
            "parent_id": "t3_1lw8lvt",
            "score": 2,
            "author_fullname": "t2_16zcsx",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "definitely helps for moe models for shuffling experts",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2c3ray",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;definitely helps for moe models for shuffling experts&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2c3ray/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752141077,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw8lvt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2c4o37",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Rich_Repeat_22",
            "can_mod_post": false,
            "created_utc": 1752141568,
            "send_replies": true,
            "parent_id": "t3_1lw8lvt",
            "score": 2,
            "author_fullname": "t2_viufiki6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "a) 96GB is more than 32GB so can load bigger MOE, speed isn't that much different when comes to dual channel CPUs. Now if we are talking about 8channel then yes, there is big difference :)  \n  \nb) I would advice to get a second used 7900XT as the first opportunity instead of an ancient 6800 16GB.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2c4o37",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;a) 96GB is more than 32GB so can load bigger MOE, speed isn&amp;#39;t that much different when comes to dual channel CPUs. Now if we are talking about 8channel then yes, there is big difference :)  &lt;/p&gt;\n\n&lt;p&gt;b) I would advice to get a second used 7900XT as the first opportunity instead of an ancient 6800 16GB.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2c4o37/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752141568,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw8lvt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2c3gma",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "reacusn",
            "can_mod_post": false,
            "created_utc": 1752140916,
            "send_replies": true,
            "parent_id": "t3_1lw8lvt",
            "score": 1,
            "author_fullname": "t2_1ppg6hcqm8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I don't think the difference between 6000 and 6400 is going to have much of an impact.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2c3gma",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t think the difference between 6000 and 6400 is going to have much of an impact.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2c3gma/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752140916,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw8lvt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2ceysw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Aphid_red",
            "can_mod_post": false,
            "created_utc": 1752146580,
            "send_replies": true,
            "parent_id": "t3_1lw8lvt",
            "score": 2,
            "author_fullname": "t2_csn2q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It depends on what you're doing. \n\nAre you just letting NVidia's/AMD's automated fallback do what it does? Then it doesn't matter *at all*, you're bottlenecked by (at most) 16GB/s PCI-e speeds. Are you using koboldcpp/llama.cpp to manually tune how much (layers) to run on the CPU? Then it *does* matter. In fact, it'll dominate your tps. Let me show you how: \n\nImagine your CPU and GPU are a turtle and a cheetah working together in a relay race. When the race is complete, one token is generated. The proportion of the model that is in RAM is the part of the race done by the turtle. \n\nAdding another slower GPU is like taking a chunk of the race and letting a greyhound do that part of the race. Going for slightly faster RAM is like speeding the turtle's movements up by 5%. \n\nTPS is determined by memory speeds. The more fast memory you have, the more tokens per second your machine can crank out. You can divide your model between slower and faster memory, but you always have to wait for the 'smallest pipe' in the whole system to throughput its data.\n\nSay you have 50GB/s RAM speed and 1000GB/s VRAM speed. You're running a model that takes 50GB for its params, and can fit 20GB of it  on your 3090. (The other 4GB are for context) The model's other 30GB goes on the CPU. \n\nTo generate one token, your CPU will take at least about 0.6 seconds of time (30GB over 50GB/s).   \nYour GPU will take at least 0.02 seconds. \n\nThe total is 0.62 seconds, or 1.61 tps. In reality you'll see about 70% of that theoretical max, or 1.1 tps.\n\nNow let's see what happens with 200GB/s RAM speeds (let's say you upgrade from a consumer DDR4 machine to a second hand epyc milan server, the best bang for your buck for RAM speeds). Here, \n\nThe CPU will take 0.15s to do its layers.   \nThe GPU will again take 0.02s. \n\nThe total is 0.17 seconds, or 5.9tps. With again about 70% real life performance 4.1tps seems about what you'd see.\n\nA similar thing applies when combining slower and faster GPUs. You won't see much of a difference from going from 2x P40 to P40 + 3090, but you *will* notice going from P40 + 3090 to 2x3090.   \n(You will also notice using tensor parallel once you go for more than one GPU if the model fits in VRAM of both).\n\n  \nNow your question: Which one of your two upgrades does more? The question should be: what model are you running? Then, you can calculate which upgrade will improve your generation speed more. It could be either one: with a very large model, most of it is in RAM and RAM speed matters. With a smaller model, adding another GPU will move a substantial portion of your calculations to GPU, perhaps even all of it. With no poor turtles in the race, the times obviously improve markedly.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2ceysw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It depends on what you&amp;#39;re doing. &lt;/p&gt;\n\n&lt;p&gt;Are you just letting NVidia&amp;#39;s/AMD&amp;#39;s automated fallback do what it does? Then it doesn&amp;#39;t matter &lt;em&gt;at all&lt;/em&gt;, you&amp;#39;re bottlenecked by (at most) 16GB/s PCI-e speeds. Are you using koboldcpp/llama.cpp to manually tune how much (layers) to run on the CPU? Then it &lt;em&gt;does&lt;/em&gt; matter. In fact, it&amp;#39;ll dominate your tps. Let me show you how: &lt;/p&gt;\n\n&lt;p&gt;Imagine your CPU and GPU are a turtle and a cheetah working together in a relay race. When the race is complete, one token is generated. The proportion of the model that is in RAM is the part of the race done by the turtle. &lt;/p&gt;\n\n&lt;p&gt;Adding another slower GPU is like taking a chunk of the race and letting a greyhound do that part of the race. Going for slightly faster RAM is like speeding the turtle&amp;#39;s movements up by 5%. &lt;/p&gt;\n\n&lt;p&gt;TPS is determined by memory speeds. The more fast memory you have, the more tokens per second your machine can crank out. You can divide your model between slower and faster memory, but you always have to wait for the &amp;#39;smallest pipe&amp;#39; in the whole system to throughput its data.&lt;/p&gt;\n\n&lt;p&gt;Say you have 50GB/s RAM speed and 1000GB/s VRAM speed. You&amp;#39;re running a model that takes 50GB for its params, and can fit 20GB of it  on your 3090. (The other 4GB are for context) The model&amp;#39;s other 30GB goes on the CPU. &lt;/p&gt;\n\n&lt;p&gt;To generate one token, your CPU will take at least about 0.6 seconds of time (30GB over 50GB/s).&lt;br/&gt;\nYour GPU will take at least 0.02 seconds. &lt;/p&gt;\n\n&lt;p&gt;The total is 0.62 seconds, or 1.61 tps. In reality you&amp;#39;ll see about 70% of that theoretical max, or 1.1 tps.&lt;/p&gt;\n\n&lt;p&gt;Now let&amp;#39;s see what happens with 200GB/s RAM speeds (let&amp;#39;s say you upgrade from a consumer DDR4 machine to a second hand epyc milan server, the best bang for your buck for RAM speeds). Here, &lt;/p&gt;\n\n&lt;p&gt;The CPU will take 0.15s to do its layers.&lt;br/&gt;\nThe GPU will again take 0.02s. &lt;/p&gt;\n\n&lt;p&gt;The total is 0.17 seconds, or 5.9tps. With again about 70% real life performance 4.1tps seems about what you&amp;#39;d see.&lt;/p&gt;\n\n&lt;p&gt;A similar thing applies when combining slower and faster GPUs. You won&amp;#39;t see much of a difference from going from 2x P40 to P40 + 3090, but you &lt;em&gt;will&lt;/em&gt; notice going from P40 + 3090 to 2x3090.&lt;br/&gt;\n(You will also notice using tensor parallel once you go for more than one GPU if the model fits in VRAM of both).&lt;/p&gt;\n\n&lt;p&gt;Now your question: Which one of your two upgrades does more? The question should be: what model are you running? Then, you can calculate which upgrade will improve your generation speed more. It could be either one: with a very large model, most of it is in RAM and RAM speed matters. With a smaller model, adding another GPU will move a substantial portion of your calculations to GPU, perhaps even all of it. With no poor turtles in the race, the times obviously improve markedly.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2ceysw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752146580,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw8lvt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n2cf01w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "FullstackSensei",
            "can_mod_post": false,
            "created_utc": 1752146595,
            "send_replies": true,
            "parent_id": "t3_1lw8lvt",
            "score": 1,
            "author_fullname": "t2_17n3nqtj56",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you're sticking to the same platform, you're splitting hairs. Latency doesn't make a difference because memory access in LLMs is purely sequential and the memory/cache controller/prefetcher will take care of prefetching the next cache lines.\n\nWhere you can gain quite a bit of performance when using system RAM is by switching to server platforms. You don't need to break the bank for that. A 2019 Cascade Lake Xeon has six memory channels running at DDR4-2933, which bring ~140GB/s theoretical peak bandwidth. An Epyc Rome from that same year has eight channels running at DDR4-3200, good for ~208GB/s theoretical bandwidth. Them two channels of DDR5-6400 have ~102GB/s theoretical bandwidth. The best part about Xeon and Epyc is that 32GB ECC RDIMM sticks cost a quarter or even less per GB vs desktop DDR5 memory. Of course, you need to do your homework beforehand and understand how those platforms work and what options you have for motherboards and CPUs, but IMO it's not that much work.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2cf01w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you&amp;#39;re sticking to the same platform, you&amp;#39;re splitting hairs. Latency doesn&amp;#39;t make a difference because memory access in LLMs is purely sequential and the memory/cache controller/prefetcher will take care of prefetching the next cache lines.&lt;/p&gt;\n\n&lt;p&gt;Where you can gain quite a bit of performance when using system RAM is by switching to server platforms. You don&amp;#39;t need to break the bank for that. A 2019 Cascade Lake Xeon has six memory channels running at DDR4-2933, which bring ~140GB/s theoretical peak bandwidth. An Epyc Rome from that same year has eight channels running at DDR4-3200, good for ~208GB/s theoretical bandwidth. Them two channels of DDR5-6400 have ~102GB/s theoretical bandwidth. The best part about Xeon and Epyc is that 32GB ECC RDIMM sticks cost a quarter or even less per GB vs desktop DDR5 memory. Of course, you need to do your homework beforehand and understand how those platforms work and what options you have for motherboards and CPUs, but IMO it&amp;#39;s not that much work.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2cf01w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752146595,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lw8lvt",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]