[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Can I make a ram based server hardware llm machine, something like a Xeon or epic with 12 channel ram. \n\nBut since I am worried about cpu prompt processing speed, can I add a gpu like a 4070, good gpu chip, kinda shit amount of vram, can I add something like that to handle the prompt processing, while leveraging the ram and bandwidth that I would get with server hardware?\n\nFrom what I know, the reason why vram is preferable to ram is memory bandwidth.\n\nWith server hardware, I can get 6 or 12 channel ddr4, which give me like 200gb/s bandwidth just for the system ram. This is fine enough for me, but I’m afrid the cpu prompt processing speed will be bad, so yeah\n\nDoes this work? If it doesn’t, why not?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Gpu just for prompt processing?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m92di7",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.75,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_rn6co7q5m",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753457684,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Can I make a ram based server hardware llm machine, something like a Xeon or epic with 12 channel ram. &lt;/p&gt;\n\n&lt;p&gt;But since I am worried about cpu prompt processing speed, can I add a gpu like a 4070, good gpu chip, kinda shit amount of vram, can I add something like that to handle the prompt processing, while leveraging the ram and bandwidth that I would get with server hardware?&lt;/p&gt;\n\n&lt;p&gt;From what I know, the reason why vram is preferable to ram is memory bandwidth.&lt;/p&gt;\n\n&lt;p&gt;With server hardware, I can get 6 or 12 channel ddr4, which give me like 200gb/s bandwidth just for the system ram. This is fine enough for me, but I’m afrid the cpu prompt processing speed will be bad, so yeah&lt;/p&gt;\n\n&lt;p&gt;Does this work? If it doesn’t, why not?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1m92di7",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "opoot_",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/",
            "subreddit_subscribers": 504692,
            "created_utc": 1753457684,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n53r159",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AppearanceHeavy6724",
            "can_mod_post": false,
            "created_utc": 1753457908,
            "send_replies": true,
            "parent_id": "t3_1m92di7",
            "score": 2,
            "author_fullname": "t2_uz37qfx5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes it will work, but not super well.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n53r159",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes it will work, but not super well.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n53r159/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753457908,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m92di7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n53t5x3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "smflx",
            "can_mod_post": false,
            "created_utc": 1753458493,
            "send_replies": true,
            "parent_id": "t3_1m92di7",
            "score": 2,
            "author_fullname": "t2_16qe65sqwe",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yes, prompt processing will be slow on CPU. So, you think of putting a GPU for faster prompt processing. It's faster, but not enough. Now, communication bandwidth between CPU &amp; GPU is bottleneck.\n\nEven for token generation, 200GB/s is far less than VRAM bandwidth.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n53t5x3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, prompt processing will be slow on CPU. So, you think of putting a GPU for faster prompt processing. It&amp;#39;s faster, but not enough. Now, communication bandwidth between CPU &amp;amp; GPU is bottleneck.&lt;/p&gt;\n\n&lt;p&gt;Even for token generation, 200GB/s is far less than VRAM bandwidth.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n53t5x3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753458493,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m92di7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n53tr0e",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lacerating_aura",
            "can_mod_post": false,
            "created_utc": 1753458655,
            "send_replies": true,
            "parent_id": "t3_1m92di7",
            "score": 1,
            "author_fullname": "t2_9nex5np2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sure you can. This will be especially useful for MoE models. Load the experts in ram while dense layers and cache is kept in vram. This also works for regular dense models, keeping only KV cache in vram. You can even keep the cache in ram and use the GPU only for prompt processing, which takes minimal vram, like 4gb or something. Although speed would obviously take hit. You would want your GPU to be connected at maximum PCIe link that it supports so the data transfer between ram and vram can be fast. This is a guess.\n\nPersonally I have tried this with 70b models, like using Q4kl quant, I keep the weights in ram and the cache in vram, which takes about 12gb at 32k fp16 context. This gives somewhat equal speed to partial offloading in my tests. \n\nI also tried the opposite, keeping weights in vram, like 70b iq3xs quant for 16x2gb split, and keeping the cache in ram, but this config seems unstable as after filing about 8k context, the software (kcpp) crashes randomly.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n53tr0e",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sure you can. This will be especially useful for MoE models. Load the experts in ram while dense layers and cache is kept in vram. This also works for regular dense models, keeping only KV cache in vram. You can even keep the cache in ram and use the GPU only for prompt processing, which takes minimal vram, like 4gb or something. Although speed would obviously take hit. You would want your GPU to be connected at maximum PCIe link that it supports so the data transfer between ram and vram can be fast. This is a guess.&lt;/p&gt;\n\n&lt;p&gt;Personally I have tried this with 70b models, like using Q4kl quant, I keep the weights in ram and the cache in vram, which takes about 12gb at 32k fp16 context. This gives somewhat equal speed to partial offloading in my tests. &lt;/p&gt;\n\n&lt;p&gt;I also tried the opposite, keeping weights in vram, like 70b iq3xs quant for 16x2gb split, and keeping the cache in ram, but this config seems unstable as after filing about 8k context, the software (kcpp) crashes randomly.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n53tr0e/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753458655,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m92di7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n56ed5b",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Willing_Landscape_61",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5593cp",
                                "score": 1,
                                "author_fullname": "t2_8lvrytgw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Unfortunately, I only have 1 x 4090 and it's not obvious to scale perf from 1 GPU to N GPU because especially with MoE you offload first the most critical layers and have then diminishing returns.\nI'll soon have 3 or 4 MI100 with supposedly comparable perf to 3090. ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n56ed5b",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Unfortunately, I only have 1 x 4090 and it&amp;#39;s not obvious to scale perf from 1 GPU to N GPU because especially with MoE you offload first the most critical layers and have then diminishing returns.\nI&amp;#39;ll soon have 3 or 4 MI100 with supposedly comparable perf to 3090. &lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1m92di7",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n56ed5b/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753486208,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753486208,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5593cp",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Leflakk",
                      "can_mod_post": false,
                      "created_utc": 1753473279,
                      "send_replies": true,
                      "parent_id": "t1_n54f2af",
                      "score": 1,
                      "author_fullname": "t2_udr659irv",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I am not the OP but I already got 4x3090 (and can't afford DDR5 setup) then I am actually wondering how it could go with an Epyc Gen2 + 8 DDR4 (3200?) for a model like Deepseek or the new Qwen3 coder. So I am interested to get more details on your results, thank you!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5593cp",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I am not the OP but I already got 4x3090 (and can&amp;#39;t afford DDR5 setup) then I am actually wondering how it could go with an Epyc Gen2 + 8 DDR4 (3200?) for a model like Deepseek or the new Qwen3 coder. So I am interested to get more details on your results, thank you!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m92di7",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n5593cp/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753473279,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n54f2af",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1753464607,
            "send_replies": true,
            "parent_id": "t3_1m92di7",
            "score": 1,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You can easily lookup benchmarks of such servers. What kind of models/quants do you want to run? How much context?\nWhat pp speed is acceptable to you?\nI may give you relevant info about what to expect with my own Epyc Gen 2 8x DDR4+ 1x 4090 server.\n\n\nFor a DeepSeek Q4 you might expect from 80 to 60 t/s of pp depending on context size (0 to 32k ).",
            "edited": 1753465062,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n54f2af",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can easily lookup benchmarks of such servers. What kind of models/quants do you want to run? How much context?\nWhat pp speed is acceptable to you?\nI may give you relevant info about what to expect with my own Epyc Gen 2 8x DDR4+ 1x 4090 server.&lt;/p&gt;\n\n&lt;p&gt;For a DeepSeek Q4 you might expect from 80 to 60 t/s of pp depending on context size (0 to 32k ).&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n54f2af/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753464607,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m92di7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n55cecj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1753474250,
            "send_replies": true,
            "parent_id": "t3_1m92di7",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Go 3090 or at least the 4070 TI with 16GB, or you're going to get limited on context that fits into the card. The KV cache being local to the GPU is how you make use of the compute to speed up PP. 12GB single card you may not be able to do 128k context even with -ngl 0.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n55cecj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Go 3090 or at least the 4070 TI with 16GB, or you&amp;#39;re going to get limited on context that fits into the card. The KV cache being local to the GPU is how you make use of the compute to speed up PP. 12GB single card you may not be able to do 128k context even with -ngl 0.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n55cecj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753474250,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m92di7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ef488598-491f-11ef-a847-9a3dd315819c",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n549q6p",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "panchovix",
            "can_mod_post": false,
            "created_utc": 1753463146,
            "send_replies": true,
            "parent_id": "t3_1m92di7",
            "score": -1,
            "author_fullname": "t2_j1kqr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It will work but the bottleneck is PCIe bandwidth then. So on a 4070 you are limited to about 26-28 GiB/s or so.\n\nA 5070 i.e. at X16 5.0 (and if your board + CPU supports PCIe 5.0) then it is 2x tha, at about 53-56 GiB/s which is a lot better.\n\nI got literally 2x PP t/s when offloading, going from X8 4.0 to X8 5.0.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n549q6p",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 405B"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It will work but the bottleneck is PCIe bandwidth then. So on a 4070 you are limited to about 26-28 GiB/s or so.&lt;/p&gt;\n\n&lt;p&gt;A 5070 i.e. at X16 5.0 (and if your board + CPU supports PCIe 5.0) then it is 2x tha, at about 53-56 GiB/s which is a lot better.&lt;/p&gt;\n\n&lt;p&gt;I got literally 2x PP t/s when offloading, going from X8 4.0 to X8 5.0.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m92di7/gpu_just_for_prompt_processing/n549q6p/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753463146,
            "author_flair_text": "Llama 405B",
            "treatment_tags": [],
            "link_id": "t3_1m92di7",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": -1
          }
        }
      ],
      "before": null
    }
  }
]