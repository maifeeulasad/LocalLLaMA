[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "1. Does converting to GGUF further quantize it, or just change the format? https://github.com/ggml-org/llama.cpp/pull/15091#issuecomment-3156098490\n\n2. This guy mentions f16 gguf \"works just fine\". How do you get a 16bit quant from a 4bit starting point? Does it improve the intelligence? https://github.com/ggml-org/llama.cpp/pull/15091#issuecomment-3156118810\n\nAnyways, I generally only go for 8bit quants, or at worst 6bit. The last charts I remember showed that intelligence/capabilities tank significantly below the large/high version of 6bit quantization. So...ü§∑‚Äç‚ôÇÔ∏è",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "GPT OSS: They're 4bit quantized by default (giga yikes üò¨), but these comments on the GGUF PR raise some questions.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mink69",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.69,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_kvniqgt7",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754433974,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754433171,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;&lt;p&gt;Does converting to GGUF further quantize it, or just change the format? &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15091#issuecomment-3156098490\"&gt;https://github.com/ggml-org/llama.cpp/pull/15091#issuecomment-3156098490&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;li&gt;&lt;p&gt;This guy mentions f16 gguf &amp;quot;works just fine&amp;quot;. How do you get a 16bit quant from a 4bit starting point? Does it improve the intelligence? &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15091#issuecomment-3156118810\"&gt;https://github.com/ggml-org/llama.cpp/pull/15091#issuecomment-3156118810&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Anyways, I generally only go for 8bit quants, or at worst 6bit. The last charts I remember showed that intelligence/capabilities tank significantly below the large/high version of 6bit quantization. So...ü§∑‚Äç‚ôÇÔ∏è&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0.png?auto=webp&amp;s=cd0f8ca9fd0c7f81d6076fccb5c6d1d0326670c1",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bda696394a3347daacb18a8a6a074a4900474307",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ff53d3362d28a1c6301445dabaa1ff5915bc119",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=3f0a35eddee4e5f20f033f1a4014fd9008e80130",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e4c56def5c919fda31730429221f4618fb38af1b",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a6252e350dc05cea44bcc1f85bb8fc5b7a67616c",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7e2312ece35f696d04c366202c78165e477e1e1e",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "uoAlW2qpCt3e8xck14MuY5UZnRkqWuVhQs4fD_GJKW0"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mink69",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Virtamancer",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mink69/gpt_oss_theyre_4bit_quantized_by_default_giga/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mink69/gpt_oss_theyre_4bit_quantized_by_default_giga/",
            "subreddit_subscribers": 511364,
            "created_utc": 1754433171,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7566ro",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Virtamancer",
                      "can_mod_post": false,
                      "created_utc": 1754439031,
                      "send_replies": true,
                      "parent_id": "t1_n7551m0",
                      "score": 2,
                      "author_fullname": "t2_kvniqgt7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Super helpful if true, considering you there are contrary claims out there.\n\nReally hoping for a lossless mlx conversion asap ü§û",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7566ro",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Super helpful if true, considering you there are contrary claims out there.&lt;/p&gt;\n\n&lt;p&gt;Really hoping for a lossless mlx conversion asap ü§û&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mink69",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mink69/gpt_oss_theyre_4bit_quantized_by_default_giga/n7566ro/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754439031,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7551m0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754438642,
            "send_replies": true,
            "parent_id": "t3_1mink69",
            "score": 4,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "They aren't quantized by default; they are trained in MXFP4 (according to OpenAI - I've seen claims to the contrary but haven't seen evidence).  It's just that this is considered a quantization because fp4 is unstable on its own and so includes a block scale much like a Q4_0 quant.  However because all the values are updated with the training it's still the true full precision model.\n\nConverting to gguf _can_ quantize it, but usually you convert without precision loss and then quantize it further from the 'raw' gguf.  It's interesting to note that some tensors get promoted from bf16 to fp32 for some reason when converting from the safe tensors resulting in a larger model with no benefit.  Right now there isn't a way to convert from mxfp4 to anything else afaict, however it would be lossless to convert to bf16.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7551m0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They aren&amp;#39;t quantized by default; they are trained in MXFP4 (according to OpenAI - I&amp;#39;ve seen claims to the contrary but haven&amp;#39;t seen evidence).  It&amp;#39;s just that this is considered a quantization because fp4 is unstable on its own and so includes a block scale much like a Q4_0 quant.  However because all the values are updated with the training it&amp;#39;s still the true full precision model.&lt;/p&gt;\n\n&lt;p&gt;Converting to gguf &lt;em&gt;can&lt;/em&gt; quantize it, but usually you convert without precision loss and then quantize it further from the &amp;#39;raw&amp;#39; gguf.  It&amp;#39;s interesting to note that some tensors get promoted from bf16 to fp32 for some reason when converting from the safe tensors resulting in a larger model with no benefit.  Right now there isn&amp;#39;t a way to convert from mxfp4 to anything else afaict, however it would be lossless to convert to bf16.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mink69/gpt_oss_theyre_4bit_quantized_by_default_giga/n7551m0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754438642,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mink69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n74r0gb",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "shokuninstudio",
            "can_mod_post": false,
            "created_utc": 1754433995,
            "send_replies": true,
            "parent_id": "t3_1mink69",
            "score": 1,
            "author_fullname": "t2_4xzh04rz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "[https://www.reddit.com/r/LocalLLaMA/comments/1milkqp/comment/n74ag4d/?context=3](https://www.reddit.com/r/LocalLLaMA/comments/1milkqp/comment/n74ag4d/?context=3)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n74r0gb",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1milkqp/comment/n74ag4d/?context=3\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1milkqp/comment/n74ag4d/?context=3&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mink69/gpt_oss_theyre_4bit_quantized_by_default_giga/n74r0gb/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754433995,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mink69",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]