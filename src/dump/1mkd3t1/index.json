[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I want to compare some models for on an italian medical quiz benchmark (with text and some images as well for vision models) I'm creating and I'm looking for suggestions, both open and closed source. \n\nMedgemma is a must, then the most important families of models: gemini from pro to flash-lite, open AI new gpt5 and oss models, R1 and V3, but after this I'm unsure. \n\nI think I'm gonna skip anthropic for now since those are code focused and not that cheap.\n\nWhat qwen models do you reccomend? Also, GLM-4.5 yes or no?\n\nOther less known models?\n\nI will share all results here. Thank you all",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Reccomendation for new medical benchmark",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mkd3t1",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_15qzm1",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754603207,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I want to compare some models for on an italian medical quiz benchmark (with text and some images as well for vision models) I&amp;#39;m creating and I&amp;#39;m looking for suggestions, both open and closed source. &lt;/p&gt;\n\n&lt;p&gt;Medgemma is a must, then the most important families of models: gemini from pro to flash-lite, open AI new gpt5 and oss models, R1 and V3, but after this I&amp;#39;m unsure. &lt;/p&gt;\n\n&lt;p&gt;I think I&amp;#39;m gonna skip anthropic for now since those are code focused and not that cheap.&lt;/p&gt;\n\n&lt;p&gt;What qwen models do you reccomend? Also, GLM-4.5 yes or no?&lt;/p&gt;\n\n&lt;p&gt;Other less known models?&lt;/p&gt;\n\n&lt;p&gt;I will share all results here. Thank you all&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mkd3t1",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "sebastianmicu24",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mkd3t1/reccomendation_for_new_medical_benchmark/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mkd3t1/reccomendation_for_new_medical_benchmark/",
            "subreddit_subscribers": 513813,
            "created_utc": 1754603207,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ibq3k",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754609262,
            "send_replies": true,
            "parent_id": "t3_1mkd3t1",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Qwen3-32B did pretty well with the medical questions in my standard test:\n\nhttp://ciar.org/h/test.1746856197.q3.txt\n\nFind within that document \"biomed:\" to see how it answered those questions.  Note that each prompt is repeated five times to get a good idea of reliability and outlier behavior.\n\nYou might want to also include Tulu3-70B or Tulu3-405B if those are not too old (they are STEM fine-tunes of Llama-3.1):\n\nhttp://ciar.org/h/test.1743405489.t370.txt\n\nI cannot say about the newer Qwen3 MoE, as I have not yet assessed them.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ibq3k",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3-32B did pretty well with the medical questions in my standard test:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://ciar.org/h/test.1746856197.q3.txt\"&gt;http://ciar.org/h/test.1746856197.q3.txt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Find within that document &amp;quot;biomed:&amp;quot; to see how it answered those questions.  Note that each prompt is repeated five times to get a good idea of reliability and outlier behavior.&lt;/p&gt;\n\n&lt;p&gt;You might want to also include Tulu3-70B or Tulu3-405B if those are not too old (they are STEM fine-tunes of Llama-3.1):&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://ciar.org/h/test.1743405489.t370.txt\"&gt;http://ciar.org/h/test.1743405489.t370.txt&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;I cannot say about the newer Qwen3 MoE, as I have not yet assessed them.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkd3t1/reccomendation_for_new_medical_benchmark/n7ibq3k/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754609262,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mkd3t1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7icj94",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ttkciar",
            "can_mod_post": false,
            "created_utc": 1754609537,
            "send_replies": true,
            "parent_id": "t3_1mkd3t1",
            "score": 1,
            "author_fullname": "t2_cpegz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Also, if you do include MedGemma-27B in your quiz (which I strongly recommend; it is an excellent model) you should give it a system prompt which tells it it is advising a medical professional, otherwise it will tell you to go ask a real doctor.\n\nHere's the wrapper script I used with it.  You can see the various commented-out `PREAMBLE` strings I tried, which were highly effective at shaping the kinds of answers it gave:\n\nhttp://ciar.org/h/mg",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7icj94",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also, if you do include MedGemma-27B in your quiz (which I strongly recommend; it is an excellent model) you should give it a system prompt which tells it it is advising a medical professional, otherwise it will tell you to go ask a real doctor.&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s the wrapper script I used with it.  You can see the various commented-out &lt;code&gt;PREAMBLE&lt;/code&gt; strings I tried, which were highly effective at shaping the kinds of answers it gave:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"http://ciar.org/h/mg\"&gt;http://ciar.org/h/mg&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkd3t1/reccomendation_for_new_medical_benchmark/n7icj94/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754609537,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mkd3t1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7kgtv8",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Affectionate-Cap-600",
            "can_mod_post": false,
            "created_utc": 1754642830,
            "send_replies": true,
            "parent_id": "t3_1mkd3t1",
            "score": 1,
            "author_fullname": "t2_5oltmr5b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "hey. I'm in Italian med student and I'm working a lot to integrate llm.\nwould you like to share some findings / info? (even in dm if you prefer)\n\nbtw, glm-4.5 is really powerful (I prefer it to qwen 3 235B).\nhave you tried minimax-M1? it is the only model that can handle really long context (other than gemini) and it is open weights.\n\nI found that even Nemotron ultra 253B v1 has of knowledge and it has one of the best writing style in Italian (compared to modern MoEs). it is derived from llama 405B using Neural Architecture Search (llama 405B is probably still a good base model, lot of knowledge and amazing for language other than English. I'll have to try the new Cogito 405B but I can't find it hosted anywhere).\nyou can try nemotron ultra on Nvidia Nimm for free (and even use a lot of API calls for free)\nit still has some problems (ie, sometimes it ignore the instruction that should turn 'on' the reasoning), I hope that we will get an updated v1_5. they released nemotron 49B v1_5, and on an huggingface discussion one of the authors that they are working on new models but that they obviously can't comment about specific models... let's hope.",
            "edited": 1754643343,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7kgtv8",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;hey. I&amp;#39;m in Italian med student and I&amp;#39;m working a lot to integrate llm.\nwould you like to share some findings / info? (even in dm if you prefer)&lt;/p&gt;\n\n&lt;p&gt;btw, glm-4.5 is really powerful (I prefer it to qwen 3 235B).\nhave you tried minimax-M1? it is the only model that can handle really long context (other than gemini) and it is open weights.&lt;/p&gt;\n\n&lt;p&gt;I found that even Nemotron ultra 253B v1 has of knowledge and it has one of the best writing style in Italian (compared to modern MoEs). it is derived from llama 405B using Neural Architecture Search (llama 405B is probably still a good base model, lot of knowledge and amazing for language other than English. I&amp;#39;ll have to try the new Cogito 405B but I can&amp;#39;t find it hosted anywhere).\nyou can try nemotron ultra on Nvidia Nimm for free (and even use a lot of API calls for free)\nit still has some problems (ie, sometimes it ignore the instruction that should turn &amp;#39;on&amp;#39; the reasoning), I hope that we will get an updated v1_5. they released nemotron 49B v1_5, and on an huggingface discussion one of the authors that they are working on new models but that they obviously can&amp;#39;t comment about specific models... let&amp;#39;s hope.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mkd3t1/reccomendation_for_new_medical_benchmark/n7kgtv8/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754642830,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mkd3t1",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]