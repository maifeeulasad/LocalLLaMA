[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I have an old laptop i5 1145g7 11gen 2x8gb ddr4 ram iris xe igpu 8bg shared vram. I recently came across [intel article to run llms utilizing igpu in 11,12,13 gen](https://www.intel.com/content/www/us/en/developer/articles/technical/run-llms-on-gpus-using-llama-cpp.html). I have been trying to run [this](https://huggingface.co/mradermacher/UIGEN-X-4B-0729-GGUF) model which i have used a lot on ollama but it takes really long. Saw posts here telling to use llama.cpp so i decided to give it a shot. i downloaded sycl zip from llama.cpp github and i can see the igpu working but dont see any improvement in performance it takes similar or maybe more time than ollama to generate output. one issue i noticed is that on default context size 4096 whenever it reached the limit, It would just repeat the last token in loop whereas in ollama, the same default context size did cause loop but never repeated the same token infact it would give a coherent code which works fantastically and then would proceed to answer again in loop and not stopping. \n\nAs im new to all this i used gemini deepthink and came up with the following but it doesnt work at all. Any help would be greatly appreciated and also if anyone has managed to successfully increased token/s using sycl backend please let me know if it was worth it or not thanks.\n\nWhat gemini deepthink recommended:\n\nllama-cli.exe -m \"E:\\\\llama sycl\\\\models\\\\unigenx4bq4s.gguf\" -p \"Create a breath taking saas page with modern features, glassmorphism design, cyberpunk aesthetic, modern Css animations/transitions and make responsive, functional buttons\" --ctx-size 8192 -ngl 99 -fa -t 8 --mlock --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0.0 --presence-penalty 1.5 --repeat-penalty 1.05 --repeat-last-n 256 --cache-type-k q4\\_0 --cache-type-v q4\\_0 --no-mmap",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Is llama.cpp sycl backend really worth it?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mi1tns",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.86,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 5,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1jiqvabt65",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 5,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754375620,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I have an old laptop i5 1145g7 11gen 2x8gb ddr4 ram iris xe igpu 8bg shared vram. I recently came across &lt;a href=\"https://www.intel.com/content/www/us/en/developer/articles/technical/run-llms-on-gpus-using-llama-cpp.html\"&gt;intel article to run llms utilizing igpu in 11,12,13 gen&lt;/a&gt;. I have been trying to run &lt;a href=\"https://huggingface.co/mradermacher/UIGEN-X-4B-0729-GGUF\"&gt;this&lt;/a&gt; model which i have used a lot on ollama but it takes really long. Saw posts here telling to use llama.cpp so i decided to give it a shot. i downloaded sycl zip from llama.cpp github and i can see the igpu working but dont see any improvement in performance it takes similar or maybe more time than ollama to generate output. one issue i noticed is that on default context size 4096 whenever it reached the limit, It would just repeat the last token in loop whereas in ollama, the same default context size did cause loop but never repeated the same token infact it would give a coherent code which works fantastically and then would proceed to answer again in loop and not stopping. &lt;/p&gt;\n\n&lt;p&gt;As im new to all this i used gemini deepthink and came up with the following but it doesnt work at all. Any help would be greatly appreciated and also if anyone has managed to successfully increased token/s using sycl backend please let me know if it was worth it or not thanks.&lt;/p&gt;\n\n&lt;p&gt;What gemini deepthink recommended:&lt;/p&gt;\n\n&lt;p&gt;llama-cli.exe -m &amp;quot;E:\\llama sycl\\models\\unigenx4bq4s.gguf&amp;quot; -p &amp;quot;Create a breath taking saas page with modern features, glassmorphism design, cyberpunk aesthetic, modern Css animations/transitions and make responsive, functional buttons&amp;quot; --ctx-size 8192 -ngl 99 -fa -t 8 --mlock --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0.0 --presence-penalty 1.5 --repeat-penalty 1.05 --repeat-last-n 256 --cache-type-k q4_0 --cache-type-v q4_0 --no-mmap&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?auto=webp&amp;s=7ff6c745939c452914047ff3d2df441274bd34cb",
                    "width": 1920,
                    "height": 1920
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=89ff655453a7c1ebf52a06d95507c865ac6aab18",
                      "width": 108,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0fb3beddf7df72d642bad515b481f4c1d0c95547",
                      "width": 216,
                      "height": 216
                    },
                    {
                      "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=469c0192771376534793365f556747a418d28ecd",
                      "width": 320,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1b1e620beec81c523bfb35cc5443ec0a0159ff65",
                      "width": 640,
                      "height": 640
                    },
                    {
                      "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8fa2193b23b44c7ed205780e62af28a37aba2405",
                      "width": 960,
                      "height": 960
                    },
                    {
                      "url": "https://external-preview.redd.it/dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=68cb581f80f80d878acfee3903a1799931dbc330",
                      "width": 1080,
                      "height": 1080
                    }
                  ],
                  "variants": {},
                  "id": "dxXECHarBSKz1hyLrBwlOcCTI-CwnnfIQLajz9bmY9Y"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mi1tns",
            "is_robot_indexable": true,
            "num_duplicates": 1,
            "report_reasons": null,
            "author": "Sweet_Eggplant4659",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mi1tns/is_llamacpp_sycl_backend_really_worth_it/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi1tns/is_llamacpp_sycl_backend_really_worth_it/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754375620,
            "num_crossposts": 1,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n70spvi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Sweet_Eggplant4659",
                      "can_mod_post": false,
                      "created_utc": 1754384532,
                      "send_replies": true,
                      "parent_id": "t1_n70pmdf",
                      "score": 2,
                      "author_fullname": "t2_1jiqvabt65",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "When I read the article, I got really excited that Intel might be onto something… I really hope they take this seriously.  \nAnyway, thank you so much for your response—I'll definitely try what you mentioned :)  \nAlso, huge thanks for your contribution to SYCL. You're the GOAT for those of us who GPU poor",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n70spvi",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When I read the article, I got really excited that Intel might be onto something… I really hope they take this seriously.&lt;br/&gt;\nAnyway, thank you so much for your response—I&amp;#39;ll definitely try what you mentioned :)&lt;br/&gt;\nAlso, huge thanks for your contribution to SYCL. You&amp;#39;re the GOAT for those of us who GPU poor&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mi1tns",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mi1tns/is_llamacpp_sycl_backend_really_worth_it/n70spvi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754384532,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n70wvif",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "smahs9",
                      "can_mod_post": false,
                      "created_utc": 1754386964,
                      "send_replies": true,
                      "parent_id": "t1_n70pmdf",
                      "score": 2,
                      "author_fullname": "t2_neyagc1uz",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "There are a lot of laptops sold with those iGPUs, with some having more than 700 shaders which can seriously speed up matmuls (compared to the avx2 CPUs they are bundled with). The sycl backend in llama.cpp is super helpful to use long prompts on these chips.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n70wvif",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;There are a lot of laptops sold with those iGPUs, with some having more than 700 shaders which can seriously speed up matmuls (compared to the avx2 CPUs they are bundled with). The sycl backend in llama.cpp is super helpful to use long prompts on these chips.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mi1tns",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mi1tns/is_llamacpp_sycl_backend_really_worth_it/n70wvif/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754386964,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n70pmdf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "qnixsynapse",
            "can_mod_post": false,
            "created_utc": 1754382733,
            "send_replies": true,
            "parent_id": "t3_1mi1tns",
            "score": 6,
            "author_fullname": "t2_x16oj02mp",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hi. I am one of the few maintainers of SYCL in llama.cpp. Please note that not all operations are supported and the backend even today lacks flash attention support(I noticed that Gemini deepthink is suggesting  you to use kvcache quantization which is not supported).\n\n\nCan't say about ollama since I never used it. \n\nI think this should be enough: `&lt;path to llama-cli&gt;/llama-cli -m &lt;path to model&gt; -ngl 99 --no-mmap` \nTbh, as an open source contributor, I noticed not \"enough \" interest in llama.cpp from Intel. I think that those who were/are from Intel maintaining the backend are doing so voluntarily in the freetime. I wish they were serious.",
            "edited": 1754383078,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70pmdf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hi. I am one of the few maintainers of SYCL in llama.cpp. Please note that not all operations are supported and the backend even today lacks flash attention support(I noticed that Gemini deepthink is suggesting  you to use kvcache quantization which is not supported).&lt;/p&gt;\n\n&lt;p&gt;Can&amp;#39;t say about ollama since I never used it. &lt;/p&gt;\n\n&lt;p&gt;I think this should be enough: &lt;code&gt;&amp;lt;path to llama-cli&amp;gt;/llama-cli -m &amp;lt;path to model&amp;gt; -ngl 99 --no-mmap&lt;/code&gt; \nTbh, as an open source contributor, I noticed not &amp;quot;enough &amp;quot; interest in llama.cpp from Intel. I think that those who were/are from Intel maintaining the backend are doing so voluntarily in the freetime. I wish they were serious.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi1tns/is_llamacpp_sycl_backend_really_worth_it/n70pmdf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754382733,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mi1tns",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        }
      ],
      "before": null
    }
  }
]