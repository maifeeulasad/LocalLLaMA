[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hey everyone,\n\nI’ve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.\n\nFrom everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I’m using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).\n\n\n\nThe idea:\n\nI want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.\n\n\n\nBut here’s the problem:\n\nThe results are pretty disappointing.\n\nEven though the PDFs were embedded correctly and the system says they’re indexed, the answers I get are vague, or clearly not based on my previous content. It doesn't really use the documents meaningfully – it feels like the bot is just hallucinating or ignoring them.\n\nI even tested it with just one document: my current résumé, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like \"What is my professional background?\" or \"What are my main skills?\" – which it should have easily pulled from the text.\n\nI’ve tried re-uploading, adjusting the chunk size, checking the document scope –&gt; but no real improvement.\n\nSo my question is:\n\nAm I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?\n\nHas anyone had better results using a different local setup for tasks like this?\n\n\n\nWould love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Still getting bad results with PDFs in AnythingLLM + Llama 3 – Am I doing something wrong, or is there a better setup?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgfuf3",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.67,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_oq1nrgj7x",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754216527,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\n\n&lt;p&gt;I’ve been doing some research on setting up a local, privacy-friendly LLM assistant, ideally something that can help me write job applications using my previous resumes and cover letters as a base.&lt;/p&gt;\n\n&lt;p&gt;From everything I read, it sounded really promising to combine AnythingLLM with Llama 3 (I’m using the LLaMA 3 8B). I installed it all locally, configured the settings properly in AnythingLLM (enabled local embeddings, context windows, etc.), and successfully loaded several PDFs (my old cover letters, resumes, etc.).&lt;/p&gt;\n\n&lt;p&gt;The idea:&lt;/p&gt;\n\n&lt;p&gt;I want to paste in a job posting and ask the chatbot to draft a personalized cover letter using my own documents as a knowledge base. Basically, a smart assistant that reuses my past writing and adapts it to the job description.&lt;/p&gt;\n\n&lt;p&gt;But here’s the problem:&lt;/p&gt;\n\n&lt;p&gt;The results are pretty disappointing.&lt;/p&gt;\n\n&lt;p&gt;Even though the PDFs were embedded correctly and the system says they’re indexed, the answers I get are vague, or clearly not based on my previous content. It doesn&amp;#39;t really use the documents meaningfully – it feels like the bot is just hallucinating or ignoring them.&lt;/p&gt;\n\n&lt;p&gt;I even tested it with just one document: my current résumé, uploaded as both PDF and plain .txt, and it still failed to accurately reflect the content when I asked basic questions like &amp;quot;What is my professional background?&amp;quot; or &amp;quot;What are my main skills?&amp;quot; – which it should have easily pulled from the text.&lt;/p&gt;\n\n&lt;p&gt;I’ve tried re-uploading, adjusting the chunk size, checking the document scope –&amp;gt; but no real improvement.&lt;/p&gt;\n\n&lt;p&gt;So my question is:&lt;/p&gt;\n\n&lt;p&gt;Am I doing something wrong? Or is this kind of task just too much for AnythingLLM + Llama 3 right now?&lt;/p&gt;\n\n&lt;p&gt;Has anyone had better results using a different local setup for tasks like this?&lt;/p&gt;\n\n&lt;p&gt;Would love to hear your tips or setups that work better for writing support based on personal document libraries. Thanks in advance!&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgfuf3",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Lazy_Fig_6244",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/",
            "subreddit_subscribers": 509291,
            "created_utc": 1754216527,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6o7ymy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Low-Opening25",
            "can_mod_post": false,
            "created_utc": 1754216739,
            "send_replies": true,
            "parent_id": "t3_1mgfuf3",
            "score": 1,
            "author_fullname": "t2_ebfjvj5t",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "even commercial models struggle with it and your are asking what are effectively models for ants to carry a human size load.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6o7ymy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;even commercial models struggle with it and your are asking what are effectively models for ants to carry a human size load.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/n6o7ymy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754216739,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgfuf3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "fe89e94a-13f2-11f0-a9de-6262c74956cf",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "richtext",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": "fe89e94a-13f2-11f0-a9de-6262c74956cf",
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6om9hb",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Asleep-Ratio7535",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6okk9k",
                                "score": 1,
                                "author_fullname": "t2_1lfyddwf0c",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "But RAG is just like how searching works, it can't know all the context if it's not injected directly. You should ask AI to explain it to you and check your solutions. Maybe add keywords when you draft your documents if you can only use anythingllm. ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6om9hb",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [
                                  {
                                    "e": "text",
                                    "t": "Llama 4"
                                  }
                                ],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;But RAG is just like how searching works, it can&amp;#39;t know all the context if it&amp;#39;s not injected directly. You should ask AI to explain it to you and check your solutions. Maybe add keywords when you draft your documents if you can only use anythingllm. &lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgfuf3",
                                "unrepliable_reason": null,
                                "author_flair_text_color": "light",
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/n6om9hb/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754223752,
                                "author_flair_text": "Llama 4",
                                "treatment_tags": [],
                                "created_utc": 1754223752,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": "#b0ae9b",
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6okk9k",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Lazy_Fig_6244",
                      "can_mod_post": false,
                      "created_utc": 1754223031,
                      "send_replies": true,
                      "parent_id": "t1_n6oiqu3",
                      "score": 1,
                      "author_fullname": "t2_oq1nrgj7x",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "When I pasted the résumé text directly into the chat, the output was much better and more accurate – you're right about that.\n\nHowever, my long-term goal is to load **multiple documents** (like several PDFs: old cover letters, resumes, project summaries) and use them as a **knowledge base**. Based on that, I’d like to generate new **cover letters** or even **project proposals** at work using previous documents as reference.\n\nSince simply copy-pasting text isn’t really a sustainable solution for that kind of workflow, I’m wondering whether it’s **normal** that the output is weak when working with uploaded PDFs in tools like **AnythingLLM**, or whether others have had better results – or even use a different **local document-based assistant setup** entirely.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6okk9k",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;When I pasted the résumé text directly into the chat, the output was much better and more accurate – you&amp;#39;re right about that.&lt;/p&gt;\n\n&lt;p&gt;However, my long-term goal is to load &lt;strong&gt;multiple documents&lt;/strong&gt; (like several PDFs: old cover letters, resumes, project summaries) and use them as a &lt;strong&gt;knowledge base&lt;/strong&gt;. Based on that, I’d like to generate new &lt;strong&gt;cover letters&lt;/strong&gt; or even &lt;strong&gt;project proposals&lt;/strong&gt; at work using previous documents as reference.&lt;/p&gt;\n\n&lt;p&gt;Since simply copy-pasting text isn’t really a sustainable solution for that kind of workflow, I’m wondering whether it’s &lt;strong&gt;normal&lt;/strong&gt; that the output is weak when working with uploaded PDFs in tools like &lt;strong&gt;AnythingLLM&lt;/strong&gt;, or whether others have had better results – or even use a different &lt;strong&gt;local document-based assistant setup&lt;/strong&gt; entirely.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgfuf3",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/n6okk9k/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754223031,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6oiqu3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Asleep-Ratio7535",
            "can_mod_post": false,
            "created_utc": 1754222231,
            "send_replies": true,
            "parent_id": "t3_1mgfuf3",
            "score": 1,
            "author_fullname": "t2_1lfyddwf0c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Have you put professional skills or background in your file? Like background: xxx skills: list. If not I think it's okay not to give you an answer. You can just copy paste all your resume. I am sure it's better.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6oiqu3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 4"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Have you put professional skills or background in your file? Like background: xxx skills: list. If not I think it&amp;#39;s okay not to give you an answer. You can just copy paste all your resume. I am sure it&amp;#39;s better.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgfuf3/still_getting_bad_results_with_pdfs_in/n6oiqu3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754222231,
            "author_flair_text": "Llama 4",
            "treatment_tags": [],
            "link_id": "t3_1mgfuf3",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#b0ae9b",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]