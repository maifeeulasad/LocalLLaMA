[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "The purpose of this post is twofold.  To give hope to those with older video cards, and to solicit further optimizations from the larger community.  Here's the script:\n\n    cat qwen.sh \n    #!/bin/bash\n\n    # Configuration Variables\n    # ---------------------\n    MODEL_PATH=\"../models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf\"     # Path to your GGUF model file\n    LLAMA_SERVER_PATH=\"./build/bin/llama-server\" # Path to your llama-server executable\n    N_GPU_LAYERS=99 # Number of layers to offload to GPU (use 99 to offload as much as possible)\n    N_CTX=14384 # Context window size (tokens). Adjust based on VRAM and model needs.\n    PORT=NNNNN # Port for the llama-server API\n\n    # --- Performance Tuning Variables ---\n    # Set these based on your system's hardware.\n    # Use 'lscpu' or similar commands to find the number of CPU cores.\n    N_THREADS=4 # Number of CPU threads to use. 'nproc' gets the number of available processors.\n    N_BATCH=300 # Batch size for prompt processing. A larger value can improve initial prompt processing speed.\n\n    # --- Script Logic ---\n\n    echo \"--- Starting optimized llama-server ---\"\n    echo \"Model: $MODEL_PATH\"\n    echo \"GPU Layers: $N_GPU_LAYERS\"\n    echo \"Context Size: $N_CTX\"\n    echo \"Threads: $N_THREADS\"\n    echo \"Batch Size: $N_BATCH\"\n    echo \"Port: $PORT\"\n    echo \"-------------------------------------\"\n\n    # Check if the model file exists\n    if [ ! -f \"$MODEL_PATH\" ]; then\n    echo \"ERROR: Model file not found at $MODEL_PATH\"\n    echo \"Please ensure the model path is correct and the model exists.\"\n    exit 1\n    fi\n\n    # Check if the llama-server executable exists\n    if [ ! -f \"$LLAMA_SERVER_PATH\" ]; then\n    echo \"ERROR: llama-server executable not found at $LLAMA_SERVER_PATH\"\n    echo \"Please ensure llama.cpp is built and the path is correct.\"\n    exit 1\n    fi\n\n    # Launch llama-server with specified parameters\n    # The '&amp;' sends the process to the background, allowing the script to exit.\n    # You might want to remove '&amp;' if you want to see logs directly in the terminal.\n    # You can also redirect output to a log file: &gt; server.log 2&gt;&amp;1 &amp;\n    \"$LLAMA_SERVER_PATH\" \\\n    -m \"$MODEL_PATH\" \\\n    --host 0.0.0.0 \\\n    --port \"$PORT\" \\\n    --n-gpu-layers \"$N_GPU_LAYERS\" \\\n    --ctx-size \"$N_CTX\" \\\n    --embedding \\\n    --threads \"$N_THREADS\" \\\n    --batch-size \"$N_BATCH\" \\\n    --flash-attn \\\n    --no-mmap \\\n    # The --no-mmap flag can sometimes prevent issues on certain file systems.\n    # It can slightly increase load time but ensures the whole model is in memory.\n\n    # Provide instructions to the user\n    echo \"\"\n    echo \"llama-server has been launched. It might take a moment to load the model.\"\n    echo \"You can check its status by visiting http://localhost:$PORT/health in your browser.\"\n    echo \"To interact with it, you'll need a separate client script (e.g., Python) that makes API calls to http://localhost:$PORT/v1/chat/completions\"\n    echo \"To stop the server, find its process ID (e.g., using 'pgrep llama-server') and use 'kill &lt;PID&gt;'.\"\n    echo \"\"\n    echo \"--- Server output will appear above this line if not backgrounded ---\"\n\n\nPrompt\n- Tokens: 9818\n- Time: 12921.578 ms\n- Speed: 759.8 t/s\nGeneration\n- Tokens: 748\n- Time: 35828.869 ms\n- Speed: 20.9 t/s\n\nUse case is C programming buddy, a rubber duck that talks back and sometimes has useful ideas.  Sometimes ... astoundingly good ideas prompting me to explore solutions I would not have thought of on my own.\n\nOutput is faster than I can read, and for lengthy processing it's fire and forget.  You'll hear the GPU unload when it's done.\n\nTLDR:  Restatement of purpose.  Provide a path for those seeking a good model on an under served segment of our community.  A request for help from those who have found optimizations I have yet to discover.\n\nchar *user = \"lazy\toptomas\";//  Yeah, it's sloppy.  Yeah, it has stuff in it I don't use anymore.  Yeah, the instructions are dumb and out of date.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Rejoice, GPU poor brethren. RTX 3060 12BG, llama-cpp, Model: DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mk0w9f",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.5,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_36ebd",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754575273,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;The purpose of this post is twofold.  To give hope to those with older video cards, and to solicit further optimizations from the larger community.  Here&amp;#39;s the script:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;cat qwen.sh \n#!/bin/bash\n\n# Configuration Variables\n# ---------------------\nMODEL_PATH=&amp;quot;../models/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf&amp;quot;     # Path to your GGUF model file\nLLAMA_SERVER_PATH=&amp;quot;./build/bin/llama-server&amp;quot; # Path to your llama-server executable\nN_GPU_LAYERS=99 # Number of layers to offload to GPU (use 99 to offload as much as possible)\nN_CTX=14384 # Context window size (tokens). Adjust based on VRAM and model needs.\nPORT=NNNNN # Port for the llama-server API\n\n# --- Performance Tuning Variables ---\n# Set these based on your system&amp;#39;s hardware.\n# Use &amp;#39;lscpu&amp;#39; or similar commands to find the number of CPU cores.\nN_THREADS=4 # Number of CPU threads to use. &amp;#39;nproc&amp;#39; gets the number of available processors.\nN_BATCH=300 # Batch size for prompt processing. A larger value can improve initial prompt processing speed.\n\n# --- Script Logic ---\n\necho &amp;quot;--- Starting optimized llama-server ---&amp;quot;\necho &amp;quot;Model: $MODEL_PATH&amp;quot;\necho &amp;quot;GPU Layers: $N_GPU_LAYERS&amp;quot;\necho &amp;quot;Context Size: $N_CTX&amp;quot;\necho &amp;quot;Threads: $N_THREADS&amp;quot;\necho &amp;quot;Batch Size: $N_BATCH&amp;quot;\necho &amp;quot;Port: $PORT&amp;quot;\necho &amp;quot;-------------------------------------&amp;quot;\n\n# Check if the model file exists\nif [ ! -f &amp;quot;$MODEL_PATH&amp;quot; ]; then\necho &amp;quot;ERROR: Model file not found at $MODEL_PATH&amp;quot;\necho &amp;quot;Please ensure the model path is correct and the model exists.&amp;quot;\nexit 1\nfi\n\n# Check if the llama-server executable exists\nif [ ! -f &amp;quot;$LLAMA_SERVER_PATH&amp;quot; ]; then\necho &amp;quot;ERROR: llama-server executable not found at $LLAMA_SERVER_PATH&amp;quot;\necho &amp;quot;Please ensure llama.cpp is built and the path is correct.&amp;quot;\nexit 1\nfi\n\n# Launch llama-server with specified parameters\n# The &amp;#39;&amp;amp;&amp;#39; sends the process to the background, allowing the script to exit.\n# You might want to remove &amp;#39;&amp;amp;&amp;#39; if you want to see logs directly in the terminal.\n# You can also redirect output to a log file: &amp;gt; server.log 2&amp;gt;&amp;amp;1 &amp;amp;\n&amp;quot;$LLAMA_SERVER_PATH&amp;quot; \\\n-m &amp;quot;$MODEL_PATH&amp;quot; \\\n--host 0.0.0.0 \\\n--port &amp;quot;$PORT&amp;quot; \\\n--n-gpu-layers &amp;quot;$N_GPU_LAYERS&amp;quot; \\\n--ctx-size &amp;quot;$N_CTX&amp;quot; \\\n--embedding \\\n--threads &amp;quot;$N_THREADS&amp;quot; \\\n--batch-size &amp;quot;$N_BATCH&amp;quot; \\\n--flash-attn \\\n--no-mmap \\\n# The --no-mmap flag can sometimes prevent issues on certain file systems.\n# It can slightly increase load time but ensures the whole model is in memory.\n\n# Provide instructions to the user\necho &amp;quot;&amp;quot;\necho &amp;quot;llama-server has been launched. It might take a moment to load the model.&amp;quot;\necho &amp;quot;You can check its status by visiting http://localhost:$PORT/health in your browser.&amp;quot;\necho &amp;quot;To interact with it, you&amp;#39;ll need a separate client script (e.g., Python) that makes API calls to http://localhost:$PORT/v1/chat/completions&amp;quot;\necho &amp;quot;To stop the server, find its process ID (e.g., using &amp;#39;pgrep llama-server&amp;#39;) and use &amp;#39;kill &amp;lt;PID&amp;gt;&amp;#39;.&amp;quot;\necho &amp;quot;&amp;quot;\necho &amp;quot;--- Server output will appear above this line if not backgrounded ---&amp;quot;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Prompt\n- Tokens: 9818\n- Time: 12921.578 ms\n- Speed: 759.8 t/s\nGeneration\n- Tokens: 748\n- Time: 35828.869 ms\n- Speed: 20.9 t/s&lt;/p&gt;\n\n&lt;p&gt;Use case is C programming buddy, a rubber duck that talks back and sometimes has useful ideas.  Sometimes ... astoundingly good ideas prompting me to explore solutions I would not have thought of on my own.&lt;/p&gt;\n\n&lt;p&gt;Output is faster than I can read, and for lengthy processing it&amp;#39;s fire and forget.  You&amp;#39;ll hear the GPU unload when it&amp;#39;s done.&lt;/p&gt;\n\n&lt;p&gt;TLDR:  Restatement of purpose.  Provide a path for those seeking a good model on an under served segment of our community.  A request for help from those who have found optimizations I have yet to discover.&lt;/p&gt;\n\n&lt;p&gt;char *user = &amp;quot;lazy  optomas&amp;quot;;//  Yeah, it&amp;#39;s sloppy.  Yeah, it has stuff in it I don&amp;#39;t use anymore.  Yeah, the instructions are dumb and out of date.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mk0w9f",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "optomas",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/",
            "subreddit_subscribers": 513417,
            "created_utc": 1754575273,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7fl3yd",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "optomas",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7fd2pr",
                                "score": 1,
                                "author_fullname": "t2_36ebd",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Seems like all the models released lately are well beyond the scale these cards are capable of.  I mean, [come on](https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF).  Really?  I cant even store that, let alone run it.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7fl3yd",
                                "is_submitter": true,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Seems like all the models released lately are well beyond the scale these cards are capable of.  I mean, &lt;a href=\"https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-1M-GGUF\"&gt;come on&lt;/a&gt;.  Really?  I cant even store that, let alone run it.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mk0w9f",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7fl3yd/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754580062,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754580062,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7fd2pr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "TheYeetsterboi",
                      "can_mod_post": false,
                      "created_utc": 1754577770,
                      "send_replies": true,
                      "parent_id": "t1_n7f6nx4",
                      "score": 2,
                      "author_fullname": "t2_bjh756rm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "An rtx 3060 could most likely run 12 battlestars lol",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7fd2pr",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;An rtx 3060 could most likely run 12 battlestars lol&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mk0w9f",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7fd2pr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754577770,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7f6nx4",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "optomas",
            "can_mod_post": false,
            "created_utc": 1754575895,
            "send_replies": true,
            "parent_id": "t3_1mk0w9f",
            "score": 2,
            "author_fullname": "t2_36ebd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "That's 12 Giga Bytes, not 12 Big Gorillas.  It's pretty common to confuse the two terms as I apparently did there.  Banana Gauges?  Battlestar Galacticas?  There *are* 12 battlestars!",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7f6nx4",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s 12 Giga Bytes, not 12 Big Gorillas.  It&amp;#39;s pretty common to confuse the two terms as I apparently did there.  Banana Gauges?  Battlestar Galacticas?  There &lt;em&gt;are&lt;/em&gt; 12 battlestars!&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7f6nx4/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754575895,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk0w9f",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7gu0na",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "optomas",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7gra7u",
                                          "score": 1,
                                          "author_fullname": "t2_36ebd",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I tried 30B  It's just too much for this machine.  20 tk/s is about the cutoff, more or less.  I've found extended context length is better for programming, anyhow.  Cheers!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7gu0na",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I tried 30B  It&amp;#39;s just too much for this machine.  20 tk/s is about the cutoff, more or less.  I&amp;#39;ve found extended context length is better for programming, anyhow.  Cheers!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mk0w9f",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7gu0na/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754592758,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754592758,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7gra7u",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ArsNeph",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7g97rr",
                                "score": 2,
                                "author_fullname": "t2_vt0xkv60d",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "No problem at all XD Qwen 3 14B is not necessarily the best model overall for coding, but it should be quite a bit better than the Deepseek distill of 2.5 when used properly. The deep seeks distilled models are all Official from Deepseek themselves, but unlike last time when they released distills of a whole suite, this time they only distilled Qwen 3 8B, and in all honesty the distill did not do much to improve its performance.\n\nQwen 3 14B has not gotten a 2507 update yet though, while 30B has. If you have enough RAM to partially offload it, you should get similar speeds, but it should be quite a bit more intelligent thanks to the new data. You'll have to choose between the instruct and reasoning versions though, they're not hybrid anymore.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7gra7u",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No problem at all XD Qwen 3 14B is not necessarily the best model overall for coding, but it should be quite a bit better than the Deepseek distill of 2.5 when used properly. The deep seeks distilled models are all Official from Deepseek themselves, but unlike last time when they released distills of a whole suite, this time they only distilled Qwen 3 8B, and in all honesty the distill did not do much to improve its performance.&lt;/p&gt;\n\n&lt;p&gt;Qwen 3 14B has not gotten a 2507 update yet though, while 30B has. If you have enough RAM to partially offload it, you should get similar speeds, but it should be quite a bit more intelligent thanks to the new data. You&amp;#39;ll have to choose between the instruct and reasoning versions though, they&amp;#39;re not hybrid anymore.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mk0w9f",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7gra7u/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754591970,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754591970,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7g97rr",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "optomas",
                      "can_mod_post": false,
                      "created_utc": 1754586979,
                      "send_replies": true,
                      "parent_id": "t1_n7g0icd",
                      "score": 1,
                      "author_fullname": "t2_36ebd",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's pretty embarrassing, thank you.  I had thought I *was* using a deepseek distill of qwen3 14b.  You are correct, the base is from qwen2.5.  \n\nI guess this was exactly why I made this post, nice catch!  \n\nTBF, I had tested qwen3 14b and it was not doing well.  I've learned quite a bit since then.  Definitely worth another spin!  \n\nAnyone else that comes across this, the model in the post title *does* work very well.  From 20 to 30 tokens a second.  It's a good rubber duck that sometimes comes up with solutions to problem I might not have come up with on my own.\n\nYou know, like spotting the fact that I am using a model from last year when I thought I was using the latest and greatest.  \n\nSeriously, thanks /u/ArsNeph!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7g97rr",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s pretty embarrassing, thank you.  I had thought I &lt;em&gt;was&lt;/em&gt; using a deepseek distill of qwen3 14b.  You are correct, the base is from qwen2.5.  &lt;/p&gt;\n\n&lt;p&gt;I guess this was exactly why I made this post, nice catch!  &lt;/p&gt;\n\n&lt;p&gt;TBF, I had tested qwen3 14b and it was not doing well.  I&amp;#39;ve learned quite a bit since then.  Definitely worth another spin!  &lt;/p&gt;\n\n&lt;p&gt;Anyone else that comes across this, the model in the post title &lt;em&gt;does&lt;/em&gt; work very well.  From 20 to 30 tokens a second.  It&amp;#39;s a good rubber duck that sometimes comes up with solutions to problem I might not have come up with on my own.&lt;/p&gt;\n\n&lt;p&gt;You know, like spotting the fact that I am using a model from last year when I thought I was using the latest and greatest.  &lt;/p&gt;\n\n&lt;p&gt;Seriously, thanks &lt;a href=\"/u/ArsNeph\"&gt;/u/ArsNeph&lt;/a&gt;!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mk0w9f",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7g97rr/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754586979,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7g0icd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ArsNeph",
            "can_mod_post": false,
            "created_utc": 1754584500,
            "send_replies": true,
            "parent_id": "t3_1mk0w9f",
            "score": 2,
            "author_fullname": "t2_vt0xkv60d",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Not really a optimization, but is there any reason you're using R1 distill as opposed to Qwen 3 14B or 30B MoE? Both of those should be much better at coding and fit in relatively the same footprint with the same speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7g0icd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not really a optimization, but is there any reason you&amp;#39;re using R1 distill as opposed to Qwen 3 14B or 30B MoE? Both of those should be much better at coding and fit in relatively the same footprint with the same speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7g0icd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754584500,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk0w9f",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7ggapc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "optomas",
            "can_mod_post": false,
            "created_utc": 1754588878,
            "send_replies": true,
            "parent_id": "t3_1mk0w9f",
            "score": 1,
            "author_fullname": "t2_36ebd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Corrected a glaring mistake.  Here's  DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf:\n\n    Prompt\n    - Tokens: 9818\n    - Time: 12921.578 ms\n    - Speed: 759.8 t/s\n    Generation\n    - Tokens: 748\n    - Time: 35828.869 ms\n    - Speed: 20.9 t/s\n\n\n Here's Qwen3-14B-Q4_K_M.gguf:\n\n    Prompt\n    - Tokens: 10158\n    - Time: 12721.917 ms\n    - Speed: 798.5 t/s\n    Generation\n    - Tokens: 2471\n    - Time: 117352.085 ms\n    - Speed: 21.1 t/s\n\nSame 48K tag file.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7ggapc",
            "is_submitter": true,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Corrected a glaring mistake.  Here&amp;#39;s  DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Prompt\n- Tokens: 9818\n- Time: 12921.578 ms\n- Speed: 759.8 t/s\nGeneration\n- Tokens: 748\n- Time: 35828.869 ms\n- Speed: 20.9 t/s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Here&amp;#39;s Qwen3-14B-Q4_K_M.gguf:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;Prompt\n- Tokens: 10158\n- Time: 12721.917 ms\n- Speed: 798.5 t/s\nGeneration\n- Tokens: 2471\n- Time: 117352.085 ms\n- Speed: 21.1 t/s\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Same 48K tag file.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mk0w9f/rejoice_gpu_poor_brethren_rtx_3060_12bg_llamacpp/n7ggapc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754588878,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mk0w9f",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]