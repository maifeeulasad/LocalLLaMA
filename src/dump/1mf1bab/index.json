[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "70B dense model fits into a 48GB but it’s harder for me to wrap my mind around if a 109B-A13B model would fit into 48GB since not all the params are active.\n\nAlso does llama cpp automatically load the active parameters onto the GPU and keep the inactive ones in RAM?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How much VRAM does MOE models take comparative to dense models?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mf1bab",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_10rvna3i1t",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754066202,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;70B dense model fits into a 48GB but it’s harder for me to wrap my mind around if a 109B-A13B model would fit into 48GB since not all the params are active.&lt;/p&gt;\n\n&lt;p&gt;Also does llama cpp automatically load the active parameters onto the GPU and keep the inactive ones in RAM?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mf1bab",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Glittering-Bag-4662",
            "discussion_type": null,
            "num_comments": 3,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mf1bab/how_much_vram_does_moe_models_take_comparative_to/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf1bab/how_much_vram_does_moe_models_take_comparative_to/",
            "subreddit_subscribers": 508771,
            "created_utc": 1754066202,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6dmoua",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "jacek2023",
            "can_mod_post": false,
            "created_utc": 1754066771,
            "send_replies": true,
            "parent_id": "t3_1mf1bab",
            "score": 7,
            "author_fullname": "t2_vqgbql9w",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MoE means that only part of the model is used each time. So memory requirement stays same as dense, it is just faster\n\n70B models use over 70GB in Q8 but 35GB in Q4\n\nThere is a way (-ot parameter) to optimize CPU offload for MoE",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dmoua",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MoE means that only part of the model is used each time. So memory requirement stays same as dense, it is just faster&lt;/p&gt;\n\n&lt;p&gt;70B models use over 70GB in Q8 but 35GB in Q4&lt;/p&gt;\n\n&lt;p&gt;There is a way (-ot parameter) to optimize CPU offload for MoE&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1bab/how_much_vram_does_moe_models_take_comparative_to/n6dmoua/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754066771,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1mf1bab",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6f8flk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Double_Cause4609",
            "can_mod_post": false,
            "created_utc": 1754083725,
            "send_replies": true,
            "parent_id": "t3_1mf1bab",
            "score": 1,
            "author_fullname": "t2_1kubzxt2ww",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "MoE models require, by default, their listed total parameter count in memory.\n\nHowever, they are as fast as their active parameter count.\n\nNot all tensors in a model are made equal, so in LlamaCPP for example, you can put the fast (but huge) tensors on CPU, but the small (and hard to calculate) ones on GPU, for the best balance of performance to cost.\n\nAdditionally, one weird part of MoE models is that the experts are big blocks of weights, right? Well, each layer has its own set of blocks. Curiously, between two tokens, only a few layer's blocks will switch to a new block (select a new expert).\n\nThis means that (at least on Linux), even if you don't have enough total system RAM for the model, it can still run quite well until you hit around half the system RAM as you would expect from the size of the model file in GB.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6f8flk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MoE models require, by default, their listed total parameter count in memory.&lt;/p&gt;\n\n&lt;p&gt;However, they are as fast as their active parameter count.&lt;/p&gt;\n\n&lt;p&gt;Not all tensors in a model are made equal, so in LlamaCPP for example, you can put the fast (but huge) tensors on CPU, but the small (and hard to calculate) ones on GPU, for the best balance of performance to cost.&lt;/p&gt;\n\n&lt;p&gt;Additionally, one weird part of MoE models is that the experts are big blocks of weights, right? Well, each layer has its own set of blocks. Curiously, between two tokens, only a few layer&amp;#39;s blocks will switch to a new block (select a new expert).&lt;/p&gt;\n\n&lt;p&gt;This means that (at least on Linux), even if you don&amp;#39;t have enough total system RAM for the model, it can still run quite well until you hit around half the system RAM as you would expect from the size of the model file in GB.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1bab/how_much_vram_does_moe_models_take_comparative_to/n6f8flk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754083725,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1bab",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6dnl5s",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754067028,
            "send_replies": true,
            "parent_id": "t3_1mf1bab",
            "score": 1,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Basically a 109B-A13B takes 109B worth.  The active parameters are per per token and per layer, so it's a random sampling ~60 times per token generated.\n\nThat said, there are specific tensors that _aren't_ routed.  Some models have a shared expert, for example, that is always selected and all models have attention tensors that are always active too.  From the handful of models I've looked at these are roughly 1/3.  So while I haven't checked GLM4.5-Air (I might do that in a bit and edit this), you can estimate that ~5B are always used and a random 8B are selected from the remaining 104B.\n\nThus MoE can benefit a lot from partial GPU offload: even a 1000B model like Kimi can easily offload the ~12B common parameters to a consumer GPU so the CPU only needs to handle 20B active parameters instead of all 32B.  When it comes to a 106B model, you'll need to test/tune with you exact setup, but if can offload the common tensors and 1/2 the experts you'll get a ~3x speed up vs CPU alone.  (Which is going to be a lot slower than GPU alone, but hey, VRAM isn't cheap...)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dnl5s",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Basically a 109B-A13B takes 109B worth.  The active parameters are per per token and per layer, so it&amp;#39;s a random sampling ~60 times per token generated.&lt;/p&gt;\n\n&lt;p&gt;That said, there are specific tensors that &lt;em&gt;aren&amp;#39;t&lt;/em&gt; routed.  Some models have a shared expert, for example, that is always selected and all models have attention tensors that are always active too.  From the handful of models I&amp;#39;ve looked at these are roughly 1/3.  So while I haven&amp;#39;t checked GLM4.5-Air (I might do that in a bit and edit this), you can estimate that ~5B are always used and a random 8B are selected from the remaining 104B.&lt;/p&gt;\n\n&lt;p&gt;Thus MoE can benefit a lot from partial GPU offload: even a 1000B model like Kimi can easily offload the ~12B common parameters to a consumer GPU so the CPU only needs to handle 20B active parameters instead of all 32B.  When it comes to a 106B model, you&amp;#39;ll need to test/tune with you exact setup, but if can offload the common tensors and 1/2 the experts you&amp;#39;ll get a ~3x speed up vs CPU alone.  (Which is going to be a lot slower than GPU alone, but hey, VRAM isn&amp;#39;t cheap...)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1bab/how_much_vram_does_moe_models_take_comparative_to/n6dnl5s/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754067028,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1bab",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]