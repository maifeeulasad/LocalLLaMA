[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Sharing notes in case it helps anyone. I don't often find people talking about models like Jamba and we have access to it, so figure it might be useful.\n\n\\-  \n  \nBeen testing local models for drafting first-pass answers to internal RFPs. The source material is rough. Basically a mix of PDF exports, old responses in docx, inconsistent product specs, wiki dumps and suchlike.\n\nI'm running a basic RAG pipeline over it using section-level chunking and a semantic search index. Nothing too exotic. Retrieval pulls five chunks per query and I'm prompting each model to answer strictly from the provided input. Tried Jamba, Mistral 7B and Mixtral on the same prompts.\n\nMy findings:\n\nMixtral gave the most natural writing style. Handled formatting like bullet points well, but when chunks were overlapping or contradicting, it sometimes mashed them together. Sounded coherent, but didn't track to any one source.\n\nMistral played it safer but the answers often felt incomplete. Would stop early or skip chunks if they weren't clearly relevant. Better than Mixtral at avoiding noise but I had to rerun prompts more often to get full coverage.\n\nJamba was slightly slower and more verbose, but I could actually trace the language back to the retrieved text most of the time. It didn't try to fill in gaps with guesswork and it stayed anchored to the input without inventing policy language. It was more useful in review. Didn't have to figure out where something came from.\n\nStill experimenting with reranking to clean up the retrieval layer. Jamba has been the most consistent in situations where accuracy matters more than polish. Might try pairing it with. post-processing model to tighten up the tone without losing the original source trail.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Drafting RFP answers with Jamba, Mistral, Mixtral",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lpqyra",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1kwk178bd9",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1751446846,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sharing notes in case it helps anyone. I don&amp;#39;t often find people talking about models like Jamba and we have access to it, so figure it might be useful.&lt;/p&gt;\n\n&lt;p&gt;-  &lt;/p&gt;\n\n&lt;p&gt;Been testing local models for drafting first-pass answers to internal RFPs. The source material is rough. Basically a mix of PDF exports, old responses in docx, inconsistent product specs, wiki dumps and suchlike.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m running a basic RAG pipeline over it using section-level chunking and a semantic search index. Nothing too exotic. Retrieval pulls five chunks per query and I&amp;#39;m prompting each model to answer strictly from the provided input. Tried Jamba, Mistral 7B and Mixtral on the same prompts.&lt;/p&gt;\n\n&lt;p&gt;My findings:&lt;/p&gt;\n\n&lt;p&gt;Mixtral gave the most natural writing style. Handled formatting like bullet points well, but when chunks were overlapping or contradicting, it sometimes mashed them together. Sounded coherent, but didn&amp;#39;t track to any one source.&lt;/p&gt;\n\n&lt;p&gt;Mistral played it safer but the answers often felt incomplete. Would stop early or skip chunks if they weren&amp;#39;t clearly relevant. Better than Mixtral at avoiding noise but I had to rerun prompts more often to get full coverage.&lt;/p&gt;\n\n&lt;p&gt;Jamba was slightly slower and more verbose, but I could actually trace the language back to the retrieved text most of the time. It didn&amp;#39;t try to fill in gaps with guesswork and it stayed anchored to the input without inventing policy language. It was more useful in review. Didn&amp;#39;t have to figure out where something came from.&lt;/p&gt;\n\n&lt;p&gt;Still experimenting with reranking to clean up the retrieval layer. Jamba has been the most consistent in situations where accuracy matters more than polish. Might try pairing it with. post-processing model to tighten up the tone without losing the original source trail.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1lpqyra",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "NullPointerJack",
            "discussion_type": null,
            "num_comments": 0,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lpqyra/drafting_rfp_answers_with_jamba_mistral_mixtral/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lpqyra/drafting_rfp_answers_with_jamba_mistral_mixtral/",
            "subreddit_subscribers": 494001,
            "created_utc": 1751446846,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [],
      "before": null
    }
  }
]