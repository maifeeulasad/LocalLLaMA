[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Qwen3 A3B 30B MOE...did not expect it to work. Ryzen 5700G CPU running at 55% utilization.\n\nhttps://preview.redd.it/hmbfaob6ncgf1.png?width=1203&amp;format=png&amp;auto=webp&amp;s=6ade3144a807decb0b799502c2d4025c1d97ad63\n\nhttps://preview.redd.it/ot9fdob6ncgf1.png?width=750&amp;format=png&amp;auto=webp&amp;s=1ca5e7cdb86c10c0be54f91a1d8b7a164ed85583\n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Bought RTX 5070 to run 30B AI and it worked with 18 tokens/s",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 140,
            "top_awarded_type": null,
            "hide_score": false,
            "media_metadata": {
              "ot9fdob6ncgf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 146,
                    "x": 108,
                    "u": "https://preview.redd.it/ot9fdob6ncgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=00437014275f5a60ab2cbe60a55e8215bce4eddc"
                  },
                  {
                    "y": 292,
                    "x": 216,
                    "u": "https://preview.redd.it/ot9fdob6ncgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=73befff76d8fb2a62d227242491bdc8dd0b64891"
                  },
                  {
                    "y": 433,
                    "x": 320,
                    "u": "https://preview.redd.it/ot9fdob6ncgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8567e36c97a299c3c921b8c06f951dca8df87fbb"
                  },
                  {
                    "y": 866,
                    "x": 640,
                    "u": "https://preview.redd.it/ot9fdob6ncgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d8f4c2d24474cf4d6c7ce07a8a5c053d7c3bc40"
                  }
                ],
                "s": {
                  "y": 1016,
                  "x": 750,
                  "u": "https://preview.redd.it/ot9fdob6ncgf1.png?width=750&amp;format=png&amp;auto=webp&amp;s=1ca5e7cdb86c10c0be54f91a1d8b7a164ed85583"
                },
                "id": "ot9fdob6ncgf1"
              },
              "hmbfaob6ncgf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 144,
                    "x": 108,
                    "u": "https://preview.redd.it/hmbfaob6ncgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b7d8a4bd60f9d2fb31d4ca421b9591b1c4cf2836"
                  },
                  {
                    "y": 289,
                    "x": 216,
                    "u": "https://preview.redd.it/hmbfaob6ncgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7afe161b553aa4a84f04bae19419736891ce46c6"
                  },
                  {
                    "y": 429,
                    "x": 320,
                    "u": "https://preview.redd.it/hmbfaob6ncgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=266d701981ce9076c9e76416ac9c11e14db49e31"
                  },
                  {
                    "y": 858,
                    "x": 640,
                    "u": "https://preview.redd.it/hmbfaob6ncgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bfd811ad1bc875c15cd94dd2246973e19f4d9689"
                  },
                  {
                    "y": 1287,
                    "x": 960,
                    "u": "https://preview.redd.it/hmbfaob6ncgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=53eb07ddfaead4b28d619a9d46003115c70e65ea"
                  },
                  {
                    "y": 1448,
                    "x": 1080,
                    "u": "https://preview.redd.it/hmbfaob6ncgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e797ec78dbcedbf617efcda2e8967cf83cec976"
                  }
                ],
                "s": {
                  "y": 1613,
                  "x": 1203,
                  "u": "https://preview.redd.it/hmbfaob6ncgf1.png?width=1203&amp;format=png&amp;auto=webp&amp;s=6ade3144a807decb0b799502c2d4025c1d97ad63"
                },
                "id": "hmbfaob6ncgf1"
              }
            },
            "name": "t3_1meostj",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.56,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_7m7dnrxx",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/82Ar4dMTgrQJXm6uKN0-oUCwMltCaeP9HhjFmtJlTGo.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754028556,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Qwen3 A3B 30B MOE...did not expect it to work. Ryzen 5700G CPU running at 55% utilization.&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/hmbfaob6ncgf1.png?width=1203&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ade3144a807decb0b799502c2d4025c1d97ad63\"&gt;https://preview.redd.it/hmbfaob6ncgf1.png?width=1203&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ade3144a807decb0b799502c2d4025c1d97ad63&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/ot9fdob6ncgf1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ca5e7cdb86c10c0be54f91a1d8b7a164ed85583\"&gt;https://preview.redd.it/ot9fdob6ncgf1.png?width=750&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ca5e7cdb86c10c0be54f91a1d8b7a164ed85583&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1meostj",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "OldEffective9726",
            "discussion_type": null,
            "num_comments": 23,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/",
            "subreddit_subscribers": 508191,
            "created_utc": 1754028556,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6c1k2r",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Secure_Reflection409",
                      "can_mod_post": false,
                      "created_utc": 1754049098,
                      "send_replies": true,
                      "parent_id": "t1_n6azf20",
                      "score": 1,
                      "author_fullname": "t2_by77ogdhr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Every time I touch KV cache, my responses start drifting into the wrong lane.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6c1k2r",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Every time I touch KV cache, my responses start drifting into the wrong lane.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6c1k2r/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754049098,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6azf20",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "DorphinPack",
            "can_mod_post": false,
            "created_utc": 1754029117,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 3,
            "author_fullname": "t2_zebuyjw9s",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Seconded on turning on FA and re: quantizing the cache squeezing the V lower than the K works best in my experience. Even for code generation I can get away with K:q8_0 V:q5_1 on llama.cpp compiled with all the fa quantization options. Not sure what LMStudio has available. \n\nBut when at Q8 you’ll claw back more VRAM for layers which means speeeeed.",
            "edited": 1754029316,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6azf20",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Seconded on turning on FA and re: quantizing the cache squeezing the V lower than the K works best in my experience. Even for code generation I can get away with K:q8_0 V:q5_1 on llama.cpp compiled with all the fa quantization options. Not sure what LMStudio has available. &lt;/p&gt;\n\n&lt;p&gt;But when at Q8 you’ll claw back more VRAM for layers which means speeeeed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6azf20/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754029117,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6b77df",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "tmvr",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6azu7v",
                                "score": 2,
                                "author_fullname": "t2_11qlhv",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "That's weird, because setting K and V to Q8 would reduce memory requirements so you should be able to put more layers into VRAM.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6b77df",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s weird, because setting K and V to Q8 would reduce memory requirements so you should be able to put more layers into VRAM.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1meostj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b77df/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754033383,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754033383,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6azu7v",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OldEffective9726",
                      "can_mod_post": false,
                      "created_utc": 1754029342,
                      "send_replies": true,
                      "parent_id": "t1_n6ayuzy",
                      "score": 2,
                      "author_fullname": "t2_7m7dnrxx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "7 tokens/s when all 3 experimental features were turned on. same prompt",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6azu7v",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;7 tokens/s when all 3 experimental features were turned on. same prompt&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6azu7v/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754029342,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ayuzy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "External-Stretch7315",
            "can_mod_post": false,
            "created_utc": 1754028825,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 2,
            "author_fullname": "t2_6ld89ekm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "why dont u turn on cache and flash attention?? can you post how much u get with those?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ayuzy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;why dont u turn on cache and flash attention?? can you post how much u get with those?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6ayuzy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754028825,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6b0qo2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OldEffective9726",
                      "can_mod_post": false,
                      "created_utc": 1754029831,
                      "send_replies": true,
                      "parent_id": "t1_n6ayxa0",
                      "score": 2,
                      "author_fullname": "t2_7m7dnrxx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "No...5070 has only 12 GB vram... it dropped to 1 tokens/s using 30/48 gpu offloading",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6b0qo2",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No...5070 has only 12 GB vram... it dropped to 1 tokens/s using 30/48 gpu offloading&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b0qo2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754029831,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ayxa0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "LA_rent_Aficionado",
            "can_mod_post": false,
            "created_utc": 1754028858,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 2,
            "author_fullname": "t2_t8zbiflk",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Put more layers on GPU if you can",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ayxa0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Put more layers on GPU if you can&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6ayxa0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754028858,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6bb1um",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "MaxKruse96",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6ba9k5",
                                                    "score": 8,
                                                    "author_fullname": "t2_pfi81",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "What needs to be done to get the GPU to be used efficiently is to not use LMStudio, and instead use llamacpp, with the -ot flags to load specific layers of the model onto the GPU where they get executed much faster. see here [https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont\\_offload\\_gguf\\_layers\\_offload\\_tensors\\_200\\_gen/?utm\\_source=chatgpt.com](https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/?utm_source=chatgpt.com)",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6bb1um",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What needs to be done to get the GPU to be used efficiently is to not use LMStudio, and instead use llamacpp, with the -ot flags to load specific layers of the model onto the GPU where they get executed much faster. see here &lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/?utm_source=chatgpt.com\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ki7tg7/dont_offload_gguf_layers_offload_tensors_200_gen/?utm_source=chatgpt.com&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1meostj",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6bb1um/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754035527,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754035527,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 8
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6ba9k5",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "jackdareel",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6b7e49",
                                          "score": 1,
                                          "author_fullname": "t2_1puly589vf",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I upvoted your reply for the effort, but I notice someone else has downvoted, presumably because despite the length of the reply you don't actually explain what is being done wrong. You explain what can be seen in the screenshot, that the speed indicates CPU is being used, but what is causing this in the settings? What needs to be done different to get the GPU to do its work?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6ba9k5",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I upvoted your reply for the effort, but I notice someone else has downvoted, presumably because despite the length of the reply you don&amp;#39;t actually explain what is being done wrong. You explain what can be seen in the screenshot, that the speed indicates CPU is being used, but what is causing this in the settings? What needs to be done different to get the GPU to do its work?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1meostj",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6ba9k5/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754035086,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754035086,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6b7e49",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "MaxKruse96",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6b5s0a",
                                "score": 4,
                                "author_fullname": "t2_pfi81",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "1. They mention their cpu: Ryzen 5700G CPU  \n2. they mention their GPU, a 5070 with 12gb  \n3. The model is qwen3 30b its not entirely evident which quantization they use, but the inference speed does not look like they are using q2 or smaller, as that would mean it fits fully into vram and lmstudio will try to offload 40/48 in that case  \n4. 15t/s on 3b active parameters is... cpu territory, especially at the context size and the type of prompt they used.\n\nFrom experience:\n\nOn my 5800x3d, a cpu about as similar as it gets to theirs, i also got 18t/s on 100% CPU inference. on ddr4.  \nOn my 7950x with ddr5, i would get around 22t/s for this usecase.\n\nEven if they put the slider to load some into gpu memory, the gpu does nothing for 99.9% of the inference time. it does not compute. only a few of the experts are on the gpu here. and there is no way to \"just load the experts that are used\" into vram, in case thats an idea some people have here. Thats not feasable.\n\nso yea, their gpu choice has 0 meaning in all of this, except due to their settings, it holds the context in vram, which i give them credit for, means they can load 64k context without overspilling, but at that point inference crawls to 10t/s if they are lucky.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6b7e49",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;ol&gt;\n&lt;li&gt;They mention their cpu: Ryzen 5700G CPU&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;they mention their GPU, a 5070 with 12gb&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;The model is qwen3 30b its not entirely evident which quantization they use, but the inference speed does not look like they are using q2 or smaller, as that would mean it fits fully into vram and lmstudio will try to offload 40/48 in that case&lt;br/&gt;&lt;/li&gt;\n&lt;li&gt;15t/s on 3b active parameters is... cpu territory, especially at the context size and the type of prompt they used.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;From experience:&lt;/p&gt;\n\n&lt;p&gt;On my 5800x3d, a cpu about as similar as it gets to theirs, i also got 18t/s on 100% CPU inference. on ddr4.&lt;br/&gt;\nOn my 7950x with ddr5, i would get around 22t/s for this usecase.&lt;/p&gt;\n\n&lt;p&gt;Even if they put the slider to load some into gpu memory, the gpu does nothing for 99.9% of the inference time. it does not compute. only a few of the experts are on the gpu here. and there is no way to &amp;quot;just load the experts that are used&amp;quot; into vram, in case thats an idea some people have here. Thats not feasable.&lt;/p&gt;\n\n&lt;p&gt;so yea, their gpu choice has 0 meaning in all of this, except due to their settings, it holds the context in vram, which i give them credit for, means they can load 64k context without overspilling, but at that point inference crawls to 10t/s if they are lucky.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1meostj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b7e49/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754033488,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754033488,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6b5s0a",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "jackdareel",
                      "can_mod_post": false,
                      "created_utc": 1754032588,
                      "send_replies": true,
                      "parent_id": "t1_n6b1v57",
                      "score": 2,
                      "author_fullname": "t2_1puly589vf",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Please share what the OP is doing wrong. I can't tell from the screenshots.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6b5s0a",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Please share what the OP is doing wrong. I can&amp;#39;t tell from the screenshots.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b5s0a/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754032588,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6b1v57",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "MaxKruse96",
            "can_mod_post": false,
            "created_utc": 1754030447,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 2,
            "author_fullname": "t2_pfi81",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "i can guarantee you that your gpu does almost nothing here. you bought this gpu for no reason at all. 30b runs with 15-20t/s on ddr5.\n\ni am sometimes baffled that people dont understand how the basics of loading a model work still - there is so much information here that you can even ask chatgpt to search reddit for the info...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6b1v57",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;i can guarantee you that your gpu does almost nothing here. you bought this gpu for no reason at all. 30b runs with 15-20t/s on ddr5.&lt;/p&gt;\n\n&lt;p&gt;i am sometimes baffled that people dont understand how the basics of loading a model work still - there is so much information here that you can even ask chatgpt to search reddit for the info...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b1v57/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754030447,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6bxv41",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "kironlau",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6butyi",
                                "score": 1,
                                "author_fullname": "t2_tb0dz2ds",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yes, the llama-server.exe of ik\\_llama.cpp could just serve an openai-format api, you could connect to frontend.\n\nStill, there are some fork of ik\\_llama.cpp support gui without command prompt:  \n[Release Croco.Cpp v1.97020\\_b6014\\_IKLpr624\\_RMv1.14.9m · Nexesenex/croco.cpp](https://github.com/Nexesenex/croco.cpp/releases/tag/v1.97020_b6014_IKLpr624_RMv1.14.9m)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6bxv41",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yes, the llama-server.exe of ik_llama.cpp could just serve an openai-format api, you could connect to frontend.&lt;/p&gt;\n\n&lt;p&gt;Still, there are some fork of ik_llama.cpp support gui without command prompt:&lt;br/&gt;\n&lt;a href=\"https://github.com/Nexesenex/croco.cpp/releases/tag/v1.97020_b6014_IKLpr624_RMv1.14.9m\"&gt;Release Croco.Cpp v1.97020_b6014_IKLpr624_RMv1.14.9m · Nexesenex/croco.cpp&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1meostj",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6bxv41/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754047571,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754047571,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6butyi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "wooden-guy",
                      "can_mod_post": false,
                      "created_utc": 1754046230,
                      "send_replies": true,
                      "parent_id": "t1_n6bru5b",
                      "score": 1,
                      "author_fullname": "t2_16to413y2o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not op, but I have a quick question cause I'm one of those who always fears the command prompt, does ik llama have an api so say I can connect it to a frontend?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6butyi",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not op, but I have a quick question cause I&amp;#39;m one of those who always fears the command prompt, does ik llama have an api so say I can connect it to a frontend?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6butyi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754046230,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6bru5b",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "kironlau",
            "can_mod_post": false,
            "created_utc": 1754044810,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 1,
            "author_fullname": "t2_tb0dz2ds",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://preview.redd.it/fq0ufb9tydgf1.png?width=1164&amp;format=png&amp;auto=webp&amp;s=63257c294ac54ecae30cddf91893ecaecf01d995\n\nUse ik\\_llama.cpp, you will get faster (at least double your token speed as you are smiliar hardware of me),  \nmodel: ubergarm\\\\Qwen3-30B-A3B-Instruct-2507-GGUF\\\\Qwen3-30B-A3B-Instruct-2507-IQ4\\_KSS.gguf   \nContext size: 32768  \ngeneration eval time =    6414.71 ms /   160 runs   (   40.09 ms per token,    24.94 tokens per second)  \nmy system config: \n\n\\-CPU: amd 5700x, GPU: 4070 12gb, RAM: 8\\*2+16\\*2 DDR4@3733 (OC)\n\nif you are interested, I could share my config on ik\\_llama.cpp",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6bru5b",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://preview.redd.it/fq0ufb9tydgf1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63257c294ac54ecae30cddf91893ecaecf01d995\"&gt;https://preview.redd.it/fq0ufb9tydgf1.png?width=1164&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63257c294ac54ecae30cddf91893ecaecf01d995&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Use ik_llama.cpp, you will get faster (at least double your token speed as you are smiliar hardware of me),&lt;br/&gt;\nmodel: ubergarm\\Qwen3-30B-A3B-Instruct-2507-GGUF\\Qwen3-30B-A3B-Instruct-2507-IQ4_KSS.gguf&lt;br/&gt;\nContext size: 32768&lt;br/&gt;\ngeneration eval time =    6414.71 ms /   160 runs   (   40.09 ms per token,    24.94 tokens per second)&lt;br/&gt;\nmy system config: &lt;/p&gt;\n\n&lt;p&gt;-CPU: amd 5700x, GPU: 4070 12gb, RAM: 8*2+16*2 DDR4@3733 (OC)&lt;/p&gt;\n\n&lt;p&gt;if you are interested, I could share my config on ik_llama.cpp&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6bru5b/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754044810,
            "media_metadata": {
              "fq0ufb9tydgf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 82,
                    "x": 108,
                    "u": "https://preview.redd.it/fq0ufb9tydgf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b2602c8626c093b74cf882e3b8a39fc169ff7b74"
                  },
                  {
                    "y": 164,
                    "x": 216,
                    "u": "https://preview.redd.it/fq0ufb9tydgf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2adb3c85d8a2d62616c750bd5187ac9dca1c5a08"
                  },
                  {
                    "y": 243,
                    "x": 320,
                    "u": "https://preview.redd.it/fq0ufb9tydgf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=89075526385e2f66b0d79f5fcac9668a0c4eac23"
                  },
                  {
                    "y": 487,
                    "x": 640,
                    "u": "https://preview.redd.it/fq0ufb9tydgf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0988df2ffef8e88d07056bf2b90a37e8c4dd95bd"
                  },
                  {
                    "y": 730,
                    "x": 960,
                    "u": "https://preview.redd.it/fq0ufb9tydgf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7a1db4e0754a6aebe6e7a2b9a6b1c311b3757d2"
                  },
                  {
                    "y": 822,
                    "x": 1080,
                    "u": "https://preview.redd.it/fq0ufb9tydgf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=df7a275b95b9ab5a5427d5acca002d0c3f118d68"
                  }
                ],
                "s": {
                  "y": 886,
                  "x": 1164,
                  "u": "https://preview.redd.it/fq0ufb9tydgf1.png?width=1164&amp;format=png&amp;auto=webp&amp;s=63257c294ac54ecae30cddf91893ecaecf01d995"
                },
                "id": "fq0ufb9tydgf1"
              }
            },
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6c5w6z",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "XLIICXX",
            "can_mod_post": false,
            "created_utc": 1754050749,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 1,
            "author_fullname": "t2_2npt3azr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Something seems off. I get around 28 t/s on a Ryzen 2700X (32GB RAM @ DDR4 3200) and a 3070 (8GB VRAM). CPU only I get around 15 t/s.\n\nEDIT: Using the unsloth UD-Q4_K_XL quant",
            "edited": 1754050957,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6c5w6z",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Something seems off. I get around 28 t/s on a Ryzen 2700X (32GB RAM @ DDR4 3200) and a 3070 (8GB VRAM). CPU only I get around 15 t/s.&lt;/p&gt;\n\n&lt;p&gt;EDIT: Using the unsloth UD-Q4_K_XL quant&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6c5w6z/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754050749,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6bg5iq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Thomas-Lore",
                      "can_mod_post": false,
                      "created_utc": 1754038436,
                      "send_replies": true,
                      "parent_id": "t1_n6b16mx",
                      "score": 2,
                      "author_fullname": "t2_5hobp6m4",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not possible for vram bandwidth be the issue here, I run it at 18tps from ddr5. And ddr5 is way slower than whatever memory rtx 5070 has.",
                      "edited": 1754038949,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6bg5iq",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not possible for vram bandwidth be the issue here, I run it at 18tps from ddr5. And ddr5 is way slower than whatever memory rtx 5070 has.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6bg5iq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754038436,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6b1dnh",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "AppearanceHeavy6724",
                      "can_mod_post": false,
                      "created_utc": 1754030180,
                      "send_replies": true,
                      "parent_id": "t1_n6b16mx",
                      "score": 2,
                      "author_fullname": "t2_uz37qfx5",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Are you ok? it is a moe, bandwidth does not matter here.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6b1dnh",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you ok? it is a moe, bandwidth does not matter here.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b1dnh/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754030180,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6b16mx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Daemonix00",
            "can_mod_post": false,
            "created_utc": 1754030072,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 1,
            "author_fullname": "t2_h89ec",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "GPU memory bandwidth. You could better get a 3090 (might be cheaper too?)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6b16mx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GPU memory bandwidth. You could better get a 3090 (might be cheaper too?)&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b16mx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754030072,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6bull7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "wooden-guy",
                      "can_mod_post": false,
                      "created_utc": 1754046126,
                      "send_replies": true,
                      "parent_id": "t1_n6azoin",
                      "score": 1,
                      "author_fullname": "t2_16to413y2o",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Way dumber too.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6bull7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Way dumber too.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6bull7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754046126,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6azoin",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "zipperlein",
            "can_mod_post": false,
            "created_utc": 1754029258,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 0,
            "author_fullname": "t2_x3duw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "[https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-UD-IQ2\\_XXS.gguf](https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-UD-IQ2_XXS.gguf)  \nWould be way faster.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6azoin",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-UD-IQ2_XXS.gguf\"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF/blob/main/Qwen3-30B-A3B-Instruct-2507-UD-IQ2_XXS.gguf&lt;/a&gt;&lt;br/&gt;\nWould be way faster.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6azoin/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754029258,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6btv78",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "CheatCodesOfLife",
                      "can_mod_post": false,
                      "created_utc": 1754045783,
                      "send_replies": true,
                      "parent_id": "t1_n6b7em9",
                      "score": 1,
                      "author_fullname": "t2_32el727b",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Seems slow, what quant? I get &gt; 40t/s with just cpu. 3090 should be &gt; 80t/s",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6btv78",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Seems slow, what quant? I get &amp;gt; 40t/s with just cpu. 3090 should be &amp;gt; 80t/s&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1meostj",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6btv78/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754045783,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6b7em9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "vulcan4d",
            "can_mod_post": false,
            "created_utc": 1754033496,
            "send_replies": true,
            "parent_id": "t3_1meostj",
            "score": 0,
            "author_fullname": "t2_a5y20",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Sorry to say you need more Vram.  The 3090 on the 30b-a3b gets around 50t/s.  You also need to take into account context.  Larger prompts for research or coding will need plenty of context.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6b7em9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sorry to say you need more Vram.  The 3090 on the 30b-a3b gets around 50t/s.  You also need to take into account context.  Larger prompts for research or coding will need plenty of context.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1meostj/bought_rtx_5070_to_run_30b_ai_and_it_worked_with/n6b7em9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754033496,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1meostj",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]