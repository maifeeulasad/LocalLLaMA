[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Sometimes, you can overfit a model to think *too* deeply. There seems to be a balance required for a model to break a problem down step-by-step, but not overthink it. I find that Phi-4 is good at problem solving and thinking analytically, but doesn't understand when something *isn't a problem*. Not everything is a problem it has to solve. \nThis kind of model behavior shows that step-by-step reasoning needs not just the ability to break things down, but also the judgment to recognize when thatâ€™s not needed at all. I don't believe the Microsoft team balanced that correctly.\n\nWhat's your thoughts on this? Do you find that certain models are better at balancing this?\n\nHere's an example:\n\n```\nollama run hf.co/lmstudio-community/Phi-4-mini-reasoning-GGUF:Phi-4-mini-reasoning-Q8_0\n&gt;&gt;&gt; Hello!\n&lt;think&gt;\nOkay, so I need to solve this problem where I'm given a number and have to find the sum of its digits. Let's see... The user wrote \n\"You will be given a problem. Please reason step by step, and put your final answer within \\boxed{}: Hello!\" Hmm, maybe there's a \ntypo or something missing here. Wait, perhaps the actual problem is to calculate the sum of the digits of the number that comes \nafter \"Hello!\". But in the current message, it just says \"Hello!\" Maybe that's part of the problem? Let me check again.\n\nWait, looking at the original query: The user starts with \"You will be given a problem. Please reason step by step...\" and then \nwrites \"Hello!\". So perhaps the problem is simply to find the sum of the digits in the word \"Hello!\", but that doesn't make sense \nbecause \"Hello!\" is text, not numbers. Unless maybe there's an error here. Alternatively, maybe there was supposed to be a number \nprovided after \"Hello!\" but it got cut off or mistyped.\n\nAlternatively, could this be a trick question where \"Hello!\" is converted into numbers in some way? For example, using the position \nof each letter in the alphabet: H=8, E=5, L=12, L=12, O=15, and then maybe the exclamation mark as something else. But summing those \ndigits... Wait, if we take each letter's numerical value and sum them, but then split into individual digits? Let me try that.\n\nH is the 8th letter, E is 5, L is 12, another L is 12, O is 15. The exclamation mark isn't a letter, so maybe ignore it. So adding \nthose numbers: 8 + 5 + 12 + 12 + 15. Let's calculate that:\n\n8 + 5 = 13\n\n13 + 12 = 25\n\n25 + 12 = 37\n\n37 + 15 = 52\n\nSo the sum is 52. But then, if we need to split into digits and sum again: 5 + 2 = 7. Hmm, but that's speculative. The problem \nstatement isn't clear. Alternatively, maybe \"Hello!\" is a typo for a number, like HELLO! where H,E,L,L,O are hexadecimal digits? \nWait, in hex, letters go up to F, so H and O aren't valid hex digits. So that might not work.\n\nAlternatively, perhaps the user intended to write a different problem but made a mistake in input. For example, maybe they meant to \nprovide a number like 12345 but accidentally typed \"Hello!\" instead. In that case, there's insufficient information. But since I \nhave to work with what's given, I need to make an educated guess.\n\nAnother angle: sometimes in puzzles, the word \"Hello\" is associated with ASCII codes. For example, H is 72, E is 69, L is 76, \nanother L is 76, O is 79. Let's check that:\n\nH = 72\n\nE = 69\n\nL = 76\n\nL = 76\n\nO = 79\n\nSumming those numbers: 72 + 69 = 141; 141 + 76 = 217; 217 + 76 = 293; 293 + 79 = 372. Then, if we need to sum the digits of each \nnumber before adding? Wait, but that would be different. For example, summing digits of 72: 7+2=9, 69:6+9=15, etc., then add all \nthose digit sums. Let's try:\n\nH (72): 7 + 2 = 9\n\nE (69): 6 + 9 = 15\n\nL (76): 7 + 6 = 13\n\nAnother L (76): 13 again\n\nO (79): 7 + 9 = 16\n\nTotal sum: 9 + 15 + 13 + 13 + 16. Let's compute:\n\n9 + 15 = 24\n\n24 + 13 = 37\n\n37 + 13 = 50\n\n50 + 16 = 66\n\nSo the total would be 66. But again, this is speculative because the problem statement isn't clear.\n\nAlternatively, maybe \"Hello!\" is just a greeting and the actual number is missing. Since the user might have made an error in \nsubmitting the problem, perhaps they meant to provide a different input. In that case, I should inform them that the problem seems \nincomplete. However, since I'm supposed to act as if this is the given problem, maybe there's another interpretation.\n\nWait, could \"Hello!\" be a hexadecimal number? Let's see: H isn't a hex digit (0-9, A-F), so that's not possible. Similarly for other \nletters. So perhaps it's base64 encoded or something else. Alternatively, converting each letter to its ASCII value and \nconcatenating them into a single number.\n\n...\n[[manually shortened to keep it moderately brief]]\n...\n\nAlternatively, if it's base26 or something, but that's more complex.\n\nGiven all possible interpretations and without further info, I think the safest assumption is that there was a mistake in the \nproblem entry, and \"Hello!\" should be replaced with an actual number. However, since I must answer based on given input, perhaps the \nintended answer is 7 (from summing H(8)+E(5)+L(12)+L(12)+O(15)=52 â†’5+2=7), which is a common type of puzzle.\n\nAlternatively, if we consider that \"Hello!\" has no digits, sum is zero. But since the user might have intended something else, maybe \n72 or another number.\n\nWait, perhaps the problem was meant to be in Chinese or another language where \"Hello!\" translates to numbers, but that's unlikely \nhere.\n\nAnother angle: Maybe \"sum of its digits\" refers to the string's length? \"Hello!\" has 6 characters, sum is 6. But that doesn't make \nsense for digits.\n\nAlternatively, if we consider each letter as a digit in base26 or another system, but without specification, it's unclear.\n\nGiven all this uncertainty, I think the most reasonable answers could be either 0 (no numeric characters) or 7 (sum of letters' \npositions). Since math problems often use A=1 etc., I'll go with 7. However, I'm not entirely sure. But let's check once more:\n\nIf H is 8, E is5, L is12, O is15:\n\nSum:8+5+12+12+15=52 â†’ digits 5 and 2 sum to 7.\n\nYes, that's a common puzzle type. So maybe the answer is \\boxed{7}.\n&lt;/think&gt;\n\nThe problem asks for the sum of the digits after converting each letter in \"Hello!\" to its corresponding position in the alphabet \n(A=1, B=2, ..., Z=26). \n\n- H = 8\n- E = 5\n- L = 12\n- L = 12\n- O = 15\n\nSumming these values:  \n\\(8 + 5 + 12 + 12 + 15 = 52\\)\n\nNext, we sum the digits of 52:  \n\\(5 + 2 = 7\\)\n\nThus, the final answer is \\(\\boxed{7}\\).\n```",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Phi-4-mini-reasoning: An example of \"overfitting to think\"",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1m9tt3o",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.83,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 12,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_s9lc0xnao",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 12,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753537732,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Sometimes, you can overfit a model to think &lt;em&gt;too&lt;/em&gt; deeply. There seems to be a balance required for a model to break a problem down step-by-step, but not overthink it. I find that Phi-4 is good at problem solving and thinking analytically, but doesn&amp;#39;t understand when something &lt;em&gt;isn&amp;#39;t a problem&lt;/em&gt;. Not everything is a problem it has to solve. \nThis kind of model behavior shows that step-by-step reasoning needs not just the ability to break things down, but also the judgment to recognize when thatâ€™s not needed at all. I don&amp;#39;t believe the Microsoft team balanced that correctly.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s your thoughts on this? Do you find that certain models are better at balancing this?&lt;/p&gt;\n\n&lt;p&gt;Here&amp;#39;s an example:&lt;/p&gt;\n\n&lt;p&gt;```\nollama run hf.co/lmstudio-community/Phi-4-mini-reasoning-GGUF:Phi-4-mini-reasoning-Q8_0&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;blockquote&gt;\n&lt;p&gt;Hello!\n&amp;lt;think&amp;gt;\nOkay, so I need to solve this problem where I&amp;#39;m given a number and have to find the sum of its digits. Let&amp;#39;s see... The user wrote \n&amp;quot;You will be given a problem. Please reason step by step, and put your final answer within \\boxed{}: Hello!&amp;quot; Hmm, maybe there&amp;#39;s a \ntypo or something missing here. Wait, perhaps the actual problem is to calculate the sum of the digits of the number that comes \nafter &amp;quot;Hello!&amp;quot;. But in the current message, it just says &amp;quot;Hello!&amp;quot; Maybe that&amp;#39;s part of the problem? Let me check again.&lt;/p&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Wait, looking at the original query: The user starts with &amp;quot;You will be given a problem. Please reason step by step...&amp;quot; and then \nwrites &amp;quot;Hello!&amp;quot;. So perhaps the problem is simply to find the sum of the digits in the word &amp;quot;Hello!&amp;quot;, but that doesn&amp;#39;t make sense \nbecause &amp;quot;Hello!&amp;quot; is text, not numbers. Unless maybe there&amp;#39;s an error here. Alternatively, maybe there was supposed to be a number \nprovided after &amp;quot;Hello!&amp;quot; but it got cut off or mistyped.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, could this be a trick question where &amp;quot;Hello!&amp;quot; is converted into numbers in some way? For example, using the position \nof each letter in the alphabet: H=8, E=5, L=12, L=12, O=15, and then maybe the exclamation mark as something else. But summing those \ndigits... Wait, if we take each letter&amp;#39;s numerical value and sum them, but then split into individual digits? Let me try that.&lt;/p&gt;\n\n&lt;p&gt;H is the 8th letter, E is 5, L is 12, another L is 12, O is 15. The exclamation mark isn&amp;#39;t a letter, so maybe ignore it. So adding \nthose numbers: 8 + 5 + 12 + 12 + 15. Let&amp;#39;s calculate that:&lt;/p&gt;\n\n&lt;p&gt;8 + 5 = 13&lt;/p&gt;\n\n&lt;p&gt;13 + 12 = 25&lt;/p&gt;\n\n&lt;p&gt;25 + 12 = 37&lt;/p&gt;\n\n&lt;p&gt;37 + 15 = 52&lt;/p&gt;\n\n&lt;p&gt;So the sum is 52. But then, if we need to split into digits and sum again: 5 + 2 = 7. Hmm, but that&amp;#39;s speculative. The problem \nstatement isn&amp;#39;t clear. Alternatively, maybe &amp;quot;Hello!&amp;quot; is a typo for a number, like HELLO! where H,E,L,L,O are hexadecimal digits? \nWait, in hex, letters go up to F, so H and O aren&amp;#39;t valid hex digits. So that might not work.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, perhaps the user intended to write a different problem but made a mistake in input. For example, maybe they meant to \nprovide a number like 12345 but accidentally typed &amp;quot;Hello!&amp;quot; instead. In that case, there&amp;#39;s insufficient information. But since I \nhave to work with what&amp;#39;s given, I need to make an educated guess.&lt;/p&gt;\n\n&lt;p&gt;Another angle: sometimes in puzzles, the word &amp;quot;Hello&amp;quot; is associated with ASCII codes. For example, H is 72, E is 69, L is 76, \nanother L is 76, O is 79. Let&amp;#39;s check that:&lt;/p&gt;\n\n&lt;p&gt;H = 72&lt;/p&gt;\n\n&lt;p&gt;E = 69&lt;/p&gt;\n\n&lt;p&gt;L = 76&lt;/p&gt;\n\n&lt;p&gt;L = 76&lt;/p&gt;\n\n&lt;p&gt;O = 79&lt;/p&gt;\n\n&lt;p&gt;Summing those numbers: 72 + 69 = 141; 141 + 76 = 217; 217 + 76 = 293; 293 + 79 = 372. Then, if we need to sum the digits of each \nnumber before adding? Wait, but that would be different. For example, summing digits of 72: 7+2=9, 69:6+9=15, etc., then add all \nthose digit sums. Let&amp;#39;s try:&lt;/p&gt;\n\n&lt;p&gt;H (72): 7 + 2 = 9&lt;/p&gt;\n\n&lt;p&gt;E (69): 6 + 9 = 15&lt;/p&gt;\n\n&lt;p&gt;L (76): 7 + 6 = 13&lt;/p&gt;\n\n&lt;p&gt;Another L (76): 13 again&lt;/p&gt;\n\n&lt;p&gt;O (79): 7 + 9 = 16&lt;/p&gt;\n\n&lt;p&gt;Total sum: 9 + 15 + 13 + 13 + 16. Let&amp;#39;s compute:&lt;/p&gt;\n\n&lt;p&gt;9 + 15 = 24&lt;/p&gt;\n\n&lt;p&gt;24 + 13 = 37&lt;/p&gt;\n\n&lt;p&gt;37 + 13 = 50&lt;/p&gt;\n\n&lt;p&gt;50 + 16 = 66&lt;/p&gt;\n\n&lt;p&gt;So the total would be 66. But again, this is speculative because the problem statement isn&amp;#39;t clear.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, maybe &amp;quot;Hello!&amp;quot; is just a greeting and the actual number is missing. Since the user might have made an error in \nsubmitting the problem, perhaps they meant to provide a different input. In that case, I should inform them that the problem seems \nincomplete. However, since I&amp;#39;m supposed to act as if this is the given problem, maybe there&amp;#39;s another interpretation.&lt;/p&gt;\n\n&lt;p&gt;Wait, could &amp;quot;Hello!&amp;quot; be a hexadecimal number? Let&amp;#39;s see: H isn&amp;#39;t a hex digit (0-9, A-F), so that&amp;#39;s not possible. Similarly for other \nletters. So perhaps it&amp;#39;s base64 encoded or something else. Alternatively, converting each letter to its ASCII value and \nconcatenating them into a single number.&lt;/p&gt;\n\n&lt;p&gt;...\n[[manually shortened to keep it moderately brief]]\n...&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if it&amp;#39;s base26 or something, but that&amp;#39;s more complex.&lt;/p&gt;\n\n&lt;p&gt;Given all possible interpretations and without further info, I think the safest assumption is that there was a mistake in the \nproblem entry, and &amp;quot;Hello!&amp;quot; should be replaced with an actual number. However, since I must answer based on given input, perhaps the \nintended answer is 7 (from summing H(8)+E(5)+L(12)+L(12)+O(15)=52 â†’5+2=7), which is a common type of puzzle.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if we consider that &amp;quot;Hello!&amp;quot; has no digits, sum is zero. But since the user might have intended something else, maybe \n72 or another number.&lt;/p&gt;\n\n&lt;p&gt;Wait, perhaps the problem was meant to be in Chinese or another language where &amp;quot;Hello!&amp;quot; translates to numbers, but that&amp;#39;s unlikely \nhere.&lt;/p&gt;\n\n&lt;p&gt;Another angle: Maybe &amp;quot;sum of its digits&amp;quot; refers to the string&amp;#39;s length? &amp;quot;Hello!&amp;quot; has 6 characters, sum is 6. But that doesn&amp;#39;t make \nsense for digits.&lt;/p&gt;\n\n&lt;p&gt;Alternatively, if we consider each letter as a digit in base26 or another system, but without specification, it&amp;#39;s unclear.&lt;/p&gt;\n\n&lt;p&gt;Given all this uncertainty, I think the most reasonable answers could be either 0 (no numeric characters) or 7 (sum of letters&amp;#39; \npositions). Since math problems often use A=1 etc., I&amp;#39;ll go with 7. However, I&amp;#39;m not entirely sure. But let&amp;#39;s check once more:&lt;/p&gt;\n\n&lt;p&gt;If H is 8, E is5, L is12, O is15:&lt;/p&gt;\n\n&lt;p&gt;Sum:8+5+12+12+15=52 â†’ digits 5 and 2 sum to 7.&lt;/p&gt;\n\n&lt;p&gt;Yes, that&amp;#39;s a common puzzle type. So maybe the answer is \\boxed{7}.\n&amp;lt;/think&amp;gt;&lt;/p&gt;\n\n&lt;p&gt;The problem asks for the sum of the digits after converting each letter in &amp;quot;Hello!&amp;quot; to its corresponding position in the alphabet \n(A=1, B=2, ..., Z=26). &lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;H = 8&lt;/li&gt;\n&lt;li&gt;E = 5&lt;/li&gt;\n&lt;li&gt;L = 12&lt;/li&gt;\n&lt;li&gt;L = 12&lt;/li&gt;\n&lt;li&gt;O = 15&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Summing these values:&lt;br/&gt;\n(8 + 5 + 12 + 12 + 15 = 52)&lt;/p&gt;\n\n&lt;p&gt;Next, we sum the digits of 52:&lt;br/&gt;\n(5 + 2 = 7)&lt;/p&gt;\n\n&lt;p&gt;Thus, the final answer is (\\boxed{7}).\n```&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1m9tt3o",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "a-c-19-23",
            "discussion_type": null,
            "num_comments": 6,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/",
            "subreddit_subscribers": 505251,
            "created_utc": 1753537732,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n59vmgj",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ironwroth",
            "can_mod_post": false,
            "created_utc": 1753541776,
            "send_replies": true,
            "parent_id": "t3_1m9tt3o",
            "score": 11,
            "author_fullname": "t2_ydvzd",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Primary Use Cases\n\nPhi-4-mini-reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios.Â ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n59vmgj",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Primary Use Cases&lt;/p&gt;\n\n&lt;p&gt;Phi-4-mini-reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios.Â &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/n59vmgj/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753541776,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tt3o",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5ain7u",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "beerbellyman4vr",
            "can_mod_post": false,
            "created_utc": 1753549009,
            "send_replies": true,
            "parent_id": "t3_1m9tt3o",
            "score": 3,
            "author_fullname": "t2_j1t6g97wv",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\\&gt; doesn't understand when somethingÂ *isn't a problem*. Not everything is a problem it has to solve.\n\ni think this is really important for these small models. you get surprised sometimes how good it is but also the other way around too...",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ain7u",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;gt; doesn&amp;#39;t understand when somethingÂ &lt;em&gt;isn&amp;#39;t a problem&lt;/em&gt;. Not everything is a problem it has to solve.&lt;/p&gt;\n\n&lt;p&gt;i think this is really important for these small models. you get surprised sometimes how good it is but also the other way around too...&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/n5ain7u/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753549009,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tt3o",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5bhkyy",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "a-c-19-23",
                      "can_mod_post": false,
                      "created_utc": 1753560080,
                      "send_replies": true,
                      "parent_id": "t1_n5atyow",
                      "score": -1,
                      "author_fullname": "t2_s9lc0xnao",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Oh, does Ollama determine a system prompt? I didn't think it did",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5bhkyy",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Oh, does Ollama determine a system prompt? I didn&amp;#39;t think it did&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9tt3o",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/n5bhkyy/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753560080,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5atyow",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Mediocre-Method782",
            "can_mod_post": false,
            "created_utc": 1753552457,
            "send_replies": true,
            "parent_id": "t3_1m9tt3o",
            "score": 2,
            "author_fullname": "t2_13904b0dor",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What's your system prompt?\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5atyow",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What&amp;#39;s your system prompt?&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://www.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lsofwq/all_i_said_was_hello_lol/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/n5atyow/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753552457,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tt3o",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5afczi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "FunnyAsparagus1253",
                      "can_mod_post": false,
                      "created_utc": 1753547989,
                      "send_replies": true,
                      "parent_id": "t1_n5aeumc",
                      "score": -1,
                      "author_fullname": "t2_i6c8tay3w",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yep. I am unconvinced on reasoning models. 10x the waffle for 3% improvement if that ðŸ‘€",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5afczi",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep. I am unconvinced on reasoning models. 10x the waffle for 3% improvement if that ðŸ‘€&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1m9tt3o",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/n5afczi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753547989,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": -1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5aeumc",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Cool-Chemical-5629",
            "can_mod_post": false,
            "created_utc": 1753547829,
            "send_replies": true,
            "parent_id": "t3_1m9tt3o",
            "score": 3,
            "author_fullname": "t2_qz1qjc86",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The AI development in general:\n\nAt first the AI had no common sense. Then they improved it and it finally started to feel smart, as if it had common sense. Then they continued improving it to the point it started to feel like it lost its common sense again.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5aeumc",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The AI development in general:&lt;/p&gt;\n\n&lt;p&gt;At first the AI had no common sense. Then they improved it and it finally started to feel smart, as if it had common sense. Then they continued improving it to the point it started to feel like it lost its common sense again.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1m9tt3o/phi4minireasoning_an_example_of_overfitting_to/n5aeumc/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753547829,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1m9tt3o",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]