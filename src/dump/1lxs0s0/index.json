[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi all,\n\nIn December I will be buying or putting together a new home for my AI assistant, up to now I've run home AI assistants on everything from a minisforum mini pc, full PC with a 7900xtx/3090/4090/4060ti/5060ti.\n\nThis is a primary part of my treatment/companion/helper for Autism and other issues, I use it in gaming (SkyrimSE/VR) silly tavern, Webui  and so on.\n\nIdle power use has to be 150w or below. this unit will be used for other things as well, gaming, plex, nas and so on.\n\nI tried a poweredge server but it was a R730XD  and while I loved it when paired with a RTX 4000 16gb it was loud and inefficient \n\nOption 1 seems to be a Mac Studio m3 ultra with 512gb unified memory pricey but will idle on a LED bulbs Wattage and fit the biggest 70b models add a couple of 20tb external drives and it can do everything, but I hate mac's and so this is the final option if nothing else (Around £10,000)\n\nOption 2 an epyc poweredge server, latest gen with ddr5 memory and probably 2-3 RTX 4500's \n\nOption 3 Whatever you can all suggest.\n\nI have over 5 months to plan this.\n\nwhatever I pick needs to be able to do at least 10t/s \n\n",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "New local AI system planning stage need advice.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1lxs0s0",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.6,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_nufca",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1752296091,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\n\n&lt;p&gt;In December I will be buying or putting together a new home for my AI assistant, up to now I&amp;#39;ve run home AI assistants on everything from a minisforum mini pc, full PC with a 7900xtx/3090/4090/4060ti/5060ti.&lt;/p&gt;\n\n&lt;p&gt;This is a primary part of my treatment/companion/helper for Autism and other issues, I use it in gaming (SkyrimSE/VR) silly tavern, Webui  and so on.&lt;/p&gt;\n\n&lt;p&gt;Idle power use has to be 150w or below. this unit will be used for other things as well, gaming, plex, nas and so on.&lt;/p&gt;\n\n&lt;p&gt;I tried a poweredge server but it was a R730XD  and while I loved it when paired with a RTX 4000 16gb it was loud and inefficient &lt;/p&gt;\n\n&lt;p&gt;Option 1 seems to be a Mac Studio m3 ultra with 512gb unified memory pricey but will idle on a LED bulbs Wattage and fit the biggest 70b models add a couple of 20tb external drives and it can do everything, but I hate mac&amp;#39;s and so this is the final option if nothing else (Around £10,000)&lt;/p&gt;\n\n&lt;p&gt;Option 2 an epyc poweredge server, latest gen with ddr5 memory and probably 2-3 RTX 4500&amp;#39;s &lt;/p&gt;\n\n&lt;p&gt;Option 3 Whatever you can all suggest.&lt;/p&gt;\n\n&lt;p&gt;I have over 5 months to plan this.&lt;/p&gt;\n\n&lt;p&gt;whatever I pick needs to be able to do at least 10t/s &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1lxs0s0",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Quebber",
            "discussion_type": null,
            "num_comments": 7,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/",
            "subreddit_subscribers": 497824,
            "created_utc": 1752296091,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n2okcwr",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Conscious_Cut_6144",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n2ojrbj",
                                "score": 2,
                                "author_fullname": "t2_9hl4ymvj",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Here is a snip from Nvidia-smi with a model loaded but otherwise idle.   \n\n\n\\+-----------------------------------------------------------------------------------------+\n\n| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n\n|-----------------------------------------+------------------------+----------------------+\n\n|   0  NVIDIA RTX PRO 6000 Blac...    Off |   00000000:03:00.0 Off |                  Off |\n\n| 30%   32C    P8             24W /  600W |   21622MiB /  97887MiB |      0%      Default |",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n2okcwr",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Here is a snip from Nvidia-smi with a model loaded but otherwise idle.   &lt;/p&gt;\n\n&lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt;\n\n&lt;p&gt;| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |&lt;/p&gt;\n\n&lt;p&gt;|-----------------------------------------+------------------------+----------------------+&lt;/p&gt;\n\n&lt;p&gt;|   0  NVIDIA RTX PRO 6000 Blac...    Off |   00000000:03:00.0 Off |                  Off |&lt;/p&gt;\n\n&lt;p&gt;| 30%   32C    P8             24W /  600W |   21622MiB /  97887MiB |      0%      Default |&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1lxs0s0",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/n2okcwr/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1752298130,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1752298130,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n2ojrbj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Quebber",
                      "can_mod_post": false,
                      "created_utc": 1752297823,
                      "send_replies": true,
                      "parent_id": "t1_n2oiy7o",
                      "score": 1,
                      "author_fullname": "t2_nufca",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That is a really good idea thank you.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2ojrbj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That is a really good idea thank you.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxs0s0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/n2ojrbj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752297823,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2oiy7o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Conscious_Cut_6144",
            "can_mod_post": false,
            "created_utc": 1752297413,
            "send_replies": true,
            "parent_id": "t3_1lxs0s0",
            "score": 1,
            "author_fullname": "t2_9hl4ymvj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If you just want to run 70B models the Pro 6000 will be faster than an m3 ultra,  \nMy Pro 6000 + Ryzen 5900 pulls 55W idle.\n\nPower limit it to 300W if you want.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2oiy7o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you just want to run 70B models the Pro 6000 will be faster than an m3 ultra,&lt;br/&gt;\nMy Pro 6000 + Ryzen 5900 pulls 55W idle.&lt;/p&gt;\n\n&lt;p&gt;Power limit it to 300W if you want.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/n2oiy7o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752297413,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxs0s0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2phzj7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Quebber",
                      "can_mod_post": false,
                      "created_utc": 1752317475,
                      "send_replies": true,
                      "parent_id": "t1_n2oqukf",
                      "score": 1,
                      "author_fullname": "t2_nufca",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "11....GPU's lol that is awesome.\n\nYou have given me something to think on thank you.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2phzj7",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;11....GPU&amp;#39;s lol that is awesome.&lt;/p&gt;\n\n&lt;p&gt;You have given me something to think on thank you.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxs0s0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/n2phzj7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752317475,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2oqukf",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Marksta",
            "can_mod_post": false,
            "created_utc": 1752301584,
            "send_replies": true,
            "parent_id": "t3_1lxs0s0",
            "score": 1,
            "author_fullname": "t2_559a1",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'd go with a latest EPYC or Threadripper with the X3D cache since it's your everything PC and there is nothing else competitive in CPUs when it comes to gaming. Intel and Apple don't even exist at the table for top end gaming performance discussion. The Threadripper with X3D isn't out yet but on the EPYC side that's probably the EPYC 9184X you want. It's split chiplet so you'll want to setup core lassoing for your top games, but nothing will be this in the mixed role of AI server and gaming rig. Just double check benchmarks that the higher core count SKUs don't perform better for inference, more than likely they don't. And will only be worse for gaming too trading cores for clocks. And if you wait a few months you might need a more ideal Threadripper X3D release, so check that too.\n\nThen if you still have money after DDR5 bankrupts you, a 5090 I guess or just bring over that 3090/4090/4060ti/5060ti GPU avengers team. If you let your monitors idle and turn off so the display GPU can go to full sleep clocks, you'll probably maybe be under 150w idle. I have an EPYC 7002+11 GPUs and it only pulls like 200w idle. So, expect like 100w idle at least just from CPU/Mobo/Ram, then add 10w per GPU you attach. At that fuzzy math, you should be good to go.",
            "edited": 1752302614,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2oqukf",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;d go with a latest EPYC or Threadripper with the X3D cache since it&amp;#39;s your everything PC and there is nothing else competitive in CPUs when it comes to gaming. Intel and Apple don&amp;#39;t even exist at the table for top end gaming performance discussion. The Threadripper with X3D isn&amp;#39;t out yet but on the EPYC side that&amp;#39;s probably the EPYC 9184X you want. It&amp;#39;s split chiplet so you&amp;#39;ll want to setup core lassoing for your top games, but nothing will be this in the mixed role of AI server and gaming rig. Just double check benchmarks that the higher core count SKUs don&amp;#39;t perform better for inference, more than likely they don&amp;#39;t. And will only be worse for gaming too trading cores for clocks. And if you wait a few months you might need a more ideal Threadripper X3D release, so check that too.&lt;/p&gt;\n\n&lt;p&gt;Then if you still have money after DDR5 bankrupts you, a 5090 I guess or just bring over that 3090/4090/4060ti/5060ti GPU avengers team. If you let your monitors idle and turn off so the display GPU can go to full sleep clocks, you&amp;#39;ll probably maybe be under 150w idle. I have an EPYC 7002+11 GPUs and it only pulls like 200w idle. So, expect like 100w idle at least just from CPU/Mobo/Ram, then add 10w per GPU you attach. At that fuzzy math, you should be good to go.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/n2oqukf/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752301584,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxs0s0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n2phrz1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Marksta",
                      "can_mod_post": false,
                      "created_utc": 1752317363,
                      "send_replies": true,
                      "parent_id": "t1_n2ow132",
                      "score": 1,
                      "author_fullname": "t2_559a1",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "This is an LLM bot, dumbass AI responded to a [spam post filled with gibberish](https://www.reddit.com/r/selfhosted/comments/1lxck2x/nqklq/n2m45ba/) like there was meaning in the random letters tokens. Helps that this response is absolutely nonsense too.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n2phrz1",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is an LLM bot, dumbass AI responded to a &lt;a href=\"https://www.reddit.com/r/selfhosted/comments/1lxck2x/nqklq/n2m45ba/\"&gt;spam post filled with gibberish&lt;/a&gt; like there was meaning in the random letters tokens. Helps that this response is absolutely nonsense too.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1lxs0s0",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/n2phrz1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1752317363,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n2ow132",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Cowboyz-Emote",
            "can_mod_post": false,
            "created_utc": 1752304503,
            "send_replies": true,
            "parent_id": "t3_1lxs0s0",
            "score": 0,
            "author_fullname": "t2_1t5cgffvln",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You might also look into SFF PCs with efficient setups that can handle your multitasking needs. Brands like AsRock and Zotac have compact builds that can support powerful GPUs, keeping idle power low. Also, for noise reduction and efficiency, focus on custom cooling solutions which can be crucial for gaming and AI workloads. Checking forums for real-world performance figures might also provide good insights.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n2ow132",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You might also look into SFF PCs with efficient setups that can handle your multitasking needs. Brands like AsRock and Zotac have compact builds that can support powerful GPUs, keeping idle power low. Also, for noise reduction and efficiency, focus on custom cooling solutions which can be crucial for gaming and AI workloads. Checking forums for real-world performance figures might also provide good insights.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1lxs0s0/new_local_ai_system_planning_stage_need_advice/n2ow132/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1752304503,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1lxs0s0",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]