[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Hi guys!\n\nHow much do PCIe Lanes really matter?\n\nAs far as i understand, just for inference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.\n\nSo basically, if using multiple gpus, its enough when they are connected via PCIe x1-x4 - or do i oversee something here?\n\nThanks for input!\n\nEdit: I'm planning to use AMD Mi50s",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "How much do PCIe Lanes matter?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mf1lfv",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.69,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 6,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_fe2ok1q3",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 6,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": 1754067983,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754066851,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Hi guys!&lt;/p&gt;\n\n&lt;p&gt;How much do PCIe Lanes really matter?&lt;/p&gt;\n\n&lt;p&gt;As far as i understand, just for inference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.&lt;/p&gt;\n\n&lt;p&gt;So basically, if using multiple gpus, its enough when they are connected via PCIe x1-x4 - or do i oversee something here?&lt;/p&gt;\n\n&lt;p&gt;Thanks for input!&lt;/p&gt;\n\n&lt;p&gt;Edit: I&amp;#39;m planning to use AMD Mi50s&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mf1lfv",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "MrCatberry",
            "discussion_type": null,
            "num_comments": 19,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/",
            "subreddit_subscribers": 508541,
            "created_utc": 1754066851,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": {
                                            "kind": "Listing",
                                            "data": {
                                              "after": null,
                                              "dist": null,
                                              "modhash": "",
                                              "geo_filter": "",
                                              "children": [
                                                {
                                                  "kind": "t1",
                                                  "data": {
                                                    "subreddit_id": "t5_81eyvm",
                                                    "approved_at_utc": null,
                                                    "author_is_blocked": false,
                                                    "comment_type": null,
                                                    "awarders": [],
                                                    "mod_reason_by": null,
                                                    "banned_by": null,
                                                    "author_flair_type": "text",
                                                    "total_awards_received": 0,
                                                    "subreddit": "LocalLLaMA",
                                                    "author_flair_template_id": null,
                                                    "distinguished": null,
                                                    "likes": null,
                                                    "replies": "",
                                                    "user_reports": [],
                                                    "saved": false,
                                                    "id": "n6fqjaf",
                                                    "banned_at_utc": null,
                                                    "mod_reason_title": null,
                                                    "gilded": 0,
                                                    "archived": false,
                                                    "collapsed_reason_code": null,
                                                    "no_follow": true,
                                                    "author": "No-Refrigerator-1672",
                                                    "can_mod_post": false,
                                                    "send_replies": true,
                                                    "parent_id": "t1_n6faib9",
                                                    "score": 1,
                                                    "author_fullname": "t2_baavelp5",
                                                    "removal_reason": null,
                                                    "approved_by": null,
                                                    "mod_note": null,
                                                    "all_awardings": [],
                                                    "body": "Basically correct, but there are some caveats. First, if you want to utilize ROCm (I bet you do), then AMD demands you to disable the iGPU in Ryzen processors; so you either go Intel or plan to install a discrete gpu, preferrably Nvidia, to avoid dealing with ROCm GPU management - it sucks. Also, for inference engines, you can only count to use llama.cpp. There is a vllm fork called \"vllm-gfx906\" for those cards, but it's only usable for text-only models, which are less and less frequent those days. Llama.cpp does work reliably, but only in &gt;!\\--split-mode layer!&lt;. For me, &gt;!\\--split-mode row!&lt; always produces gibberish - I've tried multiple ROCm versions, multiple builds, multiple models - all of them don't work with row split; but, I've seen at least one person claiming that &gt;!\\--split-mode row!&lt; works for him, so getting a multi-gpu speedup on llama.cpp is basically a coin flip. I feel like you would want to know those little details before investing your time and money into it.",
                                                    "edited": false,
                                                    "author_flair_css_class": null,
                                                    "name": "t1_n6fqjaf",
                                                    "is_submitter": false,
                                                    "downs": 0,
                                                    "author_flair_richtext": [],
                                                    "author_patreon_flair": false,
                                                    "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Basically correct, but there are some caveats. First, if you want to utilize ROCm (I bet you do), then AMD demands you to disable the iGPU in Ryzen processors; so you either go Intel or plan to install a discrete gpu, preferrably Nvidia, to avoid dealing with ROCm GPU management - it sucks. Also, for inference engines, you can only count to use llama.cpp. There is a vllm fork called &amp;quot;vllm-gfx906&amp;quot; for those cards, but it&amp;#39;s only usable for text-only models, which are less and less frequent those days. Llama.cpp does work reliably, but only in &lt;span class=\"md-spoiler-text\"&gt;--split-mode layer&lt;/span&gt;. For me, &lt;span class=\"md-spoiler-text\"&gt;--split-mode row&lt;/span&gt; always produces gibberish - I&amp;#39;ve tried multiple ROCm versions, multiple builds, multiple models - all of them don&amp;#39;t work with row split; but, I&amp;#39;ve seen at least one person claiming that &lt;span class=\"md-spoiler-text\"&gt;--split-mode row&lt;/span&gt; works for him, so getting a multi-gpu speedup on llama.cpp is basically a coin flip. I feel like you would want to know those little details before investing your time and money into it.&lt;/p&gt;\n&lt;/div&gt;",
                                                    "gildings": {},
                                                    "collapsed_reason": null,
                                                    "link_id": "t3_1mf1lfv",
                                                    "associated_award": null,
                                                    "stickied": false,
                                                    "author_premium": false,
                                                    "can_gild": false,
                                                    "top_awarded_type": null,
                                                    "unrepliable_reason": null,
                                                    "author_flair_text_color": null,
                                                    "treatment_tags": [],
                                                    "score_hidden": false,
                                                    "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6fqjaf/",
                                                    "subreddit_type": "public",
                                                    "locked": false,
                                                    "report_reasons": null,
                                                    "created": 1754089745,
                                                    "author_flair_text": null,
                                                    "collapsed": false,
                                                    "created_utc": 1754089745,
                                                    "subreddit_name_prefixed": "r/LocalLLaMA",
                                                    "controversiality": 0,
                                                    "depth": 4,
                                                    "author_flair_background_color": null,
                                                    "collapsed_because_crowd_control": null,
                                                    "mod_reports": [],
                                                    "num_reports": null,
                                                    "ups": 1
                                                  }
                                                }
                                              ],
                                              "before": null
                                            }
                                          },
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6faib9",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "MrCatberry",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6ena4m",
                                          "score": 1,
                                          "author_fullname": "t2_fe2ok1q3",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Will likely go threeway Mi50s, got a good bundle price, and 96GB VRAM sounds nice.\n\nSo, with only 100MB/s needed, i basically coule use a Miningboard as platform? Maybe even looking forward putting more of these cards in if i ever get a good deal again?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6faib9",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Will likely go threeway Mi50s, got a good bundle price, and 96GB VRAM sounds nice.&lt;/p&gt;\n\n&lt;p&gt;So, with only 100MB/s needed, i basically coule use a Miningboard as platform? Maybe even looking forward putting more of these cards in if i ever get a good deal again?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mf1lfv",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6faib9/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754084389,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754084389,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6ena4m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "No-Refrigerator-1672",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6dqmkz",
                                "score": 1,
                                "author_fullname": "t2_baavelp5",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "The cheapest Infinity Link for Mi50s that I could find is like 800 eur, and it's only compatible with quarduple card setup - no dual cards. So, basically, forget it, the link is more expensive that the cards. But, on the contrary, the PCIe speed does not matter - with dual Mi50 setup, llama.cpp goes only up to 100 MB/s during inference, so basically any PCIe type in existence can handle it no problem.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6ena4m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The cheapest Infinity Link for Mi50s that I could find is like 800 eur, and it&amp;#39;s only compatible with quarduple card setup - no dual cards. So, basically, forget it, the link is more expensive that the cards. But, on the contrary, the PCIe speed does not matter - with dual Mi50 setup, llama.cpp goes only up to 100 MB/s during inference, so basically any PCIe type in existence can handle it no problem.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mf1lfv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6ena4m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754077357,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754077357,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6dqmkz",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MrCatberry",
                      "can_mod_post": false,
                      "created_utc": 1754067907,
                      "send_replies": true,
                      "parent_id": "t1_n6dozwe",
                      "score": 1,
                      "author_fullname": "t2_fe2ok1q3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I was thinking about AMD Mi50s with 32GB.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6dqmkz",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I was thinking about AMD Mi50s with 32GB.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf1lfv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dqmkz/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754067907,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6dozwe",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ArtisticHamster",
            "can_mod_post": false,
            "created_utc": 1754067435,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 8,
            "author_fullname": "t2_2t2xbyfm",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "&gt;As far as i understand, just for interference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.\n\nThe problem is that the model might not fit into one GPU. Then, you get into splitting it across GPUs. As far as I know, the latest gen NVidia consumer/workstation cards have no NVLInk, so your only option is PCIe.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dozwe",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;As far as i understand, just for interference, with for example ollama, they are only really needed when the model is loaded into VRAM - after that everything is done on the card itself.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;The problem is that the model might not fit into one GPU. Then, you get into splitting it across GPUs. As far as I know, the latest gen NVidia consumer/workstation cards have no NVLInk, so your only option is PCIe.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dozwe/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754067435,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6dxm32",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754069862,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 7,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Depends on your application (which I'm assuming is inference):\n\n- single GPU with smaller models - load time suffers but nothing else matters\n- multi GPU pipeline parallelism (run at 1x speed with Nx VRAM) - basically the same as single GPU\n- multi GPU tensors parallelism (run at Nx speed with Nx VRAM) - important but PCIe4x8 should be fine.  x4 maybe too\n- single GPU + CPU for large models - actually very important since the GPU will/can be used for prompt processing which requires streaming to model to GPU for every batch and is usually limited by PCIe.\n\nTensor parallelism has a wrinkle where it's not just but bandwidth but also the latency.  Some benchmarks I've seen indicates the volume of data is in the ~5GBps magnitude which PCIe4x4 could support, however you also need to wait for that data to transfer before running the next operation which can have a moderate impact.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dxm32",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Depends on your application (which I&amp;#39;m assuming is inference):&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;single GPU with smaller models - load time suffers but nothing else matters&lt;/li&gt;\n&lt;li&gt;multi GPU pipeline parallelism (run at 1x speed with Nx VRAM) - basically the same as single GPU&lt;/li&gt;\n&lt;li&gt;multi GPU tensors parallelism (run at Nx speed with Nx VRAM) - important but PCIe4x8 should be fine.  x4 maybe too&lt;/li&gt;\n&lt;li&gt;single GPU + CPU for large models - actually very important since the GPU will/can be used for prompt processing which requires streaming to model to GPU for every batch and is usually limited by PCIe.&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;Tensor parallelism has a wrinkle where it&amp;#39;s not just but bandwidth but also the latency.  Some benchmarks I&amp;#39;ve seen indicates the volume of data is in the ~5GBps magnitude which PCIe4x4 could support, however you also need to wait for that data to transfer before running the next operation which can have a moderate impact.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dxm32/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754069862,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "d2642412-d9ce-11ed-ae30-32b11309f5bd",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6e4rzx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "ortegaalfredo",
            "can_mod_post": false,
            "created_utc": 1754071903,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 5,
            "author_fullname": "t2_g177e",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have an octominer system with 12 PCI 1.0 (yes, 1.0) and 1X. Just for laughs I tried to run Qwen3-235B AWQ on it using many 3090s.\n\nGot \\~25 tok/s using pipeline parallel and vllm. Tensor parallel get 4 or 5 tok/s. But PP almost don't get affected except in load times. Also I tried running GLM-4.5-air-FP8 on it. Got 25 tok/s. Upgraded to another machine with PCI 3.0 1X, same setup, same software and got 35 tok/s (Likely the bottleneck is the CPU). Currently this system it's doing batching inference on GLM with 100-130 tok/s in total.\n\nIf you look at the GPU transfers using nvtop during PP inference its about 10 Mb/s, and PCI 1.0 1X has about 250 Mb/s.",
            "edited": 1754072341,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6e4rzx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Alpaca"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have an octominer system with 12 PCI 1.0 (yes, 1.0) and 1X. Just for laughs I tried to run Qwen3-235B AWQ on it using many 3090s.&lt;/p&gt;\n\n&lt;p&gt;Got ~25 tok/s using pipeline parallel and vllm. Tensor parallel get 4 or 5 tok/s. But PP almost don&amp;#39;t get affected except in load times. Also I tried running GLM-4.5-air-FP8 on it. Got 25 tok/s. Upgraded to another machine with PCI 3.0 1X, same setup, same software and got 35 tok/s (Likely the bottleneck is the CPU). Currently this system it&amp;#39;s doing batching inference on GLM with 100-130 tok/s in total.&lt;/p&gt;\n\n&lt;p&gt;If you look at the GPU transfers using nvtop during PP inference its about 10 Mb/s, and PCI 1.0 1X has about 250 Mb/s.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6e4rzx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754071903,
            "author_flair_text": "Alpaca",
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bd9e9e",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 5
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6dwjvx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lemondrops9",
            "can_mod_post": false,
            "created_utc": 1754069565,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 3,
            "author_fullname": "t2_6kroii95",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a 3060 ti in the 16x slot and 3090 in a 4x slot. I do this because of heat issues where the cards sit physically. I did some tests last week with the 3090 in the 16x and the 3060 ti in the 4x and it was maybe 5%...10% at most faster from some quick tests.\n\nI should note they are PCIe 4.0.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dwjvx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a 3060 ti in the 16x slot and 3090 in a 4x slot. I do this because of heat issues where the cards sit physically. I did some tests last week with the 3090 in the 16x and the 3060 ti in the 4x and it was maybe 5%...10% at most faster from some quick tests.&lt;/p&gt;\n\n&lt;p&gt;I should note they are PCIe 4.0.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dwjvx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754069565,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6eazv3",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Willing_Landscape_61",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6dqgls",
                                "score": 2,
                                "author_fullname": "t2_8lvrytgw",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Does matter much then. Only loading time should be affected with pipeline parallelism.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6eazv3",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does matter much then. Only loading time should be affected with pipeline parallelism.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mf1lfv",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6eazv3/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754073716,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754073716,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6dqgls",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MrCatberry",
                      "can_mod_post": false,
                      "created_utc": 1754067859,
                      "send_replies": true,
                      "parent_id": "t1_n6dorpt",
                      "score": 1,
                      "author_fullname": "t2_fe2ok1q3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Inference",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6dqgls",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Inference&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf1lfv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dqgls/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754067859,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6dorpt",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Willing_Landscape_61",
            "can_mod_post": false,
            "created_utc": 1754067369,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 1,
            "author_fullname": "t2_8lvrytgw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Inference or training?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dorpt",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Inference or training?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dorpt/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754067369,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6dqs80",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MrCatberry",
                      "can_mod_post": false,
                      "created_utc": 1754067953,
                      "send_replies": true,
                      "parent_id": "t1_n6dpbvi",
                      "score": 1,
                      "author_fullname": "t2_fe2ok1q3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Is this still a problem with AMD Mi50s? Those definitely not able to use tensor parallelism.\n\nEdit: Sorry, was talking bs, was thinking to much about tensor cores...",
                      "edited": 1754068738,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6dqs80",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Is this still a problem with AMD Mi50s? Those definitely not able to use tensor parallelism.&lt;/p&gt;\n\n&lt;p&gt;Edit: Sorry, was talking bs, was thinking to much about tensor cores...&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf1lfv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dqs80/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754067953,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6dpbvi",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Lissanro",
            "can_mod_post": false,
            "created_utc": 1754067531,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 1,
            "author_fullname": "t2_fpfao9g",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "For inference, PCI-E lanes matter a lot if using tensor parallelism. Then having at least x8 PCI-E 4.0 or x16 PCI-E 3.0 is highly recommended.\n\nOtherwise, you can use low bandwidth connection, you will experience slower loading and there still will be some performance loss due to data transfer delays but within 5%-20% range, not too catastrophic for inference without tensor parallelism.\n\nOn my previous rig, I had cards connected using x8 x8 x4 x1 PCI-E 3.0, and I cannot recommend it, but if you low on funds, it is an option. Since I migrated to x16 x16 x16 x16 PCI-E 4.0, even when not using tensor parallelism, things work so much better - I get full performance out of my cards (3090) and loading times are fast.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6dpbvi",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For inference, PCI-E lanes matter a lot if using tensor parallelism. Then having at least x8 PCI-E 4.0 or x16 PCI-E 3.0 is highly recommended.&lt;/p&gt;\n\n&lt;p&gt;Otherwise, you can use low bandwidth connection, you will experience slower loading and there still will be some performance loss due to data transfer delays but within 5%-20% range, not too catastrophic for inference without tensor parallelism.&lt;/p&gt;\n\n&lt;p&gt;On my previous rig, I had cards connected using x8 x8 x4 x1 PCI-E 3.0, and I cannot recommend it, but if you low on funds, it is an option. Since I migrated to x16 x16 x16 x16 PCI-E 4.0, even when not using tensor parallelism, things work so much better - I get full performance out of my cards (3090) and loading times are fast.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dpbvi/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754067531,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6fbbez",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MrCatberry",
                      "can_mod_post": false,
                      "created_utc": 1754084648,
                      "send_replies": true,
                      "parent_id": "t1_n6fa9d9",
                      "score": 1,
                      "author_fullname": "t2_fe2ok1q3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I think, currently, there is no consense about that.\nIt seems like dependend on which parallism is used.\nSome say x1 is fine, some say at least x8 it needed but i guess this really depends on whats the use case.\n\nMy reseach showed that with 1 user and only 1 task it shouldnt really matter.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6fbbez",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think, currently, there is no consense about that.\nIt seems like dependend on which parallism is used.\nSome say x1 is fine, some say at least x8 it needed but i guess this really depends on whats the use case.&lt;/p&gt;\n\n&lt;p&gt;My reseach showed that with 1 user and only 1 task it shouldnt really matter.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf1lfv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6fbbez/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754084648,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6fa9d9",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "JellyfishAutomatic25",
            "can_mod_post": false,
            "created_utc": 1754084310,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 1,
            "author_fullname": "t2_clyuifd5",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You all had me and then lost me.  Lol\n\n\nI have two slots.  PCIe 16 and PCIe 4\n\nAre you basically saying that using the 16 vs the 4 doesn't matter in some applications?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6fa9d9",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You all had me and then lost me.  Lol&lt;/p&gt;\n\n&lt;p&gt;I have two slots.  PCIe 16 and PCIe 4&lt;/p&gt;\n\n&lt;p&gt;Are you basically saying that using the 16 vs the 4 doesn&amp;#39;t matter in some applications?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6fa9d9/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754084310,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6fuy2w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Outpost_Underground",
            "can_mod_post": false,
            "created_utc": 1754091261,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 1,
            "author_fullname": "t2_op6if04r",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Not super scientific, but we did a build using 4 GPUs on a mining mobo with 1x PCIe risers. The video is geared to non-tech savvy so we used Ollama and Open-WebUI, but with 27b q4 K_M models and ~24k context we got decent results. \n\nhttps://youtu.be/J1e3XQxRUa0?si=Dzr7BcQpjRmi57ab",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6fuy2w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not super scientific, but we did a build using 4 GPUs on a mining mobo with 1x PCIe risers. The video is geared to non-tech savvy so we used Ollama and Open-WebUI, but with 27b q4 K_M models and ~24k context we got decent results. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://youtu.be/J1e3XQxRUa0?si=Dzr7BcQpjRmi57ab\"&gt;https://youtu.be/J1e3XQxRUa0?si=Dzr7BcQpjRmi57ab&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6fuy2w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754091261,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n6dtoe3",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MrCatberry",
                      "can_mod_post": false,
                      "created_utc": 1754068766,
                      "send_replies": true,
                      "parent_id": "t1_n6ds3y5",
                      "score": 1,
                      "author_fullname": "t2_fe2ok1q3",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "But only with single card i guess, what about multiple cards?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6dtoe3",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;But only with single card i guess, what about multiple cards?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mf1lfv",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6dtoe3/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754068766,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6ds3y5",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Hornet_1227",
            "can_mod_post": false,
            "created_utc": 1754068328,
            "send_replies": true,
            "parent_id": "t3_1mf1lfv",
            "score": 0,
            "author_fullname": "t2_qmyu63x3f",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hmmm for RTX5090 according to tests you can lose 25% in content creation workloads but doesnt affect LLMs.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6ds3y5",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hmmm for RTX5090 according to tests you can lose 25% in content creation workloads but doesnt affect LLMs.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mf1lfv/how_much_do_pcie_lanes_matter/n6ds3y5/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754068328,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mf1lfv",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]