[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I'm working on a robot that uses a server-based LLM for voice conversations, but I'm planning to add an on-device LLM as a fallback when there's no internet connection.\n\nHere are the current specs:\n\n* **CPU**: Cortex-A53 x 4 @ 1.8GHz\n* **RAM**: 8GB LPDDR4\n* **OS**: Android (AOSP-based)\n\nI've asked models like ChatGPT and Gemini, and got mixed answers. Some say it's possible to run a 4-bit quantized model on a Cortex-A53, while others say it's not feasible.\n\nAlso, when it comes to natural voice interaction, some say **5 tokens per second (TPS)** is enough, while others insist you need at least **30 TPS** for smooth conversations. I'm a bit confused.\n\nFor lightweight, auxiliary voice interactions, what TPS rate would be considered sufficient? And what kind of hardware specs would realistically support that?",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Confused About TPS Needs for On-Device LLM: 5 vs 30 TPS for Voice?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mi1wwr",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 1,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 3,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1v0amj15h7",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 3,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754375967,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m working on a robot that uses a server-based LLM for voice conversations, but I&amp;#39;m planning to add an on-device LLM as a fallback when there&amp;#39;s no internet connection.&lt;/p&gt;\n\n&lt;p&gt;Here are the current specs:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Cortex-A53 x 4 @ 1.8GHz&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 8GB LPDDR4&lt;/li&gt;\n&lt;li&gt;&lt;strong&gt;OS&lt;/strong&gt;: Android (AOSP-based)&lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;I&amp;#39;ve asked models like ChatGPT and Gemini, and got mixed answers. Some say it&amp;#39;s possible to run a 4-bit quantized model on a Cortex-A53, while others say it&amp;#39;s not feasible.&lt;/p&gt;\n\n&lt;p&gt;Also, when it comes to natural voice interaction, some say &lt;strong&gt;5 tokens per second (TPS)&lt;/strong&gt; is enough, while others insist you need at least &lt;strong&gt;30 TPS&lt;/strong&gt; for smooth conversations. I&amp;#39;m a bit confused.&lt;/p&gt;\n\n&lt;p&gt;For lightweight, auxiliary voice interactions, what TPS rate would be considered sufficient? And what kind of hardware specs would realistically support that?&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mi1wwr",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Public_Paint8683",
            "discussion_type": null,
            "num_comments": 4,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/",
            "subreddit_subscribers": 510540,
            "created_utc": 1754375967,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "2b12e2b8-fdc0-11ee-9a03-6e2f48afd456",
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n70jlu7",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "sharp1120",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n70gz4g",
                                "score": 1,
                                "author_fullname": "t2_7rhidfsb",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "5TPS may be acceptable for slower spoken speech, but it's definitely not ideal.\n\nFrom a precursory google search, it seems that with optimization, you could maybe hope for ~3 tokens per second on that hardware with a tiny 0.5B model.\n\nText to speech/speech to text on device would also require processing power, further decreasing the odds of such a setup having acceptable latency.\n\nEdit: For reference, something like the raspberry pi 5 has roughly 6 times the multi-threaded performance and should be able to go up to at least a ~3B model at ~5 tokens per second(or something smaller faster).",
                                "edited": 1754379980,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n70jlu7",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;5TPS may be acceptable for slower spoken speech, but it&amp;#39;s definitely not ideal.&lt;/p&gt;\n\n&lt;p&gt;From a precursory google search, it seems that with optimization, you could maybe hope for ~3 tokens per second on that hardware with a tiny 0.5B model.&lt;/p&gt;\n\n&lt;p&gt;Text to speech/speech to text on device would also require processing power, further decreasing the odds of such a setup having acceptable latency.&lt;/p&gt;\n\n&lt;p&gt;Edit: For reference, something like the raspberry pi 5 has roughly 6 times the multi-threaded performance and should be able to go up to at least a ~3B model at ~5 tokens per second(or something smaller faster).&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mi1wwr",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/n70jlu7/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754379254,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754379254,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n710n7m",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Aaaaaaaaaeeeee",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n70gz4g",
                                "score": 1,
                                "author_fullname": "t2_el5pibmej",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I say 5TPS, here's a demo for you at the end of the post: https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/ \nThis didn't record the voice part But a transcription occurs as soon as I finish speaking: I did it with earphones to avoid interruption. I also don't really want to publish my voice lol. \n\n\nHow slow you can go depends on the speed of the voice set, and needs the engine to break the stream into several sentences to sent to a lightweight TTS. ",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n710n7m",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I say 5TPS, here&amp;#39;s a demo for you at the end of the post: &lt;a href=\"https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/%C2%A0\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1jk64d7/installation_commands_for_whispercpps_talkllama/ &lt;/a&gt;\nThis didn&amp;#39;t record the voice part But a transcription occurs as soon as I finish speaking: I did it with earphones to avoid interruption. I also don&amp;#39;t really want to publish my voice lol. &lt;/p&gt;\n\n&lt;p&gt;How slow you can go depends on the speed of the voice set, and needs the engine to break the stream into several sentences to sent to a lightweight TTS. &lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mi1wwr",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/n710n7m/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754389008,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754389008,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n70gz4g",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Public_Paint8683",
                      "can_mod_post": false,
                      "created_utc": 1754377759,
                      "send_replies": true,
                      "parent_id": "t1_n70fokk",
                      "score": 1,
                      "author_fullname": "t2_1v0amj15h7",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the helpful responses. The robot is a toy-like assistant that students can carry around and ask questions to. It normally uses a server-based LLM to generate answers.\n\nHowever, since internet connectivity can be unreliable while moving around, I'm considering adding an on-device LLM for lightweight fallback conversations.\n\nI'm not sure how fast 5TPS really feels in practice, but if it doesn't cause noticeable delays in casual dialogue, it might be acceptable given the current hardware specs.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n70gz4g",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the helpful responses. The robot is a toy-like assistant that students can carry around and ask questions to. It normally uses a server-based LLM to generate answers.&lt;/p&gt;\n\n&lt;p&gt;However, since internet connectivity can be unreliable while moving around, I&amp;#39;m considering adding an on-device LLM for lightweight fallback conversations.&lt;/p&gt;\n\n&lt;p&gt;I&amp;#39;m not sure how fast 5TPS really feels in practice, but if it doesn&amp;#39;t cause noticeable delays in casual dialogue, it might be acceptable given the current hardware specs.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mi1wwr",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/n70gz4g/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754377759,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n70fokk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Former-Ad-5757",
            "can_mod_post": false,
            "created_utc": 1754377031,
            "send_replies": true,
            "parent_id": "t3_1mi1wwr",
            "score": 1,
            "author_fullname": "t2_ihsdiwk6k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "It all depends on what you find acceptable. For normal usage I would say you need 30 TPS, which makes it real problematic to run on 4 Cortex-a53's. \n\nBut if it only is for fallback when there's no internet connection then I don't see why 5 TPS would not be acceptable. I would imagine your robot being very limited without internet connection, simply because it can't reach up-to-date info anymore. Maybe this is reachable with 4 Cortex-a53's\n\nP.s. I am thinking from the perspective of an industrial robot, where it for 99% of the time needs a good internet connection to reach a database and other systems. From this perspective a robot without internet basically has to say that it has no internet and it needs to try and move to an area where there is internet again.  \nFor a toy-robot it may be different.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n70fokk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "Llama 3"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;It all depends on what you find acceptable. For normal usage I would say you need 30 TPS, which makes it real problematic to run on 4 Cortex-a53&amp;#39;s. &lt;/p&gt;\n\n&lt;p&gt;But if it only is for fallback when there&amp;#39;s no internet connection then I don&amp;#39;t see why 5 TPS would not be acceptable. I would imagine your robot being very limited without internet connection, simply because it can&amp;#39;t reach up-to-date info anymore. Maybe this is reachable with 4 Cortex-a53&amp;#39;s&lt;/p&gt;\n\n&lt;p&gt;P.s. I am thinking from the perspective of an industrial robot, where it for 99% of the time needs a good internet connection to reach a database and other systems. From this perspective a robot without internet basically has to say that it has no internet and it needs to try and move to an area where there is internet again.&lt;br/&gt;\nFor a toy-robot it may be different.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mi1wwr/confused_about_tps_needs_for_ondevice_llm_5_vs_30/n70fokk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754377031,
            "author_flair_text": "Llama 3",
            "treatment_tags": [],
            "link_id": "t3_1mi1wwr",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#c7b594",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]