[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "I‚Äôm here to warn everybody that Docker Model Runnner is the friend she told you not to worry about who is sneaking in the back door and about to steal your girl‚Äôs inference (sorry, that sounds way dirtier than I meant it to). \n\nReal talk tho, Ollama seems to have kind of fell off the last month or so. They haven‚Äôt dropped a new ‚Äúofficial‚Äù model release since Mistral Small 3.2, Sure, you can pull a lot of huggingface models direct now, but dang nobody wants to mess with those long ass model names, right? \n\nI don‚Äôt feel like Ollama have been incorporating the latest llama.cpp updates as fast as they used to. It used to be like a new llama.cpp would drop, and then new Ollama update would come out like one day later, hasn‚Äôt seemed like that lately though. The whole vibe over on r/Ollama seems a little off right now TBH.\n\nDocker Model Runner just kinda showed up inside Docker Desktop a little while ago as an experimental feature, now it‚Äôs taken its shoes off and made itself at home as part of both Docker Desktop and Docker Engine. \n\nWhile we all were busy oohing and ahhhing over all these new models, Docker Model runner:\n\n- Was added to Hugging Face under pretty much every GGUF‚Äôs ‚ÄúUse this model‚Äù dropdown list with easy copy/paste access making it dead simple to pull and run ANY GGUF model.\n- Started developing its own Docker AI Model Hub which reduces any friction that may have existed for pulling and running a model. \n- Added an MCP Server and hub to the mix as well. \n\nThis was a pretty bold move on Docker‚Äôs part. They just added inference as a feature to the product a lot of us were already using to serve AI container apps. \n\nNow, I‚Äôm not sure how good the model swapping capabilities are yet because I haven‚Äôt done a ton of testing, but they are there as features and from what I understand, the whole thing is highly-configurable if you need that kind of thing and don‚Äôt mind building Docker Compose or YAML files or whatever. \n\nI‚Äôm assuming that since it‚Äôs llama.cop based that it‚Äôll incorporate llama.cpp updates fairly quickly, but you never know. \n\nAre any of y‚Äôall using Docker Model Runner? Do you like it better or worse than Ollama or LM Studio, or even plain ole Llama.cop? \n\nHere‚Äôs their doc site if anyone wants to read up on it:\n\nhttps://docs.docker.com/ai/model-runner/",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Docker Model Runner is going to steal your girl‚Äôs inference.",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Other"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mcnhtc",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.25,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_y35oj",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Other",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753823012,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I‚Äôm here to warn everybody that Docker Model Runnner is the friend she told you not to worry about who is sneaking in the back door and about to steal your girl‚Äôs inference (sorry, that sounds way dirtier than I meant it to). &lt;/p&gt;\n\n&lt;p&gt;Real talk tho, Ollama seems to have kind of fell off the last month or so. They haven‚Äôt dropped a new ‚Äúofficial‚Äù model release since Mistral Small 3.2, Sure, you can pull a lot of huggingface models direct now, but dang nobody wants to mess with those long ass model names, right? &lt;/p&gt;\n\n&lt;p&gt;I don‚Äôt feel like Ollama have been incorporating the latest llama.cpp updates as fast as they used to. It used to be like a new llama.cpp would drop, and then new Ollama update would come out like one day later, hasn‚Äôt seemed like that lately though. The whole vibe over on &lt;a href=\"/r/Ollama\"&gt;r/Ollama&lt;/a&gt; seems a little off right now TBH.&lt;/p&gt;\n\n&lt;p&gt;Docker Model Runner just kinda showed up inside Docker Desktop a little while ago as an experimental feature, now it‚Äôs taken its shoes off and made itself at home as part of both Docker Desktop and Docker Engine. &lt;/p&gt;\n\n&lt;p&gt;While we all were busy oohing and ahhhing over all these new models, Docker Model runner:&lt;/p&gt;\n\n&lt;ul&gt;\n&lt;li&gt;Was added to Hugging Face under pretty much every GGUF‚Äôs ‚ÄúUse this model‚Äù dropdown list with easy copy/paste access making it dead simple to pull and run ANY GGUF model.&lt;/li&gt;\n&lt;li&gt;Started developing its own Docker AI Model Hub which reduces any friction that may have existed for pulling and running a model. &lt;/li&gt;\n&lt;li&gt;Added an MCP Server and hub to the mix as well. &lt;/li&gt;\n&lt;/ul&gt;\n\n&lt;p&gt;This was a pretty bold move on Docker‚Äôs part. They just added inference as a feature to the product a lot of us were already using to serve AI container apps. &lt;/p&gt;\n\n&lt;p&gt;Now, I‚Äôm not sure how good the model swapping capabilities are yet because I haven‚Äôt done a ton of testing, but they are there as features and from what I understand, the whole thing is highly-configurable if you need that kind of thing and don‚Äôt mind building Docker Compose or YAML files or whatever. &lt;/p&gt;\n\n&lt;p&gt;I‚Äôm assuming that since it‚Äôs llama.cop based that it‚Äôll incorporate llama.cpp updates fairly quickly, but you never know. &lt;/p&gt;\n\n&lt;p&gt;Are any of y‚Äôall using Docker Model Runner? Do you like it better or worse than Ollama or LM Studio, or even plain ole Llama.cop? &lt;/p&gt;\n\n&lt;p&gt;Here‚Äôs their doc site if anyone wants to read up on it:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.docker.com/ai/model-runner/\"&gt;https://docs.docker.com/ai/model-runner/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "7a7848d2-bf8e-11ed-8c2f-765d15199f78",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#94e044",
            "id": "1mcnhtc",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Porespellar",
            "discussion_type": null,
            "num_comments": 9,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/",
            "subreddit_subscribers": 506711,
            "created_utc": 1753823012,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5vtnt1",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Porespellar",
                      "can_mod_post": false,
                      "created_utc": 1753830763,
                      "send_replies": true,
                      "parent_id": "t1_n5vawn7",
                      "score": 1,
                      "author_fullname": "t2_y35oj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Bruh, I just like the project, but I‚Äôll try and curb my enthusiasm LOL. üòÇ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5vtnt1",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Bruh, I just like the project, but I‚Äôll try and curb my enthusiasm LOL. üòÇ&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mcnhtc",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5vtnt1/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753830763,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5vawn7",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "SpacemanCraig3",
            "can_mod_post": false,
            "created_utc": 1753824876,
            "send_replies": true,
            "parent_id": "t3_1mcnhtc",
            "score": 9,
            "author_fullname": "t2_13jvln",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "\"Hey chatgpt, hype up this project for a reddit post\"",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5vawn7",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&amp;quot;Hey chatgpt, hype up this project for a reddit post&amp;quot;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5vawn7/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753824876,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcnhtc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 9
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5vczqq",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "disillusioned_okapi",
            "can_mod_post": false,
            "created_utc": 1753825500,
            "send_replies": true,
            "parent_id": "t3_1mcnhtc",
            "score": 3,
            "author_fullname": "t2_wy3w8",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "quite a lot of LLM software today is built by very smart people who luckily haven't spent time in the complex and treacherous world of infosec, and as such haven't given security much thought. MCP's default recommendation of running arbitrary binaries off the internet is a good example of that.¬†\n\n\nirrespective of how any of us feel about Docker, they are still one of the larger players in the secure sandboxing business.¬†¬†\nIf LLMs are to succeed, security needs to improve significantly. and I'd prefer someone like Docker (or CNCF or LF) leading that, instead of any of the VM and Anti-Virus companies.\n\nIdeally the community would lead on that, but that just doesn't seem to be happening so far.¬†\n\n\nSo, as long this is good enough as Olama, I wish them success.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5vczqq",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;quite a lot of LLM software today is built by very smart people who luckily haven&amp;#39;t spent time in the complex and treacherous world of infosec, and as such haven&amp;#39;t given security much thought. MCP&amp;#39;s default recommendation of running arbitrary binaries off the internet is a good example of that.¬†&lt;/p&gt;\n\n&lt;p&gt;irrespective of how any of us feel about Docker, they are still one of the larger players in the secure sandboxing business.¬†¬†\nIf LLMs are to succeed, security needs to improve significantly. and I&amp;#39;d prefer someone like Docker (or CNCF or LF) leading that, instead of any of the VM and Anti-Virus companies.&lt;/p&gt;\n\n&lt;p&gt;Ideally the community would lead on that, but that just doesn&amp;#39;t seem to be happening so far.¬†&lt;/p&gt;\n\n&lt;p&gt;So, as long this is good enough as Olama, I wish them success.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5vczqq/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753825500,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcnhtc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5v5l8o",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "o5mfiHTNsH748KVq",
            "can_mod_post": false,
            "created_utc": 1753823312,
            "send_replies": true,
            "parent_id": "t3_1mcnhtc",
            "score": 2,
            "author_fullname": "t2_e11zi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "The fact that it‚Äôs configurable from docker compose is actually huge\n\nhttps://docs.docker.com/ai/compose/models-and-compose/",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5v5l8o",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The fact that it‚Äôs configurable from docker compose is actually huge&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.docker.com/ai/compose/models-and-compose/\"&gt;https://docs.docker.com/ai/compose/models-and-compose/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": true,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5v5l8o/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753823312,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcnhtc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5vdmte",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "ShengrenR",
            "can_mod_post": false,
            "created_utc": 1753825694,
            "send_replies": true,
            "parent_id": "t3_1mcnhtc",
            "score": 3,
            "author_fullname": "t2_ji4n4",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ollama is like the ultimate \"I'm new to computers\" backend.. and you want to go to... checks notes.. docker? why not go all in and get them set up with Kserve and some kubernetes+vllm, that'll teach 'em",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5vdmte",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ollama is like the ultimate &amp;quot;I&amp;#39;m new to computers&amp;quot; backend.. and you want to go to... checks notes.. docker? why not go all in and get them set up with Kserve and some kubernetes+vllm, that&amp;#39;ll teach &amp;#39;em&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5vdmte/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753825694,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcnhtc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5v7cx3",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "StupidityCanFly",
            "can_mod_post": false,
            "created_utc": 1753823823,
            "send_replies": true,
            "parent_id": "t3_1mcnhtc",
            "score": 2,
            "author_fullname": "t2_sydekv9b",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Doesn‚Äôt support ROCm, unfortunately.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5v7cx3",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Doesn‚Äôt support ROCm, unfortunately.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5v7cx3/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753823823,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcnhtc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5vtsv9",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Careless-Car_",
                      "can_mod_post": false,
                      "created_utc": 1753830807,
                      "send_replies": true,
                      "parent_id": "t1_n5v7i0c",
                      "score": 2,
                      "author_fullname": "t2_2eugqr60",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Because vLLM is great for prod, but not every developer has access to the enterprise grade GPUs required for vLLM?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5vtsv9",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because vLLM is great for prod, but not every developer has access to the enterprise grade GPUs required for vLLM?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mcnhtc",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5vtsv9/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753830807,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5vu9ju",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Porespellar",
                      "can_mod_post": false,
                      "created_utc": 1753830957,
                      "send_replies": true,
                      "parent_id": "t1_n5v7i0c",
                      "score": 2,
                      "author_fullname": "t2_y35oj",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "vLLM can be finicky and a pain in the butt especially for non dev types.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5vu9ju",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;vLLM can be finicky and a pain in the butt especially for non dev types.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mcnhtc",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5vu9ju/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753830957,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5v7i0c",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Candid_Payment_4094",
            "can_mod_post": false,
            "created_utc": 1753823864,
            "send_replies": true,
            "parent_id": "t3_1mcnhtc",
            "score": 1,
            "author_fullname": "t2_anbsqj0od",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You're overhyping it. I really don't get the appeal of Ollama, and now this. What is so difficult about running a local vLLM server in a docker container? Even for development? No one is going to steal anything. People that need to run their models beyond a single user are not going to bother with this. And while you're developing and testing use cases, why not already stick with something that can go to prod?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5v7i0c",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You&amp;#39;re overhyping it. I really don&amp;#39;t get the appeal of Ollama, and now this. What is so difficult about running a local vLLM server in a docker container? Even for development? No one is going to steal anything. People that need to run their models beyond a single user are not going to bother with this. And while you&amp;#39;re developing and testing use cases, why not already stick with something that can go to prod?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mcnhtc/docker_model_runner_is_going_to_steal_your_girls/n5v7i0c/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753823864,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mcnhtc",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]