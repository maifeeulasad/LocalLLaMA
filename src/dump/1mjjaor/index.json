[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "What a ride! Been a big 24h. Now that the dust has barely settled, I just wanted some clarification (and I'm sure there are many of us) around which of the major GPT-OSS releases should we be using for best quality-performance? (rather than speed)\n\nThere's llama.cpp native support: [https://github.com/ggml-org/llama.cpp/discussions/15095](https://github.com/ggml-org/llama.cpp/discussions/15095)  \nI presume this means I can just run the native models dropped by OpenAI on hugging face here: [https://huggingface.co/openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) \n\nBut then there is GGML: [https://github.com/ggml-org/llama.cpp/pull/15091](https://github.com/ggml-org/llama.cpp/pull/15091)  \nWith the models here: [https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf](https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf)\n\nAnd there's Unsloth: [https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune](https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune)  \nTheir models are gguf: [https://huggingface.co/unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)  \nThey mention chat template fixes have have different quants.\n\nIs the right combo the OpenAI quants with the Unsloth chat template fixes? (I'm using LMStudio on a 128 M4 Max for what that's worth).\n\nAlso, shoutout to everyone involved to the organisations involved above, woking your absolute asses off at the moment.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Where are we at running the GPT-OSS models locally?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjjaor",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.56,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 1,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1gzoposi1r",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 1,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754520512,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What a ride! Been a big 24h. Now that the dust has barely settled, I just wanted some clarification (and I&amp;#39;m sure there are many of us) around which of the major GPT-OSS releases should we be using for best quality-performance? (rather than speed)&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s llama.cpp native support: &lt;a href=\"https://github.com/ggml-org/llama.cpp/discussions/15095\"&gt;https://github.com/ggml-org/llama.cpp/discussions/15095&lt;/a&gt;&lt;br/&gt;\nI presume this means I can just run the native models dropped by OpenAI on hugging face here: &lt;a href=\"https://huggingface.co/openai/gpt-oss-120b\"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;But then there is GGML: &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15091\"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;br/&gt;\nWith the models here: &lt;a href=\"https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf\"&gt;https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And there&amp;#39;s Unsloth: &lt;a href=\"https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune\"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune&lt;/a&gt;&lt;br/&gt;\nTheir models are gguf: &lt;a href=\"https://huggingface.co/unsloth/gpt-oss-20b-GGUF\"&gt;https://huggingface.co/unsloth/gpt-oss-20b-GGUF&lt;/a&gt;&lt;br/&gt;\nThey mention chat template fixes have have different quants.&lt;/p&gt;\n\n&lt;p&gt;Is the right combo the OpenAI quants with the Unsloth chat template fixes? (I&amp;#39;m using LMStudio on a 128 M4 Max for what that&amp;#39;s worth).&lt;/p&gt;\n\n&lt;p&gt;Also, shoutout to everyone involved to the organisations involved above, woking your absolute asses off at the moment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?auto=webp&amp;s=021ac90e342e7ce24176d8fc1d8f982df536ec3a",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b2771ee5257111e4de088311cb5195ef52c7b24",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=546ac16b2e0ddee9d735b54e7252ea7704f0aa25",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d3b1de829365122de157087d1354ddc7ce0a097",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ab49a1bcbed40a5d13899b9b4cca4f76dd3f536",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=26c58e1a03de3b0334cae1d33f50e3bfa61973c5",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c58046231d9a33ac45a28213537010cf1e217fd",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjjaor",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Suspicious_Young8152",
            "discussion_type": null,
            "num_comments": 12,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/",
            "subreddit_subscribers": 512425,
            "created_utc": 1754520512,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bqdry",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "BumbleSlob",
                      "can_mod_post": false,
                      "created_utc": 1754523696,
                      "send_replies": true,
                      "parent_id": "t1_n7blb2w",
                      "score": 1,
                      "author_fullname": "t2_1j7fhlcqkp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Seems to be that because of MXFP4 native hardware support, this model is the first one which should make the 50x series of cards worthwhile for inference (by differentiating from 40x gens not supporting it)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bqdry",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Seems to be that because of MXFP4 native hardware support, this model is the first one which should make the 50x series of cards worthwhile for inference (by differentiating from 40x gens not supporting it)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bqdry/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754523696,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bsxrv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "TitwitMuffbiscuit",
                      "can_mod_post": false,
                      "created_utc": 1754524543,
                      "send_replies": true,
                      "parent_id": "t1_n7blb2w",
                      "score": 1,
                      "author_fullname": "t2_liojy",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "llama.cpp, windows, UD-Q4 K XL from unsloth (which is pretty much the same as the other quants when you see the file sizes)\n\n12 t/s on a single 3060 12 gb and 64 GB of DDR4 3200 and a 12100F with 16k context.\n\n`llama-server.exe --no-mmap -t 7 -ngl 37 -c 16384 -n -1 -fa --jinja --reasoning-format none --temp 1.0 --top-k 0 --top-p 1.0 --min-p 0 --jinja -m gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf --n-cpu-moe 31`\n\nI haven't tried ik\\_llama.cpp or transformers yet.\n\nThis shit box has a purpose now. I don't care about the hate train, it's smart for it's size.",
                      "edited": 1754526082,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bsxrv",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llama.cpp, windows, UD-Q4 K XL from unsloth (which is pretty much the same as the other quants when you see the file sizes)&lt;/p&gt;\n\n&lt;p&gt;12 t/s on a single 3060 12 gb and 64 GB of DDR4 3200 and a 12100F with 16k context.&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;llama-server.exe --no-mmap -t 7 -ngl 37 -c 16384 -n -1 -fa --jinja --reasoning-format none --temp 1.0 --top-k 0 --top-p 1.0 --min-p 0 --jinja -m gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf --n-cpu-moe 31&lt;/code&gt;&lt;/p&gt;\n\n&lt;p&gt;I haven&amp;#39;t tried ik_llama.cpp or transformers yet.&lt;/p&gt;\n\n&lt;p&gt;This shit box has a purpose now. I don&amp;#39;t care about the hate train, it&amp;#39;s smart for it&amp;#39;s size.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bsxrv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754524543,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7blb2w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Wrong-Historian",
            "can_mod_post": false,
            "created_utc": 1754522042,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 4,
            "author_fullname": "t2_69r67vj3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "llama-cpp, OpenAI gguff 120B\n\n25T/s on a single 3090 and 96GB DDR5 6800 and 14900K\n\nPretty awesome",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7blb2w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llama-cpp, OpenAI gguff 120B&lt;/p&gt;\n\n&lt;p&gt;25T/s on a single 3090 and 96GB DDR5 6800 and 14900K&lt;/p&gt;\n\n&lt;p&gt;Pretty awesome&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7blb2w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754522042,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7bjoe0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1754521507,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 3,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm running Unsloth their 120b at 20 t/s on my budget workstation (16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000). 20 t/s, that is for the first 1K tokens, it slows down to 13 t/s at 30K context.  \nThat's 80% faster than GLM Air, I'm waiting for a new Qwen 32b coder.",
            "edited": 1754522228,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bjoe0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running Unsloth their 120b at 20 t/s on my budget workstation (16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000). 20 t/s, that is for the first 1K tokens, it slows down to 13 t/s at 30K context.&lt;br/&gt;\nThat&amp;#39;s 80% faster than GLM Air, I&amp;#39;m waiting for a new Qwen 32b coder.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bjoe0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754521507,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bnqxq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Suspicious_Young8152",
                      "can_mod_post": false,
                      "created_utc": 1754522830,
                      "send_replies": true,
                      "parent_id": "t1_n7bjcy1",
                      "score": 1,
                      "author_fullname": "t2_1gzoposi1r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ha, no, but the last commit was 15 minutes ago, dust certainly hasn't fully settled yet!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bnqxq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ha, no, but the last commit was 15 minutes ago, dust certainly hasn&amp;#39;t fully settled yet!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bnqxq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754522830,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7bjcy1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lakySK",
            "can_mod_post": false,
            "created_utc": 1754521398,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 1,
            "author_fullname": "t2_y9y2q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've just used the OpenAI gguf and it seemed to work well. Haven't played with the template. Do you know what exactly Unsloth changed?\n\nEdit: Is this related? [https://github.com/ggml-org/llama.cpp/issues/15110](https://github.com/ggml-org/llama.cpp/issues/15110)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bjcy1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just used the OpenAI gguf and it seemed to work well. Haven&amp;#39;t played with the template. Do you know what exactly Unsloth changed?&lt;/p&gt;\n\n&lt;p&gt;Edit: Is this related? &lt;a href=\"https://github.com/ggml-org/llama.cpp/issues/15110\"&gt;https://github.com/ggml-org/llama.cpp/issues/15110&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bjcy1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754521398,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7br323",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Baldur-Norddahl",
            "can_mod_post": false,
            "created_utc": 1754523926,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 1,
            "author_fullname": "t2_bvqb8ng0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "unsloth q6 ud is significantly faster than the original. The model file is not much smaller however. The difference is that the bf16 layers of the original are quantized to q6. These layers are the shared layers and therefore run on every token. Reducing them in size will have a big impact on memory bandwidth used per token. The experts are still mxfp4 as in the original.\n\nYou asked about performance. Reducing from bf16 to q6 is obviously going to reduce that slightly. But probably not noticeably. It is an option anyway to get a little extra speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7br323",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;unsloth q6 ud is significantly faster than the original. The model file is not much smaller however. The difference is that the bf16 layers of the original are quantized to q6. These layers are the shared layers and therefore run on every token. Reducing them in size will have a big impact on memory bandwidth used per token. The experts are still mxfp4 as in the original.&lt;/p&gt;\n\n&lt;p&gt;You asked about performance. Reducing from bf16 to q6 is obviously going to reduce that slightly. But probably not noticeably. It is an option anyway to get a little extra speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7br323/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754523926,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bj12x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "lakySK",
                      "can_mod_post": false,
                      "created_utc": 1754521286,
                      "send_replies": true,
                      "parent_id": "t1_n7bhdsk",
                      "score": 5,
                      "author_fullname": "t2_y9y2q",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not sure I agree. On a 128GB Macbook, this thing is as quick as 30B Qwen and definitely reasons better (and a lot shorter!). Plus I still have half of the RAM free to use it as a normal computer, unlike with Qwen 235B or GLM Air where I need to try hard to squeeze them in and keep them running at a decent speed. I'm definitely going to be giving it a shot for myself.  \n\n\nEdit: Plus so much better with more obscure languages like Slovak. It's a night and day between GPT-OSS and Qwen 3 🤯",
                      "edited": 1754521922,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bj12x",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not sure I agree. On a 128GB Macbook, this thing is as quick as 30B Qwen and definitely reasons better (and a lot shorter!). Plus I still have half of the RAM free to use it as a normal computer, unlike with Qwen 235B or GLM Air where I need to try hard to squeeze them in and keep them running at a decent speed. I&amp;#39;m definitely going to be giving it a shot for myself.  &lt;/p&gt;\n\n&lt;p&gt;Edit: Plus so much better with more obscure languages like Slovak. It&amp;#39;s a night and day between GPT-OSS and Qwen 3 🤯&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bj12x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754521286,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 5
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7brcd2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MoneyPowerNexis",
                      "can_mod_post": false,
                      "created_utc": 1754524011,
                      "send_replies": true,
                      "parent_id": "t1_n7bhdsk",
                      "score": 1,
                      "author_fullname": "t2_635g2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "For coding I'm finding it as good or better at some things and worse than some things vs GLM 4.5 and Qwen.\n\nIn particular it made a pretty good wave function collapse demo where as my other best models struggled. I think GLM makes the nicest user interfaces and Qwen seems a bit better at coding all-round but  GPT-OSS is so damn fast I know I'll be using it for coding until something better comes along at that speed range but I wont be relying on its creativity just its ability to quickly code edit and not mess up existing code it if its given a large block. \n\nbut I certainly wont be using  GPT-OSS it for creative writing, maybe for proof reading though.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7brcd2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For coding I&amp;#39;m finding it as good or better at some things and worse than some things vs GLM 4.5 and Qwen.&lt;/p&gt;\n\n&lt;p&gt;In particular it made a pretty good wave function collapse demo where as my other best models struggled. I think GLM makes the nicest user interfaces and Qwen seems a bit better at coding all-round but  GPT-OSS is so damn fast I know I&amp;#39;ll be using it for coding until something better comes along at that speed range but I wont be relying on its creativity just its ability to quickly code edit and not mess up existing code it if its given a large block. &lt;/p&gt;\n\n&lt;p&gt;but I certainly wont be using  GPT-OSS it for creative writing, maybe for proof reading though.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7brcd2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754524011,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bn3se",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Suspicious_Young8152",
                      "can_mod_post": false,
                      "created_utc": 1754522619,
                      "send_replies": true,
                      "parent_id": "t1_n7bhdsk",
                      "score": 1,
                      "author_fullname": "t2_1gzoposi1r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Because personally I'm more in need of a technical expert than a waifu.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bn3se",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because personally I&amp;#39;m more in need of a technical expert than a waifu.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bn3se/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754522619,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7bhdsk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ambitious-Profit855",
            "can_mod_post": false,
            "created_utc": 1754520733,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 0,
            "author_fullname": "t2_8hl761dc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why are you so eager to run it? All I read so far about it was pretty mediocre and the Qwen models are the better choice. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bhdsk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why are you so eager to run it? All I read so far about it was pretty mediocre and the Qwen models are the better choice. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bhdsk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754520733,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 0
          }
        }
      ],
      "before": null
    }
  }
]