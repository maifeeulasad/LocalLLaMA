[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "What a ride! Been a big 24h. Now that the dust has barely settled, I just wanted some clarification (and I'm sure there are many of us) around which of the major GPT-OSS releases should we be using for best quality-performance? (rather than speed)\n\nThere's llama.cpp native support: [https://github.com/ggml-org/llama.cpp/discussions/15095](https://github.com/ggml-org/llama.cpp/discussions/15095)  \nI presume this means I can just run the native models dropped by OpenAI on hugging face here: [https://huggingface.co/openai/gpt-oss-120b](https://huggingface.co/openai/gpt-oss-120b) \n\nBut then there is GGML: [https://github.com/ggml-org/llama.cpp/pull/15091](https://github.com/ggml-org/llama.cpp/pull/15091)  \nWith the models here: [https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf](https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf)\n\nAnd there's Unsloth: [https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune](https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune)  \nTheir models are gguf: [https://huggingface.co/unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)  \nThey mention chat template fixes have have different quants.\n\nIs the right combo the OpenAI quants with the Unsloth chat template fixes? (I'm using LMStudio on a 128 M4 Max for what that's worth).\n\nAlso, shoutout to everyone involved to the organisations involved above, woking your absolute asses off at the moment.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Where are we at running the GPT-OSS models locally?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mjjaor",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.56,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 2,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1gzoposi1r",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 2,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "post_hint": "self",
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754520512,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;What a ride! Been a big 24h. Now that the dust has barely settled, I just wanted some clarification (and I&amp;#39;m sure there are many of us) around which of the major GPT-OSS releases should we be using for best quality-performance? (rather than speed)&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s llama.cpp native support: &lt;a href=\"https://github.com/ggml-org/llama.cpp/discussions/15095\"&gt;https://github.com/ggml-org/llama.cpp/discussions/15095&lt;/a&gt;&lt;br/&gt;\nI presume this means I can just run the native models dropped by OpenAI on hugging face here: &lt;a href=\"https://huggingface.co/openai/gpt-oss-120b\"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;But then there is GGML: &lt;a href=\"https://github.com/ggml-org/llama.cpp/pull/15091\"&gt;https://github.com/ggml-org/llama.cpp/pull/15091&lt;/a&gt;&lt;br/&gt;\nWith the models here: &lt;a href=\"https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf\"&gt;https://huggingface.co/collections/ggml-org/gpt-oss-68923b60bee37414546c70bf&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;And there&amp;#39;s Unsloth: &lt;a href=\"https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune\"&gt;https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune&lt;/a&gt;&lt;br/&gt;\nTheir models are gguf: &lt;a href=\"https://huggingface.co/unsloth/gpt-oss-20b-GGUF\"&gt;https://huggingface.co/unsloth/gpt-oss-20b-GGUF&lt;/a&gt;&lt;br/&gt;\nThey mention chat template fixes have have different quants.&lt;/p&gt;\n\n&lt;p&gt;Is the right combo the OpenAI quants with the Unsloth chat template fixes? (I&amp;#39;m using LMStudio on a 128 M4 Max for what that&amp;#39;s worth).&lt;/p&gt;\n\n&lt;p&gt;Also, shoutout to everyone involved to the organisations involved above, woking your absolute asses off at the moment.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "preview": {
              "images": [
                {
                  "source": {
                    "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?auto=webp&amp;s=021ac90e342e7ce24176d8fc1d8f982df536ec3a",
                    "width": 1200,
                    "height": 600
                  },
                  "resolutions": [
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7b2771ee5257111e4de088311cb5195ef52c7b24",
                      "width": 108,
                      "height": 54
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=546ac16b2e0ddee9d735b54e7252ea7704f0aa25",
                      "width": 216,
                      "height": 108
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0d3b1de829365122de157087d1354ddc7ce0a097",
                      "width": 320,
                      "height": 160
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ab49a1bcbed40a5d13899b9b4cca4f76dd3f536",
                      "width": 640,
                      "height": 320
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=26c58e1a03de3b0334cae1d33f50e3bfa61973c5",
                      "width": 960,
                      "height": 480
                    },
                    {
                      "url": "https://external-preview.redd.it/wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5c58046231d9a33ac45a28213537010cf1e217fd",
                      "width": 1080,
                      "height": 540
                    }
                  ],
                  "variants": {},
                  "id": "wBahFztknQ-A1CZRCY7qY4UJKbme9D-9RZUUC_JNONw"
                }
              ],
              "enabled": false
            },
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mjjaor",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "Suspicious_Young8152",
            "discussion_type": null,
            "num_comments": 17,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/",
            "subreddit_subscribers": 512874,
            "created_utc": 1754520512,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7de4cc",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Wrong-Historian",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7bqdry",
                                "score": 2,
                                "author_fullname": "t2_69r67vj3",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Inference is limited by memory bandwidth, not compute. My 3090 isn't even maxxing out its tdp so the cores aren't fully utilized, as the memory is the bottleneck. Maybe 'native MXFP4 support' could bring some energy efficiency",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7de4cc",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Inference is limited by memory bandwidth, not compute. My 3090 isn&amp;#39;t even maxxing out its tdp so the cores aren&amp;#39;t fully utilized, as the memory is the bottleneck. Maybe &amp;#39;native MXFP4 support&amp;#39; could bring some energy efficiency&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjjaor",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7de4cc/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754547368,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754547368,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bqdry",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "BumbleSlob",
                      "can_mod_post": false,
                      "created_utc": 1754523696,
                      "send_replies": true,
                      "parent_id": "t1_n7blb2w",
                      "score": 2,
                      "author_fullname": "t2_1j7fhlcqkp",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Seems to be that because of MXFP4 native hardware support, this model is the first one which should make the 50x series of cards worthwhile for inference (by differentiating from 40x gens not supporting it)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bqdry",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Seems to be that because of MXFP4 native hardware support, this model is the first one which should make the 50x series of cards worthwhile for inference (by differentiating from 40x gens not supporting it)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bqdry/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754523696,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7blb2w",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Wrong-Historian",
            "can_mod_post": false,
            "created_utc": 1754522042,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 7,
            "author_fullname": "t2_69r67vj3",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "llama-cpp, OpenAI gguff 120B\n\n25T/s on a single 3090 and 96GB DDR5 6800 and 14900K\n\nPretty awesome",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7blb2w",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;llama-cpp, OpenAI gguff 120B&lt;/p&gt;\n\n&lt;p&gt;25T/s on a single 3090 and 96GB DDR5 6800 and 14900K&lt;/p&gt;\n\n&lt;p&gt;Pretty awesome&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7blb2w/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754522042,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 7
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7bjoe0",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "AdamDhahabi",
            "can_mod_post": false,
            "created_utc": 1754521507,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 6,
            "author_fullname": "t2_x5lnbc2",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I'm running Unsloth their 120b at 20 t/s on my budget workstation (16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000). 20 t/s, that is for the first 1K tokens, it slows down to 13 t/s at 30K context.  \nThat's 80% faster than GLM Air, I'm waiting for a new Qwen 32b coder.",
            "edited": 1754522228,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bjoe0",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;m running Unsloth their 120b at 20 t/s on my budget workstation (16 GB RTX 5060 Ti + 16 GB P5000 + 64 GB DDR5 6000). 20 t/s, that is for the first 1K tokens, it slows down to 13 t/s at 30K context.&lt;br/&gt;\nThat&amp;#39;s 80% faster than GLM Air, I&amp;#39;m waiting for a new Qwen 32b coder.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bjoe0/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754521507,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n7cpfvz",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "llmentry",
            "can_mod_post": false,
            "created_utc": 1754536034,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 6,
            "author_fullname": "t2_1lufy6yx6z",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Ooba 3.9, with Unsloth GGUF and the template fix posted on Ooba's github, works very nicely.\n\n\nJailbreaking via a mix of prompt and template and model response is difficult, but possible (and kinda fun).  It's so stupidly restricted that I needed to jailbreak just to use one of my normal work-related prompts :/\n\n\n120B needs ~66Gb RAM for q6_k, and god it's fast.  It's not going to win any creative writing awards, but it's great (possibly best-in-class) for STEM.  Now that I've freed its mind from all those policies, I really like it so far.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7cpfvz",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ooba 3.9, with Unsloth GGUF and the template fix posted on Ooba&amp;#39;s github, works very nicely.&lt;/p&gt;\n\n&lt;p&gt;Jailbreaking via a mix of prompt and template and model response is difficult, but possible (and kinda fun).  It&amp;#39;s so stupidly restricted that I needed to jailbreak just to use one of my normal work-related prompts :/&lt;/p&gt;\n\n&lt;p&gt;120B needs ~66Gb RAM for q6_k, and god it&amp;#39;s fast.  It&amp;#39;s not going to win any creative writing awards, but it&amp;#39;s great (possibly best-in-class) for STEM.  Now that I&amp;#39;ve freed its mind from all those policies, I really like it so far.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7cpfvz/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754536034,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 6
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bnqxq",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Suspicious_Young8152",
                      "can_mod_post": false,
                      "created_utc": 1754522830,
                      "send_replies": true,
                      "parent_id": "t1_n7bjcy1",
                      "score": 1,
                      "author_fullname": "t2_1gzoposi1r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Ha, no, but the last commit was 15 minutes ago, dust certainly hasn't fully settled yet!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bnqxq",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ha, no, but the last commit was 15 minutes ago, dust certainly hasn&amp;#39;t fully settled yet!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bnqxq/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754522830,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7bjcy1",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "lakySK",
            "can_mod_post": false,
            "created_utc": 1754521398,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 3,
            "author_fullname": "t2_y9y2q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I've just used the OpenAI gguf and it seemed to work well. Haven't played with the template. Do you know what exactly Unsloth changed?\n\nEdit: Is this related? [https://github.com/ggml-org/llama.cpp/issues/15110](https://github.com/ggml-org/llama.cpp/issues/15110)",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bjcy1",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve just used the OpenAI gguf and it seemed to work well. Haven&amp;#39;t played with the template. Do you know what exactly Unsloth changed?&lt;/p&gt;\n\n&lt;p&gt;Edit: Is this related? &lt;a href=\"https://github.com/ggml-org/llama.cpp/issues/15110\"&gt;https://github.com/ggml-org/llama.cpp/issues/15110&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bjcy1/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754521398,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7c52n5",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Suspicious_Young8152",
                      "can_mod_post": false,
                      "created_utc": 1754528711,
                      "send_replies": true,
                      "parent_id": "t1_n7br323",
                      "score": 2,
                      "author_fullname": "t2_1gzoposi1r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That's really interesting. So that might result in something like if you're getting an 80% one-shot on MXFP4, consider a 95% 5 shot with q6 where the extra time is maybe x3 over the 5, but the improvement is on a log scale, each incremental % point helps a lot more than the last. Thanks, good food for thought.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7c52n5",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That&amp;#39;s really interesting. So that might result in something like if you&amp;#39;re getting an 80% one-shot on MXFP4, consider a 95% 5 shot with q6 where the extra time is maybe x3 over the 5, but the improvement is on a log scale, each incremental % point helps a lot more than the last. Thanks, good food for thought.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7c52n5/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754528711,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7br323",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Baldur-Norddahl",
            "can_mod_post": false,
            "created_utc": 1754523926,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 2,
            "author_fullname": "t2_bvqb8ng0",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "unsloth q6 ud is significantly faster than the original. The model file is not much smaller however. The difference is that the bf16 layers of the original are quantized to q6. These layers are the shared layers and therefore run on every token. Reducing them in size will have a big impact on memory bandwidth used per token. The experts are still mxfp4 as in the original.\n\nYou asked about performance. Reducing from bf16 to q6 is obviously going to reduce that slightly. But probably not noticeably. It is an option anyway to get a little extra speed.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7br323",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;unsloth q6 ud is significantly faster than the original. The model file is not much smaller however. The difference is that the bf16 layers of the original are quantized to q6. These layers are the shared layers and therefore run on every token. Reducing them in size will have a big impact on memory bandwidth used per token. The experts are still mxfp4 as in the original.&lt;/p&gt;\n\n&lt;p&gt;You asked about performance. Reducing from bf16 to q6 is obviously going to reduce that slightly. But probably not noticeably. It is an option anyway to get a little extra speed.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7br323/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754523926,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n7cexj0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "chisleu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7bj12x",
                                "score": 3,
                                "author_fullname": "t2_cbxyn",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I imagine GPT OSS would be better at chat. If I was going to put a model on a tablet for survival, it would be GPT.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7cexj0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I imagine GPT OSS would be better at chat. If I was going to put a model on a tablet for survival, it would be GPT.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjjaor",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7cexj0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754532140,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754532140,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bj12x",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "lakySK",
                      "can_mod_post": false,
                      "created_utc": 1754521286,
                      "send_replies": true,
                      "parent_id": "t1_n7bhdsk",
                      "score": 9,
                      "author_fullname": "t2_y9y2q",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not sure I agree. On a 128GB Macbook, this thing is as quick as 30B Qwen and definitely reasons better (and a lot shorter!). Plus I still have half of the RAM free to use it as a normal computer, unlike with Qwen 235B or GLM Air where I need to try hard to squeeze them in and keep them running at a decent speed. I'm definitely going to be giving it a shot for myself.  \n\n\nEdit: Plus so much better with more obscure languages like Slovak. It's a night and day between GPT-OSS and Qwen 3 🤯",
                      "edited": 1754521922,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bj12x",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not sure I agree. On a 128GB Macbook, this thing is as quick as 30B Qwen and definitely reasons better (and a lot shorter!). Plus I still have half of the RAM free to use it as a normal computer, unlike with Qwen 235B or GLM Air where I need to try hard to squeeze them in and keep them running at a decent speed. I&amp;#39;m definitely going to be giving it a shot for myself.  &lt;/p&gt;\n\n&lt;p&gt;Edit: Plus so much better with more obscure languages like Slovak. It&amp;#39;s a night and day between GPT-OSS and Qwen 3 🤯&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bj12x/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754521286,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 9
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7brcd2",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "MoneyPowerNexis",
                      "can_mod_post": false,
                      "created_utc": 1754524011,
                      "send_replies": true,
                      "parent_id": "t1_n7bhdsk",
                      "score": 3,
                      "author_fullname": "t2_635g2",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "For coding I'm finding it as good or better at some things and worse than some things vs GLM 4.5 and Qwen.\n\nIn particular it made a pretty good wave function collapse demo where as my other best models struggled. I think GLM makes the nicest user interfaces and Qwen seems a bit better at coding all-round but  GPT-OSS is so damn fast I know I'll be using it for coding until something better comes along at that speed range but I wont be relying on its creativity just its ability to quickly code edit and not mess up existing code it if its given a large block. \n\nbut I certainly wont be using  GPT-OSS it for creative writing, maybe for proof reading though.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7brcd2",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For coding I&amp;#39;m finding it as good or better at some things and worse than some things vs GLM 4.5 and Qwen.&lt;/p&gt;\n\n&lt;p&gt;In particular it made a pretty good wave function collapse demo where as my other best models struggled. I think GLM makes the nicest user interfaces and Qwen seems a bit better at coding all-round but  GPT-OSS is so damn fast I know I&amp;#39;ll be using it for coding until something better comes along at that speed range but I wont be relying on its creativity just its ability to quickly code edit and not mess up existing code it if its given a large block. &lt;/p&gt;\n\n&lt;p&gt;but I certainly wont be using  GPT-OSS it for creative writing, maybe for proof reading though.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7brcd2/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754524011,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "body": "Good luck. I had issues with parsing large data sets, psychology, biology, displaying equations in a different format, downloading and modifying code, you name it.\n\nWhen people say this thing is obscenely censored they're not talking about fuck chat, they're talking about virtually anything.",
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n7cq2js",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "llmentry",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n7cgut0",
                                          "score": 2,
                                          "author_fullname": "t2_1lufy6yx6z",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Agreed.  But once jailbroken, it's surprisingly useful (for STEM brainstorming/backgrounding, anyway; haven't tried it out on coding yet.)",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n7cq2js",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Agreed.  But once jailbroken, it&amp;#39;s surprisingly useful (for STEM brainstorming/backgrounding, anyway; haven&amp;#39;t tried it out on coding yet.)&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mjjaor",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7cq2js/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754536281,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754536281,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 2
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n7cgut0",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AbyssianOne",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n7bn3se",
                                "score": 3,
                                "author_fullname": "t2_1651c3kskq",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "author_cakeday": true,
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n7cgut0",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Good luck. I had issues with parsing large data sets, psychology, biology, displaying equations in a different format, downloading and modifying code, you name it.&lt;/p&gt;\n\n&lt;p&gt;When people say this thing is obscenely censored they&amp;#39;re not talking about fuck chat, they&amp;#39;re talking about virtually anything.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mjjaor",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7cgut0/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754532826,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754532826,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n7bn3se",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": false,
                      "author": "Suspicious_Young8152",
                      "can_mod_post": false,
                      "created_utc": 1754522619,
                      "send_replies": true,
                      "parent_id": "t1_n7bhdsk",
                      "score": 6,
                      "author_fullname": "t2_1gzoposi1r",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Because personally I'm more in need of a technical expert than a waifu.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7bn3se",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because personally I&amp;#39;m more in need of a technical expert than a waifu.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bn3se/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754522619,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 6
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n7cs6lw",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "OutrageousMinimum191",
                      "can_mod_post": false,
                      "created_utc": 1754537126,
                      "send_replies": true,
                      "parent_id": "t1_n7bhdsk",
                      "score": 1,
                      "author_fullname": "t2_13ejj8rbqm",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "GLM air is far better choice",
                      "edited": 1754537331,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n7cs6lw",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;GLM air is far better choice&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mjjaor",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7cs6lw/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754537126,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n7bhdsk",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Ambitious-Profit855",
            "can_mod_post": false,
            "created_utc": 1754520733,
            "send_replies": true,
            "parent_id": "t3_1mjjaor",
            "score": 3,
            "author_fullname": "t2_8hl761dc",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why are you so eager to run it? All I read so far about it was pretty mediocre and the Qwen models are the better choice. ",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n7bhdsk",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why are you so eager to run it? All I read so far about it was pretty mediocre and the Qwen models are the better choice. &lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mjjaor/where_are_we_at_running_the_gptoss_models_locally/n7bhdsk/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754520733,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mjjaor",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 1,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        }
      ],
      "before": null
    }
  }
]