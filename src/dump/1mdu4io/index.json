[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Like the title says, I ran **GLM 4.5 Air Q4** on my local machine using **RooCode** inside **VS Code**, and I was able to build a functional CRUD-style web application.\n\nUsers can register with a password, log in, and log out from the client side. All authentication is handled using JWTs.\n\nThe experience honestly exceeded my expectations. Compared to my past attempts with local LLMs and RooCode (which sometimes struggled to generate even a basic webpage), this felt like a major step forward. The results were genuinely satisfying.\n\nThe entire app took about an hour to generate, with a bit of debugging and prompt tweaking along the way. With more deliberate prompting and a little more patience, I think I could have pushed it further. But for now, it’s a solid starting point.\n\nIf anyone else is experimenting with local models for full stack projects, I’d love to hear how it’s going. Happy to answer questions or share what I’ve learned.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "I Built a Full Stack App Using a Local LLM (GLM 4.5 Air) and RooCode. Here's How It Went",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Discussion"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mdu4io",
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.91,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 9,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_fz3utn30",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Discussion",
            "can_mod_post": false,
            "score": 9,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753943029,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Like the title says, I ran &lt;strong&gt;GLM 4.5 Air Q4&lt;/strong&gt; on my local machine using &lt;strong&gt;RooCode&lt;/strong&gt; inside &lt;strong&gt;VS Code&lt;/strong&gt;, and I was able to build a functional CRUD-style web application.&lt;/p&gt;\n\n&lt;p&gt;Users can register with a password, log in, and log out from the client side. All authentication is handled using JWTs.&lt;/p&gt;\n\n&lt;p&gt;The experience honestly exceeded my expectations. Compared to my past attempts with local LLMs and RooCode (which sometimes struggled to generate even a basic webpage), this felt like a major step forward. The results were genuinely satisfying.&lt;/p&gt;\n\n&lt;p&gt;The entire app took about an hour to generate, with a bit of debugging and prompt tweaking along the way. With more deliberate prompting and a little more patience, I think I could have pushed it further. But for now, it’s a solid starting point.&lt;/p&gt;\n\n&lt;p&gt;If anyone else is experimenting with local models for full stack projects, I’d love to hear how it’s going. Happy to answer questions or share what I’ve learned.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "5f921ea4-c7bc-11ed-9c23-3a00622979b4",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#646d73",
            "id": "1mdu4io",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "gamblingapocalypse",
            "discussion_type": null,
            "num_comments": 16,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/",
            "subreddit_subscribers": 507935,
            "created_utc": 1753943029,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n66rpi0",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "gamblingapocalypse",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n65zu0g",
                                          "score": 1,
                                          "author_fullname": "t2_fz3utn30",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Very cool!  I'm impressed with the depth of Roo myself.  Thanks for the tips!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n66rpi0",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Very cool!  I&amp;#39;m impressed with the depth of Roo myself.  Thanks for the tips!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdu4io",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n66rpi0/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753978172,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753978172,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n65zu0g",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "dfgvbsrdfgaregzf",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n64dxvk",
                                "score": 2,
                                "author_fullname": "t2_pup33y79c",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Roo Code is IMHO by far the best agentic coder in VSCode. The flow they use, the structure with To Dos, the ability to easily define new agents etc is much better than Copilot.\n\n (And you can use the Copilot models/subscription in Roo Code by setting the LLM provider to \"VS Code LM API\".)",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n65zu0g",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Roo Code is IMHO by far the best agentic coder in VSCode. The flow they use, the structure with To Dos, the ability to easily define new agents etc is much better than Copilot.&lt;/p&gt;\n\n&lt;p&gt;(And you can use the Copilot models/subscription in Roo Code by setting the LLM provider to &amp;quot;VS Code LM API&amp;quot;.)&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdu4io",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n65zu0g/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753970263,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753970263,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n64dxvk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gamblingapocalypse",
                      "can_mod_post": false,
                      "created_utc": 1753944193,
                      "send_replies": true,
                      "parent_id": "t1_n64ckzy",
                      "score": 2,
                      "author_fullname": "t2_fz3utn30",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "That’s really interesting. I’ve been focused mostly on local models.  I haven’t tried RooCode with cloud hosted models like Opus yet.  I was surprised how well GLM 4.5 performed in a local setup, though. It’s encouraging to hear RooCode scales well with different backends.  Fun times! :)",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64dxvk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;That’s really interesting. I’ve been focused mostly on local models.  I haven’t tried RooCode with cloud hosted models like Opus yet.  I was surprised how well GLM 4.5 performed in a local setup, though. It’s encouraging to hear RooCode scales well with different backends.  Fun times! :)&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64dxvk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753944193,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64ckzy",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "dfgvbsrdfgaregzf",
            "can_mod_post": false,
            "created_utc": 1753943448,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 8,
            "author_fullname": "t2_pup33y79c",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Roo Code is extremely powerful with the right model. I benchmarked it using full context length models from OpenRouter and it equaled Cursor with Opus 4 Max in a stress test.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64ckzy",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Roo Code is extremely powerful with the right model. I benchmarked it using full context length models from OpenRouter and it equaled Cursor with Opus 4 Max in a stress test.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64ckzy/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753943448,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 8
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n64jyv7",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "robberviet",
                      "can_mod_post": false,
                      "created_utc": 1753947543,
                      "send_replies": true,
                      "parent_id": "t1_n64hxk2",
                      "score": 3,
                      "author_fullname": "t2_jxc5a",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "You can use it. Use and tell us what is the difference. \n\nEach person feel different. I feel Cline is better than VSCode. RooCode and Cline are the same.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n64jyv7",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You can use it. Use and tell us what is the difference. &lt;/p&gt;\n\n&lt;p&gt;Each person feel different. I feel Cline is better than VSCode. RooCode and Cline are the same.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64jyv7/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753947543,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 3
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n65u5wi",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gamblingapocalypse",
                      "can_mod_post": false,
                      "created_utc": 1753968526,
                      "send_replies": true,
                      "parent_id": "t1_n64hxk2",
                      "score": 1,
                      "author_fullname": "t2_fz3utn30",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "I have better luck with roo, really it’s just preference.  ",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n65u5wi",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have better luck with roo, really it’s just preference.  &lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n65u5wi/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753968526,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64hxk2",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Glittering-Call8746",
            "can_mod_post": false,
            "created_utc": 1753946386,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 1,
            "author_fullname": "t2_tqwl6sawb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Why can't I just use vs code insiders ? What's the difference between the AI extensions ? Ie Cline and Roo",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64hxk2",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Why can&amp;#39;t I just use vs code insiders ? What&amp;#39;s the difference between the AI extensions ? Ie Cline and Roo&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64hxk2/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753946386,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n69e29f",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "FriendlyUser_",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n66t8bb",
                                "score": 1,
                                "author_fullname": "t2_1papeut323",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "oh you can download the models directly from hf, if it is gguf you even can select ollama there and it pops up a selection with several quants to select and it will give you the whole ollama pull command for that particular model and quant",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n69e29f",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;oh you can download the models directly from hf, if it is gguf you even can select ollama there and it pops up a selection with several quants to select and it will give you the whole ollama pull command for that particular model and quant&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdu4io",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n69e29f/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754005858,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754005858,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n66t8bb",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gamblingapocalypse",
                      "can_mod_post": false,
                      "created_utc": 1753978603,
                      "send_replies": true,
                      "parent_id": "t1_n64uk6q",
                      "score": 1,
                      "author_fullname": "t2_fz3utn30",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Honestly, I just installed the Roo extension and was good to go after that (Using LM Studio which I think is a bit better than Ollama.  LM Studio grants you access to a wider range of models, and allows you to download them with other quantization techniques, without doing them yourself.  Ollama for the most part only gives you q4 standard quantization for all of their models, so if you want to experiment q3 or q6 you can't do that on Ollama).  I haven't used vs code insider however.  Just thinking out loud: I will say, for some reason the Roo agent adds A LOT of tokens to the initial prompt.  I'm curious if its like that for other agent developers.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n66t8bb",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Honestly, I just installed the Roo extension and was good to go after that (Using LM Studio which I think is a bit better than Ollama.  LM Studio grants you access to a wider range of models, and allows you to download them with other quantization techniques, without doing them yourself.  Ollama for the most part only gives you q4 standard quantization for all of their models, so if you want to experiment q3 or q6 you can&amp;#39;t do that on Ollama).  I haven&amp;#39;t used vs code insider however.  Just thinking out loud: I will say, for some reason the Roo agent adds A LOT of tokens to the initial prompt.  I&amp;#39;m curious if its like that for other agent developers.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n66t8bb/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753978603,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64uk6q",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Glittering-Call8746",
            "can_mod_post": false,
            "created_utc": 1753953591,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 1,
            "author_fullname": "t2_tqwl6sawb",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I can't get vs code insiders to work on remote ollama endpoint.. so just cline atm.. I only have begin my setup. It took me weeks as I'm busy working. Now I gotta figure out why vs code insiders not working with ollama.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64uk6q",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I can&amp;#39;t get vs code insiders to work on remote ollama endpoint.. so just cline atm.. I only have begin my setup. It took me weeks as I&amp;#39;m busy working. Now I gotta figure out why vs code insiders not working with ollama.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64uk6q/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753953591,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n67t69z",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "gamblingapocalypse",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n67itx8",
                                          "score": 1,
                                          "author_fullname": "t2_fz3utn30",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "I did not stuck in a loop.  For more detail I did use the q4 version.  If that helps.\n\nAlso, I did not take a single query.  I built in stages.  Another thing is I put the settings to maximize the context window from LM Studio's side.",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n67t69z",
                                          "is_submitter": true,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I did not stuck in a loop.  For more detail I did use the q4 version.  If that helps.&lt;/p&gt;\n\n&lt;p&gt;Also, I did not take a single query.  I built in stages.  Another thing is I put the settings to maximize the context window from LM Studio&amp;#39;s side.&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mdu4io",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n67t69z/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753988584,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753988584,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n67itx8",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "AstroZombie138",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n679m9i",
                                "score": 1,
                                "author_fullname": "t2_3sicn",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Did you get any roo stuck in a loop type errors?  I just loaded up GLM 4.5-Air and am getting that on my first attempts.  I don't get that when using Claude on the same prompts.\n\nEdit:  Just in case anyone searches for this in the future, my issue is because the context window was set too low.",
                                "edited": 1754008379,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n67itx8",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Did you get any roo stuck in a loop type errors?  I just loaded up GLM 4.5-Air and am getting that on my first attempts.  I don&amp;#39;t get that when using Claude on the same prompts.&lt;/p&gt;\n\n&lt;p&gt;Edit:  Just in case anyone searches for this in the future, my issue is because the context window was set too low.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mdu4io",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n67itx8/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753985651,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753985651,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n679m9i",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gamblingapocalypse",
                      "can_mod_post": false,
                      "created_utc": 1753983136,
                      "send_replies": true,
                      "parent_id": "t1_n67441m",
                      "score": 2,
                      "author_fullname": "t2_fz3utn30",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Yes, 4.5 Air for both.  I ran using an m4 max chip (laptop) with 128 gigs of ram.  Getting about 40 tps, but tended to slow down (to maybe 20 or so) towards the end of the context length (128k tokens I think?).  Time to first token was pretty quick, but that also slowed down when I reached towards the end of the context window.",
                      "edited": 1753983528,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n679m9i",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, 4.5 Air for both.  I ran using an m4 max chip (laptop) with 128 gigs of ram.  Getting about 40 tps, but tended to slow down (to maybe 20 or so) towards the end of the context length (128k tokens I think?).  Time to first token was pretty quick, but that also slowed down when I reached towards the end of the context window.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n679m9i/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753983136,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n67441m",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "AstroZombie138",
            "can_mod_post": false,
            "created_utc": 1753981649,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 1,
            "author_fullname": "t2_3sicn",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Are you using GLM 4.5 Air for both the architect and code agents?  What kind of machine are you running it on and what does the performance feel like?",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n67441m",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you using GLM 4.5 Air for both the architect and code agents?  What kind of machine are you running it on and what does the performance feel like?&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n67441m/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753981649,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n65tr9o",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gamblingapocalypse",
                      "can_mod_post": false,
                      "created_utc": 1753968401,
                      "send_replies": true,
                      "parent_id": "t1_n64frnp",
                      "score": 2,
                      "author_fullname": "t2_fz3utn30",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for your insight.  I look forward to the day where one day we’ll have local models (or strategies) that would work just as well as Sonnet today.  But if that were to happen, the big models would be so much better (maybe).",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n65tr9o",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for your insight.  I look forward to the day where one day we’ll have local models (or strategies) that would work just as well as Sonnet today.  But if that were to happen, the big models would be so much better (maybe).&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mdu4io",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n65tr9o/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753968401,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n64frnp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fueled_by_caffeine",
            "can_mod_post": false,
            "created_utc": 1753945184,
            "send_replies": true,
            "parent_id": "t3_1mdu4io",
            "score": 1,
            "author_fullname": "t2_7fkqhhi",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I spent a couple of hours playing yesterday, it worked a lot better than any other model I’d tried previously for agentic stuff, but it’s still quite a lot worse than Claude 4 sonnet in terms of coding strategy and architecture (far more interventions required with incorrect approaches needing to be intercepted vs Claude) and on my M4 Max MBP with 128GB RAM q4 with 128k context felt too slow to be useful professionally though honestly I feel like that could be more on LM Studio than anything else as in direct chat rather than API I was seeing a little over 40 tps.  Overall the model felt very very thinky with often entire responses being synthesized as thoughts before being regenerated again as an actual response.\n\nComing on leaps and bounds but I couldn’t use it over a bigger hosted model yet.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n64frnp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I spent a couple of hours playing yesterday, it worked a lot better than any other model I’d tried previously for agentic stuff, but it’s still quite a lot worse than Claude 4 sonnet in terms of coding strategy and architecture (far more interventions required with incorrect approaches needing to be intercepted vs Claude) and on my M4 Max MBP with 128GB RAM q4 with 128k context felt too slow to be useful professionally though honestly I feel like that could be more on LM Studio than anything else as in direct chat rather than API I was seeing a little over 40 tps.  Overall the model felt very very thinky with often entire responses being synthesized as thoughts before being regenerated again as an actual response.&lt;/p&gt;\n\n&lt;p&gt;Coming on leaps and bounds but I couldn’t use it over a bigger hosted model yet.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mdu4io/i_built_a_full_stack_app_using_a_local_llm_glm_45/n64frnp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753945184,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mdu4io",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]