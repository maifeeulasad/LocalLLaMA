[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "**Full specs**:\n\n**GPU**: RTX 4070 TI Super (16 GB VRAM)\n\n**CPU**: i7 14700K\n\n**System RAM**: 96 GB DDR5 @ 6200 MT/s (total usage, including all Windows processes, is 61 GB, so only having 64GB RAM is probably sufficient)\n\n**OS**: Windows 11\n\n**Model runner**: LM Studio (see settings in third screenshot)\n\n  \nWhen I saw that OpenAI released a 120b parameter model, my assumption was that running it wouldn't be realistic for people with consumer-grade hardware. After some experimentation, I was *partly* proven wrong- 13 t/s is a speed that I'd consider \"usable\" on days where I'm feeling relatively patient. I'd imagine that people running RTX 5090's and/or faster system RAM are getting speeds that are truly usable for a lot of people, a lot of the time. If anyone has this setup, I'd love to hear what kind of speeds you're getting. ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "is_gallery": true,
            "title": "gpt-oss-120b performance with only 16 GB VRAM- surprisingly decent",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "New Model"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": 114,
            "top_awarded_type": null,
            "name": "t3_1miprwe",
            "media_metadata": {
              "i0atmotcgahf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 88,
                    "x": 108,
                    "u": "https://preview.redd.it/i0atmotcgahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ae2fdd72226ff436307c407112f2825a6a2885d"
                  },
                  {
                    "y": 176,
                    "x": 216,
                    "u": "https://preview.redd.it/i0atmotcgahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2b006d9e18fb382960d218f8475eda8d6af7a4e5"
                  },
                  {
                    "y": 262,
                    "x": 320,
                    "u": "https://preview.redd.it/i0atmotcgahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2334591ddfde6c24b6127e8e024c358eb7c96f23"
                  }
                ],
                "s": {
                  "y": 344,
                  "x": 420,
                  "u": "https://preview.redd.it/i0atmotcgahf1.png?width=420&amp;format=png&amp;auto=webp&amp;s=82d5ca666a8ad3f6bc713112ec84cd299bb4abc7"
                },
                "id": "i0atmotcgahf1"
              },
              "i1yjttaggahf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 147,
                    "x": 108,
                    "u": "https://preview.redd.it/i1yjttaggahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=139ffdbef2afdfb2f27c515606e13b62126698ba"
                  },
                  {
                    "y": 295,
                    "x": 216,
                    "u": "https://preview.redd.it/i1yjttaggahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=89d9ce76d1ff7eebf94964ec2de7f9b6d5ce8060"
                  },
                  {
                    "y": 437,
                    "x": 320,
                    "u": "https://preview.redd.it/i1yjttaggahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c6c628f2b1dab58f75b68c746437470e5a64e628"
                  }
                ],
                "s": {
                  "y": 592,
                  "x": 433,
                  "u": "https://preview.redd.it/i1yjttaggahf1.png?width=433&amp;format=png&amp;auto=webp&amp;s=bf4c25d2d3d63ecc3d632e14af48ae2fdd260edd"
                },
                "id": "i1yjttaggahf1"
              },
              "uin3n9rjgahf1": {
                "status": "valid",
                "e": "Image",
                "m": "image/png",
                "p": [
                  {
                    "y": 60,
                    "x": 108,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8f6a117776c5a38995291509599b083970cbab99"
                  },
                  {
                    "y": 120,
                    "x": 216,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4ac27f81fd04f331e34425b0686ef826313fd641"
                  },
                  {
                    "y": 178,
                    "x": 320,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0a9f2eeac062cba3d74ea30788fd59f2b770f304"
                  },
                  {
                    "y": 356,
                    "x": 640,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b812cd15ad1a75cfab386d5c7ee8b7720cb298d7"
                  },
                  {
                    "y": 534,
                    "x": 960,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7b252d0d7d01a708c51fb555d6da0ba7a275e22"
                  },
                  {
                    "y": 601,
                    "x": 1080,
                    "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=894f3af2e68991bca0d2b8352c7e80d3e3d0871a"
                  }
                ],
                "s": {
                  "y": 684,
                  "x": 1229,
                  "u": "https://preview.redd.it/uin3n9rjgahf1.png?width=1229&amp;format=png&amp;auto=webp&amp;s=b28d0dca527df3e3dac4939295d592c858f6aec2"
                },
                "id": "uin3n9rjgahf1"
              }
            },
            "hide_score": false,
            "quarantine": false,
            "link_flair_text_color": "light",
            "upvote_ratio": 0.69,
            "author_flair_background_color": null,
            "ups": 18,
            "domain": "reddit.com",
            "media_embed": {},
            "thumbnail_width": 140,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_1t2n2s9f",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "gallery_data": {
              "items": [
                {
                  "media_id": "i0atmotcgahf1",
                  "id": 722144194
                },
                {
                  "media_id": "i1yjttaggahf1",
                  "id": 722144195
                },
                {
                  "media_id": "uin3n9rjgahf1",
                  "id": 722144196
                }
              ]
            },
            "link_flair_text": "New Model",
            "can_mod_post": false,
            "score": 18,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "https://b.thumbs.redditmedia.com/3XwIvatwftLg0e--y7jCH8mLR2VOGeduF77wVTFjpug.jpg",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": false,
            "subreddit_type": "public",
            "created": 1754438808,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;&lt;strong&gt;Full specs&lt;/strong&gt;:&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: RTX 4070 TI Super (16 GB VRAM)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;: i7 14700K&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;System RAM&lt;/strong&gt;: 96 GB DDR5 @ 6200 MT/s (total usage, including all Windows processes, is 61 GB, so only having 64GB RAM is probably sufficient)&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;OS&lt;/strong&gt;: Windows 11&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;Model runner&lt;/strong&gt;: LM Studio (see settings in third screenshot)&lt;/p&gt;\n\n&lt;p&gt;When I saw that OpenAI released a 120b parameter model, my assumption was that running it wouldn&amp;#39;t be realistic for people with consumer-grade hardware. After some experimentation, I was &lt;em&gt;partly&lt;/em&gt; proven wrong- 13 t/s is a speed that I&amp;#39;d consider &amp;quot;usable&amp;quot; on days where I&amp;#39;m feeling relatively patient. I&amp;#39;d imagine that people running RTX 5090&amp;#39;s and/or faster system RAM are getting speeds that are truly usable for a lot of people, a lot of the time. If anyone has this setup, I&amp;#39;d love to hear what kind of speeds you&amp;#39;re getting. &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "url_overridden_by_dest": "https://www.reddit.com/gallery/1miprwe",
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "ced98442-f5d3-11ed-b657-66d3b15490c6",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "mod_note": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "num_reports": null,
            "removal_reason": null,
            "link_flair_background_color": "#ffb000",
            "id": "1miprwe",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "gigaflops_",
            "discussion_type": null,
            "num_comments": 15,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/",
            "stickied": false,
            "url": "https://www.reddit.com/gallery/1miprwe",
            "subreddit_subscribers": 511886,
            "created_utc": 1754438808,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n757kzp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "IxinDow",
            "can_mod_post": false,
            "created_utc": 1754439507,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 11,
            "author_fullname": "t2_lt5ci87n",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "seems pretty safe",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n757kzp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;seems pretty safe&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n757kzp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754439507,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 11
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "richtext",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": "ed89e5c6-72f1-11ee-9954-1697022cd89d",
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n75apvr",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "random-tomato",
            "can_mod_post": false,
            "created_utc": 1754440589,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 4,
            "author_fullname": "t2_fmd6oq5v6",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Just posting my numbers too! 5090 + 60GB of DDR5, 22 cpu moe layers offloaded:\n\n    srv  params_from_: Chat format: GPT-OSS\n    slot launch_slot_: id  0 | task 5966 | processing task\n    slot update_slots: id  0 | task 5966 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 112\n    slot update_slots: id  0 | task 5966 | kv cache rm [104, end)\n    slot update_slots: id  0 | task 5966 | prompt processing progress, n_past = 112, n_tokens = 8, progress = 0.071429\n    slot update_slots: id  0 | task 5966 | prompt done, n_past = 112, n_tokens = 8\n    slot      release: id  0 | task 5966 | stop processing: n_past = 970, truncated = 0\n    slot print_timing: id  0 | task 5966 | \n    prompt eval time =     167.26 ms /     8 tokens (   20.91 ms per token,    47.83 tokens per second)\n           eval time =   23464.92 ms /   859 tokens (   27.32 ms per token,    36.61 tokens per second)\n          total time =   23632.18 ms /   867 tokens",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75apvr",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [
              {
                "e": "text",
                "t": "llama.cpp"
              }
            ],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just posting my numbers too! 5090 + 60GB of DDR5, 22 cpu moe layers offloaded:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;srv  params_from_: Chat format: GPT-OSS\nslot launch_slot_: id  0 | task 5966 | processing task\nslot update_slots: id  0 | task 5966 | new prompt, n_ctx_slot = 32768, n_keep = 0, n_prompt_tokens = 112\nslot update_slots: id  0 | task 5966 | kv cache rm [104, end)\nslot update_slots: id  0 | task 5966 | prompt processing progress, n_past = 112, n_tokens = 8, progress = 0.071429\nslot update_slots: id  0 | task 5966 | prompt done, n_past = 112, n_tokens = 8\nslot      release: id  0 | task 5966 | stop processing: n_past = 970, truncated = 0\nslot print_timing: id  0 | task 5966 | \nprompt eval time =     167.26 ms /     8 tokens (   20.91 ms per token,    47.83 tokens per second)\n       eval time =   23464.92 ms /   859 tokens (   27.32 ms per token,    36.61 tokens per second)\n      total time =   23632.18 ms /   867 tokens\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": "light",
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75apvr/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754440589,
            "author_flair_text": "llama.cpp",
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": "#bbbdbf",
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 4
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n75um8q",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Pro-editor-1105",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n75h3zc",
                                "score": 3,
                                "author_fullname": "t2_uptissiz",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "7700x with a 64gb of ram.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n75um8q",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;7700x with a 64gb of ram.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1miprwe",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75um8q/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754447482,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754447482,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 3
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n75h3zc",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "gigaflops_",
                      "can_mod_post": false,
                      "created_utc": 1754442777,
                      "send_replies": true,
                      "parent_id": "t1_n75berx",
                      "score": 2,
                      "author_fullname": "t2_1t2n2s9f",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "MXFP4\n\nThis one: [https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF](https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF) \n\nhttps://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;format=png&amp;auto=webp&amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c\n\n  \nWhat CPU and system RAM speed do you have? Since a substantial amount of the model is still run on the CPU, I wonder if that could be your bottleneck?",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n75h3zc",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MXFP4&lt;/p&gt;\n\n&lt;p&gt;This one: &lt;a href=\"https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF\"&gt;https://huggingface.co/lmstudio-community/gpt-oss-120b-GGUF&lt;/a&gt; &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c\"&gt;https://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;What CPU and system RAM speed do you have? Since a substantial amount of the model is still run on the CPU, I wonder if that could be your bottleneck?&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miprwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75h3zc/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754442777,
                      "media_metadata": {
                        "rp0hn4aruahf1": {
                          "status": "valid",
                          "e": "Image",
                          "m": "image/png",
                          "p": [
                            {
                              "y": 83,
                              "x": 108,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a01bcee1c17aae70164b8aed2b7c73ff98ff6031"
                            },
                            {
                              "y": 166,
                              "x": 216,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ec7833521fc8619c7cb1ff04a4d057159b7f97c8"
                            },
                            {
                              "y": 246,
                              "x": 320,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=38e13e3b3100510917d7c261c63af7fc5ea62e41"
                            },
                            {
                              "y": 493,
                              "x": 640,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=44f6495b75b1034f57ae02d6bf402a62aaf1cb69"
                            },
                            {
                              "y": 739,
                              "x": 960,
                              "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=2842f10d7c8645f13e701176463afb1d98e29e15"
                            }
                          ],
                          "s": {
                            "y": 785,
                            "x": 1019,
                            "u": "https://preview.redd.it/rp0hn4aruahf1.png?width=1019&amp;format=png&amp;auto=webp&amp;s=ea3520c435640b23d50de365a7187e2b157e8e9c"
                          },
                          "id": "rp0hn4aruahf1"
                        }
                      },
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n75berx",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Pro-editor-1105",
            "can_mod_post": false,
            "created_utc": 1754440827,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 3,
            "author_fullname": "t2_uptissiz",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What quant are you using? I am using llama.cpp with unsloth and getting 8tps on a 4090 with 16k context. 64GB of ram.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75berx",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What quant are you using? I am using llama.cpp with unsloth and getting 8tps on a 4090 with 16k context. 64GB of ram.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75berx/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754440827,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n762hqu",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Abject-Ad-5400",
                      "can_mod_post": false,
                      "created_utc": 1754450416,
                      "send_replies": true,
                      "parent_id": "t1_n75u1bd",
                      "score": 1,
                      "author_fullname": "t2_6wm5hi6q",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "For the future noob who comes across this - Context window and KV Cache.   \n  \nlmstudio defaulted to lower context length letting me load it all on VRAM. I was dancing around making a custom model in Ollama earlier to adjust these params, but swapping to lmstudio did it for me. As soon as I turned the context up to 128k the model failed to load. Now to tweak it.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n762hqu",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For the future noob who comes across this - Context window and KV Cache.   &lt;/p&gt;\n\n&lt;p&gt;lmstudio defaulted to lower context length letting me load it all on VRAM. I was dancing around making a custom model in Ollama earlier to adjust these params, but swapping to lmstudio did it for me. As soon as I turned the context up to 128k the model failed to load. Now to tweak it.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miprwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n762hqu/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754450416,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n76g59m",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "LuciusCentauri",
                      "can_mod_post": false,
                      "created_utc": 1754456252,
                      "send_replies": true,
                      "parent_id": "t1_n75u1bd",
                      "score": 1,
                      "author_fullname": "t2_o9hjlici",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Also 10t/s on my 16Gb VRAM 3080 Laptop with ollama but 60t/s on m4 pro mac with lmstudio",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n76g59m",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Also 10t/s on my 16Gb VRAM 3080 Laptop with ollama but 60t/s on m4 pro mac with lmstudio&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miprwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n76g59m/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754456252,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n75u1bd",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Abject-Ad-5400",
            "can_mod_post": false,
            "created_utc": 1754447276,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 2,
            "author_fullname": "t2_6wm5hi6q",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "EDIT - see obvious fix in reply. Leaving this here for seo just incase\n\nMy jank 3080 10GB + 2070 8GB + 32GB DDR5 7800x3d was working pulling like 11 tok/sec on gpt-oss:20b via Ollama earlier this afternoon. Wanted to check out lm-studio, but performance was about the same.\n\nThen I updated to lm-studio 03.22 on linux and it's night and day. I have no clue what changed, I haven't tweaked anything but I'm at 68 tok/s now.  Best I can tell is that the whole model, or at least a greater portion is fitting on my 18GB VRAM. I'm a noob but wanted to post incase it saves anyone who tried and quit earlier today. So far I don't see anything online talking about this big of a performance jump out of the blue.\n\n68.83 tok/sec•2951 tokens•0.21s to first token•Stop reason: EOS Token Found\n\nWhen the model is loaded in lmstudio it shows \\~17gb loaded to vram and \\~1gb allocated to cpu, not sure if it's for the model or the app. Running on Ollama is still stuck in the dust using 7G gpu1, 5G gpu2, and 8G RAM. The difference is like 5-8min per query on ollama vs 20-30 seconds with lmstudio 03.22",
            "edited": 1754450835,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n75u1bd",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;EDIT - see obvious fix in reply. Leaving this here for seo just incase&lt;/p&gt;\n\n&lt;p&gt;My jank 3080 10GB + 2070 8GB + 32GB DDR5 7800x3d was working pulling like 11 tok/sec on gpt-oss:20b via Ollama earlier this afternoon. Wanted to check out lm-studio, but performance was about the same.&lt;/p&gt;\n\n&lt;p&gt;Then I updated to lm-studio 03.22 on linux and it&amp;#39;s night and day. I have no clue what changed, I haven&amp;#39;t tweaked anything but I&amp;#39;m at 68 tok/s now.  Best I can tell is that the whole model, or at least a greater portion is fitting on my 18GB VRAM. I&amp;#39;m a noob but wanted to post incase it saves anyone who tried and quit earlier today. So far I don&amp;#39;t see anything online talking about this big of a performance jump out of the blue.&lt;/p&gt;\n\n&lt;p&gt;68.83 tok/sec•2951 tokens•0.21s to first token•Stop reason: EOS Token Found&lt;/p&gt;\n\n&lt;p&gt;When the model is loaded in lmstudio it shows ~17gb loaded to vram and ~1gb allocated to cpu, not sure if it&amp;#39;s for the model or the app. Running on Ollama is still stuck in the dust using 7G gpu1, 5G gpu2, and 8G RAM. The difference is like 5-8min per query on ollama vs 20-30 seconds with lmstudio 03.22&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n75u1bd/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754447276,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n76eyot",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "logseventyseven",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n76eh5z",
                                "score": 1,
                                "author_fullname": "t2_x4ih8rkff",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "yeah, I'll just wait for 36-36-36-96 stock",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n76eyot",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;yeah, I&amp;#39;ll just wait for 36-36-36-96 stock&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1miprwe",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n76eyot/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754455705,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754455705,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            },
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n76iwo6",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "ProfessionUpbeat4500",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n76eh5z",
                                "score": 1,
                                "author_fullname": "t2_h79wu0k74",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "I own 64gb...96gb upgrade worth?\n\nI also own 4070 ti super 16gb",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n76iwo6",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I own 64gb...96gb upgrade worth?&lt;/p&gt;\n\n&lt;p&gt;I also own 4070 ti super 16gb&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1miprwe",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n76iwo6/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754457574,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754457574,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n76eh5z",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Crafty-Celery-2466",
                      "can_mod_post": false,
                      "created_utc": 1754455480,
                      "send_replies": true,
                      "parent_id": "t1_n76e31l",
                      "score": 1,
                      "author_fullname": "t2_8x13k917",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Not worth it adding more with different timings.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n76eh5z",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Not worth it adding more with different timings.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1miprwe",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n76eh5z/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754455480,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n76e31l",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "logseventyseven",
            "can_mod_post": false,
            "created_utc": 1754455296,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 2,
            "author_fullname": "t2_x4ih8rkff",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Hey man, I'm looking to upgrade my ram to 96 gigs to run this model. Currently I have 16x2 DDR5 6000Mhz CL36-36-36-96 sticks. I'm looking for a 32x2 pair but I'm only able to find 32x2 DDR5 6000Mhz CL30-40-40-96. I'm on AM5. Do you think it will work? I'm worried about stability because of the difference in timings.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76e31l",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Hey man, I&amp;#39;m looking to upgrade my ram to 96 gigs to run this model. Currently I have 16x2 DDR5 6000Mhz CL36-36-36-96 sticks. I&amp;#39;m looking for a 32x2 pair but I&amp;#39;m only able to find 32x2 DDR5 6000Mhz CL30-40-40-96. I&amp;#39;m on AM5. Do you think it will work? I&amp;#39;m worried about stability because of the difference in timings.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n76e31l/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754455296,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n757i8t",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "Admirable-Star7088",
            "can_mod_post": false,
            "created_utc": 1754439481,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 1,
            "author_fullname": "t2_qhlcbiy3k",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Yup, pretty fast model for its size, and despite that I do not use a fully functional quant yet it has performed overall really well for me, especially in creative writing where it's quite impressive.\n\nWill download a more stable and bug-free quant tomorrow and test this model some more.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n757i8t",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yup, pretty fast model for its size, and despite that I do not use a fully functional quant yet it has performed overall really well for me, especially in creative writing where it&amp;#39;s quite impressive.&lt;/p&gt;\n\n&lt;p&gt;Will download a more stable and bug-free quant tomorrow and test this model some more.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n757i8t/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754439481,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n76xh3g",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "fungnoth",
            "can_mod_post": false,
            "created_utc": 1754465309,
            "send_replies": true,
            "parent_id": "t3_1miprwe",
            "score": 1,
            "author_fullname": "t2_13v3uw",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "If that’s true, this might be their biggest contribution to the open source community. Others might be able to replicate how they make it inference so fast",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n76xh3g",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If that’s true, this might be their biggest contribution to the open source community. Others might be able to replicate how they make it inference so fast&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1miprwe/gptoss120b_performance_with_only_16_gb_vram/n76xh3g/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754465309,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1miprwe",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]