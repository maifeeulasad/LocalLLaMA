[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I'm getting about 15t/s generation speeds, but I've seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? ",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "Fastest way to run Qwen 3 30B A3B on 32GB RAM+10GB VRAM in LM Studio?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1mgocw6",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.82,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 7,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_3bzzdk93",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 7,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1754240445,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;Currently using 30B A3B on a Windows 11 system with 32GB DDR4, Ryzen 5600 and 3080 10GB and I&amp;#39;m getting about 15t/s generation speeds, but I&amp;#39;ve seen other people claim they can get 20t/s-25t/s. Is 15t/s typical for my setup or is there any way I can squeeze more speed out of it? &lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": false,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1mgocw6",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "yungfishstick",
            "discussion_type": null,
            "num_comments": 8,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/",
            "subreddit_subscribers": 509625,
            "created_utc": 1754240445,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n6qj0f7",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "DorphinPack",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n6qhh66",
                                          "score": 1,
                                          "author_fullname": "t2_zebuyjw9s",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "Ah that helps my mental model a lot thanks!",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n6qj0f7",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ah that helps my mental model a lot thanks!&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1mgocw6",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6qj0f7/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1754245778,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1754245778,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n6qhh66",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qcryd",
                                "score": 2,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "No, but probably very little.  When if comes to LLMs the main issue is that you need to read _billions_ of parameters per token.  These reads occur in large blocks so there isn't really any need to incur the latency that a small read/write to a totally random location in memory would.  Like a lot of things, RAM is somewhat pipelined so you can set up the next access while data is being read out to avoid latency.",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6qhh66",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;No, but probably very little.  When if comes to LLMs the main issue is that you need to read &lt;em&gt;billions&lt;/em&gt; of parameters per token.  These reads occur in large blocks so there isn&amp;#39;t really any need to incur the latency that a small read/write to a totally random location in memory would.  Like a lot of things, RAM is somewhat pipelined so you can set up the next access while data is being read out to avoid latency.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgocw6",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6qhh66/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754245308,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754245308,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 2
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6qcryd",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "DorphinPack",
                      "can_mod_post": false,
                      "created_utc": 1754243878,
                      "send_replies": true,
                      "parent_id": "t1_n6q63ww",
                      "score": 2,
                      "author_fullname": "t2_zebuyjw9s",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Do you know how much timings affect this workload?\n\nI remember being taught that cutting your latency by getting the right timings would yield more “speed” than chasing higher freqs to the detriment of timing.\n\nSpeed is in quotes because this was from before I REALLY grokked how to optimize a workload.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6qcryd",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Do you know how much timings affect this workload?&lt;/p&gt;\n\n&lt;p&gt;I remember being taught that cutting your latency by getting the right timings would yield more “speed” than chasing higher freqs to the detriment of timing.&lt;/p&gt;\n\n&lt;p&gt;Speed is in quotes because this was from before I REALLY grokked how to optimize a workload.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgocw6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6qcryd/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754243878,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 2
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": "",
                                "user_reports": [],
                                "saved": false,
                                "id": "n6qqvoe",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": false,
                                "author": "eloquentemu",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n6qaxzj",
                                "score": 4,
                                "author_fullname": "t2_lpdsy",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "Suppose you offload everything but the experts.  It's 48 layers, 8 active experts, experts are 3\\*768\\*2048.  Supposing you're using Q4_K each expert is 2.9MB (one actually ends up q6 due to block sizing).  48\\*8\\*2.9 = 1120MB of experts per token.  Your 51GBps memory / 1.12GB/tok gives 45t/s *in theory*.\n\nNow, most Q4 MoE seem to benchmark at roughly half theoretical bandwidth but those are larger.  The 30B-A3B has such a small number of active parameters that all the little pipeline stalls really hurt it.  Like benchmarking CPU only on my machine (it's an Epyc with much faster memory so don't expect this to exactly map to yours):\n\n| model                          |       size |     params | backend    |    test |           t/s |\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: |\n| qwen3moe 30B.A3B Q4_K - Medium |  17.28 GiB |    30.53 B | CPU        |   tg128 |  49.24 ± 0.00 |\n| qwen3moe 30B.A3B Q8_0          |  30.25 GiB |    30.53 B | CPU        |   tg128 |  42.41 ± 0.11 |\n| qwen3moe 30B.A3B BF16          |  56.89 GiB |    30.53 B | CPU        |   tg128 |  29.53 ± 0.00 |\n| qwen3moe 30B.A3B all F32       | 113.74 GiB |    30.53 B | CPU        |   tg128 |  18.86 ± 0.07 |\n\nYou can see that the difference between Q4 and Q8 is basically nothing, meaning that memory bandwidth isn't limiting for Q4 30B-A3B for most operations.  The bf16 probably shows the largest discrepancy 3.3x larger but the Q4 is only 1.5x faster.  Only the silly float32 \"quant\" got me to ~50% of theoretical bandwidth like I'd see for 235B-A22B.  Supposing your CPU ends up with similar efficiency as mine (I get ~75t/s with GPU), you'd maybe expect ~14t/s with everything but the experts on GPU.  Since that's only like 1.5GB of VRAM you should be able to offload several layers worth of experts too, and get a solid speedup from that.  You can probably only fit about 1/3 in your remaining VRAM so maybe 14*3/2 = 21t/s which is in the ballpark of others' claims.",
                                "edited": 1754250134,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n6qqvoe",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Suppose you offload everything but the experts.  It&amp;#39;s 48 layers, 8 active experts, experts are 3*768*2048.  Supposing you&amp;#39;re using Q4_K each expert is 2.9MB (one actually ends up q6 due to block sizing).  48*8*2.9 = 1120MB of experts per token.  Your 51GBps memory / 1.12GB/tok gives 45t/s &lt;em&gt;in theory&lt;/em&gt;.&lt;/p&gt;\n\n&lt;p&gt;Now, most Q4 MoE seem to benchmark at roughly half theoretical bandwidth but those are larger.  The 30B-A3B has such a small number of active parameters that all the little pipeline stalls really hurt it.  Like benchmarking CPU only on my machine (it&amp;#39;s an Epyc with much faster memory so don&amp;#39;t expect this to exactly map to yours):&lt;/p&gt;\n\n&lt;table&gt;&lt;thead&gt;\n&lt;tr&gt;\n&lt;th&gt;model&lt;/th&gt;\n&lt;th align=\"right\"&gt;size&lt;/th&gt;\n&lt;th align=\"right\"&gt;params&lt;/th&gt;\n&lt;th&gt;backend&lt;/th&gt;\n&lt;th align=\"right\"&gt;test&lt;/th&gt;\n&lt;th align=\"right\"&gt;t/s&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt;\n&lt;td align=\"right\"&gt;17.28 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;30.53 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"right\"&gt;49.24 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt;\n&lt;td align=\"right\"&gt;30.25 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;30.53 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"right\"&gt;42.41 ± 0.11&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3moe 30B.A3B BF16&lt;/td&gt;\n&lt;td align=\"right\"&gt;56.89 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;30.53 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"right\"&gt;29.53 ± 0.00&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;qwen3moe 30B.A3B all F32&lt;/td&gt;\n&lt;td align=\"right\"&gt;113.74 GiB&lt;/td&gt;\n&lt;td align=\"right\"&gt;30.53 B&lt;/td&gt;\n&lt;td&gt;CPU&lt;/td&gt;\n&lt;td align=\"right\"&gt;tg128&lt;/td&gt;\n&lt;td align=\"right\"&gt;18.86 ± 0.07&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;&lt;/table&gt;\n\n&lt;p&gt;You can see that the difference between Q4 and Q8 is basically nothing, meaning that memory bandwidth isn&amp;#39;t limiting for Q4 30B-A3B for most operations.  The bf16 probably shows the largest discrepancy 3.3x larger but the Q4 is only 1.5x faster.  Only the silly float32 &amp;quot;quant&amp;quot; got me to ~50% of theoretical bandwidth like I&amp;#39;d see for 235B-A22B.  Supposing your CPU ends up with similar efficiency as mine (I get ~75t/s with GPU), you&amp;#39;d maybe expect ~14t/s with everything but the experts on GPU.  Since that&amp;#39;s only like 1.5GB of VRAM you should be able to offload several layers worth of experts too, and get a solid speedup from that.  You can probably only fit about 1/3 in your remaining VRAM so maybe 14*3/2 = 21t/s which is in the ballpark of others&amp;#39; claims.&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1mgocw6",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6qqvoe/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1754248219,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1754248219,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 4
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n6qaxzj",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "yungfishstick",
                      "can_mod_post": false,
                      "created_utc": 1754243339,
                      "send_replies": true,
                      "parent_id": "t1_n6q63ww",
                      "score": 1,
                      "author_fullname": "t2_3bzzdk93",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "3200mhz",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n6qaxzj",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;3200mhz&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1mgocw6",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6qaxzj/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1754243339,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n6q63ww",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "eloquentemu",
            "can_mod_post": false,
            "created_utc": 1754241902,
            "send_replies": true,
            "parent_id": "t3_1mgocw6",
            "score": 3,
            "author_fullname": "t2_lpdsy",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "DDR4 has a pretty wide range of frequencies from like 2166 to 3600+.  At a minimum confirm you are running &gt;3000 and you might want to look up a memory tuning guide.  Without knowing what freq you're running, I can't estimate theoretical.\n\nBeyond that.. How are you running now?  The current best way to run MoE models on mixed CPU+GPU is to offload everything but the experts to GPU with `-ngl 99 -ot '[2-9][0-9].*exps=CPU'`.  That's 99 layers on GPU (i.e. all) with override-tensors putting the experts from layers 20+ on CPU which uses about 8.5GB on my machine with Q4_K_M and ~0 context.   Not sure how to map that to LM Studio, but I think it uses the same arguments.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6q63ww",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;DDR4 has a pretty wide range of frequencies from like 2166 to 3600+.  At a minimum confirm you are running &amp;gt;3000 and you might want to look up a memory tuning guide.  Without knowing what freq you&amp;#39;re running, I can&amp;#39;t estimate theoretical.&lt;/p&gt;\n\n&lt;p&gt;Beyond that.. How are you running now?  The current best way to run MoE models on mixed CPU+GPU is to offload everything but the experts to GPU with &lt;code&gt;-ngl 99 -ot &amp;#39;[2-9][0-9].*exps=CPU&amp;#39;&lt;/code&gt;.  That&amp;#39;s 99 layers on GPU (i.e. all) with override-tensors putting the experts from layers 20+ on CPU which uses about 8.5GB on my machine with Q4_K_M and ~0 context.   Not sure how to map that to LM Studio, but I think it uses the same arguments.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6q63ww/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754241902,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgocw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6q8zph",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "darkassassinclone",
            "can_mod_post": false,
            "created_utc": 1754242758,
            "send_replies": true,
            "parent_id": "t3_1mgocw6",
            "score": 3,
            "author_fullname": "t2_n2tld",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have the exact same setup as you. I get about 22T/s. Here's what I use in a bat file (change the model path for your setup):\n\n    set LLAMA_SET_ROWS=1\n    llama-server.exe --api-key 1 -a qwen3 -m \"D:\\models\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf\" -ngl 999 -ot \"blk.(1[8-9]|[2-4][0-9]).ffn_.*._exps.=CPU\" -ub 768 -b 4096 -c 40960 -ctk q5_1 -ctv q5_1 -fa",
            "edited": 1754243335,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6q8zph",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have the exact same setup as you. I get about 22T/s. Here&amp;#39;s what I use in a bat file (change the model path for your setup):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;set LLAMA_SET_ROWS=1\nllama-server.exe --api-key 1 -a qwen3 -m &amp;quot;D:\\models\\Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf&amp;quot; -ngl 999 -ot &amp;quot;blk.(1[8-9]|[2-4][0-9]).ffn_.*._exps.=CPU&amp;quot; -ub 768 -b 4096 -c 40960 -ctk q5_1 -ctv q5_1 -fa\n&lt;/code&gt;&lt;/pre&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6q8zph/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754242758,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgocw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n6qphgw",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "tvetus",
            "can_mod_post": false,
            "created_utc": 1754247785,
            "send_replies": true,
            "parent_id": "t3_1mgocw6",
            "score": 1,
            "author_fullname": "t2_7ugvr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "What do you mean by fastest? response time or total tokens per second? Some inference frameworks are optimized for low latency responses. vLLM is good for parallel batch inference.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n6qphgw",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What do you mean by fastest? response time or total tokens per second? Some inference frameworks are optimized for low latency responses. vLLM is good for parallel batch inference.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1mgocw6/fastest_way_to_run_qwen_3_30b_a3b_on_32gb_ram10gb/n6qphgw/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1754247785,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1mgocw6",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]