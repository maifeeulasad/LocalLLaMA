[
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": 1,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t3",
          "data": {
            "approved_at_utc": null,
            "subreddit": "LocalLLaMA",
            "selftext": "(Rig lol) I’ve got a 2x 3090 with 128gb of Ram on a 16 core ryzen 9. What should I use so that I can fully load the GPUs and also the CPU/RAM? Will ollama automatically use what I put in front of it?\n\nI need to be able to use it to provide a local API on my network.",
            "user_reports": [],
            "saved": false,
            "mod_reason_title": null,
            "gilded": 0,
            "clicked": false,
            "title": "What inference engine should I use to fully use my budget rug?",
            "link_flair_richtext": [
              {
                "e": "text",
                "t": "Question | Help"
              }
            ],
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "hidden": false,
            "pwls": 6,
            "link_flair_css_class": "",
            "downs": 0,
            "thumbnail_height": null,
            "top_awarded_type": null,
            "hide_score": false,
            "name": "t3_1madt6e",
            "quarantine": false,
            "link_flair_text_color": "dark",
            "upvote_ratio": 0.2,
            "author_flair_background_color": null,
            "subreddit_type": "public",
            "ups": 0,
            "total_awards_received": 0,
            "media_embed": {},
            "thumbnail_width": null,
            "author_flair_template_id": null,
            "is_original_content": false,
            "author_fullname": "t2_2zxxuvbk",
            "secure_media": null,
            "is_reddit_media_domain": false,
            "is_meta": false,
            "category": null,
            "secure_media_embed": {},
            "link_flair_text": "Question | Help",
            "can_mod_post": false,
            "score": 0,
            "approved_by": null,
            "is_created_from_ads_ui": false,
            "author_premium": false,
            "thumbnail": "self",
            "edited": false,
            "author_flair_css_class": null,
            "author_flair_richtext": [],
            "gildings": {},
            "content_categories": null,
            "is_self": true,
            "mod_note": null,
            "created": 1753592339,
            "link_flair_type": "richtext",
            "wls": 6,
            "removed_by_category": null,
            "banned_by": null,
            "author_flair_type": "text",
            "domain": "self.LocalLLaMA",
            "allow_live_comments": false,
            "selftext_html": "&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;(Rig lol) I’ve got a 2x 3090 with 128gb of Ram on a 16 core ryzen 9. What should I use so that I can fully load the GPUs and also the CPU/RAM? Will ollama automatically use what I put in front of it?&lt;/p&gt;\n\n&lt;p&gt;I need to be able to use it to provide a local API on my network.&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;",
            "likes": null,
            "suggested_sort": null,
            "banned_at_utc": null,
            "view_count": null,
            "archived": false,
            "no_follow": true,
            "is_crosspostable": false,
            "pinned": false,
            "over_18": false,
            "all_awardings": [],
            "awarders": [],
            "media_only": false,
            "link_flair_template_id": "2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",
            "can_gild": false,
            "spoiler": false,
            "locked": false,
            "author_flair_text": null,
            "treatment_tags": [],
            "visited": false,
            "removed_by": null,
            "num_reports": null,
            "distinguished": null,
            "subreddit_id": "t5_81eyvm",
            "author_is_blocked": false,
            "mod_reason_by": null,
            "removal_reason": null,
            "link_flair_background_color": "#5a74cc",
            "id": "1madt6e",
            "is_robot_indexable": true,
            "num_duplicates": 0,
            "report_reasons": null,
            "author": "bidet_enthusiast",
            "discussion_type": null,
            "num_comments": 13,
            "send_replies": true,
            "media": null,
            "contest_mode": false,
            "author_patreon_flair": false,
            "author_flair_text_color": null,
            "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/",
            "stickied": false,
            "url": "https://www.reddit.com/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/",
            "subreddit_subscribers": 505616,
            "created_utc": 1753592339,
            "num_crossposts": 0,
            "mod_reports": [],
            "is_video": false
          }
        }
      ],
      "before": null
    }
  },
  {
    "kind": "Listing",
    "data": {
      "after": null,
      "dist": null,
      "modhash": "",
      "geo_filter": "",
      "children": [
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "richtext",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": "50c36eba-fdca-11ee-9735-92a88d7e3b87",
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5e8utg",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "-finnegannn-",
                      "can_mod_post": false,
                      "created_utc": 1753600267,
                      "send_replies": true,
                      "parent_id": "t1_n5dwcb6",
                      "score": 1,
                      "author_fullname": "t2_15923syckr",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "140 is wild… I need to try out VLLM… I’ve been using LM Studio and I’ve tried Ollama for my dual 3090 system, but I’ve never been able to use VLLM as it’s my main pc when it’s not being used for inference… maybe I need to dual boot Linux and give it a go… when the 30b is split across both GPUs at say Q6_K, I only get around 50 tok/s",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5e8utg",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [
                        {
                          "e": "text",
                          "t": "Ollama"
                        }
                      ],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;140 is wild… I need to try out VLLM… I’ve been using LM Studio and I’ve tried Ollama for my dual 3090 system, but I’ve never been able to use VLLM as it’s my main pc when it’s not being used for inference… maybe I need to dual boot Linux and give it a go… when the 30b is split across both GPUs at say Q6_K, I only get around 50 tok/s&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1madt6e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": "light",
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5e8utg/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753600267,
                      "author_flair_text": "Ollama",
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": "#bbbdbf",
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ehl5s",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "plankalkul-z1",
                      "can_mod_post": false,
                      "created_utc": 1753605364,
                      "send_replies": true,
                      "parent_id": "t1_n5dwcb6",
                      "score": 1,
                      "author_fullname": "t2_w73n3yrsx",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "&gt; Ollama will make use of the extra vRAM but not really the compute. From what I understand it doesn’t really support true tensor parallelism - neither does Llama.cpp from what I gather.\n\n\nThat is correct.\n\n\nOllama is fantastic in making use of all available memory (VRAM + RAM) fully automatically, but it won't help with compute on multi-GPU setups, at all.\n\n\nllama.cpp has tensor *splitting* mode that adds 10..15% of performance (on my setup, 2x RTX6000 Ada; YMMV), but that's a far cry from what is achievable with proper tensor parallelism.\n\n\nSo... For a multi-GPU setup with *same types of GPUs where number of them is a power of 2* (like OP's 2x 3090) an inference engine supporting tensor parallelism is highly recommended: like, say, vLLM or SGLang.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ehl5s",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Ollama will make use of the extra vRAM but not really the compute. From what I understand it doesn’t really support true tensor parallelism - neither does Llama.cpp from what I gather.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That is correct.&lt;/p&gt;\n\n&lt;p&gt;Ollama is fantastic in making use of all available memory (VRAM + RAM) fully automatically, but it won&amp;#39;t help with compute on multi-GPU setups, at all.&lt;/p&gt;\n\n&lt;p&gt;llama.cpp has tensor &lt;em&gt;splitting&lt;/em&gt; mode that adds 10..15% of performance (on my setup, 2x RTX6000 Ada; YMMV), but that&amp;#39;s a far cry from what is achievable with proper tensor parallelism.&lt;/p&gt;\n\n&lt;p&gt;So... For a multi-GPU setup with &lt;em&gt;same types of GPUs where number of them is a power of 2&lt;/em&gt; (like OP&amp;#39;s 2x 3090) an inference engine supporting tensor parallelism is highly recommended: like, say, vLLM or SGLang.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1madt6e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5ehl5s/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753605364,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5fehzv",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bidet_enthusiast",
                      "can_mod_post": false,
                      "created_utc": 1753621921,
                      "send_replies": true,
                      "parent_id": "t1_n5dwcb6",
                      "score": 1,
                      "author_fullname": "t2_2zxxuvbk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thanks for the tips, I will try this with vllm. My mono is running pcie4, so hopefully that will give me decent interconnect.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5fehzv",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks for the tips, I will try this with vllm. My mono is running pcie4, so hopefully that will give me decent interconnect.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1madt6e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5fehzv/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753621921,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  },
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": {
                        "kind": "Listing",
                        "data": {
                          "after": null,
                          "dist": null,
                          "modhash": "",
                          "geo_filter": "",
                          "children": [
                            {
                              "kind": "t1",
                              "data": {
                                "subreddit_id": "t5_81eyvm",
                                "approved_at_utc": null,
                                "author_is_blocked": false,
                                "comment_type": null,
                                "awarders": [],
                                "mod_reason_by": null,
                                "banned_by": null,
                                "author_flair_type": "text",
                                "total_awards_received": 0,
                                "subreddit": "LocalLLaMA",
                                "author_flair_template_id": null,
                                "likes": null,
                                "replies": {
                                  "kind": "Listing",
                                  "data": {
                                    "after": null,
                                    "dist": null,
                                    "modhash": "",
                                    "geo_filter": "",
                                    "children": [
                                      {
                                        "kind": "t1",
                                        "data": {
                                          "subreddit_id": "t5_81eyvm",
                                          "approved_at_utc": null,
                                          "author_is_blocked": false,
                                          "comment_type": null,
                                          "awarders": [],
                                          "mod_reason_by": null,
                                          "banned_by": null,
                                          "author_flair_type": "text",
                                          "total_awards_received": 0,
                                          "subreddit": "LocalLLaMA",
                                          "author_flair_template_id": null,
                                          "likes": null,
                                          "replies": "",
                                          "user_reports": [],
                                          "saved": false,
                                          "id": "n5gjgis",
                                          "banned_at_utc": null,
                                          "mod_reason_title": null,
                                          "gilded": 0,
                                          "archived": false,
                                          "collapsed_reason_code": null,
                                          "no_follow": true,
                                          "author": "Lazy-Pattern-5171",
                                          "can_mod_post": false,
                                          "send_replies": true,
                                          "parent_id": "t1_n5g6wgx",
                                          "score": 1,
                                          "author_fullname": "t2_1lyjk8is25",
                                          "removal_reason": null,
                                          "approved_by": null,
                                          "mod_note": null,
                                          "all_awardings": [],
                                          "collapsed": false,
                                          "body": "For some reason I thought this does open a ui due to the port selection of 8000. So in context of an LLM an MCP is just json ?",
                                          "edited": false,
                                          "top_awarded_type": null,
                                          "author_flair_css_class": null,
                                          "name": "t1_n5gjgis",
                                          "is_submitter": false,
                                          "downs": 0,
                                          "author_flair_richtext": [],
                                          "author_patreon_flair": false,
                                          "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;For some reason I thought this does open a ui due to the port selection of 8000. So in context of an LLM an MCP is just json ?&lt;/p&gt;\n&lt;/div&gt;",
                                          "gildings": {},
                                          "collapsed_reason": null,
                                          "distinguished": null,
                                          "associated_award": null,
                                          "stickied": false,
                                          "author_premium": false,
                                          "can_gild": false,
                                          "link_id": "t3_1madt6e",
                                          "unrepliable_reason": null,
                                          "author_flair_text_color": null,
                                          "score_hidden": false,
                                          "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5gjgis/",
                                          "subreddit_type": "public",
                                          "locked": false,
                                          "report_reasons": null,
                                          "created": 1753634786,
                                          "author_flair_text": null,
                                          "treatment_tags": [],
                                          "created_utc": 1753634786,
                                          "subreddit_name_prefixed": "r/LocalLLaMA",
                                          "controversiality": 0,
                                          "depth": 3,
                                          "author_flair_background_color": null,
                                          "collapsed_because_crowd_control": null,
                                          "mod_reports": [],
                                          "num_reports": null,
                                          "ups": 1
                                        }
                                      }
                                    ],
                                    "before": null
                                  }
                                },
                                "user_reports": [],
                                "saved": false,
                                "id": "n5g6wgx",
                                "banned_at_utc": null,
                                "mod_reason_title": null,
                                "gilded": 0,
                                "archived": false,
                                "collapsed_reason_code": null,
                                "no_follow": true,
                                "author": "Tyme4Trouble",
                                "can_mod_post": false,
                                "send_replies": true,
                                "parent_id": "t1_n5flusk",
                                "score": 1,
                                "author_fullname": "t2_973amyap",
                                "removal_reason": null,
                                "approved_by": null,
                                "mod_note": null,
                                "all_awardings": [],
                                "body": "MCP follows a client server architecture. vLLM can work with MCP if the client supports it, but it doesn’t support it on its own. \n\nQwen3 uses the same tool calling format as Hermes so that’s what’s used by vLLM. \n\nhttps://docs.vllm.ai/en/stable/features/tool_calling.html#xlam-models-xlam",
                                "edited": false,
                                "top_awarded_type": null,
                                "downs": 0,
                                "author_flair_css_class": null,
                                "name": "t1_n5g6wgx",
                                "is_submitter": false,
                                "collapsed": false,
                                "author_flair_richtext": [],
                                "author_patreon_flair": false,
                                "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;MCP follows a client server architecture. vLLM can work with MCP if the client supports it, but it doesn’t support it on its own. &lt;/p&gt;\n\n&lt;p&gt;Qwen3 uses the same tool calling format as Hermes so that’s what’s used by vLLM. &lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://docs.vllm.ai/en/stable/features/tool_calling.html#xlam-models-xlam\"&gt;https://docs.vllm.ai/en/stable/features/tool_calling.html#xlam-models-xlam&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;",
                                "gildings": {},
                                "collapsed_reason": null,
                                "distinguished": null,
                                "associated_award": null,
                                "stickied": false,
                                "author_premium": false,
                                "can_gild": false,
                                "link_id": "t3_1madt6e",
                                "unrepliable_reason": null,
                                "author_flair_text_color": null,
                                "score_hidden": false,
                                "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5g6wgx/",
                                "subreddit_type": "public",
                                "locked": false,
                                "report_reasons": null,
                                "created": 1753631110,
                                "author_flair_text": null,
                                "treatment_tags": [],
                                "created_utc": 1753631110,
                                "subreddit_name_prefixed": "r/LocalLLaMA",
                                "controversiality": 0,
                                "depth": 2,
                                "author_flair_background_color": null,
                                "collapsed_because_crowd_control": null,
                                "mod_reports": [],
                                "num_reports": null,
                                "ups": 1
                              }
                            }
                          ],
                          "before": null
                        }
                      },
                      "user_reports": [],
                      "saved": false,
                      "id": "n5flusk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "Lazy-Pattern-5171",
                      "can_mod_post": false,
                      "created_utc": 1753624502,
                      "send_replies": true,
                      "parent_id": "t1_n5dwcb6",
                      "score": 1,
                      "author_fullname": "t2_1lyjk8is25",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Does vllm support mcp? and what is Hermes tool calling. So many questions. Congratulations on 140",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5flusk",
                      "is_submitter": false,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Does vllm support mcp? and what is Hermes tool calling. So many questions. Congratulations on 140&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1madt6e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5flusk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753624502,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5dwcb6",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": false,
            "author": "Tyme4Trouble",
            "can_mod_post": false,
            "created_utc": 1753593435,
            "send_replies": true,
            "parent_id": "t3_1madt6e",
            "score": 3,
            "author_fullname": "t2_973amyap",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "I have a pretty similar setup. Ollama will make use of the extra vRAM but not really the compute. From what I understand it doesn’t really support true tensor parallelism - neither does Llama.cpp from what I gather. \n\nI’m using vLLM. Here’s the runner I’m using for Qwen3-30B at INT8 weights and activations.\n\n```\nvllm serve ramblingpolymath/Qwen3-30B-A3B-W8A8   --host 0.0.0.0   --port 8000   --tensor-parallel-size 2   --gpu-memory-utilization 0.9   --max-model-len 131072   --rope-scaling '{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}'   --max-num-seqs 8   --trust-remote-code   --disable-log-requests   --enable-chunked-prefill   --max-num-batched-tokens 512   --cuda-graph-sizes 8   --enable-prefix-caching   --max-seq-len-to-capture 32768   --enable-auto-tool-choice   --tool-call-parser hermes\n\n```\n\nWhat is the PCIe connectivity for the 3090s? If PCIe 4.0 x8 for each you’re probably fine. On mine it’s PCIe 3.0 x16 and x4 which bottlenecked tensor parallel performance on smaller models and MoE models like Qwen3-30B. In the case of the latter, an NVLink bridge pushed me from 100-140 tok/s.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5dwcb6",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I have a pretty similar setup. Ollama will make use of the extra vRAM but not really the compute. From what I understand it doesn’t really support true tensor parallelism - neither does Llama.cpp from what I gather. &lt;/p&gt;\n\n&lt;p&gt;I’m using vLLM. Here’s the runner I’m using for Qwen3-30B at INT8 weights and activations.&lt;/p&gt;\n\n&lt;p&gt;```\nvllm serve ramblingpolymath/Qwen3-30B-A3B-W8A8   --host 0.0.0.0   --port 8000   --tensor-parallel-size 2   --gpu-memory-utilization 0.9   --max-model-len 131072   --rope-scaling &amp;#39;{&amp;quot;rope_type&amp;quot;:&amp;quot;yarn&amp;quot;,&amp;quot;factor&amp;quot;:4.0,&amp;quot;original_max_position_embeddings&amp;quot;:32768}&amp;#39;   --max-num-seqs 8   --trust-remote-code   --disable-log-requests   --enable-chunked-prefill   --max-num-batched-tokens 512   --cuda-graph-sizes 8   --enable-prefix-caching   --max-seq-len-to-capture 32768   --enable-auto-tool-choice   --tool-call-parser hermes&lt;/p&gt;\n\n&lt;p&gt;```&lt;/p&gt;\n\n&lt;p&gt;What is the PCIe connectivity for the 3090s? If PCIe 4.0 x8 for each you’re probably fine. On mine it’s PCIe 3.0 x16 and x4 which bottlenecked tensor parallel performance on smaller models and MoE models like Qwen3-30B. In the case of the latter, an NVLink bridge pushed me from 100-140 tok/s.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5dwcb6/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753593435,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1madt6e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 3
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5feua6",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bidet_enthusiast",
                      "can_mod_post": false,
                      "created_utc": 1753622043,
                      "send_replies": true,
                      "parent_id": "t1_n5ejlde",
                      "score": 1,
                      "author_fullname": "t2_2zxxuvbk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thank you for the tips! This gives me some stuff to deep dive, I’m sure I’ll figure out what will be best along the way.",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5feua6",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you for the tips! This gives me some stuff to deep dive, I’m sure I’ll figure out what will be best along the way.&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1madt6e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5feua6/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753622043,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ejlde",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "No_Edge2098",
            "can_mod_post": false,
            "created_utc": 1753606564,
            "send_replies": true,
            "parent_id": "t3_1madt6e",
            "score": 2,
            "author_fullname": "t2_uaotuj04",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "You’ve got a monster rig, not a rug  Ollama’s great for plug-and-play, but it won’t max out both 3090s and that beefy CPU/RAM out of the box. For full control and GPU parallelism, look into **vLLM**, **text-generation-webui** with **ExLlama**, or **TGI**. Set up inference with model parallel or tensor parallelism via **DeepSpeed** or **Ray Serve** if needed. Then front it with FastAPI or LM Studio for a local API. Basically: Ollama for ease, vLLM + ExLlama for full send.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ejlde",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;You’ve got a monster rig, not a rug  Ollama’s great for plug-and-play, but it won’t max out both 3090s and that beefy CPU/RAM out of the box. For full control and GPU parallelism, look into &lt;strong&gt;vLLM&lt;/strong&gt;, &lt;strong&gt;text-generation-webui&lt;/strong&gt; with &lt;strong&gt;ExLlama&lt;/strong&gt;, or &lt;strong&gt;TGI&lt;/strong&gt;. Set up inference with model parallel or tensor parallelism via &lt;strong&gt;DeepSpeed&lt;/strong&gt; or &lt;strong&gt;Ray Serve&lt;/strong&gt; if needed. Then front it with FastAPI or LM Studio for a local API. Basically: Ollama for ease, vLLM + ExLlama for full send.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5ejlde/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753606564,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1madt6e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5e1a42",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "GPTshop_ai",
            "can_mod_post": false,
            "created_utc": 1753596039,
            "send_replies": true,
            "parent_id": "t3_1madt6e",
            "score": 2,
            "author_fullname": "t2_rkmud0isr",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "just try every single one, then you will see. there aren't too many.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5e1a42",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;just try every single one, then you will see. there aren&amp;#39;t too many.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5e1a42/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753596039,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1madt6e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": {
              "kind": "Listing",
              "data": {
                "after": null,
                "dist": null,
                "modhash": "",
                "geo_filter": "",
                "children": [
                  {
                    "kind": "t1",
                    "data": {
                      "subreddit_id": "t5_81eyvm",
                      "approved_at_utc": null,
                      "author_is_blocked": false,
                      "comment_type": null,
                      "awarders": [],
                      "mod_reason_by": null,
                      "banned_by": null,
                      "author_flair_type": "text",
                      "total_awards_received": 0,
                      "subreddit": "LocalLLaMA",
                      "author_flair_template_id": null,
                      "likes": null,
                      "replies": "",
                      "user_reports": [],
                      "saved": false,
                      "id": "n5ff0zk",
                      "banned_at_utc": null,
                      "mod_reason_title": null,
                      "gilded": 0,
                      "archived": false,
                      "collapsed_reason_code": null,
                      "no_follow": true,
                      "author": "bidet_enthusiast",
                      "can_mod_post": false,
                      "created_utc": 1753622109,
                      "send_replies": true,
                      "parent_id": "t1_n5ehvlm",
                      "score": 1,
                      "author_fullname": "t2_2zxxuvbk",
                      "removal_reason": null,
                      "approved_by": null,
                      "mod_note": null,
                      "all_awardings": [],
                      "body": "Thank you!",
                      "edited": false,
                      "top_awarded_type": null,
                      "author_flair_css_class": null,
                      "name": "t1_n5ff0zk",
                      "is_submitter": true,
                      "downs": 0,
                      "author_flair_richtext": [],
                      "author_patreon_flair": false,
                      "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thank you!&lt;/p&gt;\n&lt;/div&gt;",
                      "gildings": {},
                      "collapsed_reason": null,
                      "distinguished": null,
                      "associated_award": null,
                      "stickied": false,
                      "author_premium": false,
                      "can_gild": false,
                      "link_id": "t3_1madt6e",
                      "unrepliable_reason": null,
                      "author_flair_text_color": null,
                      "score_hidden": false,
                      "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5ff0zk/",
                      "subreddit_type": "public",
                      "locked": false,
                      "report_reasons": null,
                      "created": 1753622109,
                      "author_flair_text": null,
                      "treatment_tags": [],
                      "collapsed": false,
                      "subreddit_name_prefixed": "r/LocalLLaMA",
                      "controversiality": 0,
                      "depth": 1,
                      "author_flair_background_color": null,
                      "collapsed_because_crowd_control": null,
                      "mod_reports": [],
                      "num_reports": null,
                      "ups": 1
                    }
                  }
                ],
                "before": null
              }
            },
            "user_reports": [],
            "saved": false,
            "id": "n5ehvlm",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "SandboChang",
            "can_mod_post": false,
            "created_utc": 1753605540,
            "send_replies": true,
            "parent_id": "t3_1madt6e",
            "score": 2,
            "author_fullname": "t2_10icmj",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "https://github.com/turboderp-org/exllamav3\n\nExllamav3 should be the fastest for single user. \n\nYou can try using TabbyAPI to run it:\n\nhttps://github.com/theroyallab/tabbyAPI/\n\nIf you will be serving more users then vLLM/SGLang maybe better options.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5ehvlm",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"https://github.com/turboderp-org/exllamav3\"&gt;https://github.com/turboderp-org/exllamav3&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;Exllamav3 should be the fastest for single user. &lt;/p&gt;\n\n&lt;p&gt;You can try using TabbyAPI to run it:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://github.com/theroyallab/tabbyAPI/\"&gt;https://github.com/theroyallab/tabbyAPI/&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;If you will be serving more users then vLLM/SGLang maybe better options.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5ehvlm/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753605540,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1madt6e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 2
          }
        },
        {
          "kind": "t1",
          "data": {
            "subreddit_id": "t5_81eyvm",
            "approved_at_utc": null,
            "author_is_blocked": false,
            "comment_type": null,
            "awarders": [],
            "mod_reason_by": null,
            "banned_by": null,
            "author_flair_type": "text",
            "total_awards_received": 0,
            "subreddit": "LocalLLaMA",
            "author_flair_template_id": null,
            "likes": null,
            "replies": "",
            "user_reports": [],
            "saved": false,
            "id": "n5dwufp",
            "banned_at_utc": null,
            "mod_reason_title": null,
            "gilded": 0,
            "archived": false,
            "collapsed_reason_code": null,
            "no_follow": true,
            "author": "NNN_Throwaway2",
            "can_mod_post": false,
            "created_utc": 1753593694,
            "send_replies": true,
            "parent_id": "t3_1madt6e",
            "score": 1,
            "author_fullname": "t2_8rrihts9",
            "approved_by": null,
            "mod_note": null,
            "all_awardings": [],
            "collapsed": false,
            "body": "Something other than ollama.",
            "edited": false,
            "top_awarded_type": null,
            "author_flair_css_class": null,
            "name": "t1_n5dwufp",
            "is_submitter": false,
            "downs": 0,
            "author_flair_richtext": [],
            "author_patreon_flair": false,
            "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Something other than ollama.&lt;/p&gt;\n&lt;/div&gt;",
            "removal_reason": null,
            "collapsed_reason": null,
            "distinguished": null,
            "associated_award": null,
            "stickied": false,
            "author_premium": false,
            "can_gild": false,
            "gildings": {},
            "unrepliable_reason": null,
            "author_flair_text_color": null,
            "score_hidden": false,
            "permalink": "/r/LocalLLaMA/comments/1madt6e/what_inference_engine_should_i_use_to_fully_use/n5dwufp/",
            "subreddit_type": "public",
            "locked": false,
            "report_reasons": null,
            "created": 1753593694,
            "author_flair_text": null,
            "treatment_tags": [],
            "link_id": "t3_1madt6e",
            "subreddit_name_prefixed": "r/LocalLLaMA",
            "controversiality": 0,
            "depth": 0,
            "author_flair_background_color": null,
            "collapsed_because_crowd_control": null,
            "mod_reports": [],
            "num_reports": null,
            "ups": 1
          }
        }
      ],
      "before": null
    }
  }
]